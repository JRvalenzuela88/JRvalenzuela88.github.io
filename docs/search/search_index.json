{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Betfair is one of the only betting platforms in the world that demands winning clients. Unlike bookies, we don\u2019t ban you when you succeed. We need you, and we want you to be able to keep improving your strategies so you win more. We're here to help you in your automation journey, and this site is dedicated to sharing the tools and resources you need to succeed in this journey. Accessing our API \u00b6 Betfair has a set of customer-facing transactional APIs to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support. Historic Data \u00b6 We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Tutorials \u00b6 JSON to CSV Backtesting ratings Other resources \u00b6 Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data Using third party tools for automation \u00b6 Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Professional Ratings automation Market favourite automation Tipping automation Automating simultaneous markets Kelly Criterion staking Gruss Betting Assistant Ratings automation Market favourite automation Automating simultaneous markets Kelly Criterion staking Cymatic Trader Ratings automation Data modelling \u00b6 An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL ML walkthrough in Python AFL modelling in Python Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies Inspiration & information \u00b6 There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Betfair Quants Slack Group betfair quants is really active community-owned Slack group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation. Need extra help? \u00b6 If you\u2019re looking for bespoke advice or have extra questions, please contact us at automation@betfair.com.au . We have a dedicated in-house resource that is here to automate your betting strategies.","title":"The Automation Hub"},{"location":"#accessing-our-api","text":"Betfair has a set of customer-facing transactional APIs to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support.","title":"Accessing our API"},{"location":"#historic-data","text":"We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section .","title":"Historic Data"},{"location":"#tutorials","text":"JSON to CSV Backtesting ratings","title":"Tutorials"},{"location":"#other-resources","text":"Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data","title":"Other resources"},{"location":"#using-third-party-tools-for-automation","text":"Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Professional Ratings automation Market favourite automation Tipping automation Automating simultaneous markets Kelly Criterion staking Gruss Betting Assistant Ratings automation Market favourite automation Automating simultaneous markets Kelly Criterion staking Cymatic Trader Ratings automation","title":"Using third party tools for automation"},{"location":"#data-modelling","text":"An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL ML walkthrough in Python AFL modelling in Python Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies","title":"Data modelling"},{"location":"#inspiration-information","text":"There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Betfair Quants Slack Group betfair quants is really active community-owned Slack group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation.","title":"Inspiration &amp; information"},{"location":"#need-extra-help","text":"If you\u2019re looking for bespoke advice or have extra questions, please contact us at automation@betfair.com.au . We have a dedicated in-house resource that is here to automate your betting strategies.","title":"Need extra help?"},{"location":"api/apiPythontutorial/","text":"Betfair API tutorial in Python \u00b6 This tutorial will walk you through the process of connecting to Betfair's API, grabbing data and placing a bet in Python. It will utilise the betfairlightweight Python library. Requirements \u00b6 This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here . This tutorial will also assume that you have a basic understanding of what an API is. For a summary in layman's terms, read this article . Quick Links \u00b6 Here are some other useful links for accessing our API: How to create an API app key Developer Docs - the official dev docs for Betfair's API Sports API Visualiser - Useful for exploring what the API has to offer Account API Visualiser Examples using betfairlightweight There's a more complete list of resources here Getting Started \u00b6 Setting Up Your Certificates \u00b6 To use the API securely, Betfair recommends generating certificates. The betfairlightweight package requires this to login non-interactively. For detailed instructions on how to generate certificates on a windows machine, follow the instructions outlined here . For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here . You should then create a folder for your certs, perhaps named 'certs' and grab the path location. Installing betfairlightweight \u00b6 We also need to install betfairlightweight . To do this, simply use pip install betfairlightweight in the cmd prompt/terminal. If this doesn't work, you will have to Google your error. If you're just starting out with Python, you may have to add Python to your environment variables. Sending Requests to the API \u00b6 Log into the API Client \u00b6 Now we're finally ready to log in and use the API. First, we create an APIClient object and then log in. To log in, we'll need to specify where we put our certs. In this example, I'll put them in a folder named 'certs', on my desktop. You'll also need to change the username , password and app_key variables to your own. In [206]: # Import libraries import betfairlightweight from betfairlightweight import filters import pandas as pd import numpy as np import os import datetime import json # Change this certs path to wherever you're storing your certificates certs_path = r 'C:\\Users\\wardj\\Desktop\\certs' # Change these login details to your own my_username = \"your_username\" my_password = \"your_password\" my_app_key = \"your_app_key\" trading = betfairlightweight . APIClient ( username = my_username , password = my_password , app_key = my_app_key , certs = certs_path ) trading . login () Out[206]: < LoginResource > Get Event IDs \u00b6 Betfair's API has a number of operations. For example, if you want to list the market book for a market, you would use the listMarketBook operation. These endpoints are shown in the Sports API Visualiser and in the docs. They are also listed below: Sports API \u00b6 listEventTypes listCompetitions listTimeRanges listEvents listMarketTypes listCountries listVenues listMarketCatalogue listMarketBook listRunnerBook placeOrders cancelOrders updateOrders replaceOrders listCurrentOrders listClearedOrders listMarketProfitAndLoss The Account Operations API operations/endpoints can be found here . First we need to grab the 'Event Type Id'. Each sport has a different ID. Below we will find the ids for all sports by requesting the event_type_ids without a filter. In [43]: # Grab all event type ids. This will return a list which we will iterate over to print out the id and the name of the sport event_types = trading . betting . list_event_types () sport_ids = pd . DataFrame ({ 'Sport' : [ event_type_object . event_type . name for event_type_object in event_types ], 'ID' : [ event_type_object . event_type . id for event_type_object in event_types ] }) . set_index ( 'Sport' ) . sort_index () sport_ids Out[43]: Sport ID American Football 6423 Athletics 3988 Australian Rules 61420 Baseball 7511 Basketball 7522 Boxing 6 Chess 136332 Cricket 4 Cycling 11 Darts 3503 Esports 27454571 Financial Bets 6231 Gaelic Games 2152880 Golf 3 Greyhound Racing 4339 Handball 468328 Horse Racing 7 Ice Hockey 7524 Mixed Martial Arts 26420387 Motor Sport 8 Netball 606611 Politics 2378961 Rugby League 1477 Rugby Union 5 Snooker 6422 Soccer 1 Special Bets 10 Tennis 2 Volleyball 998917 If we just wanted to get the event id for horse racing, we could use the filter function from betfairlightweight as shown in the examples and below. In [50]: # Filter for just horse racing horse_racing_filter = ` betfairlightweight ` . filters . market_filter ( text_query = 'Horse Racing' ) # This returns a list horse_racing_event_type = trading . betting . list_event_types ( filter = horse_racing_filter ) # Get the first element of the list horse_racing_event_type = horse_racing_event_type [ 0 ] horse_racing_event_type_id = horse_racing_event_type . event_type . id print ( f \"The event type id for horse racing is { horse_racing_event_type_id } \" ) The event type id for horse racing is 7 Get Competition IDs \u00b6 Sometimes you may want to get markets based on the competition. An example may be the Brownlow medal, or the EPL. Let's have a look at all the soccer competitions over the next week and filter to only get the EPL Competition ID. In [90]: # Get a datetime object in a week and convert to string datetime_in_a_week = ( datetime . datetime . utcnow () + datetime . timedelta ( weeks = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) # Create a competition filter competition_filter = ` betfairlightweight ` . filters . market_filter ( event_type_ids = [ 1 ], # Soccer's event type id is 1 market_start_time = { 'to' : datetime_in_a_week }) # Get a list of competitions for soccer competitions = trading . betting . list_competitions ( filter = competition_filter ) # Iterate over the competitions and create a dataframe of competitions and competition ids soccer_competitions = pd . DataFrame ({ 'Competition' : [ competition_object . competition . name for competition_object in competitions ], 'ID' : [ competition_object . competition . id for competition_object in competitions ] }) In [94]: # Get the English Premier League Competition ID soccer_competitions [ soccer_competitions . Competition . str . contains ( 'English Premier' )] Out[94]: # Competition ID 116 English Premier League 10932509 Get Upcoming Events \u00b6 Say you want to get all the upcoming events for Thoroughbreads for the next 24 hours. We will use the listEvents operation for this. First, as before, we define a market filter, and then using the betting method from our trading object which we defined earlier. In [207]: # Define a market filter thoroughbreds_event_filter = ` betfairlightweight ` . filters . market_filter ( event_type_ids = [ horse_racing_event_type_id ], market_countries = [ 'AU' ], market_start_time = { 'to' : ( datetime . datetime . utcnow () + datetime . timedelta ( days = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) } ) # Print the filter thoroughbreds_event_filter Out[207]: { 'eventTypeIds' : [ '7' ], 'marketCountries' : [ 'AU' ], 'marketStartTime' : { 'to' : '2018-10-26T22:25:00Z' }} In [208]: # Get a list of all thoroughbred events as objects aus_thoroughbred_events = trading . betting . list_events ( filter = thoroughbreds_event_filter ) # Create a DataFrame with all the events by iterating over each event object aus_thoroughbred_events_today = pd . DataFrame ({ 'Event Name' : [ event_object . event . name for event_object in aus_thoroughbred_events ], 'Event ID' : [ event_object . event . id for event_object in aus_thoroughbred_events ], 'Event Venue' : [ event_object . event . venue for event_object in aus_thoroughbred_events ], 'Country Code' : [ event_object . event . country_code for event_object in aus_thoroughbred_events ], 'Time Zone' : [ event_object . event . time_zone for event_object in aus_thoroughbred_events ], 'Open Date' : [ event_object . event . open_date for event_object in aus_thoroughbred_events ], 'Market Count' : [ event_object . market_count for event_object in aus_thoroughbred_events ] }) aus_thoroughbred_events_today Out[208]: # Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 MVal (AUS) 26 th Oct 28971066 Moonee Valley AU Australia/Sydney 2018-10-26 07:30:00 24 1 Newc (AUS) 26 th Oct 28974559 Newcastle AU Australia/Sydney 2018-10-26 07:07:00 20 2 Bath (AUS) 26 th Oct 28974547 Bathurst AU Australia/Sydney 2018-10-26 02:43:00 16 3 Cant (AUS) 26 th Oct 28974545 Canterbury AU Australia/Sydney 2018-10-26 07:15:00 16 4 Scne (AUS) 26 th Oct 28973942 Scone AU Australia/Sydney 2018-10-26 02:25:00 16 5 Gawl (AUS) 26 th Oct 28974550 Gawler AU Australia/Adelaide 2018-10-26 04:00:00 16 6 Gatt (AUS) 26 th Oct 28974549 Gatton AU Australia/Queensland 2018-10-26 01:55:00 16 7 GlPk (AUS) 26 th Oct 28974562 Gloucester Park AU Australia/Perth 2018-10-26 09:10:00 20 8 Hoba (AUS) 26 th Oct 28974563 Hobart AU Australia/Sydney 2018-10-26 05:23:00 18 9 Echu (AUS) 26 th Oct 28974016 Echuca AU Australia/Sydney 2018-10-26 01:30:00 18 10 Melt (AUS) 26 th Oct 28974560 Melton AU Australia/Sydney 2018-10-26 07:18:00 18 11 MVal (AUS) 26 th Oct 28921730 None AU Australia/Sydney 2018-10-26 11:00:00 1 12 Redc (AUS) 26 th Oct 28974561 Redcliffe AU Australia/Queensland 2018-10-26 02:17:00 16 13 SCst (AUS) 26 th Oct 28974149 Sunshine Coast AU Australia/Queensland 2018-10-26 06:42:00 20 Get Market Types \u00b6 Say we want to know what market types a certain event is offering. To do this, we use the listMarketTypes operation. Let's take the Moonee Valley event from above (ID: 28971066). As this is a horse race we would expect that it would have Win and Place markets. In [209]: # Define a market filter market_types_filter = ` betfairlightweight ` . filters . market_filter ( event_ids = [ '28971066' ]) # Request market types market_types = trading . betting . list_market_types ( filter = market_types_filter ) # Create a DataFrame of market types market_types_mooney_valley = pd . DataFrame ({ 'Market Type' : [ market_type_object . market_type for market_type_object in market_types ], }) market_types_mooney_valley Out[209]: Market Type 0 OTHER_PLACE 1 PLACE 2 WIN Get Market Catalogues \u00b6 If we want to know the various market names that there are for a particular event, as well as how much has been matched on each market, we want to request data from the listMarketCatalogue operation. We can provide a number of filters, including the Competition ID, the Event ID, the Venue etc. to the filter. We must also specify the maximum number of results, and if we want additional data like the event data or runner data, we can also request that. For a more comprehensive understanding of the options for filters and what we can request, please have a look at the Sports API Visualiser . The options listed under market filter should be put into a filter, whilst the others should be arguments to the relevant operation function in betfairlightweight . For example, if we want all the markets for Moonee Valley, we should use the following filters and arguments. In [210]: market_catalogue_filter = ` betfairlightweight ` . filters . market_filter ( event_ids = [ '28971066' ]) market_catalogues = trading . betting . list_market_catalogue ( filter = market_catalogue_filter , max_results = '100' , sort = 'FIRST_TO_START' ) # Create a DataFrame for each market catalogue market_types_mooney_valley = pd . DataFrame ({ 'Market Name' : [ market_cat_object . market_name for market_cat_object in market_catalogues ], 'Market ID' : [ market_cat_object . market_id for market_cat_object in market_catalogues ], 'Total Matched' : [ market_cat_object . total_matched for market_cat_object in market_catalogues ], }) market_types_mooney_valley Out[210]: Market Name Market ID Total Matched 0 4 TBP 1.150090094 1 To Be Placed 1.150090092 2 R1 1000m 3yo 1.150090091 3 4 TBP 1.150090101 4 To Be Placed 1.150090099 5 R2 2040m Hcap 1.150090098 6 To Be Placed 1.150090106 7 R3 1500m Hcap 1.150090105 8 4 TBP 1.150090108 9 To Be Placed 1.150090113 10 R4 2040m Hcap 1.150090112 11 4 TBP 1.150090115 12 4 TBP 1.150090122 13 R5 955m Hcap 1.150090119 14 To Be Placed 1.150090120 15 4 TBP 1.150090129 16 To Be Placed 1.150090127 17 R6 1200m Hcap 1.150090126 18 R7 1200m Grp1 1.150038686 19 4 TBP 1.150038689 20 To Be Placed 1.150038687 21 R8 1500m Hcap 1.150090140 22 4 TBP 1.150090143 23 To Be Placed 1.150090141 Get Market Books \u00b6 If we then want to get the prices available/last traded for a market, we should use the listMarketBook operation. Let's Look at the market book for Moonee Valley R7. We will need to define a function which processes the runner books and collates the data into a DataFrame. In [212]: def process_runner_books ( runner_books ): ''' This function processes the runner books and returns a DataFrame with the best back/lay prices + vol for each runner :param runner_books: :return: ''' best_back_prices = [ runner_book . ex . available_to_back [ 0 ] . price if runner_book . ex . available_to_back [ 0 ] . price else 1.01 for runner_book in runner_books ] best_back_sizes = [ runner_book . ex . available_to_back [ 0 ] . size if runner_book . ex . available_to_back [ 0 ] . size else 1.01 for runner_book in runner_books ] best_lay_prices = [ runner_book . ex . available_to_lay [ 0 ] . price if runner_book . ex . available_to_lay [ 0 ] . price else 1000.0 for runner_book in runner_books ] best_lay_sizes = [ runner_book . ex . available_to_lay [ 0 ] . size if runner_book . ex . available_to_lay [ 0 ] . size else 1.01 for runner_book in runner_books ] selection_ids = [ runner_book . selection_id for runner_book in runner_books ] last_prices_traded = [ runner_book . last_price_traded for runner_book in runner_books ] total_matched = [ runner_book . total_matched for runner_book in runner_books ] statuses = [ runner_book . status for runner_book in runner_books ] scratching_datetimes = [ runner_book . removal_date for runner_book in runner_books ] adjustment_factors = [ runner_book . adjustment_factor for runner_book in runner_books ] df = pd . DataFrame ({ 'Selection ID' : selection_ids , 'Best Back Price' : best_back_prices , 'Best Back Size' : best_back_sizes , 'Best Lay Price' : best_lay_prices , 'Best Lay Size' : best_lay_sizes , 'Last Price Traded' : last_prices_traded , 'Total Matched' : total_matched , 'Status' : statuses , 'Removal Date' : scratching_datetimes , 'Adjustment Factor' : adjustment_factors }) return df In [213]: # Create a price filter. Get all traded and offer data price_filter = ` betfairlightweight ` . filters . price_projection ( price_data = [ 'EX_BEST_OFFERS' ] ) # Request market books market_books = trading . betting . list_market_book ( market_ids = [ '1.150038686' ], price_projection = price_filter ) # Grab the first market book from the returned list as we only requested one market market_book = market_books [ 0 ] runners_df = process_runner_books ( market_book . runners ) runners_df Out[213]: # Selection ID Best Back Price Best Back Size Best Lay Price Best Lay Size Last Price Traded Total Matched Status Removal Date Adjustment Factor 0 16905731 12.0 65.54 13.0 33.09 12.0 1226.67 ACTIVE None 8.333 1 15815968 6.6 96.64 7.0 9.00 6.6 5858.61 ACTIVE None 14.286 2 9384677 14.0 114.71 15.0 76.71 14.0 964.80 ACTIVE None 6.667 3 8198751 17.5 14.67 19.0 33.02 17.5 940.56 ACTIVE None 5.556 4 9507057 38.0 53.13 100.0 40.22 46.0 224.72 ACTIVE None 3.125 5 21283266 15.0 121.46 19.5 5.56 19.5 1102.37 ACTIVE None 7.692 6 21283267 80.0 37.58 760.0 9.70 760.0 125.30 ACTIVE None 1.087 7 21063807 6.4 1503.62 7.2 50.00 6.6 8011.44 ACTIVE None 13.333 8 21283268 48.0 54.57 60.0 51.93 50.0 150.22 ACTIVE None 2.381 9 21283269 8.8 235.77 9.4 30.40 8.8 1729.96 ACTIVE None 11.111 10 4883975 46.0 33.42 55.0 5.00 46.0 208.45 ACTIVE None 2.381 11 202351 25.0 20.00 30.0 6.00 24.0 658.09 ACTIVE None 2.632 12 21283270 19.5 69.33 22.0 20.00 19.5 825.59 ACTIVE None 4.545 13 21283271 5.3 96.14 5.7 5.03 5.3 12654.32 ACTIVE None 16.871 Orderbook Workflow \u00b6 Now that we have the market book in an easy to read DataFrame, we can go ahead and start placing orders based on the market book. Although it is a simple (and probably not profitable) strategy, in the next few sections we will be backing the favourite and adjusting our orders. Placing Orders \u00b6 To place an order we use the placeOrders operation. A handy component of placeOrders is that you can send your strategy along with the runner that you want to back, so it is extremely easy to analyse how your strategy performed later. Let's place a 5 dollar back bet on the favourite at $7 call this strategy 'back_the_fav' . Note that if you are placing a limit order you must specify a price which is allowed by Betfair. For example, the price 6.3 isn't allowed, whereas 6.4 is, as prices go up by 20c increments at that price range. You can read about tick points here . In [232]: # Get the favourite's price and selection id fav_selection_id = runners_df . loc [ runners_df [ 'Best Back Price' ] . idxmin (), 'Selection ID' ] fav_price = runners_df . loc [ runners_df [ 'Best Back Price' ] . idxmin (), 'Best Back Price' ] In [276]: # Define a limit order filter limit_order_filter = ` betfairlightweight ` . filters . limit_order ( size = 5 , price = 7 , persistence_type = 'LAPSE' ) # Define an instructions filter instructions_filter = ` betfairlightweight ` . filters . place_instruction ( selection_id = str ( fav_selection_id ), order_type = \"LIMIT\" , side = \"BACK\" , limit_order = limit_order_filter ) instructions_filter Out[276]: { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7 , 'size' : 5 }, 'orderType' : 'LIMIT' , 'selectionId' : '21283271' , 'side' : 'BACK' } In [277]: # Place the order order = trading . betting . place_orders ( market_id = '1.150038686' , # The market id we obtained from before customer_strategy_ref = 'back_the_fav' , instructions = [ instructions_filter ] # This must be a list ) Now that we've placed the other, we can check if the order placing was a success and if any has been matched. In [306]: order . __dict__ Out[306]: { '_data' : { 'instructionReports' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'instruction' : { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7.0 , 'size' : 5.0 }, 'orderType' : 'LIMIT' , 'selectionId' : 21283271 , 'side' : 'BACK' }, 'orderStatus' : 'EXECUTABLE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'sizeMatched' : 0.0 , 'status' : 'SUCCESS' }], 'marketId' : '1.150038686' , 'status' : 'SUCCESS' }, '_datetime_created' : datetime . datetime ( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), '_datetime_updated' : datetime . datetime ( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), 'customer_ref' : None , 'elapsed_time' : 1.484069 , 'error_code' : None , 'market_id' : '1.150038686' , 'place_instruction_reports' : [ < betfairlightweight . resources . bettingresources . PlaceOrderInstructionReports at 0x23e0f7952e8 > ], 'status' : 'SUCCESS' } As we can see, the status is 'SUCCESS' , whilst the sizeMatched is 0. Let's now look at our current orders. Get Current Orders \u00b6 To get our current orders, we need to use the listCurrentOrders operation. We can then use either the bet id, the market id, or the bet strategy to filter our orders. In [311]: trading . betting . list_current_orders ( customer_strategy_refs = [ 'back_the_fav' ]) . __dict__ Out [ 311 ]: { '_data' : { 'currentOrders' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'bspLiability' : 0.0 , 'customerStrategyRef' : 'back_the_fav' , 'handicap' : 0.0 , 'marketId' : '1.150038686' , 'orderType' : 'LIMIT' , 'persistenceType' : 'LAPSE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'priceSize' : { 'price' : 7.0 , 'size' : 5.0 }, 'regulatorCode' : 'MALTA LOTTERIES AND GAMBLING AUTHORITY' , 'selectionId' : 21283271 , 'side' : 'BACK' , 'sizeCancelled' : 0.0 , 'sizeLapsed' : 0.0 , 'sizeMatched' : 0.0 , 'sizeRemaining' : 5.0 , 'sizeVoided' : 0.0 , 'status' : 'EXECUTABLE' }], 'moreAvailable' : False }, '_datetime_created' : datetime . datetime ( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), '_datetime_updated' : datetime . datetime ( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), 'elapsed_time' : 1.327456 , 'more_available' : False , 'orders' : [ < betfairlightweight . resources . bettingresources . CurrentOrder at 0x23e0e7acd30 > ], 'publish_time' : None , 'streaming_unique_id' : None , 'streaming_update' : None } As we can see, we have one order which is unmatched for our strategy 'back_the_fav' Cancelling Orders \u00b6 Let's now cancel this bet. To do this, we will use the cancelOrders operation. If you pass in a market ID it will cancel all orders for that specific market ID, like you can do on the website. In [312]: cancelled_order = trading . betting . cancel_orders ( market_id = '1.150038686' ) In [328]: # Create a DataFrame to view the instruction report pd . Series ( cancelled_order . cancel_instruction_reports [ 0 ] . __dict__ ) . to_frame () . T Out[328]: # status size_cancelled cancelled_date instruction error_code 0 SUCCESS 5 2018-10-26 06:01:26 betfairlightweight.resources.bettingresources... None Get Past Orders and Results \u00b6 If we want to go back and look at past orders we have made, there are two main operations for this: listClearedOrders - this operation takes a range of data down to the individual selection ID level, and returns a summary of those specific orders listMarketProfitAndLoss - this operation is more specific, and only takes Market IDs to return the Profit/Loss for that market Alternatively, we can use the getAccountStatement operation from the Account Operations API. Let's now use both Sports API operations based on our previous orders and then compare it to the getAccountStatement operation. Get Cleared Orders \u00b6 In [346]: # listClearedOrders cleared_orders = trading . betting . list_cleared_orders ( bet_status = \"SETTLED\" , market_ids = [ \"1.150038686\" ]) In [371]: # Create a DataFrame from the orders pd . DataFrame ( cleared_orders . _data [ 'clearedOrders' ]) Out[371]: # betCount betId betOutcome eventId eventTypeId handicap lastMatchedDate marketId orderType persistenceType placedDate priceMatched priceReduced priceRequested profit selectionId settledDate side sizeSettled 0 1 142383373022 LOST 28971066 7 0.0 2018-10-26T10:31:53.000Z 1.150038686 MARKET_ON_CLOSE LAPSE 2018-10-26T00:12:03.000Z 5.74 False 5.74 -5.0 21283271 2018-10-26T10:34:39.000Z BACK 5.0 1 1 142383570640 WON 28971066 7 0.0 2018-10-26T00:16:32.000Z 1.150038686 LIMIT LAPSE 2018-10-26T00:16:31.000Z 5.40 False 5.50 5.0 21283271 2018-10-26T10:34:39.000Z LAY 5.0 Note that we can also filter for certain dates, bet ids, event ids, selection ids etc. We can also group by the event type, the event, the market, the runner, the side, the bet and the strategy, which is extremely useful if you're looking for a quick summary of how your strategy is performing. Get Market Profit and Loss \u00b6 Now let's find the Profit and Loss for the market. To do this we will use the listMarketProfitAndLoss operation. Note that this function only works with market IDs, and once the website clears the market, the operation will no longer work. However the market is generally up for about a minute after the race, so if your strategy is automated, you can check once if your bet is settled and if it is, hit the getMarketProfitAndLoss endpoint. Because of this, we will check a different market ID to the example above. In [406]: # Get the profit/loss - this returns a list pl = trading . betting . list_market_profit_and_loss ( market_ids = [ \"1.150318913\" ], include_bsp_bets = 'true' , include_settled_bets = 'true' ) In [410]: # Create a profit/loss DataFrame pl_df = pd . DataFrame ( pl [ 0 ] . _data [ 'profitAndLosses' ]) . assign ( marketId = pl [ 0 ] . market_id ) pl_df Out[410]: # ifWin selectionId marketId 0 -5.0 10065177 1.150318913 1 14.0 17029506 1.150318913 2 -5.0 5390339 1.150318913 3 -5.0 13771011 1.150318913 4 -5.0 138209 1.150318913 5 -5.0 10503541 1.150318913 6 -5.0 12165809 1.150318913 Get Account Statement \u00b6 Another method is to use the getAccountStatement , which provides an overview of all your bets over a certain time period. You can then filter this for specific dates if you wish. In [428]: # Define a date filter - get all bets for the past 4 days four_days_ago = ( datetime . datetime . utcnow () - datetime . timedelta ( days = 4 )) . strftime ( \"%Y-%m- %d T%TZ\" ) acct_statement_date_filter = ` betfairlightweight ` . filters . time_range ( from_ = four_days_ago ) # Request account statement account_statement = trading . account . get_account_statement ( item_date_range = acct_statement_date_filter ) In [450]: # Create df of recent transactions recent_transactions = pd . DataFrame ( account_statement . _data [ 'accountStatement' ]) recent_transactions Out[450]: # amount balance itemClass itemClassData itemDate legacyData refId 0 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":3.8,\"bet... 2018-10-28T23:14:28.000Z {'avgPrice': 3.8, 'betSize': 5.0, 'betType': '... 142845441633 1 5.0 261.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.4,\"bet... 2018-10-26T10:34:39.000Z {'avgPrice': 5.4, 'betSize': 5.0, 'betType': '... 142383570640 2 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.74,\"be... 2018-10-26T10:34:39.000Z {'avgPrice': 5.74, 'betSize': 5.0, 'betType': ... 142383373022 In [468]: # Create df of itemClassData - iterate over the account statement list and convert to json so that the DataFrame function # can read it correctly class_data = [ json . loads ( account_statement . account_statement [ i ] . item_class_data [ 'unknownStatementItem' ]) for i in range ( len ( account_statement . account_statement ))] In [471]: class_df = pd . DataFrame ( class_data ) class_df Out [471]: # avgPrice betCategoryType betSize betType commissionRate eventId eventTypeId fullMarketName grossBetAmount marketName marketType placedDate selectionId selectionName startDate transactionId transactionType winLose 0 3.80 M 5.0 B None 150318913 7 USA / TPara (US) 28 th Oct/ 16:06 R8 1m Allw Claim 0.0 R8 1m Allw Claim O 2018-10-28T23:02:28.000Z 17029506 Gato Guapo 2018-10-28T23:06:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST 1 5.40 E 5.0 L None 150038686 7 AUS / MVal (AUS) 26 th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:16:31.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_CREDIT 2 5.74 M 5.0 B None 150038686 7 AUS / MVal (AUS) 26 th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:12:03.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST As we can see, this DataFrame provides a much more comprehensive view of each of our bets. However, it lacks the ability to filter by strategy like the listClearedOrders operation in the Sports API.","title":"API tutorial in Python"},{"location":"api/apiPythontutorial/#betfair-api-tutorial-in-python","text":"This tutorial will walk you through the process of connecting to Betfair's API, grabbing data and placing a bet in Python. It will utilise the betfairlightweight Python library.","title":"Betfair API tutorial in Python"},{"location":"api/apiPythontutorial/#requirements","text":"This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here . This tutorial will also assume that you have a basic understanding of what an API is. For a summary in layman's terms, read this article .","title":"Requirements"},{"location":"api/apiPythontutorial/#quick-links","text":"Here are some other useful links for accessing our API: How to create an API app key Developer Docs - the official dev docs for Betfair's API Sports API Visualiser - Useful for exploring what the API has to offer Account API Visualiser Examples using betfairlightweight There's a more complete list of resources here","title":"Quick Links"},{"location":"api/apiPythontutorial/#getting-started","text":"","title":"Getting Started"},{"location":"api/apiPythontutorial/#setting-up-your-certificates","text":"To use the API securely, Betfair recommends generating certificates. The betfairlightweight package requires this to login non-interactively. For detailed instructions on how to generate certificates on a windows machine, follow the instructions outlined here . For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here . You should then create a folder for your certs, perhaps named 'certs' and grab the path location.","title":"Setting Up Your Certificates"},{"location":"api/apiPythontutorial/#installing-betfairlightweight","text":"We also need to install betfairlightweight . To do this, simply use pip install betfairlightweight in the cmd prompt/terminal. If this doesn't work, you will have to Google your error. If you're just starting out with Python, you may have to add Python to your environment variables.","title":"Installing betfairlightweight"},{"location":"api/apiPythontutorial/#sending-requests-to-the-api","text":"","title":"Sending Requests to the API"},{"location":"api/apiPythontutorial/#log-into-the-api-client","text":"Now we're finally ready to log in and use the API. First, we create an APIClient object and then log in. To log in, we'll need to specify where we put our certs. In this example, I'll put them in a folder named 'certs', on my desktop. You'll also need to change the username , password and app_key variables to your own. In [206]: # Import libraries import betfairlightweight from betfairlightweight import filters import pandas as pd import numpy as np import os import datetime import json # Change this certs path to wherever you're storing your certificates certs_path = r 'C:\\Users\\wardj\\Desktop\\certs' # Change these login details to your own my_username = \"your_username\" my_password = \"your_password\" my_app_key = \"your_app_key\" trading = betfairlightweight . APIClient ( username = my_username , password = my_password , app_key = my_app_key , certs = certs_path ) trading . login () Out[206]: < LoginResource >","title":"Log into the API Client"},{"location":"api/apiPythontutorial/#get-event-ids","text":"Betfair's API has a number of operations. For example, if you want to list the market book for a market, you would use the listMarketBook operation. These endpoints are shown in the Sports API Visualiser and in the docs. They are also listed below:","title":"Get Event IDs"},{"location":"api/apiPythontutorial/#sports-api","text":"listEventTypes listCompetitions listTimeRanges listEvents listMarketTypes listCountries listVenues listMarketCatalogue listMarketBook listRunnerBook placeOrders cancelOrders updateOrders replaceOrders listCurrentOrders listClearedOrders listMarketProfitAndLoss The Account Operations API operations/endpoints can be found here . First we need to grab the 'Event Type Id'. Each sport has a different ID. Below we will find the ids for all sports by requesting the event_type_ids without a filter. In [43]: # Grab all event type ids. This will return a list which we will iterate over to print out the id and the name of the sport event_types = trading . betting . list_event_types () sport_ids = pd . DataFrame ({ 'Sport' : [ event_type_object . event_type . name for event_type_object in event_types ], 'ID' : [ event_type_object . event_type . id for event_type_object in event_types ] }) . set_index ( 'Sport' ) . sort_index () sport_ids Out[43]: Sport ID American Football 6423 Athletics 3988 Australian Rules 61420 Baseball 7511 Basketball 7522 Boxing 6 Chess 136332 Cricket 4 Cycling 11 Darts 3503 Esports 27454571 Financial Bets 6231 Gaelic Games 2152880 Golf 3 Greyhound Racing 4339 Handball 468328 Horse Racing 7 Ice Hockey 7524 Mixed Martial Arts 26420387 Motor Sport 8 Netball 606611 Politics 2378961 Rugby League 1477 Rugby Union 5 Snooker 6422 Soccer 1 Special Bets 10 Tennis 2 Volleyball 998917 If we just wanted to get the event id for horse racing, we could use the filter function from betfairlightweight as shown in the examples and below. In [50]: # Filter for just horse racing horse_racing_filter = ` betfairlightweight ` . filters . market_filter ( text_query = 'Horse Racing' ) # This returns a list horse_racing_event_type = trading . betting . list_event_types ( filter = horse_racing_filter ) # Get the first element of the list horse_racing_event_type = horse_racing_event_type [ 0 ] horse_racing_event_type_id = horse_racing_event_type . event_type . id print ( f \"The event type id for horse racing is { horse_racing_event_type_id } \" ) The event type id for horse racing is 7","title":"Sports API"},{"location":"api/apiPythontutorial/#get-competition-ids","text":"Sometimes you may want to get markets based on the competition. An example may be the Brownlow medal, or the EPL. Let's have a look at all the soccer competitions over the next week and filter to only get the EPL Competition ID. In [90]: # Get a datetime object in a week and convert to string datetime_in_a_week = ( datetime . datetime . utcnow () + datetime . timedelta ( weeks = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) # Create a competition filter competition_filter = ` betfairlightweight ` . filters . market_filter ( event_type_ids = [ 1 ], # Soccer's event type id is 1 market_start_time = { 'to' : datetime_in_a_week }) # Get a list of competitions for soccer competitions = trading . betting . list_competitions ( filter = competition_filter ) # Iterate over the competitions and create a dataframe of competitions and competition ids soccer_competitions = pd . DataFrame ({ 'Competition' : [ competition_object . competition . name for competition_object in competitions ], 'ID' : [ competition_object . competition . id for competition_object in competitions ] }) In [94]: # Get the English Premier League Competition ID soccer_competitions [ soccer_competitions . Competition . str . contains ( 'English Premier' )] Out[94]: # Competition ID 116 English Premier League 10932509","title":"Get Competition IDs"},{"location":"api/apiPythontutorial/#get-upcoming-events","text":"Say you want to get all the upcoming events for Thoroughbreads for the next 24 hours. We will use the listEvents operation for this. First, as before, we define a market filter, and then using the betting method from our trading object which we defined earlier. In [207]: # Define a market filter thoroughbreds_event_filter = ` betfairlightweight ` . filters . market_filter ( event_type_ids = [ horse_racing_event_type_id ], market_countries = [ 'AU' ], market_start_time = { 'to' : ( datetime . datetime . utcnow () + datetime . timedelta ( days = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) } ) # Print the filter thoroughbreds_event_filter Out[207]: { 'eventTypeIds' : [ '7' ], 'marketCountries' : [ 'AU' ], 'marketStartTime' : { 'to' : '2018-10-26T22:25:00Z' }} In [208]: # Get a list of all thoroughbred events as objects aus_thoroughbred_events = trading . betting . list_events ( filter = thoroughbreds_event_filter ) # Create a DataFrame with all the events by iterating over each event object aus_thoroughbred_events_today = pd . DataFrame ({ 'Event Name' : [ event_object . event . name for event_object in aus_thoroughbred_events ], 'Event ID' : [ event_object . event . id for event_object in aus_thoroughbred_events ], 'Event Venue' : [ event_object . event . venue for event_object in aus_thoroughbred_events ], 'Country Code' : [ event_object . event . country_code for event_object in aus_thoroughbred_events ], 'Time Zone' : [ event_object . event . time_zone for event_object in aus_thoroughbred_events ], 'Open Date' : [ event_object . event . open_date for event_object in aus_thoroughbred_events ], 'Market Count' : [ event_object . market_count for event_object in aus_thoroughbred_events ] }) aus_thoroughbred_events_today Out[208]: # Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 MVal (AUS) 26 th Oct 28971066 Moonee Valley AU Australia/Sydney 2018-10-26 07:30:00 24 1 Newc (AUS) 26 th Oct 28974559 Newcastle AU Australia/Sydney 2018-10-26 07:07:00 20 2 Bath (AUS) 26 th Oct 28974547 Bathurst AU Australia/Sydney 2018-10-26 02:43:00 16 3 Cant (AUS) 26 th Oct 28974545 Canterbury AU Australia/Sydney 2018-10-26 07:15:00 16 4 Scne (AUS) 26 th Oct 28973942 Scone AU Australia/Sydney 2018-10-26 02:25:00 16 5 Gawl (AUS) 26 th Oct 28974550 Gawler AU Australia/Adelaide 2018-10-26 04:00:00 16 6 Gatt (AUS) 26 th Oct 28974549 Gatton AU Australia/Queensland 2018-10-26 01:55:00 16 7 GlPk (AUS) 26 th Oct 28974562 Gloucester Park AU Australia/Perth 2018-10-26 09:10:00 20 8 Hoba (AUS) 26 th Oct 28974563 Hobart AU Australia/Sydney 2018-10-26 05:23:00 18 9 Echu (AUS) 26 th Oct 28974016 Echuca AU Australia/Sydney 2018-10-26 01:30:00 18 10 Melt (AUS) 26 th Oct 28974560 Melton AU Australia/Sydney 2018-10-26 07:18:00 18 11 MVal (AUS) 26 th Oct 28921730 None AU Australia/Sydney 2018-10-26 11:00:00 1 12 Redc (AUS) 26 th Oct 28974561 Redcliffe AU Australia/Queensland 2018-10-26 02:17:00 16 13 SCst (AUS) 26 th Oct 28974149 Sunshine Coast AU Australia/Queensland 2018-10-26 06:42:00 20","title":"Get Upcoming Events"},{"location":"api/apiPythontutorial/#get-market-types","text":"Say we want to know what market types a certain event is offering. To do this, we use the listMarketTypes operation. Let's take the Moonee Valley event from above (ID: 28971066). As this is a horse race we would expect that it would have Win and Place markets. In [209]: # Define a market filter market_types_filter = ` betfairlightweight ` . filters . market_filter ( event_ids = [ '28971066' ]) # Request market types market_types = trading . betting . list_market_types ( filter = market_types_filter ) # Create a DataFrame of market types market_types_mooney_valley = pd . DataFrame ({ 'Market Type' : [ market_type_object . market_type for market_type_object in market_types ], }) market_types_mooney_valley Out[209]: Market Type 0 OTHER_PLACE 1 PLACE 2 WIN","title":"Get Market Types"},{"location":"api/apiPythontutorial/#get-market-catalogues","text":"If we want to know the various market names that there are for a particular event, as well as how much has been matched on each market, we want to request data from the listMarketCatalogue operation. We can provide a number of filters, including the Competition ID, the Event ID, the Venue etc. to the filter. We must also specify the maximum number of results, and if we want additional data like the event data or runner data, we can also request that. For a more comprehensive understanding of the options for filters and what we can request, please have a look at the Sports API Visualiser . The options listed under market filter should be put into a filter, whilst the others should be arguments to the relevant operation function in betfairlightweight . For example, if we want all the markets for Moonee Valley, we should use the following filters and arguments. In [210]: market_catalogue_filter = ` betfairlightweight ` . filters . market_filter ( event_ids = [ '28971066' ]) market_catalogues = trading . betting . list_market_catalogue ( filter = market_catalogue_filter , max_results = '100' , sort = 'FIRST_TO_START' ) # Create a DataFrame for each market catalogue market_types_mooney_valley = pd . DataFrame ({ 'Market Name' : [ market_cat_object . market_name for market_cat_object in market_catalogues ], 'Market ID' : [ market_cat_object . market_id for market_cat_object in market_catalogues ], 'Total Matched' : [ market_cat_object . total_matched for market_cat_object in market_catalogues ], }) market_types_mooney_valley Out[210]: Market Name Market ID Total Matched 0 4 TBP 1.150090094 1 To Be Placed 1.150090092 2 R1 1000m 3yo 1.150090091 3 4 TBP 1.150090101 4 To Be Placed 1.150090099 5 R2 2040m Hcap 1.150090098 6 To Be Placed 1.150090106 7 R3 1500m Hcap 1.150090105 8 4 TBP 1.150090108 9 To Be Placed 1.150090113 10 R4 2040m Hcap 1.150090112 11 4 TBP 1.150090115 12 4 TBP 1.150090122 13 R5 955m Hcap 1.150090119 14 To Be Placed 1.150090120 15 4 TBP 1.150090129 16 To Be Placed 1.150090127 17 R6 1200m Hcap 1.150090126 18 R7 1200m Grp1 1.150038686 19 4 TBP 1.150038689 20 To Be Placed 1.150038687 21 R8 1500m Hcap 1.150090140 22 4 TBP 1.150090143 23 To Be Placed 1.150090141","title":"Get Market Catalogues"},{"location":"api/apiPythontutorial/#get-market-books","text":"If we then want to get the prices available/last traded for a market, we should use the listMarketBook operation. Let's Look at the market book for Moonee Valley R7. We will need to define a function which processes the runner books and collates the data into a DataFrame. In [212]: def process_runner_books ( runner_books ): ''' This function processes the runner books and returns a DataFrame with the best back/lay prices + vol for each runner :param runner_books: :return: ''' best_back_prices = [ runner_book . ex . available_to_back [ 0 ] . price if runner_book . ex . available_to_back [ 0 ] . price else 1.01 for runner_book in runner_books ] best_back_sizes = [ runner_book . ex . available_to_back [ 0 ] . size if runner_book . ex . available_to_back [ 0 ] . size else 1.01 for runner_book in runner_books ] best_lay_prices = [ runner_book . ex . available_to_lay [ 0 ] . price if runner_book . ex . available_to_lay [ 0 ] . price else 1000.0 for runner_book in runner_books ] best_lay_sizes = [ runner_book . ex . available_to_lay [ 0 ] . size if runner_book . ex . available_to_lay [ 0 ] . size else 1.01 for runner_book in runner_books ] selection_ids = [ runner_book . selection_id for runner_book in runner_books ] last_prices_traded = [ runner_book . last_price_traded for runner_book in runner_books ] total_matched = [ runner_book . total_matched for runner_book in runner_books ] statuses = [ runner_book . status for runner_book in runner_books ] scratching_datetimes = [ runner_book . removal_date for runner_book in runner_books ] adjustment_factors = [ runner_book . adjustment_factor for runner_book in runner_books ] df = pd . DataFrame ({ 'Selection ID' : selection_ids , 'Best Back Price' : best_back_prices , 'Best Back Size' : best_back_sizes , 'Best Lay Price' : best_lay_prices , 'Best Lay Size' : best_lay_sizes , 'Last Price Traded' : last_prices_traded , 'Total Matched' : total_matched , 'Status' : statuses , 'Removal Date' : scratching_datetimes , 'Adjustment Factor' : adjustment_factors }) return df In [213]: # Create a price filter. Get all traded and offer data price_filter = ` betfairlightweight ` . filters . price_projection ( price_data = [ 'EX_BEST_OFFERS' ] ) # Request market books market_books = trading . betting . list_market_book ( market_ids = [ '1.150038686' ], price_projection = price_filter ) # Grab the first market book from the returned list as we only requested one market market_book = market_books [ 0 ] runners_df = process_runner_books ( market_book . runners ) runners_df Out[213]: # Selection ID Best Back Price Best Back Size Best Lay Price Best Lay Size Last Price Traded Total Matched Status Removal Date Adjustment Factor 0 16905731 12.0 65.54 13.0 33.09 12.0 1226.67 ACTIVE None 8.333 1 15815968 6.6 96.64 7.0 9.00 6.6 5858.61 ACTIVE None 14.286 2 9384677 14.0 114.71 15.0 76.71 14.0 964.80 ACTIVE None 6.667 3 8198751 17.5 14.67 19.0 33.02 17.5 940.56 ACTIVE None 5.556 4 9507057 38.0 53.13 100.0 40.22 46.0 224.72 ACTIVE None 3.125 5 21283266 15.0 121.46 19.5 5.56 19.5 1102.37 ACTIVE None 7.692 6 21283267 80.0 37.58 760.0 9.70 760.0 125.30 ACTIVE None 1.087 7 21063807 6.4 1503.62 7.2 50.00 6.6 8011.44 ACTIVE None 13.333 8 21283268 48.0 54.57 60.0 51.93 50.0 150.22 ACTIVE None 2.381 9 21283269 8.8 235.77 9.4 30.40 8.8 1729.96 ACTIVE None 11.111 10 4883975 46.0 33.42 55.0 5.00 46.0 208.45 ACTIVE None 2.381 11 202351 25.0 20.00 30.0 6.00 24.0 658.09 ACTIVE None 2.632 12 21283270 19.5 69.33 22.0 20.00 19.5 825.59 ACTIVE None 4.545 13 21283271 5.3 96.14 5.7 5.03 5.3 12654.32 ACTIVE None 16.871","title":"Get Market Books"},{"location":"api/apiPythontutorial/#orderbook-workflow","text":"Now that we have the market book in an easy to read DataFrame, we can go ahead and start placing orders based on the market book. Although it is a simple (and probably not profitable) strategy, in the next few sections we will be backing the favourite and adjusting our orders.","title":"Orderbook Workflow"},{"location":"api/apiPythontutorial/#placing-orders","text":"To place an order we use the placeOrders operation. A handy component of placeOrders is that you can send your strategy along with the runner that you want to back, so it is extremely easy to analyse how your strategy performed later. Let's place a 5 dollar back bet on the favourite at $7 call this strategy 'back_the_fav' . Note that if you are placing a limit order you must specify a price which is allowed by Betfair. For example, the price 6.3 isn't allowed, whereas 6.4 is, as prices go up by 20c increments at that price range. You can read about tick points here . In [232]: # Get the favourite's price and selection id fav_selection_id = runners_df . loc [ runners_df [ 'Best Back Price' ] . idxmin (), 'Selection ID' ] fav_price = runners_df . loc [ runners_df [ 'Best Back Price' ] . idxmin (), 'Best Back Price' ] In [276]: # Define a limit order filter limit_order_filter = ` betfairlightweight ` . filters . limit_order ( size = 5 , price = 7 , persistence_type = 'LAPSE' ) # Define an instructions filter instructions_filter = ` betfairlightweight ` . filters . place_instruction ( selection_id = str ( fav_selection_id ), order_type = \"LIMIT\" , side = \"BACK\" , limit_order = limit_order_filter ) instructions_filter Out[276]: { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7 , 'size' : 5 }, 'orderType' : 'LIMIT' , 'selectionId' : '21283271' , 'side' : 'BACK' } In [277]: # Place the order order = trading . betting . place_orders ( market_id = '1.150038686' , # The market id we obtained from before customer_strategy_ref = 'back_the_fav' , instructions = [ instructions_filter ] # This must be a list ) Now that we've placed the other, we can check if the order placing was a success and if any has been matched. In [306]: order . __dict__ Out[306]: { '_data' : { 'instructionReports' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'instruction' : { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7.0 , 'size' : 5.0 }, 'orderType' : 'LIMIT' , 'selectionId' : 21283271 , 'side' : 'BACK' }, 'orderStatus' : 'EXECUTABLE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'sizeMatched' : 0.0 , 'status' : 'SUCCESS' }], 'marketId' : '1.150038686' , 'status' : 'SUCCESS' }, '_datetime_created' : datetime . datetime ( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), '_datetime_updated' : datetime . datetime ( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), 'customer_ref' : None , 'elapsed_time' : 1.484069 , 'error_code' : None , 'market_id' : '1.150038686' , 'place_instruction_reports' : [ < betfairlightweight . resources . bettingresources . PlaceOrderInstructionReports at 0x23e0f7952e8 > ], 'status' : 'SUCCESS' } As we can see, the status is 'SUCCESS' , whilst the sizeMatched is 0. Let's now look at our current orders.","title":"Placing Orders"},{"location":"api/apiPythontutorial/#get-current-orders","text":"To get our current orders, we need to use the listCurrentOrders operation. We can then use either the bet id, the market id, or the bet strategy to filter our orders. In [311]: trading . betting . list_current_orders ( customer_strategy_refs = [ 'back_the_fav' ]) . __dict__ Out [ 311 ]: { '_data' : { 'currentOrders' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'bspLiability' : 0.0 , 'customerStrategyRef' : 'back_the_fav' , 'handicap' : 0.0 , 'marketId' : '1.150038686' , 'orderType' : 'LIMIT' , 'persistenceType' : 'LAPSE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'priceSize' : { 'price' : 7.0 , 'size' : 5.0 }, 'regulatorCode' : 'MALTA LOTTERIES AND GAMBLING AUTHORITY' , 'selectionId' : 21283271 , 'side' : 'BACK' , 'sizeCancelled' : 0.0 , 'sizeLapsed' : 0.0 , 'sizeMatched' : 0.0 , 'sizeRemaining' : 5.0 , 'sizeVoided' : 0.0 , 'status' : 'EXECUTABLE' }], 'moreAvailable' : False }, '_datetime_created' : datetime . datetime ( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), '_datetime_updated' : datetime . datetime ( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), 'elapsed_time' : 1.327456 , 'more_available' : False , 'orders' : [ < betfairlightweight . resources . bettingresources . CurrentOrder at 0x23e0e7acd30 > ], 'publish_time' : None , 'streaming_unique_id' : None , 'streaming_update' : None } As we can see, we have one order which is unmatched for our strategy 'back_the_fav'","title":"Get Current Orders"},{"location":"api/apiPythontutorial/#cancelling-orders","text":"Let's now cancel this bet. To do this, we will use the cancelOrders operation. If you pass in a market ID it will cancel all orders for that specific market ID, like you can do on the website. In [312]: cancelled_order = trading . betting . cancel_orders ( market_id = '1.150038686' ) In [328]: # Create a DataFrame to view the instruction report pd . Series ( cancelled_order . cancel_instruction_reports [ 0 ] . __dict__ ) . to_frame () . T Out[328]: # status size_cancelled cancelled_date instruction error_code 0 SUCCESS 5 2018-10-26 06:01:26 betfairlightweight.resources.bettingresources... None","title":"Cancelling Orders"},{"location":"api/apiPythontutorial/#get-past-orders-and-results","text":"If we want to go back and look at past orders we have made, there are two main operations for this: listClearedOrders - this operation takes a range of data down to the individual selection ID level, and returns a summary of those specific orders listMarketProfitAndLoss - this operation is more specific, and only takes Market IDs to return the Profit/Loss for that market Alternatively, we can use the getAccountStatement operation from the Account Operations API. Let's now use both Sports API operations based on our previous orders and then compare it to the getAccountStatement operation.","title":"Get Past Orders and Results"},{"location":"api/apiPythontutorial/#get-cleared-orders","text":"In [346]: # listClearedOrders cleared_orders = trading . betting . list_cleared_orders ( bet_status = \"SETTLED\" , market_ids = [ \"1.150038686\" ]) In [371]: # Create a DataFrame from the orders pd . DataFrame ( cleared_orders . _data [ 'clearedOrders' ]) Out[371]: # betCount betId betOutcome eventId eventTypeId handicap lastMatchedDate marketId orderType persistenceType placedDate priceMatched priceReduced priceRequested profit selectionId settledDate side sizeSettled 0 1 142383373022 LOST 28971066 7 0.0 2018-10-26T10:31:53.000Z 1.150038686 MARKET_ON_CLOSE LAPSE 2018-10-26T00:12:03.000Z 5.74 False 5.74 -5.0 21283271 2018-10-26T10:34:39.000Z BACK 5.0 1 1 142383570640 WON 28971066 7 0.0 2018-10-26T00:16:32.000Z 1.150038686 LIMIT LAPSE 2018-10-26T00:16:31.000Z 5.40 False 5.50 5.0 21283271 2018-10-26T10:34:39.000Z LAY 5.0 Note that we can also filter for certain dates, bet ids, event ids, selection ids etc. We can also group by the event type, the event, the market, the runner, the side, the bet and the strategy, which is extremely useful if you're looking for a quick summary of how your strategy is performing.","title":"Get Cleared Orders"},{"location":"api/apiPythontutorial/#get-market-profit-and-loss","text":"Now let's find the Profit and Loss for the market. To do this we will use the listMarketProfitAndLoss operation. Note that this function only works with market IDs, and once the website clears the market, the operation will no longer work. However the market is generally up for about a minute after the race, so if your strategy is automated, you can check once if your bet is settled and if it is, hit the getMarketProfitAndLoss endpoint. Because of this, we will check a different market ID to the example above. In [406]: # Get the profit/loss - this returns a list pl = trading . betting . list_market_profit_and_loss ( market_ids = [ \"1.150318913\" ], include_bsp_bets = 'true' , include_settled_bets = 'true' ) In [410]: # Create a profit/loss DataFrame pl_df = pd . DataFrame ( pl [ 0 ] . _data [ 'profitAndLosses' ]) . assign ( marketId = pl [ 0 ] . market_id ) pl_df Out[410]: # ifWin selectionId marketId 0 -5.0 10065177 1.150318913 1 14.0 17029506 1.150318913 2 -5.0 5390339 1.150318913 3 -5.0 13771011 1.150318913 4 -5.0 138209 1.150318913 5 -5.0 10503541 1.150318913 6 -5.0 12165809 1.150318913","title":"Get Market Profit and Loss"},{"location":"api/apiPythontutorial/#get-account-statement","text":"Another method is to use the getAccountStatement , which provides an overview of all your bets over a certain time period. You can then filter this for specific dates if you wish. In [428]: # Define a date filter - get all bets for the past 4 days four_days_ago = ( datetime . datetime . utcnow () - datetime . timedelta ( days = 4 )) . strftime ( \"%Y-%m- %d T%TZ\" ) acct_statement_date_filter = ` betfairlightweight ` . filters . time_range ( from_ = four_days_ago ) # Request account statement account_statement = trading . account . get_account_statement ( item_date_range = acct_statement_date_filter ) In [450]: # Create df of recent transactions recent_transactions = pd . DataFrame ( account_statement . _data [ 'accountStatement' ]) recent_transactions Out[450]: # amount balance itemClass itemClassData itemDate legacyData refId 0 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":3.8,\"bet... 2018-10-28T23:14:28.000Z {'avgPrice': 3.8, 'betSize': 5.0, 'betType': '... 142845441633 1 5.0 261.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.4,\"bet... 2018-10-26T10:34:39.000Z {'avgPrice': 5.4, 'betSize': 5.0, 'betType': '... 142383570640 2 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.74,\"be... 2018-10-26T10:34:39.000Z {'avgPrice': 5.74, 'betSize': 5.0, 'betType': ... 142383373022 In [468]: # Create df of itemClassData - iterate over the account statement list and convert to json so that the DataFrame function # can read it correctly class_data = [ json . loads ( account_statement . account_statement [ i ] . item_class_data [ 'unknownStatementItem' ]) for i in range ( len ( account_statement . account_statement ))] In [471]: class_df = pd . DataFrame ( class_data ) class_df Out [471]: # avgPrice betCategoryType betSize betType commissionRate eventId eventTypeId fullMarketName grossBetAmount marketName marketType placedDate selectionId selectionName startDate transactionId transactionType winLose 0 3.80 M 5.0 B None 150318913 7 USA / TPara (US) 28 th Oct/ 16:06 R8 1m Allw Claim 0.0 R8 1m Allw Claim O 2018-10-28T23:02:28.000Z 17029506 Gato Guapo 2018-10-28T23:06:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST 1 5.40 E 5.0 L None 150038686 7 AUS / MVal (AUS) 26 th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:16:31.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_CREDIT 2 5.74 M 5.0 B None 150038686 7 AUS / MVal (AUS) 26 th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:12:03.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST As we can see, this DataFrame provides a much more comprehensive view of each of our bets. However, it lacks the ability to filter by strategy like the listClearedOrders operation in the Sports API.","title":"Get Account Statement"},{"location":"api/apiResources/","text":"API Resources \u00b6 There are a wide range of resources available to help make it easier to interact with the Betfair API, including those created by Betfair as well as the wider community. Here are some of the resources we'd recommend taking a look at if you're building a program to interact with either the polling and/or Stream APIs. The basics \u00b6 Creating & activating your app key Dev Docs Developer Program knowledge base Developer Forum where you can share your experiences and find out what's worked for other clients API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support. Github \u00b6 Our Datascientists' repos for using R and Python to access the API There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange The UK\u2019s Github repo including libraries for other languages Visualisers \u00b6 Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Other resources \u00b6 Towards Data Science provide a decent basic walk through of how to log in to the polling API and pull market data, with some interesting commentary along the way. Betfair Quants Slack Group betfair quants is really active community-owned Slack group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation.","title":"API resources"},{"location":"api/apiResources/#api-resources","text":"There are a wide range of resources available to help make it easier to interact with the Betfair API, including those created by Betfair as well as the wider community. Here are some of the resources we'd recommend taking a look at if you're building a program to interact with either the polling and/or Stream APIs.","title":"API Resources"},{"location":"api/apiResources/#the-basics","text":"Creating & activating your app key Dev Docs Developer Program knowledge base Developer Forum where you can share your experiences and find out what's worked for other clients API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support.","title":"The basics"},{"location":"api/apiResources/#github","text":"Our Datascientists' repos for using R and Python to access the API There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange The UK\u2019s Github repo including libraries for other languages","title":"Github"},{"location":"api/apiResources/#visualisers","text":"Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries","title":"Visualisers"},{"location":"api/apiResources/#other-resources","text":"Towards Data Science provide a decent basic walk through of how to log in to the polling API and pull market data, with some interesting commentary along the way. Betfair Quants Slack Group betfair quants is really active community-owned Slack group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation.","title":"Other resources"},{"location":"api/apiRtutorial/","text":"Betfair API tutorials in R \u00b6 Betfair's API can be easily traversed in R. It allows you to retrieve market information, create/cancel bets and manage your account. Here's a collection of easy to follow API tutorials in R: Accessing the API using R Get Worldcup Odds AFL Odds PulleR Tutorial Accessing the API using R \u00b6 Set up R \u00b6 What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Required Packages \u00b6 Two R packages are required: library ( tidyverse ) library ( abettor ) The abettor package can be downloaded here . For an in-depth understanding of the package, have a read of the documentation. Instructions are also provided in the sample code. Login to Betfair \u00b6 To login to Betfair, replace the following dummy username, password and app key with your own. abettor :: loginBF ( username = \"betfair_username\" , password = \"betfair_password\" , applicationKey = \"betfair_app_key\" ) If you don't have a live app key for the API yet take a look at this page . Finding Event IDs \u00b6 In order to find data for specific markets, you will first need to know the event ID. This is easily achieved with the abettor package . To find the event IDs of events in the next 60 days: abettor :: listEventTypes ( toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" ))) This will return a DataFrame of the following structure: eventType.id eventType.name marketCount 1 Soccer 1193 2 Tennis 2184 7522 Basketball 1 4 Cricket 37 7 Horse Racing 509 61420 Australian Rules 31 4339 Greyhound Racing 527 Finding Competition IDs \u00b6 Once you have the event ID, the next logical step is to find the competition IDs for the event you want to get data for. For example, if you want to find the competition IDs for Australian Rules, you would use the following abettor :: listCompetitions ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) This will return the following structured DataFrame: competition.id competition.name marketCount competitionRegion 11516633 Brownlow Medal 2018 3 AUS 11897406 AFL 78 AUS Finding Specific Markets \u00b6 The next logical step is to find the market that you are interested in. Furthering our example above, if you want the Match Odds for all Australian Rules games over the next 60 days, simply use the Competition ID from above in the following. abettor :: listMarketCatalogue ( eventTypeIds = 61420 , marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = 11897406 , toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) This returns a large DataFrame object with each market, participants and associated odds. Get World Cup Odds Tutorial \u00b6 This tutorial walks you through the process of retrieving exchange odds for all the matches from the 2018 FIFA World Cup 2018. This can be modified for other sports and uses. You can run this script in R. ################################################### ### FIFA World Cup Datathon ### Betfair API Tutorial ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches from the upcoming FIFA World Cup 2018 ################################################### ################################################### ### Setup ################################################### ## Loading required packages library ( tidyverse ) ## package for general data manipulation - https://www.tidyverse.org/ library ( abettor ) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Enter your Betfair API Credentials below betfair_username <- \"\" betfair_password <- \"\" betfair_app_key <- \"\" ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF ( username = betfair_username , password = betfair_password , applicationKey = betfair_app_key ) ################################################### ## Retrieving all soccer competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_soccer_markets <- abettor :: listCompetitions ( eventTypeIds = 1 , ## Soccer is eventTypeId 1, toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Retrieving the competition id ## for the 2018 World Cup ################################################### world_cup_competition_id <- all_soccer_markets %>% dplyr :: pull ( competition ) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter ( name == \"2018 FIFA World Cup\" ) %>% ## Filtering for the competition we need dplyr :: pull ( id ) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the World Cup ################################################### all_world_cup_markets <- abettor :: listMarketCatalogue ( eventTypeIds = 1 , ## Soccer is eventTypeId 1 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = world_cup_competition_id , ## Restrict our search to World Cup matches only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step (World Cup Matches) ################################################### ## Creating a vector/array of all market ids all_world_cup_markets_market_ids <- all_world_cup_markets %>% pull ( marketId ) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function ( market_id ) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook ( marketIds = market_id , ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull ( runners ) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select ( lastPriceTraded ) %>% ## Extracting team and last matched odds mutate ( market_id = market_id ) %>% ## Padding market id to the data to make it unique to this match bind_cols ( data.frame ( outcome = c ( \"o_1\" , \"o_2\" , \"o_3\" ))) %>% ## Creating outcome order to maintain consistency spread ( outcome , lastPriceTraded ) %>% ## Reshaping data to make it 1 row per match rename ( team_1_odds = o_1 , team_2_odds = o_2 , draw_odds = o_3 ) %>% ##Renaming columns such that all matches can be combined into one data frame select ( market_id , team_1_odds , draw_odds , team_2_odds ) ## Ordering columns in the right order return ( odds ) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame world_cup_market_odds <- map_dfr ( .x = all_world_cup_markets_market_ids , ##Iterate over market ids .f = fetch_odds ## through function fetch_odds ) %>% bind_cols ( all_world_cup_markets %>% ## Merge with event names to identify which match odds it is pull ( event ) %>% select ( name )) %>% mutate ( team_1 = gsub ( \" v .*\" , \"\" , name ), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name )) %>% ## Extracing team 2 from match name select ( team_1 , team_2 , team_1_odds , draw_odds , team_2_odds ) ## Extracting columns that we need ## Writing output to csv file write_csv ( world_cup_market_odds , \"world_cup_market_odds.csv\" ) AFL Odds PulleR Tutorial \u00b6 This tutorial walks you through the process of retrieving exchange odds for the the next round of Australian Rules. You can run this script in R. ################################################### ### AFL Model ### Betfair API Odds GrabbR ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches for the upcoming round of AFL games ################################################### ## Loading required packages library ( tidyverse ) ## package for general data manipulation - https://www.tidyverse.org/ library ( abettor ) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF ( username = 'your_username' , password = 'your_password' , applicationKey = \"your_betfair_app_key\" ) ################################################### ## Retrieving all AFL competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_afl_markets <- abettor :: listCompetitions ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) ################################################### ## Retrieving the competition id ## for the regular AFL season ################################################### afl_competition_id <- all_afl_markets %>% dplyr :: pull ( competition ) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter ( name == \"AFL\" ) %>% ## Filtering for the competition we need dplyr :: pull ( id ) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the AFL ################################################### all_afl_markets <- abettor :: listMarketCatalogue ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = afl_competition_id , ## Restrict our search to AFL Matches Only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step ################################################### ## Creating a vector/array of all market ids all_afl_markets_market_ids <- all_afl_markets %>% pull ( marketId ) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function ( market_id ) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook ( marketIds = market_id , ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull ( runners ) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select ( lastPriceTraded ) %>% ## Extracting team and last matched odds mutate ( market_id = market_id ) %>% ## Padding market id to the data to make it unique to this match bind_cols ( data.frame ( outcome = c ( \"o_1\" , \"o_2\" ))) %>% ## Creating outcome order to maintain consistency spread ( outcome , lastPriceTraded ) %>% ## Reshaping data to make it 1 row per match rename ( team_1_odds = o_1 , team_2_odds = o_2 ) %>% ##Renaming columns such that all matches can be combined into one data frame select ( market_id , team_1_odds , team_2_odds ) ## Ordering columns in the right order return ( odds ) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame afl_market_odds <- map_dfr ( .x = all_afl_markets_market_ids , ##Iterate over market ids .f = fetch_odds ## through function fetch_odds ) %>% bind_cols ( all_afl_markets %>% ## Merge with event names to identify which match odds it is pull ( event ) %>% select ( name )) %>% mutate ( team_1 = gsub ( \" v .*\" , \"\" , name ), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name )) %>% ## Extracing team 2 from match name select ( team_1 , team_2 , team_1_odds , team_2_odds ) ## Extracting columns that we need ## Writing output to csv file write_csv ( afl_market_odds , \"weekly_afl_odds.csv\" )","title":"API tutorials in R"},{"location":"api/apiRtutorial/#betfair-api-tutorials-in-r","text":"Betfair's API can be easily traversed in R. It allows you to retrieve market information, create/cancel bets and manage your account. Here's a collection of easy to follow API tutorials in R: Accessing the API using R Get Worldcup Odds AFL Odds PulleR Tutorial","title":"Betfair API tutorials in R"},{"location":"api/apiRtutorial/#accessing-the-api-using-r","text":"","title":"Accessing the API using R"},{"location":"api/apiRtutorial/#set-up-r","text":"What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R","title":"Set up R"},{"location":"api/apiRtutorial/#required-packages","text":"Two R packages are required: library ( tidyverse ) library ( abettor ) The abettor package can be downloaded here . For an in-depth understanding of the package, have a read of the documentation. Instructions are also provided in the sample code.","title":"Required Packages"},{"location":"api/apiRtutorial/#login-to-betfair","text":"To login to Betfair, replace the following dummy username, password and app key with your own. abettor :: loginBF ( username = \"betfair_username\" , password = \"betfair_password\" , applicationKey = \"betfair_app_key\" ) If you don't have a live app key for the API yet take a look at this page .","title":"Login to Betfair"},{"location":"api/apiRtutorial/#finding-event-ids","text":"In order to find data for specific markets, you will first need to know the event ID. This is easily achieved with the abettor package . To find the event IDs of events in the next 60 days: abettor :: listEventTypes ( toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" ))) This will return a DataFrame of the following structure: eventType.id eventType.name marketCount 1 Soccer 1193 2 Tennis 2184 7522 Basketball 1 4 Cricket 37 7 Horse Racing 509 61420 Australian Rules 31 4339 Greyhound Racing 527","title":"Finding Event IDs"},{"location":"api/apiRtutorial/#finding-competition-ids","text":"Once you have the event ID, the next logical step is to find the competition IDs for the event you want to get data for. For example, if you want to find the competition IDs for Australian Rules, you would use the following abettor :: listCompetitions ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) This will return the following structured DataFrame: competition.id competition.name marketCount competitionRegion 11516633 Brownlow Medal 2018 3 AUS 11897406 AFL 78 AUS","title":"Finding Competition IDs"},{"location":"api/apiRtutorial/#finding-specific-markets","text":"The next logical step is to find the market that you are interested in. Furthering our example above, if you want the Match Odds for all Australian Rules games over the next 60 days, simply use the Competition ID from above in the following. abettor :: listMarketCatalogue ( eventTypeIds = 61420 , marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = 11897406 , toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) This returns a large DataFrame object with each market, participants and associated odds.","title":"Finding Specific Markets"},{"location":"api/apiRtutorial/#get-world-cup-odds-tutorial","text":"This tutorial walks you through the process of retrieving exchange odds for all the matches from the 2018 FIFA World Cup 2018. This can be modified for other sports and uses. You can run this script in R. ################################################### ### FIFA World Cup Datathon ### Betfair API Tutorial ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches from the upcoming FIFA World Cup 2018 ################################################### ################################################### ### Setup ################################################### ## Loading required packages library ( tidyverse ) ## package for general data manipulation - https://www.tidyverse.org/ library ( abettor ) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Enter your Betfair API Credentials below betfair_username <- \"\" betfair_password <- \"\" betfair_app_key <- \"\" ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF ( username = betfair_username , password = betfair_password , applicationKey = betfair_app_key ) ################################################### ## Retrieving all soccer competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_soccer_markets <- abettor :: listCompetitions ( eventTypeIds = 1 , ## Soccer is eventTypeId 1, toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Retrieving the competition id ## for the 2018 World Cup ################################################### world_cup_competition_id <- all_soccer_markets %>% dplyr :: pull ( competition ) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter ( name == \"2018 FIFA World Cup\" ) %>% ## Filtering for the competition we need dplyr :: pull ( id ) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the World Cup ################################################### all_world_cup_markets <- abettor :: listMarketCatalogue ( eventTypeIds = 1 , ## Soccer is eventTypeId 1 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = world_cup_competition_id , ## Restrict our search to World Cup matches only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step (World Cup Matches) ################################################### ## Creating a vector/array of all market ids all_world_cup_markets_market_ids <- all_world_cup_markets %>% pull ( marketId ) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function ( market_id ) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook ( marketIds = market_id , ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull ( runners ) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select ( lastPriceTraded ) %>% ## Extracting team and last matched odds mutate ( market_id = market_id ) %>% ## Padding market id to the data to make it unique to this match bind_cols ( data.frame ( outcome = c ( \"o_1\" , \"o_2\" , \"o_3\" ))) %>% ## Creating outcome order to maintain consistency spread ( outcome , lastPriceTraded ) %>% ## Reshaping data to make it 1 row per match rename ( team_1_odds = o_1 , team_2_odds = o_2 , draw_odds = o_3 ) %>% ##Renaming columns such that all matches can be combined into one data frame select ( market_id , team_1_odds , draw_odds , team_2_odds ) ## Ordering columns in the right order return ( odds ) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame world_cup_market_odds <- map_dfr ( .x = all_world_cup_markets_market_ids , ##Iterate over market ids .f = fetch_odds ## through function fetch_odds ) %>% bind_cols ( all_world_cup_markets %>% ## Merge with event names to identify which match odds it is pull ( event ) %>% select ( name )) %>% mutate ( team_1 = gsub ( \" v .*\" , \"\" , name ), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name )) %>% ## Extracing team 2 from match name select ( team_1 , team_2 , team_1_odds , draw_odds , team_2_odds ) ## Extracting columns that we need ## Writing output to csv file write_csv ( world_cup_market_odds , \"world_cup_market_odds.csv\" )","title":"Get World Cup Odds Tutorial"},{"location":"api/apiRtutorial/#afl-odds-puller-tutorial","text":"This tutorial walks you through the process of retrieving exchange odds for the the next round of Australian Rules. You can run this script in R. ################################################### ### AFL Model ### Betfair API Odds GrabbR ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches for the upcoming round of AFL games ################################################### ## Loading required packages library ( tidyverse ) ## package for general data manipulation - https://www.tidyverse.org/ library ( abettor ) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF ( username = 'your_username' , password = 'your_password' , applicationKey = \"your_betfair_app_key\" ) ################################################### ## Retrieving all AFL competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_afl_markets <- abettor :: listCompetitions ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) ################################################### ## Retrieving the competition id ## for the regular AFL season ################################################### afl_competition_id <- all_afl_markets %>% dplyr :: pull ( competition ) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter ( name == \"AFL\" ) %>% ## Filtering for the competition we need dplyr :: pull ( id ) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the AFL ################################################### all_afl_markets <- abettor :: listMarketCatalogue ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = afl_competition_id , ## Restrict our search to AFL Matches Only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step ################################################### ## Creating a vector/array of all market ids all_afl_markets_market_ids <- all_afl_markets %>% pull ( marketId ) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function ( market_id ) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook ( marketIds = market_id , ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull ( runners ) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select ( lastPriceTraded ) %>% ## Extracting team and last matched odds mutate ( market_id = market_id ) %>% ## Padding market id to the data to make it unique to this match bind_cols ( data.frame ( outcome = c ( \"o_1\" , \"o_2\" ))) %>% ## Creating outcome order to maintain consistency spread ( outcome , lastPriceTraded ) %>% ## Reshaping data to make it 1 row per match rename ( team_1_odds = o_1 , team_2_odds = o_2 ) %>% ##Renaming columns such that all matches can be combined into one data frame select ( market_id , team_1_odds , team_2_odds ) ## Ordering columns in the right order return ( odds ) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame afl_market_odds <- map_dfr ( .x = all_afl_markets_market_ids , ##Iterate over market ids .f = fetch_odds ## through function fetch_odds ) %>% bind_cols ( all_afl_markets %>% ## Merge with event names to identify which match odds it is pull ( event ) %>% select ( name )) %>% mutate ( team_1 = gsub ( \" v .*\" , \"\" , name ), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name )) %>% ## Extracing team 2 from match name select ( team_1 , team_2 , team_1_odds , team_2_odds ) ## Extracting columns that we need ## Writing output to csv file write_csv ( afl_market_odds , \"weekly_afl_odds.csv\" )","title":"AFL Odds PulleR Tutorial"},{"location":"api/apiappkey/","text":"How to access the Betfair API \u00b6 Betfair has it\u2019s own Exchange API . You can use it to programmatically retrieve live markets, automate successful trading strategies or create your own customised trading interface. Professional punters use it for these functions and many more. This guide helps Australian and New Zealand customers with obtaining their Betfair API Key. If you\u2019re outside of these two regions please go to the UK's Developer Program website . There are four steps involved in getting access to our API Obtain an SSOID token Register your application Obtain your app key Activate your app key API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support. Find your SSOID token \u00b6 The simplest way to setup your browser with the SSOID is to follow this link and log in - this will allow for the SSOID to be automatically populated in the next step. After logging in, you\u2019ll be sent to the main Betfair website. Note: it may not show that you\u2019re logged in on the site. You can ignore that. Proceed to step two. Register your application \u00b6 Navigate to the API-NG accounts visualiser . If you\u2019ve followed step 1 correctly, your SSOID token should be automatically populated in the visualiser. Next click on createDeveloperAppKeys in the left hand navigation. Type in an application name (this is your app key name, so make sure this is unique), then click \u2018Execute\u2019 down the bottom of the page. If you receive an error message saying that your app key couldn\u2019t be created, it\u2019s most likely because you already have one. Use the getDeveloperAppKeys method in the left hand menu to check whether there\u2019s already an app key associated with your account. Find your app key \u00b6 After your key is created, you should see in the right hand panel your application: You\u2019ll notice that two application keys have been created; Version \u2013 1.0-Delay: is a delayed app key for development purposes Version \u2013 1.0: is the live pricing app key; on yours it should have a status \u2018No\u2019 in Active. Grab the application key listed for the live price one - for the example above, that is \u2018MkcBqyZrD53V6A..\u2019 Activate your app key \u00b6 This process will generate two app keys: A developer key which is designed for development purposes. This has a variable delay of between 1 and 180 seconds, doesn\u2019t show matched volume and doesn\u2019t need to be activated prior to use. A live app key is intended for transacting on the Exchange and should only be used when you\u2019re ready to start placing bets or can no longer test your strategy effectively using the developer key. Please note that if the live key is used to pull data from the Exchange without corresponding bets being placed a delay may be automatically applied to the live key. If you\u2019re ready to start testing your strategy or placing bets, please contact api@betfair.com.au and we will be happy to assist with activating the live key and implementing your strategy.","title":"How to access the Betfair API"},{"location":"api/apiappkey/#how-to-access-the-betfair-api","text":"Betfair has it\u2019s own Exchange API . You can use it to programmatically retrieve live markets, automate successful trading strategies or create your own customised trading interface. Professional punters use it for these functions and many more. This guide helps Australian and New Zealand customers with obtaining their Betfair API Key. If you\u2019re outside of these two regions please go to the UK's Developer Program website . There are four steps involved in getting access to our API Obtain an SSOID token Register your application Obtain your app key Activate your app key API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support.","title":"How to access the Betfair API"},{"location":"api/apiappkey/#find-your-ssoid-token","text":"The simplest way to setup your browser with the SSOID is to follow this link and log in - this will allow for the SSOID to be automatically populated in the next step. After logging in, you\u2019ll be sent to the main Betfair website. Note: it may not show that you\u2019re logged in on the site. You can ignore that. Proceed to step two.","title":"Find your SSOID token"},{"location":"api/apiappkey/#register-your-application","text":"Navigate to the API-NG accounts visualiser . If you\u2019ve followed step 1 correctly, your SSOID token should be automatically populated in the visualiser. Next click on createDeveloperAppKeys in the left hand navigation. Type in an application name (this is your app key name, so make sure this is unique), then click \u2018Execute\u2019 down the bottom of the page. If you receive an error message saying that your app key couldn\u2019t be created, it\u2019s most likely because you already have one. Use the getDeveloperAppKeys method in the left hand menu to check whether there\u2019s already an app key associated with your account.","title":"Register your application"},{"location":"api/apiappkey/#find-your-app-key","text":"After your key is created, you should see in the right hand panel your application: You\u2019ll notice that two application keys have been created; Version \u2013 1.0-Delay: is a delayed app key for development purposes Version \u2013 1.0: is the live pricing app key; on yours it should have a status \u2018No\u2019 in Active. Grab the application key listed for the live price one - for the example above, that is \u2018MkcBqyZrD53V6A..\u2019","title":"Find your app key"},{"location":"api/apiappkey/#activate-your-app-key","text":"This process will generate two app keys: A developer key which is designed for development purposes. This has a variable delay of between 1 and 180 seconds, doesn\u2019t show matched volume and doesn\u2019t need to be activated prior to use. A live app key is intended for transacting on the Exchange and should only be used when you\u2019re ready to start placing bets or can no longer test your strategy effectively using the developer key. Please note that if the live key is used to pull data from the Exchange without corresponding bets being placed a delay may be automatically applied to the live key. If you\u2019re ready to start testing your strategy or placing bets, please contact api@betfair.com.au and we will be happy to assist with activating the live key and implementing your strategy.","title":"Activate your app key"},{"location":"historicData/backtestingRatingsTutorial/","text":"Backtesting wagering models with Betfair JSON stream data \u00b6 This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the JSON to CSV tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo . Set up \u00b6 I'm going to be using a jupyter notebook for this investigation which is a special type of data analysis output that is used to combine code, outputs and explanatory text in a readable single document. It's mostly closely associated with python data analysis code which is the language I'll be using here also. The entire body of python code used will be repeated at the bottom of the article where you can copy it and repurpose it for yourself. If you're not familiar with python, don't worry neither am I really! I'm inexperienced with python so if you have experience with some other programming language you should be able to follow along with the logic here too. If you don't have experience using another programming language this all might appear intimidating but it's heavy on the explanatory text so you should get something out of it. We need a few non standard python libraries so make sure to install betfairlightweight and plotly before you get started with something like pip install betfairlightweight & pip install plotly . We'll load all our libraries and do some setup here. import pandas as pd import numpy as np import requests from datetime import date , timedelta import os import re import tarfile import zipfile import bz2 import glob import logging from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import plotly.express as px import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) % config IPCompleter . greedy = True Context \u00b6 Backtesting is the life-blood of most successful wagering systems. In short it attempts to answer a single question for you: \ud835\udf0f : How much money will I win or lose if I started using this system to place bets with real money? Without a rigorous and quantitative backtesting approach it's really quite hard to estimate the answer to this question $ \\tau $ that will be even reliably on the right side of zero. You could live test your system with real bets at small stakes, however, this isn't the panacea it seems. It will take time (more than you think) for your results to converge to their long term expectation. How long? Answering this question will require some expertise with probability and statistics you might not have. Even more than that though is that depending on where you're betting your results at small stakes could be very different than at larger stakes. You might not be able get a good answer to $ \\tau $ until betting at full stakes at which point finding the answer might coincide with blowing up your gambling bankroll. Backtesting is also very hard. To perfectly backtest your own predicted probability on a historical race or sporting match you need to produce 2 things: (1) What would my predicted chance have been exactly for this selection in this market on this day in the past? (2) What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction? The devil in the detail of backtesting tends to be in those exactlys. The aim of the backtesting game is answering (2) as accurately as possible because it tells you exactly how much you would have made over your backtesting period, from there you can confidently project that rate of profitability forward. It's easy to make mistakes and small errors in the quantitative reasoning can lead you to extremely misguided projections downstream. Question (1) won't be in the scope of this notebook but it's equally (and probably more) important that (2) but it is the key challenge of all predictive modelling exercises so there's plenty of discussion about it elsewhere. Backtesting on Betfair \u00b6 Answering question (2) for betting on the Betfair Exchange is difficult. The Exchange is a dynamic system that changes from one micro second to the next. What number should you use for odds? How much could you assume to get down at those odds? The conventional and easiest approach is to backtest at the BSP. The BSP is simple because it's a single number (to use for both back and lay bets) and is a taken price (there's no uncertainty about getting matched). Depending on the liquidity of the market a reasonably sized stake might also not move the BSP very much. For some markets you may be able to safely assume you could be $10s of dollars at the BSP without moving it an inch. However, that's definitely not true of all BSP markets and you need to be generally aware that your Betfair orders in the future will change the state of the exchange, and large bets will move the BSP in an unfavourable direction. Aside from uncertainty around the liquidity and resilience of the BSP, many many markets don't have a BSP. So what do we do then? Typically what a lot of people (who have a relationship with Betfair Australia) do at this point is request a data dump. They might request an odds file for all Australian harness race win markets since June 2018 with results and 4 different price points: the BSP, the last traded price, the weighted average price (WAP) traded in 3 minutes before the race starts, and the WAP for all bets matched prior to 3 mins before the race. However, you will likely need to be an existing VIP customer to get this file and it's not a perfect solution: it might take 2 weeks to get, you can't refresh it, you can't test more hypothetical price points after your initial analysis amongst many other problems. What if you could produce this valuable data file yourself? Betfair Stream Data \u00b6 Betfair's historical stream data is an extremely rich source of data. However, in it's raw form it's difficult to handle for the uninitiated. It also might not be immediately obvious how many different things this dataset could be used for without seeing some examples. These guides will hopefully demystify how to turn this raw data into a familiar and usable format whilst also hopefully providing some inspiration for the kinds of value that can be excavated from it. This example: backtesting Betfair Hub thoroughbred model \u00b6 To illustrate how you can use the stream files to backtest the outputs of a rating system we'll use the Australian Thoroughbred Rating model available on the Betfair Hub. The most recent model iteration only goes back till Feb 28 th 2021 however as an illustrative example this is fine. We'd normally want to backtest with a lot more historical data than this, which just means in this case our estimation of future performance will be unreliable. I'm interested to see how we would have fared betting all selections rated by this model according to a few different staking schemes and also at a few different times / price points. Old ratings If you want to pull in ratings from before Feb 2021 to add to your database for more complete backtesting these are available in a data dump here . Scrape The Model Ratings \u00b6 If you travel to the Betfair hub ratings page you'll find that URL links behind the ratings download buttons have a consistent URL pattern that looks very scrape friendly. We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe. # Function to return Pandas DF of hub ratings for a particular date def getHubRatings ( dte ): # Substitute the date into the URL url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date= {} presenter=RatingsPresenter&json=true' . format ( dte ) # Convert the response into JSON responseJson = requests . get ( url ) . json () hubList = [] if not responseJson : return ( None ) # Want an normalised table (1 row per selection) # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields for meeting in responseJson [ 'meetings' ]: for race in meeting [ 'races' ]: for runner in race [ 'runners' ]: hubList . append ( { 'date' : dte , 'track' : meeting [ 'name' ], 'race_number' : race [ 'number' ], 'race_name' : race [ 'name' ], 'market_id' : race [ 'bfExchangeMarketId' ], 'selection_id' : str ( runner [ 'bfExchangeSelectionId' ]), 'selection_name' : runner [ 'name' ], 'model_odds' : runner [ 'ratedPrice' ] } ) out = pd . DataFrame ( hubList ) return ( out ) # See the response from a single day getHubRatings ( date ( 2021 , 3 , 1 )) . head ( 5 ) date track race_number race_name market_id selection_id selection_name model_odds 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620052 1. Military Affair 6.44 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 5889703 3. Proverbial 21.11 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38177688 4. A Real Wag 9.97 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620053 5. El Jay 44.12 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 37263264 6. Flying Honour 3.39 dateDFList = [] dateList = pd . date_range ( date ( 2021 , 2 , 18 ), date . today () - timedelta ( days = 1 ), freq = 'd' ) for dte in dateList : dateDFList . append ( getHubRatings ( dte )) # Concatenate (add rows to rows) all the dataframes within the list hubRatings = pd . concat ( dateDFList ) hubRatings . shape ( 32519 , 8 ) Assembling the odds file \u00b6 So part 1 was very painless. This is how we like data: served by some API or available in a nice tabular format on a webpage ready to be scraped with standard tools available in popular languages. Unfortunately, it won't be so painless to assemble our odds file. We'll find out why it's tricky as we go. The Data \u00b6 The data we'll be using is the historical Exchange data available from this website. The data available through this service is called streaming JSON data. There are a few options available relating to granularity (how many time points per second the data updates at) but we'll be using the most granular \"PRO\" set which has updates every 50 milliseconds. Essentially what the data allows us to do is, for a particular market, recreate the exact state of the Betfair Exchange at say: 150 milliseconds before the market closed. When people say the state of the Exchange they mean two things a) what are all the current open orders on all the selections b) what are the current traded volumes on each selection at each price point. We obviously don't have access to any information about which accounts are putting up which prices and other things Betfair has themselves. We're essentially getting a snapshot of everything you can see through the website by clicking on each selection manually and looking at the graphs, tables and ladders. However, with just these 2 sets of information we can build a rich view of the dynamics of exchange and also build out all of the summary metrics (WAP etc) we might have previously needed Betfair to help with. For our purposes 50 milli-second intervaled data is huge overkill. But you could imagine needing this kind of granularity for other kinds of wagering systems - eg a high frequency trading algorithm of some sort that needs to make many decisions and actions every second. Let's take a look at what the stream data looks like for a single market: So it looks pretty intractable. For this particular market there's 14,384 lines of data where each line consists of a single JSON packet of data. If you're not a data engineer (neither am I) your head might explode thinking about how you could read this into your computer and transform it into something usable. The data looks like this because it is saved from a special Betfair API called the Stream API which which is used by high end Betfair API users and which delivers fast speeds other performance improvements over the normal \"polling\" API. Now what's good about that, for the purposes of our exercise, is that the very nice python package betfairlightweight has the functionality built to not only parse the Stream API when connected live but also these historical saved versions of the stream data. Without it we'd be very far away from the finish line, with betfairlightweight we're pretty close. Unpacking / flattening the data \u00b6 Because these files are so large and unprocessed this process won't look the same as your normal data ETL in python: where you can read a raw data file (csv, JSON, text etc.) into memory and use python functions to transform into usable format. I personally had no idea how to use python and betfairlightweight to parse these data until I saw Betfair's very instructive overview which you should read for a more detailed look at some of the below code. By my count there were 4 key conceptual components that I had to get my head around to understand and be able to re-purpose that code. So if you're like me (a bit confused by some of the steps in that piece) this explanation might help. I'll assume you don't do any decompression and keep the monthly PRO files as the .tar archives as they are. Conceptually the process looks something like this: Load the \"archives\" into a \"generator\" Scan across the generator (market_ids) and the market states within those markets to extract useful objects Process those useful objects to pull out some metadata + useful summary numbers derived from the available orders and traded volumes snapshot data Write this useful summarised data to a file that can be read and understood with normal data analysis workflows First we'll run a bunch of setup code setting up my libraries and creating some utility functions that will be used throughout the main parsing component. It'll also point to the two stream files I'll be parsing for this exercise. # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = StreamListener ( max_latency = None ) ### Utility Functions # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input sample: R6 1400m Grp1 parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) # creating a flag that is True when markets are australian thoroughbreds def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) 1: .tar load \u00b6 This function I stole from Betfair's instructional article The stream files are downloaded as .tar archive files which are a special kind of file that we'll need to unpack Instead of loading each file into memory this function returns a \"generator\" which is a special python object that is to be iterated over This basically means it contains the instructions to unpack and scan over files on the fly This function also contains the logic to deal with if these files are zip archives or you've manually unpacked the archive and have the .bz2 zipped files # loading from tar and extracting files def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None 2: Scan across market states and extract useful objects \u00b6 So this function will take a special \"stream\" object which we'll create with betfairlightweight The function takes a stream object input and returns 4 instances of the market state The market state 3 mins before the scheduled off The market state immediately before it goes inplay The market state immediately before it closes for settlement The final market state with outcomes It basically just loops over all the market states and has a few checks to determine if it should save the current market state as key variables and then returns those # Extract Components From Generated Stream def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book evaluate_market = None prev_market = None postplay_market = None preplay_market = None t3m_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 3 mins before scheduled off if t3m_market is None and seconds_to_start < 3 * 60 : t3m_market = market_book # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay_market is not None and preplay_market is None : preplay_market = postplay_market return ( t3m_market , preplay_market , postplay_market , prev_market ) # Final market is last prev_market 3 & 4: Summarise those useful objects and write to .csv \u00b6 This next chunk contains a wrapper function that will do all the execution It will open a csv output file Use the load_markets utility to iterate over the .tar files Use betfairlightweight to instantiate the special stream object Pass that stream object to the extract_components_from_stream which will scan across the market states and pull out 4 key market books Convert those marketbooks into simple summary numbers or dictionaries that will be written to the output .csv file def run_stream_parsing (): # Run Pipeline with open ( \"outputs/tho-odds.csv\" , \"w+\" ) as output : # Write Column Headers To File output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m \\n \" ) for file_obj in load_markets ( data_path ): # Instantiate a \"stream\" object stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) # Extract key components according to the custom function above (outputs 4 objects) ( t3m_market , preplay_market , postplay_market , final_market ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay_market is None : continue ; # Runner metadata and key fields available from final market book runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final_market . runners ] # Last Traded Price # _____________________ # From the last marketbook before inplay or close ltp = [ runner . last_price_traded for runner in preplay_market . runners ] # Total Matched Volume # _____________________ # Calculates the traded volume across all traded price points for each selection def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) selection_traded_volume = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay_market . runners ] # Top 3 Ladder # ______________________ # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries def top_3_ladder ( availableLadder ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : 3 ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"price\" ] = price out [ \"volume\" ] = volume return ( out ) # Sometimes t-3 mins market book is empty try : atb_ladder_3m = [ top_3_ladder ( runner . ex . available_to_back ) for runner in t3m_market . runners ] atl_ladder_3m = [ top_3_ladder ( runner . ex . available_to_lay ) for runner in t3m_market . runners ] except : atb_ladder_3m = {} atl_ladder_3m = {} # Writing To CSV # ______________________ for ( runnerMeta , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ) in zip ( runner_data , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final_market . market_id ), final_market . market_definition . market_time , final_market . market_definition . country_code , final_market . market_definition . venue , final_market . market_definition . name , runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], runnerMeta [ 'selection_status' ], runnerMeta [ 'sp' ], ltp , selection_traded_volume , '\"' + str ( atb_ladder_3m ) + '\"' , # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas '\"' + str ( atl_ladder_3m ) + '\"' ) ) # This will execute the files (it took me ~2 hours for 2 months of data) #run_stream_parsing() Extending this code \u00b6 Because this process is very slow you might want to save much more information than you think you need For example I currently think I only want the best back and lay prices at t-3 mins before the off but I've saved the top 3 boxes in the available to back and lay ladders as dictionary strings From these ladders I can retroactively calculate not only just the best back and lay prices but also WAP prices and also sizes at those boxes which I could use for much more accurate backtesting if I wanted to later without having can across the entire stream files again I could easily save the entire open and traded orders ladders in the same way amongst many other ways of retaining more of the data for post-processing analysis Backtesting Analysis \u00b6 Let's take stock of where we are. We currently have model ratings (about 1.5 months worth) and Betfair Odds (2 months worth). Circling back to the original backtesting context we needed to solve for 2 key questions: What would my predicted chance have been exactly for this selection in this market on this day in the past? What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction? Backtesting with someone else's publicly available and historically logged ratings solves question 1. With these particular ratings we're fine but generally we should just be aware there are some sketchy services that might make retroactive adjustments to historical ratings to juice their performance which obviously violates 1. For the second part we now have several real Betfair odds values to combine with the ratings and some chosen staking formula to simulate actual bets. I won't dwell too much on the stake size component but it's important. Similarly we aren't out of the woods with the \"what odds exactly\" question either. I'll show performance of backtesting at the \"Last Traded Price\" however, there's literally no way of actually being the last bet matched order on every exchange market so there's some uncertainty in a few of these prices. Further, and from experience, if you placing bets at the BSP and you're using some form of proportional staking (like Kelly) then you're calculated stake size will need to include a quantity (the BSP) which you will literally never be 100% sure of. You'll need to estimate the BSP as close to market suspension as you can and place your BSP bets with a stake sized derived from that estimation. This imprecision in stake calculation WILL cost you some profit relative to your backtested expectation. These might seem like minor considerations but you should be aware of some of the gory details of the many ways becoming successful on Betfair is really difficult. To be reliably profitable on Betfair you don't just need a good model, you'll likely need to spend hours and hours thinking about these things: testing things, ironing out all these little kinks and trying to account for all your uncertainties. I'll just be running over the skeleton of what you should do. Setting up your master data \u00b6 # First we'll load and tidy our odds data # Load in odds file we created above bfOdds = pd . read_csv ( \"outputs/tho-odds.csv\" , dtype = { 'market_id' : object , 'selection_id' : object , 'atb_ladder_3m' : object , 'atl_ladder_3m' : object }) # Convert dictionary columns import ast bfOdds [ 'atb_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atb_ladder_3m' ]] bfOdds [ 'atl_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atl_ladder_3m' ]] # Convert LTP to Numeric bfOdds [ 'ltp' ] = pd . to_numeric ( bfOdds [ 'ltp' ], errors = 'coerce' ) # Filter after 18th Feb bfOdds = bfOdds . query ( 'event_date >= \"2021-02-18\"' ) bfOdds . head ( 5 ) market_id event_date country track market_name selection_id selection_name result bsp ltp matched_volume atb_ladder_3m atl_ladder_3m 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 31552374 2. Chubascos LOSER 5.84 5.9 7390.59 {'price': [6, 5.9, 5.8], 'volume': [30.99, 82.... {'price': [6.2, 6.4, 6.6], 'volume': [4.99, 22... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620171 3. Love You More LOSER 65.00 70.0 1297.27 {'price': [65, 60, 55], 'volume': [2, 2.9, 15.... {'price': [75, 80, 85], 'volume': [0.66, 3.24,... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620172 4. Splashing Rossa LOSER 10.98 10.5 2665.94 {'price': [9, 8.8, 8.6], 'volume': [21.92, 10.... {'price': [9.6, 9.8, 10], 'volume': [13.43, 7.... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620173 5. The Fairytale LOSER 54.56 50.0 221.13 {'price': [55, 50, 48], 'volume': [4.85, 2.85,... {'price': [65, 70, 75], 'volume': [2.1, 7.18, ... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620174 6. My Boy Dragon LOSER 166.90 160.0 199.00 {'price': [140, 120, 110], 'volume': [0.36, 1.... {'price': [260, 270, 340], 'volume': [1.29, 2.... When backtesting, and developing wagering systems more generally, I've found it really helpful to have a set of standard patterns or ways of representing common datasets. For a task like this it's really helpful to keep everything joined and together in a wide table. So we want a dataframe with everything we need to conduct the backtest: your model ratings, the odds you're betting at, the results on the bets, and ultimately betting logic will all become columns in a dataframe. It's helpful to have consistent column names so that the code for any new test you run looks much like previous tests and you can leverage custom functions that can be reused across tests and other projects. I like to have the following columns in my backtesting dataframe: date market_id (can be a surrogate id if dealing with fixed odds markets) selection_id (could be selection name) win (a binary win loss) model_odds model_prob market_odds market_prob bet_side stake gpl commission npl This analysis will be a little more complex as we're considering different price points so I'll leave out the market_odds and market_prob columns. # Joining the ratings data and odds data and combining rawDF = pd . merge ( hubRatings [ hubRatings [ 'market_id' ] . isin ( bfOdds . market_id . unique ())], bfOdds [[ 'market_name' , 'market_id' , 'selection_id' , 'result' , 'matched_volume' , 'bsp' , 'ltp' , 'atb_ladder_3m' , 'atl_ladder_3m' ]], on = [ 'market_id' , 'selection_id' ], how = 'inner' ) rawDF date track race_number race_name market_id selection_id selection_name model_odds market_name result matched_volume bsp ltp atb_ladder_3m atl_ladder_3m 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523320 11. Vast Kama 34.28 R1 1200m 3yo LOSER 1934.49 42.00 42.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523319 10. Triptonic 21.22 R1 1200m 3yo LOSER 1710.76 23.87 23.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 35773035 9. Right Reason 10.23 R1 1200m 3yo LOSER 5524.11 12.50 11.5 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523318 8. Off Road 40.75 R1 1200m 3yo LOSER 1506.51 35.31 34.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523317 7. More Than Value 77.49 R1 1200m 3yo LOSER 617.18 55.00 55.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28092381 11. Born A Warrior 10.67 R8 1300m Hcap LOSER 905.55 6.97 6.2 {'price': [6.2, 5.8, 5.1], 'volume': [7.98, 40... {'price': [6.8, 7, 7.8], 'volume': [6.26, 41.5... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38698010 12. Diva Bella 25.77 R8 1300m Hcap LOSER 11.06 23.60 18.5 {'price': [23, 22, 18], 'volume': [0.31, 24.91... {'price': [70, 75, 95], 'volume': [0.61, 3.5, ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28224034 13. Twice As Special 51.23 R8 1300m Hcap LOSER 52.49 36.37 26.0 {'price': [30, 29, 26], 'volume': [13.84, 5.92... {'price': [44, 50, 95], 'volume': [2.76, 1.66,... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38913296 15. Rosie Riveter 24.92 R8 1300m Hcap LOSER 58.65 9.72 11.0 {'price': [10.5, 10, 9.6], 'volume': [0.69, 28... {'price': [11, 12.5, 19], 'volume': [3.87, 2.7... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 4973624 8. Celer 26.23 R8 1300m Hcap LOSER 22.14 21.73 28.0 {'price': [24, 23, 20], 'volume': [1.55, 18.26... {'price': [30, 65, 70], 'volume': [0.55, 1.55,... df = ( rawDF # Extra Best Back + Lay 3 mins before of . assign ( best_back_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atb_ladder_3m' ]]) . assign ( best_lay_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atl_ladder_3m' ]]) # Coalesce LTP to BSP (about 60 rows) . assign ( ltp = lambda x : np . where ( x [ \"ltp\" ] . isnull (), x [ \"bsp\" ], x [ \"ltp\" ])) # Add a binary win / loss column . assign ( win = lambda x : np . where ( x [ 'result' ] == \"WINNER\" , 1 , 0 )) # Extra columns . assign ( model_prob = lambda x : 1 / x [ 'model_odds' ]) # Reorder Columns . reindex ( columns = [ 'date' , 'track' , 'race_number' , 'market_id' , 'selection_id' , 'bsp' , 'ltp' , 'best_back_3m' , 'best_lay_3m' , 'atb_ladder_3m' , 'atl_ladder_3m' , 'model_prob' , 'model_odds' , 'win' ]) ) df . head ( 5 ) date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0 Staking + Outcome Functions \u00b6 Now we can create a set of standard staking functions that a dataframe with an expected set of columns and add staking and bet outcome fields. We'll also add the ability of these functions to reference a different odds column so that we can backtest against our different price points. For simplicity we'll assume you're paying 5% commission on winnings however it could be higher or lower and depends on the MBR of the market. def bet_apply_commission ( df , com = 0.05 ): # Total Market GPL df [ 'market_gpl' ] = df . groupby ( 'market_id' )[ 'gpl' ] . transform ( sum ) # Apply 5% commission df [ 'market_commission' ] = np . where ( df [ 'market_gpl' ] <= 0 , 0 , 0.05 * df [ 'market_gpl' ]) # Sum of Market Winning Bets df [ 'floored_gpl' ] = np . where ( df [ 'gpl' ] <= 0 , 0 , df [ 'gpl' ]) df [ 'market_netwinnings' ] = df . groupby ( 'market_id' )[ 'floored_gpl' ] . transform ( sum ) # Partition Commission According to Selection GPL df [ 'commission' ] = np . where ( df [ 'market_netwinnings' ] == 0 , 0 , ( df [ 'market_commission' ] * df [ 'floored_gpl' ]) / ( df [ 'market_netwinnings' ])) # Calculate Selection NPL df [ 'npl' ] = df [ 'gpl' ] - df [ 'commission' ] # Drop excess columns df = df . drop ( columns = [ 'floored_gpl' , 'market_netwinnings' , 'market_commission' , 'market_gpl' ]) return ( df ) def bet_flat ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , # PUSH np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , stake ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) def bet_kelly ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , # PUSH np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , # PUSH 0 , np . where ( df [ 'bet_side' ] == \"B\" , ( ( 1 / df [ 'model_odds' ]) - ( 1 / df [ back_odds ]) ) / ( 1 - ( 1 / df [ back_odds ])), ( ( 1 / df [ lay_odds ]) - ( 1 / df [ 'model_odds' ]) ) / ( 1 - ( 1 / df [ lay_odds ])), ) ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) # Testing one of these functions flat_bets_bsp = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) flat_bets_bsp . head ( 5 ) date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win bet_side stake gpl commission npl 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 L 1 1.0 0.0 1.0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0 L 1 1.0 0.0 1.0 Evaluation Functions \u00b6 In my experience it's great to develop a suite of functions and analytical tools that really dig into every aspect of your simulated betting performance. You want to be as thorough and critical as possible, even when you're results are good. Another tip to guide this process is to have a reasonable benchmark. Essentially no one wins at 10% POT on thoroughbreds at the BSP so if your analysis suggests you can... there's a bug. Similarly you almost certainly won't lose at more than <-10%. Different sports and codes will have different realistic profitability ranges depending on the efficiency of the markets (will be roughly correlated to matched volume). Ruling out unreasonable results can save you a lot of time and delusion. I'm keeping it pretty simple here but you might also want to create functions to analyse: Track / distance based performance Performance across odds ranges Profit volatility (maybe using sharpe ratio to optimise volatility - adjusted profit) Date ranges (weeks / months etc) # Create simple PL and POT table def bet_eval_metrics ( d , side = False ): if side : metrics = ( d . groupby ( 'bet_side' , as_index = False ) . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) ) else : metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) # Cumulative PL by market to visually see trend and consistency def bet_eval_chart_cPl ( d ): d = ( d . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) d [ 'cNpl' ] = d . npl . cumsum () chart = px . line ( d , x = \"market_number\" , y = \"cNpl\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) return ( chart ) To illustrate these evaluation functions let's analyse flat staking at the BSP. bets = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) bet_eval_metrics ( bets , side = True ) bet_side npl stake pot B -749.493788 8356 -0.089695 L -268.499212 8592 -0.031250 bet_eval_chart_cPl ( bets ) So this isn't gonna build us an art gallery! This is to be expected though, it's not easy to make consistent profit certainly from free ratings sources available online. Testing different approaches \u00b6 We pulled those extra price points for a reason. Let's set up a little test harness that enables us to use different price points and bet using different staking functions. # We'll test a 2 different staking schemes on 3 different price points grid = { \"flat_bsp\" : ( bet_flat , \"bsp\" , \"bsp\" ), \"flat_ltp\" : ( bet_flat , \"ltp\" , \"ltp\" ), \"flat_3m\" : ( bet_flat , \"best_back_3m\" , \"best_lay_3m\" ), \"kelly_bsp\" : ( bet_kelly , \"bsp\" , \"bsp\" ), \"kelly_ltp\" : ( bet_kelly , \"ltp\" , \"ltp\" ), \"kelly_3m\" : ( bet_kelly , \"best_back_3m\" , \"best_lay_3m\" ) } metricSummary = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column # objects[0] is the staking function itself bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) # Calculate the metrics and tag with strategy label betMetrics = ( bet_eval_metrics ( bets ) . assign ( strategy = lambda x : strategy ) . reindex ( columns = [ 'strategy' , 'stake' , 'npl' , 'pot' ]) ) # Init the betMetrics df or append if already exists try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) strategy stake npl pot kelly_ltp 754.496453 -31.814150 -0.042166 kelly_bsp 732.165110 -34.613773 -0.047276 flat_bsp 16948.000000 -1017.993000 -0.060066 flat_ltp 16949.000000 -1184.546000 -0.069889 flat_3m 15712.000000 -1225.123000 -0.077974 kelly_3m 614.135601 -50.469295 -0.082179 # Compare Cumulative PL Charts cumulativePLs = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) d = ( bets . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' , 'stake' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) # Normalise to $10,000 stake for visual comparison d [ 'npl' ] = d [ 'npl' ] / ( d . stake . sum () / 10000 ) d [ 'cNpl' ] = d . npl . cumsum () d [ 'strategy' ] = strategy # Init the cumulativePLs df or append if already exists try : cumulativePLs = pd . concat ([ cumulativePLs , d ], ignore_index = True ) except : cumulativePLs = d px . line ( cumulativePLs , x = \"market_number\" , y = \"cNpl\" , color = \"strategy\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) Searching For Profit \u00b6 So this is often where you're going to arrive developing many wagering models: there's no indication of reliable long term profit. Where do you go from here? TBH I think most people give up here. Because you're not a quitter though you've got 3 main option categories: Make the underlying model better Search for better prices via detailed price analysis and clever bet placement Try to find a subset of these selections with these ratings and these price points that are sustainably profitable Obviously each situation is different but I think option 3 isn't a bad way to go initially because it will definitely help you understand your model better. For a racing model you might want to split your performance by: tracks or states track conditions or weather barriers race quality or grade odds ranges selection sample size (you likely perform worse on horses with little form for eg) perceived model value Finding a big enough slice across those dimensions that's either really profitable or really losing might reveal to you a bug in the data or workflow in your model development that you can go back and fix. As an example of a simple approach to selectiveness I'll quickly run through how being more selective about your perceived value might make a difference in final profitability. So our best performing strategy using our simple analysis above was Kelly staking at the last traded price. We'll start with that but be aware of that there's no way of implementing a LTP bet placement engine, you could imagine a proxy being placing limit bets \"just before\" the race jumps which is a whole other kettle of fish. Anyway, let's plot our profitability under this strategy at different perceived \"edges\". If we are more selective of only large overlays according to the hub's rated chance you can see we can increase the profitability. bets = bet_kelly ( df , back_odds = 'ltp' , lay_odds = 'ltp' ) metricSummary = None for bVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: for lVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: x = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( bVal , lVal )) betMetrics = bet_eval_metrics ( x , side = False ) betMetrics [ 'bVal' ] = bVal betMetrics [ 'lVal' ] = lVal try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) . head ( 4 ) npl stake pot bVal lVal -18.059813 574.944431 -0.031411 0.3 0.30 -22.887791 628.302349 -0.036428 0.3 0.20 -24.509182 669.482514 -0.036609 0.3 0.05 -22.997908 614.528386 -0.037424 0.2 0.30 betsFilters = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( 0.3 , 0.3 )) bet_eval_chart_cPl ( betsFilters ) We were doing ok till the last 200 market nightmare! Might be one to test with more data. So we still haven't found a clear profitable edge with these ratings, however we got a bit closer to break even which is positive. This step also indicates that this rating system performs better for large overlays which is a good model indicator (if you can't improve by selecting for larger overlays it's usually a sign you need to go back to the drawing board) You could imagine a few more iterations of analysis you might be able to eek out a slight edge However, be wary as these steps optimisation steps are very prone to overfitting so you need to be careful. Conclusion and Next Steps \u00b6 While using someone else's model is easy it's also not likely to end in personal riches. Developing your own model with your own tools and on a sport or racing code you know about is probably where you should start. However, hopefully this short guide helps you think about what to do when you finish the modelling component: How much money will I win or lose if I started using this system to place bets with real money? If you want to expand this backtesting analysis, here's a list (in no particular order) of things that I've omitted or angles I might look at next: Get more data -- more rating data and odds data is needed for draw a good conclusion about long term expectation Cross reference performance against race or selection metadata (track, # races run etc.) to improve performance with betting selectivity Extract more price points from the stream data to try to gain an pricing edge on these ratings Over to you \u00b6 We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space Complete code \u00b6 Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github import pandas as pd import numpy as np import requests from datetime import date , timedelta import os import re import tarfile import zipfile import bz2 import glob import logging from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import plotly.express as px import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) % config IPCompleter . greedy = True #### -------------------------- #### FUNCTIONS #### -------------------------- # Function to return Pandas DF of hub ratings for a particular date def getHubRatings ( dte ): # Substitute the date into the URL url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date= {} presenter=RatingsPresenter&json=true' . format ( dte ) # Convert the response into JSON responseJson = requests . get ( url ) . json () hubList = [] if not responseJson : return ( None ) # Want an normalised table (1 row per selection) # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields for meeting in responseJson [ 'meetings' ]: for race in meeting [ 'races' ]: for runner in race [ 'runners' ]: hubList . append ( { 'date' : dte , 'track' : meeting [ 'name' ], 'race_number' : race [ 'number' ], 'race_name' : race [ 'name' ], 'market_id' : race [ 'bfExchangeMarketId' ], 'selection_id' : str ( runner [ 'bfExchangeSelectionId' ]), 'selection_name' : runner [ 'name' ], 'model_odds' : runner [ 'ratedPrice' ] } ) out = pd . DataFrame ( hubList ) return ( out ) # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input sample: R6 1400m Grp1 parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) # creating a flag that is True when markets are australian thoroughbreds def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # loading from tar and extracting files def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # Extract Components From Generated Stream def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book evaluate_market = None prev_market = None postplay_market = None preplay_market = None t3m_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 3 mins before scheduled off if t3m_market is None and seconds_to_start < 3 * 60 : t3m_market = market_book # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay_market is not None and preplay_market is None : preplay_market = postplay_market return ( t3m_market , preplay_market , postplay_market , prev_market ) # Final market is last prev_market def run_stream_parsing (): # Run Pipeline with open ( \"outputs/tho-odds.csv\" , \"w+\" ) as output : # Write Column Headers To File output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m \\n \" ) for file_obj in load_markets ( data_path ): # Instantiate a \"stream\" object stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) # Extract key components according to the custom function above (outputs 4 objects) ( t3m_market , preplay_market , postplay_market , final_market ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay_market is None : continue ; # Runner metadata and key fields available from final market book runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final_market . runners ] # Last Traded Price # _____________________ # From the last marketbook before inplay or close ltp = [ runner . last_price_traded for runner in preplay_market . runners ] # Total Matched Volume # _____________________ # Calculates the traded volume across all traded price points for each selection def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) selection_traded_volume = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay_market . runners ] # Top 3 Ladder # ______________________ # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries def top_3_ladder ( availableLadder ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : 3 ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"price\" ] = price out [ \"volume\" ] = volume return ( out ) # Sometimes t-3 mins market book is empty try : atb_ladder_3m = [ top_3_ladder ( runner . ex . available_to_back ) for runner in t3m_market . runners ] atl_ladder_3m = [ top_3_ladder ( runner . ex . available_to_lay ) for runner in t3m_market . runners ] except : atb_ladder_3m = {} atl_ladder_3m = {} # Writing To CSV # ______________________ for ( runnerMeta , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ) in zip ( runner_data , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final_market . market_id ), final_market . market_definition . market_time , final_market . market_definition . country_code , final_market . market_definition . venue , final_market . market_definition . name , runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], runnerMeta [ 'selection_status' ], runnerMeta [ 'sp' ], ltp , selection_traded_volume , '\"' + str ( atb_ladder_3m ) + '\"' , # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas '\"' + str ( atl_ladder_3m ) + '\"' ) ) def bet_apply_commission ( df , com = 0.05 ): # Total Market GPL df [ 'market_gpl' ] = df . groupby ( 'market_id' )[ 'gpl' ] . transform ( sum ) # Apply 5% commission df [ 'market_commission' ] = np . where ( df [ 'market_gpl' ] <= 0 , 0 , 0.05 * df [ 'market_gpl' ]) # Sum of Market Winning Bets df [ 'floored_gpl' ] = np . where ( df [ 'gpl' ] <= 0 , 0 , df [ 'gpl' ]) df [ 'market_netwinnings' ] = df . groupby ( 'market_id' )[ 'floored_gpl' ] . transform ( sum ) # Partition Commission According to Selection GPL df [ 'commission' ] = np . where ( df [ 'market_netwinnings' ] == 0 , 0 , ( df [ 'market_commission' ] * df [ 'floored_gpl' ]) / ( df [ 'market_netwinnings' ])) # Calculate Selection NPL df [ 'npl' ] = df [ 'gpl' ] - df [ 'commission' ] # Drop excess columns df = df . drop ( columns = [ 'floored_gpl' , 'market_netwinnings' , 'market_commission' , 'market_gpl' ]) return ( df ) def bet_flat ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , stake ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) def bet_kelly ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , np . where ( df [ 'bet_side' ] == \"B\" , ( ( 1 / df [ 'model_odds' ]) - ( 1 / df [ back_odds ]) ) / ( 1 - ( 1 / df [ back_odds ])), ( ( 1 / df [ lay_odds ]) - ( 1 / df [ 'model_odds' ]) ) / ( 1 - ( 1 / df [ lay_odds ])), ) ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) # Create simple PL and POT table def bet_eval_metrics ( d , side = False ): if side : metrics = ( d . groupby ( 'bet_side' , as_index = False ) . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) ) else : metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) # Cumulative PL by market to visually see trend and consistency def bet_eval_chart_cPl ( d ): d = ( d . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) d [ 'cNpl' ] = d . npl . cumsum () chart = px . line ( d , x = \"market_number\" , y = \"cNpl\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) return ( chart ) #### -------------------------- #### EXECUTION #### -------------------------- # Loop through all recent history dateDFList = [] dateList = pd . date_range ( date ( 2021 , 2 , 18 ), date . today () - timedelta ( days = 1 ), freq = 'd' ) for dte in dateList : dateDFList . append ( getHubRatings ( dte )) # Concatenate (add rows to rows) all the dataframes within the list hubRatings = pd . concat ( dateDFList ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = StreamListener ( max_latency = None ) # This will execute the files (it took me ~2 hours for 2 months of data) #run_stream_parsing() # Load in odds file we created above bfOdds = pd . read_csv ( \"outputs/tho-odds.csv\" , dtype = { 'market_id' : object , 'selection_id' : object , 'atb_ladder_3m' : object , 'atl_ladder_3m' : object }) # Convert dictionary columns import ast bfOdds [ 'atb_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atb_ladder_3m' ]] bfOdds [ 'atl_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atl_ladder_3m' ]] # Convert LTP to Numeric bfOdds [ 'ltp' ] = pd . to_numeric ( bfOdds [ 'ltp' ], errors = 'coerce' ) # Filter after 18th Feb bfOdds = bfOdds . query ( 'event_date >= \"2021-02-18\"' ) # Joining the ratings data and odds data and combining rawDF = pd . merge ( hubRatings [ hubRatings [ 'market_id' ] . isin ( bfOdds . market_id . unique ())], bfOdds [[ 'market_name' , 'market_id' , 'selection_id' , 'result' , 'matched_volume' , 'bsp' , 'ltp' , 'atb_ladder_3m' , 'atl_ladder_3m' ]], on = [ 'market_id' , 'selection_id' ], how = 'inner' ) # Join and clean up columns df = ( rawDF # Extra Best Back + Lay 3 mins before of . assign ( best_back_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atb_ladder_3m' ]]) . assign ( best_lay_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atl_ladder_3m' ]]) # Coalesce LTP to BSP (about 60 rows) . assign ( ltp = lambda x : np . where ( x [ \"ltp\" ] . isnull (), x [ \"bsp\" ], x [ \"ltp\" ])) # Add a binary win / loss column . assign ( win = lambda x : np . where ( x [ 'result' ] == \"WINNER\" , 1 , 0 )) # Extra columns . assign ( model_prob = lambda x : 1 / x [ 'model_odds' ]) # Reorder Columns . reindex ( columns = [ 'date' , 'track' , 'race_number' , 'market_id' , 'selection_id' , 'bsp' , 'ltp' , 'best_back_3m' , 'best_lay_3m' , 'atb_ladder_3m' , 'atl_ladder_3m' , 'model_prob' , 'model_odds' , 'win' ]) ) bets = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) bet_eval_metrics ( bets , side = True ) bet_eval_chart_cPl ( bets ) # We'll test a 2 different staking schemes on 3 different price points grid = { \"flat_bsp\" : ( bet_flat , \"bsp\" , \"bsp\" ), \"flat_ltp\" : ( bet_flat , \"ltp\" , \"ltp\" ), \"flat_3m\" : ( bet_flat , \"best_back_3m\" , \"best_lay_3m\" ), \"kelly_bsp\" : ( bet_kelly , \"bsp\" , \"bsp\" ), \"kelly_ltp\" : ( bet_kelly , \"ltp\" , \"ltp\" ), \"kelly_3m\" : ( bet_kelly , \"best_back_3m\" , \"best_lay_3m\" ) } # Evaluate Metrics For Strategy Grid metricSummary = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column # objects[0] is the staking function itself bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) betMetrics = ( bet_eval_metrics ( bets ) . assign ( strategy = lambda x : strategy ) . reindex ( columns = [ 'strategy' , 'stake' , 'npl' , 'pot' ]) ) try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) # Compare Cumulative PL Charts cumulativePLs = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) d = ( bets . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' , 'stake' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) # Normalise to $10,000 stake for visual comparison d [ 'npl' ] = d [ 'npl' ] / ( d . stake . sum () / 10000 ) d [ 'cNpl' ] = d . npl . cumsum () d [ 'strategy' ] = strategy try : cumulativePLs = pd . concat ([ cumulativePLs , d ], ignore_index = True ) except : cumulativePLs = d px . line ( cumulativePLs , x = \"market_number\" , y = \"cNpl\" , color = \"strategy\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) bets = bet_kelly ( df , back_odds = 'ltp' , lay_odds = 'ltp' ) metricSummary = None for bVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: for lVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: x = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( bVal , lVal )) betMetrics = bet_eval_metrics ( x , side = False ) betMetrics [ 'bVal' ] = bVal betMetrics [ 'lVal' ] = lVal try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) . head ( 4 ) betsFilters = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( 0.3 , 0.3 )) bet_eval_chart_cPl ( betsFilters ) Disclaimer \u00b6 Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Backtesting ratings tutorial"},{"location":"historicData/backtestingRatingsTutorial/#backtesting-wagering-models-with-betfair-json-stream-data","text":"This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the JSON to CSV tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo .","title":"Backtesting wagering models with Betfair JSON stream data"},{"location":"historicData/backtestingRatingsTutorial/#set-up","text":"I'm going to be using a jupyter notebook for this investigation which is a special type of data analysis output that is used to combine code, outputs and explanatory text in a readable single document. It's mostly closely associated with python data analysis code which is the language I'll be using here also. The entire body of python code used will be repeated at the bottom of the article where you can copy it and repurpose it for yourself. If you're not familiar with python, don't worry neither am I really! I'm inexperienced with python so if you have experience with some other programming language you should be able to follow along with the logic here too. If you don't have experience using another programming language this all might appear intimidating but it's heavy on the explanatory text so you should get something out of it. We need a few non standard python libraries so make sure to install betfairlightweight and plotly before you get started with something like pip install betfairlightweight & pip install plotly . We'll load all our libraries and do some setup here. import pandas as pd import numpy as np import requests from datetime import date , timedelta import os import re import tarfile import zipfile import bz2 import glob import logging from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import plotly.express as px import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) % config IPCompleter . greedy = True","title":"Set up"},{"location":"historicData/backtestingRatingsTutorial/#context","text":"Backtesting is the life-blood of most successful wagering systems. In short it attempts to answer a single question for you: \ud835\udf0f : How much money will I win or lose if I started using this system to place bets with real money? Without a rigorous and quantitative backtesting approach it's really quite hard to estimate the answer to this question $ \\tau $ that will be even reliably on the right side of zero. You could live test your system with real bets at small stakes, however, this isn't the panacea it seems. It will take time (more than you think) for your results to converge to their long term expectation. How long? Answering this question will require some expertise with probability and statistics you might not have. Even more than that though is that depending on where you're betting your results at small stakes could be very different than at larger stakes. You might not be able get a good answer to $ \\tau $ until betting at full stakes at which point finding the answer might coincide with blowing up your gambling bankroll. Backtesting is also very hard. To perfectly backtest your own predicted probability on a historical race or sporting match you need to produce 2 things: (1) What would my predicted chance have been exactly for this selection in this market on this day in the past? (2) What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction? The devil in the detail of backtesting tends to be in those exactlys. The aim of the backtesting game is answering (2) as accurately as possible because it tells you exactly how much you would have made over your backtesting period, from there you can confidently project that rate of profitability forward. It's easy to make mistakes and small errors in the quantitative reasoning can lead you to extremely misguided projections downstream. Question (1) won't be in the scope of this notebook but it's equally (and probably more) important that (2) but it is the key challenge of all predictive modelling exercises so there's plenty of discussion about it elsewhere.","title":"Context"},{"location":"historicData/backtestingRatingsTutorial/#backtesting-on-betfair","text":"Answering question (2) for betting on the Betfair Exchange is difficult. The Exchange is a dynamic system that changes from one micro second to the next. What number should you use for odds? How much could you assume to get down at those odds? The conventional and easiest approach is to backtest at the BSP. The BSP is simple because it's a single number (to use for both back and lay bets) and is a taken price (there's no uncertainty about getting matched). Depending on the liquidity of the market a reasonably sized stake might also not move the BSP very much. For some markets you may be able to safely assume you could be $10s of dollars at the BSP without moving it an inch. However, that's definitely not true of all BSP markets and you need to be generally aware that your Betfair orders in the future will change the state of the exchange, and large bets will move the BSP in an unfavourable direction. Aside from uncertainty around the liquidity and resilience of the BSP, many many markets don't have a BSP. So what do we do then? Typically what a lot of people (who have a relationship with Betfair Australia) do at this point is request a data dump. They might request an odds file for all Australian harness race win markets since June 2018 with results and 4 different price points: the BSP, the last traded price, the weighted average price (WAP) traded in 3 minutes before the race starts, and the WAP for all bets matched prior to 3 mins before the race. However, you will likely need to be an existing VIP customer to get this file and it's not a perfect solution: it might take 2 weeks to get, you can't refresh it, you can't test more hypothetical price points after your initial analysis amongst many other problems. What if you could produce this valuable data file yourself?","title":"Backtesting on Betfair"},{"location":"historicData/backtestingRatingsTutorial/#betfair-stream-data","text":"Betfair's historical stream data is an extremely rich source of data. However, in it's raw form it's difficult to handle for the uninitiated. It also might not be immediately obvious how many different things this dataset could be used for without seeing some examples. These guides will hopefully demystify how to turn this raw data into a familiar and usable format whilst also hopefully providing some inspiration for the kinds of value that can be excavated from it.","title":"Betfair Stream Data"},{"location":"historicData/backtestingRatingsTutorial/#this-example-backtesting-betfair-hub-thoroughbred-model","text":"To illustrate how you can use the stream files to backtest the outputs of a rating system we'll use the Australian Thoroughbred Rating model available on the Betfair Hub. The most recent model iteration only goes back till Feb 28 th 2021 however as an illustrative example this is fine. We'd normally want to backtest with a lot more historical data than this, which just means in this case our estimation of future performance will be unreliable. I'm interested to see how we would have fared betting all selections rated by this model according to a few different staking schemes and also at a few different times / price points. Old ratings If you want to pull in ratings from before Feb 2021 to add to your database for more complete backtesting these are available in a data dump here .","title":"This example: backtesting Betfair Hub thoroughbred model"},{"location":"historicData/backtestingRatingsTutorial/#scrape-the-model-ratings","text":"If you travel to the Betfair hub ratings page you'll find that URL links behind the ratings download buttons have a consistent URL pattern that looks very scrape friendly. We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe. # Function to return Pandas DF of hub ratings for a particular date def getHubRatings ( dte ): # Substitute the date into the URL url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date= {} presenter=RatingsPresenter&json=true' . format ( dte ) # Convert the response into JSON responseJson = requests . get ( url ) . json () hubList = [] if not responseJson : return ( None ) # Want an normalised table (1 row per selection) # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields for meeting in responseJson [ 'meetings' ]: for race in meeting [ 'races' ]: for runner in race [ 'runners' ]: hubList . append ( { 'date' : dte , 'track' : meeting [ 'name' ], 'race_number' : race [ 'number' ], 'race_name' : race [ 'name' ], 'market_id' : race [ 'bfExchangeMarketId' ], 'selection_id' : str ( runner [ 'bfExchangeSelectionId' ]), 'selection_name' : runner [ 'name' ], 'model_odds' : runner [ 'ratedPrice' ] } ) out = pd . DataFrame ( hubList ) return ( out ) # See the response from a single day getHubRatings ( date ( 2021 , 3 , 1 )) . head ( 5 ) date track race_number race_name market_id selection_id selection_name model_odds 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620052 1. Military Affair 6.44 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 5889703 3. Proverbial 21.11 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38177688 4. A Real Wag 9.97 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620053 5. El Jay 44.12 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 37263264 6. Flying Honour 3.39 dateDFList = [] dateList = pd . date_range ( date ( 2021 , 2 , 18 ), date . today () - timedelta ( days = 1 ), freq = 'd' ) for dte in dateList : dateDFList . append ( getHubRatings ( dte )) # Concatenate (add rows to rows) all the dataframes within the list hubRatings = pd . concat ( dateDFList ) hubRatings . shape ( 32519 , 8 )","title":"Scrape The Model Ratings"},{"location":"historicData/backtestingRatingsTutorial/#assembling-the-odds-file","text":"So part 1 was very painless. This is how we like data: served by some API or available in a nice tabular format on a webpage ready to be scraped with standard tools available in popular languages. Unfortunately, it won't be so painless to assemble our odds file. We'll find out why it's tricky as we go.","title":"Assembling the odds file"},{"location":"historicData/backtestingRatingsTutorial/#the-data","text":"The data we'll be using is the historical Exchange data available from this website. The data available through this service is called streaming JSON data. There are a few options available relating to granularity (how many time points per second the data updates at) but we'll be using the most granular \"PRO\" set which has updates every 50 milliseconds. Essentially what the data allows us to do is, for a particular market, recreate the exact state of the Betfair Exchange at say: 150 milliseconds before the market closed. When people say the state of the Exchange they mean two things a) what are all the current open orders on all the selections b) what are the current traded volumes on each selection at each price point. We obviously don't have access to any information about which accounts are putting up which prices and other things Betfair has themselves. We're essentially getting a snapshot of everything you can see through the website by clicking on each selection manually and looking at the graphs, tables and ladders. However, with just these 2 sets of information we can build a rich view of the dynamics of exchange and also build out all of the summary metrics (WAP etc) we might have previously needed Betfair to help with. For our purposes 50 milli-second intervaled data is huge overkill. But you could imagine needing this kind of granularity for other kinds of wagering systems - eg a high frequency trading algorithm of some sort that needs to make many decisions and actions every second. Let's take a look at what the stream data looks like for a single market: So it looks pretty intractable. For this particular market there's 14,384 lines of data where each line consists of a single JSON packet of data. If you're not a data engineer (neither am I) your head might explode thinking about how you could read this into your computer and transform it into something usable. The data looks like this because it is saved from a special Betfair API called the Stream API which which is used by high end Betfair API users and which delivers fast speeds other performance improvements over the normal \"polling\" API. Now what's good about that, for the purposes of our exercise, is that the very nice python package betfairlightweight has the functionality built to not only parse the Stream API when connected live but also these historical saved versions of the stream data. Without it we'd be very far away from the finish line, with betfairlightweight we're pretty close.","title":"The Data"},{"location":"historicData/backtestingRatingsTutorial/#unpacking-flattening-the-data","text":"Because these files are so large and unprocessed this process won't look the same as your normal data ETL in python: where you can read a raw data file (csv, JSON, text etc.) into memory and use python functions to transform into usable format. I personally had no idea how to use python and betfairlightweight to parse these data until I saw Betfair's very instructive overview which you should read for a more detailed look at some of the below code. By my count there were 4 key conceptual components that I had to get my head around to understand and be able to re-purpose that code. So if you're like me (a bit confused by some of the steps in that piece) this explanation might help. I'll assume you don't do any decompression and keep the monthly PRO files as the .tar archives as they are. Conceptually the process looks something like this: Load the \"archives\" into a \"generator\" Scan across the generator (market_ids) and the market states within those markets to extract useful objects Process those useful objects to pull out some metadata + useful summary numbers derived from the available orders and traded volumes snapshot data Write this useful summarised data to a file that can be read and understood with normal data analysis workflows First we'll run a bunch of setup code setting up my libraries and creating some utility functions that will be used throughout the main parsing component. It'll also point to the two stream files I'll be parsing for this exercise. # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = StreamListener ( max_latency = None ) ### Utility Functions # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input sample: R6 1400m Grp1 parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) # creating a flag that is True when markets are australian thoroughbreds def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' )","title":"Unpacking / flattening the data"},{"location":"historicData/backtestingRatingsTutorial/#1-tar-load","text":"This function I stole from Betfair's instructional article The stream files are downloaded as .tar archive files which are a special kind of file that we'll need to unpack Instead of loading each file into memory this function returns a \"generator\" which is a special python object that is to be iterated over This basically means it contains the instructions to unpack and scan over files on the fly This function also contains the logic to deal with if these files are zip archives or you've manually unpacked the archive and have the .bz2 zipped files # loading from tar and extracting files def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None","title":"1: .tar load"},{"location":"historicData/backtestingRatingsTutorial/#2-scan-across-market-states-and-extract-useful-objects","text":"So this function will take a special \"stream\" object which we'll create with betfairlightweight The function takes a stream object input and returns 4 instances of the market state The market state 3 mins before the scheduled off The market state immediately before it goes inplay The market state immediately before it closes for settlement The final market state with outcomes It basically just loops over all the market states and has a few checks to determine if it should save the current market state as key variables and then returns those # Extract Components From Generated Stream def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book evaluate_market = None prev_market = None postplay_market = None preplay_market = None t3m_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 3 mins before scheduled off if t3m_market is None and seconds_to_start < 3 * 60 : t3m_market = market_book # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay_market is not None and preplay_market is None : preplay_market = postplay_market return ( t3m_market , preplay_market , postplay_market , prev_market ) # Final market is last prev_market","title":"2: Scan across market states and extract useful objects"},{"location":"historicData/backtestingRatingsTutorial/#3-4-summarise-those-useful-objects-and-write-to-csv","text":"This next chunk contains a wrapper function that will do all the execution It will open a csv output file Use the load_markets utility to iterate over the .tar files Use betfairlightweight to instantiate the special stream object Pass that stream object to the extract_components_from_stream which will scan across the market states and pull out 4 key market books Convert those marketbooks into simple summary numbers or dictionaries that will be written to the output .csv file def run_stream_parsing (): # Run Pipeline with open ( \"outputs/tho-odds.csv\" , \"w+\" ) as output : # Write Column Headers To File output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m \\n \" ) for file_obj in load_markets ( data_path ): # Instantiate a \"stream\" object stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) # Extract key components according to the custom function above (outputs 4 objects) ( t3m_market , preplay_market , postplay_market , final_market ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay_market is None : continue ; # Runner metadata and key fields available from final market book runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final_market . runners ] # Last Traded Price # _____________________ # From the last marketbook before inplay or close ltp = [ runner . last_price_traded for runner in preplay_market . runners ] # Total Matched Volume # _____________________ # Calculates the traded volume across all traded price points for each selection def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) selection_traded_volume = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay_market . runners ] # Top 3 Ladder # ______________________ # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries def top_3_ladder ( availableLadder ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : 3 ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"price\" ] = price out [ \"volume\" ] = volume return ( out ) # Sometimes t-3 mins market book is empty try : atb_ladder_3m = [ top_3_ladder ( runner . ex . available_to_back ) for runner in t3m_market . runners ] atl_ladder_3m = [ top_3_ladder ( runner . ex . available_to_lay ) for runner in t3m_market . runners ] except : atb_ladder_3m = {} atl_ladder_3m = {} # Writing To CSV # ______________________ for ( runnerMeta , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ) in zip ( runner_data , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final_market . market_id ), final_market . market_definition . market_time , final_market . market_definition . country_code , final_market . market_definition . venue , final_market . market_definition . name , runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], runnerMeta [ 'selection_status' ], runnerMeta [ 'sp' ], ltp , selection_traded_volume , '\"' + str ( atb_ladder_3m ) + '\"' , # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas '\"' + str ( atl_ladder_3m ) + '\"' ) ) # This will execute the files (it took me ~2 hours for 2 months of data) #run_stream_parsing()","title":"3 &amp; 4: Summarise those useful objects and write to .csv"},{"location":"historicData/backtestingRatingsTutorial/#extending-this-code","text":"Because this process is very slow you might want to save much more information than you think you need For example I currently think I only want the best back and lay prices at t-3 mins before the off but I've saved the top 3 boxes in the available to back and lay ladders as dictionary strings From these ladders I can retroactively calculate not only just the best back and lay prices but also WAP prices and also sizes at those boxes which I could use for much more accurate backtesting if I wanted to later without having can across the entire stream files again I could easily save the entire open and traded orders ladders in the same way amongst many other ways of retaining more of the data for post-processing analysis","title":"Extending this code"},{"location":"historicData/backtestingRatingsTutorial/#backtesting-analysis","text":"Let's take stock of where we are. We currently have model ratings (about 1.5 months worth) and Betfair Odds (2 months worth). Circling back to the original backtesting context we needed to solve for 2 key questions: What would my predicted chance have been exactly for this selection in this market on this day in the past? What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction? Backtesting with someone else's publicly available and historically logged ratings solves question 1. With these particular ratings we're fine but generally we should just be aware there are some sketchy services that might make retroactive adjustments to historical ratings to juice their performance which obviously violates 1. For the second part we now have several real Betfair odds values to combine with the ratings and some chosen staking formula to simulate actual bets. I won't dwell too much on the stake size component but it's important. Similarly we aren't out of the woods with the \"what odds exactly\" question either. I'll show performance of backtesting at the \"Last Traded Price\" however, there's literally no way of actually being the last bet matched order on every exchange market so there's some uncertainty in a few of these prices. Further, and from experience, if you placing bets at the BSP and you're using some form of proportional staking (like Kelly) then you're calculated stake size will need to include a quantity (the BSP) which you will literally never be 100% sure of. You'll need to estimate the BSP as close to market suspension as you can and place your BSP bets with a stake sized derived from that estimation. This imprecision in stake calculation WILL cost you some profit relative to your backtested expectation. These might seem like minor considerations but you should be aware of some of the gory details of the many ways becoming successful on Betfair is really difficult. To be reliably profitable on Betfair you don't just need a good model, you'll likely need to spend hours and hours thinking about these things: testing things, ironing out all these little kinks and trying to account for all your uncertainties. I'll just be running over the skeleton of what you should do.","title":"Backtesting Analysis"},{"location":"historicData/backtestingRatingsTutorial/#setting-up-your-master-data","text":"# First we'll load and tidy our odds data # Load in odds file we created above bfOdds = pd . read_csv ( \"outputs/tho-odds.csv\" , dtype = { 'market_id' : object , 'selection_id' : object , 'atb_ladder_3m' : object , 'atl_ladder_3m' : object }) # Convert dictionary columns import ast bfOdds [ 'atb_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atb_ladder_3m' ]] bfOdds [ 'atl_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atl_ladder_3m' ]] # Convert LTP to Numeric bfOdds [ 'ltp' ] = pd . to_numeric ( bfOdds [ 'ltp' ], errors = 'coerce' ) # Filter after 18th Feb bfOdds = bfOdds . query ( 'event_date >= \"2021-02-18\"' ) bfOdds . head ( 5 ) market_id event_date country track market_name selection_id selection_name result bsp ltp matched_volume atb_ladder_3m atl_ladder_3m 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 31552374 2. Chubascos LOSER 5.84 5.9 7390.59 {'price': [6, 5.9, 5.8], 'volume': [30.99, 82.... {'price': [6.2, 6.4, 6.6], 'volume': [4.99, 22... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620171 3. Love You More LOSER 65.00 70.0 1297.27 {'price': [65, 60, 55], 'volume': [2, 2.9, 15.... {'price': [75, 80, 85], 'volume': [0.66, 3.24,... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620172 4. Splashing Rossa LOSER 10.98 10.5 2665.94 {'price': [9, 8.8, 8.6], 'volume': [21.92, 10.... {'price': [9.6, 9.8, 10], 'volume': [13.43, 7.... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620173 5. The Fairytale LOSER 54.56 50.0 221.13 {'price': [55, 50, 48], 'volume': [4.85, 2.85,... {'price': [65, 70, 75], 'volume': [2.1, 7.18, ... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620174 6. My Boy Dragon LOSER 166.90 160.0 199.00 {'price': [140, 120, 110], 'volume': [0.36, 1.... {'price': [260, 270, 340], 'volume': [1.29, 2.... When backtesting, and developing wagering systems more generally, I've found it really helpful to have a set of standard patterns or ways of representing common datasets. For a task like this it's really helpful to keep everything joined and together in a wide table. So we want a dataframe with everything we need to conduct the backtest: your model ratings, the odds you're betting at, the results on the bets, and ultimately betting logic will all become columns in a dataframe. It's helpful to have consistent column names so that the code for any new test you run looks much like previous tests and you can leverage custom functions that can be reused across tests and other projects. I like to have the following columns in my backtesting dataframe: date market_id (can be a surrogate id if dealing with fixed odds markets) selection_id (could be selection name) win (a binary win loss) model_odds model_prob market_odds market_prob bet_side stake gpl commission npl This analysis will be a little more complex as we're considering different price points so I'll leave out the market_odds and market_prob columns. # Joining the ratings data and odds data and combining rawDF = pd . merge ( hubRatings [ hubRatings [ 'market_id' ] . isin ( bfOdds . market_id . unique ())], bfOdds [[ 'market_name' , 'market_id' , 'selection_id' , 'result' , 'matched_volume' , 'bsp' , 'ltp' , 'atb_ladder_3m' , 'atl_ladder_3m' ]], on = [ 'market_id' , 'selection_id' ], how = 'inner' ) rawDF date track race_number race_name market_id selection_id selection_name model_odds market_name result matched_volume bsp ltp atb_ladder_3m atl_ladder_3m 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523320 11. Vast Kama 34.28 R1 1200m 3yo LOSER 1934.49 42.00 42.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523319 10. Triptonic 21.22 R1 1200m 3yo LOSER 1710.76 23.87 23.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 35773035 9. Right Reason 10.23 R1 1200m 3yo LOSER 5524.11 12.50 11.5 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523318 8. Off Road 40.75 R1 1200m 3yo LOSER 1506.51 35.31 34.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523317 7. More Than Value 77.49 R1 1200m 3yo LOSER 617.18 55.00 55.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28092381 11. Born A Warrior 10.67 R8 1300m Hcap LOSER 905.55 6.97 6.2 {'price': [6.2, 5.8, 5.1], 'volume': [7.98, 40... {'price': [6.8, 7, 7.8], 'volume': [6.26, 41.5... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38698010 12. Diva Bella 25.77 R8 1300m Hcap LOSER 11.06 23.60 18.5 {'price': [23, 22, 18], 'volume': [0.31, 24.91... {'price': [70, 75, 95], 'volume': [0.61, 3.5, ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28224034 13. Twice As Special 51.23 R8 1300m Hcap LOSER 52.49 36.37 26.0 {'price': [30, 29, 26], 'volume': [13.84, 5.92... {'price': [44, 50, 95], 'volume': [2.76, 1.66,... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38913296 15. Rosie Riveter 24.92 R8 1300m Hcap LOSER 58.65 9.72 11.0 {'price': [10.5, 10, 9.6], 'volume': [0.69, 28... {'price': [11, 12.5, 19], 'volume': [3.87, 2.7... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 4973624 8. Celer 26.23 R8 1300m Hcap LOSER 22.14 21.73 28.0 {'price': [24, 23, 20], 'volume': [1.55, 18.26... {'price': [30, 65, 70], 'volume': [0.55, 1.55,... df = ( rawDF # Extra Best Back + Lay 3 mins before of . assign ( best_back_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atb_ladder_3m' ]]) . assign ( best_lay_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atl_ladder_3m' ]]) # Coalesce LTP to BSP (about 60 rows) . assign ( ltp = lambda x : np . where ( x [ \"ltp\" ] . isnull (), x [ \"bsp\" ], x [ \"ltp\" ])) # Add a binary win / loss column . assign ( win = lambda x : np . where ( x [ 'result' ] == \"WINNER\" , 1 , 0 )) # Extra columns . assign ( model_prob = lambda x : 1 / x [ 'model_odds' ]) # Reorder Columns . reindex ( columns = [ 'date' , 'track' , 'race_number' , 'market_id' , 'selection_id' , 'bsp' , 'ltp' , 'best_back_3m' , 'best_lay_3m' , 'atb_ladder_3m' , 'atl_ladder_3m' , 'model_prob' , 'model_odds' , 'win' ]) ) df . head ( 5 ) date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0","title":"Setting up your master data"},{"location":"historicData/backtestingRatingsTutorial/#staking-outcome-functions","text":"Now we can create a set of standard staking functions that a dataframe with an expected set of columns and add staking and bet outcome fields. We'll also add the ability of these functions to reference a different odds column so that we can backtest against our different price points. For simplicity we'll assume you're paying 5% commission on winnings however it could be higher or lower and depends on the MBR of the market. def bet_apply_commission ( df , com = 0.05 ): # Total Market GPL df [ 'market_gpl' ] = df . groupby ( 'market_id' )[ 'gpl' ] . transform ( sum ) # Apply 5% commission df [ 'market_commission' ] = np . where ( df [ 'market_gpl' ] <= 0 , 0 , 0.05 * df [ 'market_gpl' ]) # Sum of Market Winning Bets df [ 'floored_gpl' ] = np . where ( df [ 'gpl' ] <= 0 , 0 , df [ 'gpl' ]) df [ 'market_netwinnings' ] = df . groupby ( 'market_id' )[ 'floored_gpl' ] . transform ( sum ) # Partition Commission According to Selection GPL df [ 'commission' ] = np . where ( df [ 'market_netwinnings' ] == 0 , 0 , ( df [ 'market_commission' ] * df [ 'floored_gpl' ]) / ( df [ 'market_netwinnings' ])) # Calculate Selection NPL df [ 'npl' ] = df [ 'gpl' ] - df [ 'commission' ] # Drop excess columns df = df . drop ( columns = [ 'floored_gpl' , 'market_netwinnings' , 'market_commission' , 'market_gpl' ]) return ( df ) def bet_flat ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , # PUSH np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , stake ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) def bet_kelly ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , # PUSH np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , # PUSH 0 , np . where ( df [ 'bet_side' ] == \"B\" , ( ( 1 / df [ 'model_odds' ]) - ( 1 / df [ back_odds ]) ) / ( 1 - ( 1 / df [ back_odds ])), ( ( 1 / df [ lay_odds ]) - ( 1 / df [ 'model_odds' ]) ) / ( 1 - ( 1 / df [ lay_odds ])), ) ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) # Testing one of these functions flat_bets_bsp = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) flat_bets_bsp . head ( 5 ) date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win bet_side stake gpl commission npl 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 L 1 1.0 0.0 1.0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0 L 1 1.0 0.0 1.0","title":"Staking + Outcome Functions"},{"location":"historicData/backtestingRatingsTutorial/#evaluation-functions","text":"In my experience it's great to develop a suite of functions and analytical tools that really dig into every aspect of your simulated betting performance. You want to be as thorough and critical as possible, even when you're results are good. Another tip to guide this process is to have a reasonable benchmark. Essentially no one wins at 10% POT on thoroughbreds at the BSP so if your analysis suggests you can... there's a bug. Similarly you almost certainly won't lose at more than <-10%. Different sports and codes will have different realistic profitability ranges depending on the efficiency of the markets (will be roughly correlated to matched volume). Ruling out unreasonable results can save you a lot of time and delusion. I'm keeping it pretty simple here but you might also want to create functions to analyse: Track / distance based performance Performance across odds ranges Profit volatility (maybe using sharpe ratio to optimise volatility - adjusted profit) Date ranges (weeks / months etc) # Create simple PL and POT table def bet_eval_metrics ( d , side = False ): if side : metrics = ( d . groupby ( 'bet_side' , as_index = False ) . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) ) else : metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) # Cumulative PL by market to visually see trend and consistency def bet_eval_chart_cPl ( d ): d = ( d . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) d [ 'cNpl' ] = d . npl . cumsum () chart = px . line ( d , x = \"market_number\" , y = \"cNpl\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) return ( chart ) To illustrate these evaluation functions let's analyse flat staking at the BSP. bets = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) bet_eval_metrics ( bets , side = True ) bet_side npl stake pot B -749.493788 8356 -0.089695 L -268.499212 8592 -0.031250 bet_eval_chart_cPl ( bets ) So this isn't gonna build us an art gallery! This is to be expected though, it's not easy to make consistent profit certainly from free ratings sources available online.","title":"Evaluation Functions"},{"location":"historicData/backtestingRatingsTutorial/#testing-different-approaches","text":"We pulled those extra price points for a reason. Let's set up a little test harness that enables us to use different price points and bet using different staking functions. # We'll test a 2 different staking schemes on 3 different price points grid = { \"flat_bsp\" : ( bet_flat , \"bsp\" , \"bsp\" ), \"flat_ltp\" : ( bet_flat , \"ltp\" , \"ltp\" ), \"flat_3m\" : ( bet_flat , \"best_back_3m\" , \"best_lay_3m\" ), \"kelly_bsp\" : ( bet_kelly , \"bsp\" , \"bsp\" ), \"kelly_ltp\" : ( bet_kelly , \"ltp\" , \"ltp\" ), \"kelly_3m\" : ( bet_kelly , \"best_back_3m\" , \"best_lay_3m\" ) } metricSummary = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column # objects[0] is the staking function itself bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) # Calculate the metrics and tag with strategy label betMetrics = ( bet_eval_metrics ( bets ) . assign ( strategy = lambda x : strategy ) . reindex ( columns = [ 'strategy' , 'stake' , 'npl' , 'pot' ]) ) # Init the betMetrics df or append if already exists try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) strategy stake npl pot kelly_ltp 754.496453 -31.814150 -0.042166 kelly_bsp 732.165110 -34.613773 -0.047276 flat_bsp 16948.000000 -1017.993000 -0.060066 flat_ltp 16949.000000 -1184.546000 -0.069889 flat_3m 15712.000000 -1225.123000 -0.077974 kelly_3m 614.135601 -50.469295 -0.082179 # Compare Cumulative PL Charts cumulativePLs = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) d = ( bets . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' , 'stake' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) # Normalise to $10,000 stake for visual comparison d [ 'npl' ] = d [ 'npl' ] / ( d . stake . sum () / 10000 ) d [ 'cNpl' ] = d . npl . cumsum () d [ 'strategy' ] = strategy # Init the cumulativePLs df or append if already exists try : cumulativePLs = pd . concat ([ cumulativePLs , d ], ignore_index = True ) except : cumulativePLs = d px . line ( cumulativePLs , x = \"market_number\" , y = \"cNpl\" , color = \"strategy\" , title = 'Cumulative Net Profit' , template = 'simple_white' )","title":"Testing different approaches"},{"location":"historicData/backtestingRatingsTutorial/#searching-for-profit","text":"So this is often where you're going to arrive developing many wagering models: there's no indication of reliable long term profit. Where do you go from here? TBH I think most people give up here. Because you're not a quitter though you've got 3 main option categories: Make the underlying model better Search for better prices via detailed price analysis and clever bet placement Try to find a subset of these selections with these ratings and these price points that are sustainably profitable Obviously each situation is different but I think option 3 isn't a bad way to go initially because it will definitely help you understand your model better. For a racing model you might want to split your performance by: tracks or states track conditions or weather barriers race quality or grade odds ranges selection sample size (you likely perform worse on horses with little form for eg) perceived model value Finding a big enough slice across those dimensions that's either really profitable or really losing might reveal to you a bug in the data or workflow in your model development that you can go back and fix. As an example of a simple approach to selectiveness I'll quickly run through how being more selective about your perceived value might make a difference in final profitability. So our best performing strategy using our simple analysis above was Kelly staking at the last traded price. We'll start with that but be aware of that there's no way of implementing a LTP bet placement engine, you could imagine a proxy being placing limit bets \"just before\" the race jumps which is a whole other kettle of fish. Anyway, let's plot our profitability under this strategy at different perceived \"edges\". If we are more selective of only large overlays according to the hub's rated chance you can see we can increase the profitability. bets = bet_kelly ( df , back_odds = 'ltp' , lay_odds = 'ltp' ) metricSummary = None for bVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: for lVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: x = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( bVal , lVal )) betMetrics = bet_eval_metrics ( x , side = False ) betMetrics [ 'bVal' ] = bVal betMetrics [ 'lVal' ] = lVal try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) . head ( 4 ) npl stake pot bVal lVal -18.059813 574.944431 -0.031411 0.3 0.30 -22.887791 628.302349 -0.036428 0.3 0.20 -24.509182 669.482514 -0.036609 0.3 0.05 -22.997908 614.528386 -0.037424 0.2 0.30 betsFilters = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( 0.3 , 0.3 )) bet_eval_chart_cPl ( betsFilters ) We were doing ok till the last 200 market nightmare! Might be one to test with more data. So we still haven't found a clear profitable edge with these ratings, however we got a bit closer to break even which is positive. This step also indicates that this rating system performs better for large overlays which is a good model indicator (if you can't improve by selecting for larger overlays it's usually a sign you need to go back to the drawing board) You could imagine a few more iterations of analysis you might be able to eek out a slight edge However, be wary as these steps optimisation steps are very prone to overfitting so you need to be careful.","title":"Searching For Profit"},{"location":"historicData/backtestingRatingsTutorial/#conclusion-and-next-steps","text":"While using someone else's model is easy it's also not likely to end in personal riches. Developing your own model with your own tools and on a sport or racing code you know about is probably where you should start. However, hopefully this short guide helps you think about what to do when you finish the modelling component: How much money will I win or lose if I started using this system to place bets with real money? If you want to expand this backtesting analysis, here's a list (in no particular order) of things that I've omitted or angles I might look at next: Get more data -- more rating data and odds data is needed for draw a good conclusion about long term expectation Cross reference performance against race or selection metadata (track, # races run etc.) to improve performance with betting selectivity Extract more price points from the stream data to try to gain an pricing edge on these ratings","title":"Conclusion and Next Steps"},{"location":"historicData/backtestingRatingsTutorial/#over-to-you","text":"We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space","title":"Over to you"},{"location":"historicData/backtestingRatingsTutorial/#complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github import pandas as pd import numpy as np import requests from datetime import date , timedelta import os import re import tarfile import zipfile import bz2 import glob import logging from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import plotly.express as px import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) % config IPCompleter . greedy = True #### -------------------------- #### FUNCTIONS #### -------------------------- # Function to return Pandas DF of hub ratings for a particular date def getHubRatings ( dte ): # Substitute the date into the URL url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date= {} presenter=RatingsPresenter&json=true' . format ( dte ) # Convert the response into JSON responseJson = requests . get ( url ) . json () hubList = [] if not responseJson : return ( None ) # Want an normalised table (1 row per selection) # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields for meeting in responseJson [ 'meetings' ]: for race in meeting [ 'races' ]: for runner in race [ 'runners' ]: hubList . append ( { 'date' : dte , 'track' : meeting [ 'name' ], 'race_number' : race [ 'number' ], 'race_name' : race [ 'name' ], 'market_id' : race [ 'bfExchangeMarketId' ], 'selection_id' : str ( runner [ 'bfExchangeSelectionId' ]), 'selection_name' : runner [ 'name' ], 'model_odds' : runner [ 'ratedPrice' ] } ) out = pd . DataFrame ( hubList ) return ( out ) # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input sample: R6 1400m Grp1 parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) # creating a flag that is True when markets are australian thoroughbreds def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # loading from tar and extracting files def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # Extract Components From Generated Stream def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book evaluate_market = None prev_market = None postplay_market = None preplay_market = None t3m_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 3 mins before scheduled off if t3m_market is None and seconds_to_start < 3 * 60 : t3m_market = market_book # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay_market is not None and preplay_market is None : preplay_market = postplay_market return ( t3m_market , preplay_market , postplay_market , prev_market ) # Final market is last prev_market def run_stream_parsing (): # Run Pipeline with open ( \"outputs/tho-odds.csv\" , \"w+\" ) as output : # Write Column Headers To File output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m \\n \" ) for file_obj in load_markets ( data_path ): # Instantiate a \"stream\" object stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) # Extract key components according to the custom function above (outputs 4 objects) ( t3m_market , preplay_market , postplay_market , final_market ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay_market is None : continue ; # Runner metadata and key fields available from final market book runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final_market . runners ] # Last Traded Price # _____________________ # From the last marketbook before inplay or close ltp = [ runner . last_price_traded for runner in preplay_market . runners ] # Total Matched Volume # _____________________ # Calculates the traded volume across all traded price points for each selection def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) selection_traded_volume = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay_market . runners ] # Top 3 Ladder # ______________________ # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries def top_3_ladder ( availableLadder ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : 3 ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"price\" ] = price out [ \"volume\" ] = volume return ( out ) # Sometimes t-3 mins market book is empty try : atb_ladder_3m = [ top_3_ladder ( runner . ex . available_to_back ) for runner in t3m_market . runners ] atl_ladder_3m = [ top_3_ladder ( runner . ex . available_to_lay ) for runner in t3m_market . runners ] except : atb_ladder_3m = {} atl_ladder_3m = {} # Writing To CSV # ______________________ for ( runnerMeta , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ) in zip ( runner_data , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final_market . market_id ), final_market . market_definition . market_time , final_market . market_definition . country_code , final_market . market_definition . venue , final_market . market_definition . name , runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], runnerMeta [ 'selection_status' ], runnerMeta [ 'sp' ], ltp , selection_traded_volume , '\"' + str ( atb_ladder_3m ) + '\"' , # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas '\"' + str ( atl_ladder_3m ) + '\"' ) ) def bet_apply_commission ( df , com = 0.05 ): # Total Market GPL df [ 'market_gpl' ] = df . groupby ( 'market_id' )[ 'gpl' ] . transform ( sum ) # Apply 5% commission df [ 'market_commission' ] = np . where ( df [ 'market_gpl' ] <= 0 , 0 , 0.05 * df [ 'market_gpl' ]) # Sum of Market Winning Bets df [ 'floored_gpl' ] = np . where ( df [ 'gpl' ] <= 0 , 0 , df [ 'gpl' ]) df [ 'market_netwinnings' ] = df . groupby ( 'market_id' )[ 'floored_gpl' ] . transform ( sum ) # Partition Commission According to Selection GPL df [ 'commission' ] = np . where ( df [ 'market_netwinnings' ] == 0 , 0 , ( df [ 'market_commission' ] * df [ 'floored_gpl' ]) / ( df [ 'market_netwinnings' ])) # Calculate Selection NPL df [ 'npl' ] = df [ 'gpl' ] - df [ 'commission' ] # Drop excess columns df = df . drop ( columns = [ 'floored_gpl' , 'market_netwinnings' , 'market_commission' , 'market_gpl' ]) return ( df ) def bet_flat ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , stake ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) def bet_kelly ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , np . where ( df [ 'bet_side' ] == \"B\" , ( ( 1 / df [ 'model_odds' ]) - ( 1 / df [ back_odds ]) ) / ( 1 - ( 1 / df [ back_odds ])), ( ( 1 / df [ lay_odds ]) - ( 1 / df [ 'model_odds' ]) ) / ( 1 - ( 1 / df [ lay_odds ])), ) ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) # Create simple PL and POT table def bet_eval_metrics ( d , side = False ): if side : metrics = ( d . groupby ( 'bet_side' , as_index = False ) . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) ) else : metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) # Cumulative PL by market to visually see trend and consistency def bet_eval_chart_cPl ( d ): d = ( d . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) d [ 'cNpl' ] = d . npl . cumsum () chart = px . line ( d , x = \"market_number\" , y = \"cNpl\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) return ( chart ) #### -------------------------- #### EXECUTION #### -------------------------- # Loop through all recent history dateDFList = [] dateList = pd . date_range ( date ( 2021 , 2 , 18 ), date . today () - timedelta ( days = 1 ), freq = 'd' ) for dte in dateList : dateDFList . append ( getHubRatings ( dte )) # Concatenate (add rows to rows) all the dataframes within the list hubRatings = pd . concat ( dateDFList ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = StreamListener ( max_latency = None ) # This will execute the files (it took me ~2 hours for 2 months of data) #run_stream_parsing() # Load in odds file we created above bfOdds = pd . read_csv ( \"outputs/tho-odds.csv\" , dtype = { 'market_id' : object , 'selection_id' : object , 'atb_ladder_3m' : object , 'atl_ladder_3m' : object }) # Convert dictionary columns import ast bfOdds [ 'atb_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atb_ladder_3m' ]] bfOdds [ 'atl_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atl_ladder_3m' ]] # Convert LTP to Numeric bfOdds [ 'ltp' ] = pd . to_numeric ( bfOdds [ 'ltp' ], errors = 'coerce' ) # Filter after 18th Feb bfOdds = bfOdds . query ( 'event_date >= \"2021-02-18\"' ) # Joining the ratings data and odds data and combining rawDF = pd . merge ( hubRatings [ hubRatings [ 'market_id' ] . isin ( bfOdds . market_id . unique ())], bfOdds [[ 'market_name' , 'market_id' , 'selection_id' , 'result' , 'matched_volume' , 'bsp' , 'ltp' , 'atb_ladder_3m' , 'atl_ladder_3m' ]], on = [ 'market_id' , 'selection_id' ], how = 'inner' ) # Join and clean up columns df = ( rawDF # Extra Best Back + Lay 3 mins before of . assign ( best_back_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atb_ladder_3m' ]]) . assign ( best_lay_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atl_ladder_3m' ]]) # Coalesce LTP to BSP (about 60 rows) . assign ( ltp = lambda x : np . where ( x [ \"ltp\" ] . isnull (), x [ \"bsp\" ], x [ \"ltp\" ])) # Add a binary win / loss column . assign ( win = lambda x : np . where ( x [ 'result' ] == \"WINNER\" , 1 , 0 )) # Extra columns . assign ( model_prob = lambda x : 1 / x [ 'model_odds' ]) # Reorder Columns . reindex ( columns = [ 'date' , 'track' , 'race_number' , 'market_id' , 'selection_id' , 'bsp' , 'ltp' , 'best_back_3m' , 'best_lay_3m' , 'atb_ladder_3m' , 'atl_ladder_3m' , 'model_prob' , 'model_odds' , 'win' ]) ) bets = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) bet_eval_metrics ( bets , side = True ) bet_eval_chart_cPl ( bets ) # We'll test a 2 different staking schemes on 3 different price points grid = { \"flat_bsp\" : ( bet_flat , \"bsp\" , \"bsp\" ), \"flat_ltp\" : ( bet_flat , \"ltp\" , \"ltp\" ), \"flat_3m\" : ( bet_flat , \"best_back_3m\" , \"best_lay_3m\" ), \"kelly_bsp\" : ( bet_kelly , \"bsp\" , \"bsp\" ), \"kelly_ltp\" : ( bet_kelly , \"ltp\" , \"ltp\" ), \"kelly_3m\" : ( bet_kelly , \"best_back_3m\" , \"best_lay_3m\" ) } # Evaluate Metrics For Strategy Grid metricSummary = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column # objects[0] is the staking function itself bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) betMetrics = ( bet_eval_metrics ( bets ) . assign ( strategy = lambda x : strategy ) . reindex ( columns = [ 'strategy' , 'stake' , 'npl' , 'pot' ]) ) try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) # Compare Cumulative PL Charts cumulativePLs = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) d = ( bets . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' , 'stake' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) # Normalise to $10,000 stake for visual comparison d [ 'npl' ] = d [ 'npl' ] / ( d . stake . sum () / 10000 ) d [ 'cNpl' ] = d . npl . cumsum () d [ 'strategy' ] = strategy try : cumulativePLs = pd . concat ([ cumulativePLs , d ], ignore_index = True ) except : cumulativePLs = d px . line ( cumulativePLs , x = \"market_number\" , y = \"cNpl\" , color = \"strategy\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) bets = bet_kelly ( df , back_odds = 'ltp' , lay_odds = 'ltp' ) metricSummary = None for bVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: for lVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: x = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( bVal , lVal )) betMetrics = bet_eval_metrics ( x , side = False ) betMetrics [ 'bVal' ] = bVal betMetrics [ 'lVal' ] = lVal try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) . head ( 4 ) betsFilters = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( 0.3 , 0.3 )) bet_eval_chart_cPl ( betsFilters )","title":"Complete code"},{"location":"historicData/backtestingRatingsTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"historicData/dataSources/","text":"Historical Data Sources \u00b6 We know that your automated strategies and models are only as good as your data. We work hard to make sure you have access to the data you need to allow you to achieve what you're setting out to in your automation and modelling projects. There\u2019s a huge variety of historic pricing data available, and hopefully this page shows you how to access what you're looking for. For more information on how to use this data to make your own predictive model, take a look at our modelling section . Historical Stream API data \u00b6 Betfair UK give access to all the historical Stream API data since 2016. It is excellent to use in building models and back testing strategies, however isn't necessarily in an easily accessible format for everyone. What you need to know about this data source: \u00b6 JSON format, downloads as TAR files (zipped) Australian and overseas racing, plus soccer, tennis, cricket, golf and \u2018other sport\u2019 data All Exchange markets included since the Stream API was introduced in 2016 Time-stamped odds and volume data Able to filter by Event ID, market type and other parameters 3 tiers of access: Basic free tier \u2013 1 minute intervals for odds, no volume (free) Advanced tier \u2013 1 second intervals for odds, volume included (cost associated) Pro tier \u2013 50 millisecond intervals for odds, volume included (cost associated) Includes a Historic Data API endpoint for download management Tool available to convert the free data tier TAR files to CSV files Supporting resources to help you access this data: \u00b6 How to download and access the data files Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API Historical racing data \u00b6 This is an excellent resource if you are interested in racing and like to see market level data in a CSV format. What you need to know about this data source: \u00b6 CSV format Free to download All Australian and overseas races, dating back to the beginning of the Exchange Available as a single file per day, per country, win or place market Market snapshot by runner, including Max and min matched prices and volume, pre-play and in-play Weighted average price, pre-play and in-play BSP Winner Select Australian sports data \u00b6 We provide a range of data on Australian sports in an accessible CSV format. CSV format Select Australian sports including AFL, NRL and Super Rugby All matches 2011 \u2013 2018 Market level snapshot, including Max and min matched prices and volume, pre-play and in-play Weighted average price If none of these options suit your needs please contact us at bdp@betfair.com.au to discuss other potential options.","title":"Historic Data Sources"},{"location":"historicData/dataSources/#historical-data-sources","text":"We know that your automated strategies and models are only as good as your data. We work hard to make sure you have access to the data you need to allow you to achieve what you're setting out to in your automation and modelling projects. There\u2019s a huge variety of historic pricing data available, and hopefully this page shows you how to access what you're looking for. For more information on how to use this data to make your own predictive model, take a look at our modelling section .","title":"Historical Data Sources"},{"location":"historicData/dataSources/#historical-stream-api-data","text":"Betfair UK give access to all the historical Stream API data since 2016. It is excellent to use in building models and back testing strategies, however isn't necessarily in an easily accessible format for everyone.","title":"Historical Stream API data"},{"location":"historicData/dataSources/#what-you-need-to-know-about-this-data-source","text":"JSON format, downloads as TAR files (zipped) Australian and overseas racing, plus soccer, tennis, cricket, golf and \u2018other sport\u2019 data All Exchange markets included since the Stream API was introduced in 2016 Time-stamped odds and volume data Able to filter by Event ID, market type and other parameters 3 tiers of access: Basic free tier \u2013 1 minute intervals for odds, no volume (free) Advanced tier \u2013 1 second intervals for odds, volume included (cost associated) Pro tier \u2013 50 millisecond intervals for odds, volume included (cost associated) Includes a Historic Data API endpoint for download management Tool available to convert the free data tier TAR files to CSV files","title":"What you need to know about this data source:"},{"location":"historicData/dataSources/#supporting-resources-to-help-you-access-this-data","text":"How to download and access the data files Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API","title":"Supporting resources to help you access this data:"},{"location":"historicData/dataSources/#historical-racing-data","text":"This is an excellent resource if you are interested in racing and like to see market level data in a CSV format.","title":"Historical racing data"},{"location":"historicData/dataSources/#what-you-need-to-know-about-this-data-source_1","text":"CSV format Free to download All Australian and overseas races, dating back to the beginning of the Exchange Available as a single file per day, per country, win or place market Market snapshot by runner, including Max and min matched prices and volume, pre-play and in-play Weighted average price, pre-play and in-play BSP Winner","title":"What you need to know about this data source:"},{"location":"historicData/dataSources/#select-australian-sports-data","text":"We provide a range of data on Australian sports in an accessible CSV format. CSV format Select Australian sports including AFL, NRL and Super Rugby All matches 2011 \u2013 2018 Market level snapshot, including Max and min matched prices and volume, pre-play and in-play Weighted average price If none of these options suit your needs please contact us at bdp@betfair.com.au to discuss other potential options.","title":"Select Australian sports data"},{"location":"historicData/jsonToCsvTutorial/","text":"JSON to CSV tutorial: making a market summary \u00b6 The historic pricing data available on the Betfair Historic Data site is an excellent resource, including almost every market offered on the Exchange back to 2016. We do appreciate though that the JSON format of the data sets can make it challenging to find value in the data, especially if you're not confident in working with large data sets. In this tutorial we're going to step through the process of using the Python betfairlightweight library to take in a compressed tar folder, process the historic JSON files, and convert the data into a simple csv output, including basic market summary data for each runner split into pre play and in play values. We're also going to include a filter function, to allow us to filter out markets we're not interested in. The idea of this tutorial is to share a way of using existing libraries to make working with the JSON data sets easier, and hopefully the provide a foundation that you can build your own code base and data sets from. We'll be focusing on horse racing data; what we want to produce is a csv output that includes one row per runner for each market we're interested in, along with summary pre-play and in-play data for the runner. We'll step through the issues we encountered and how we went about solving the various challenges, including sharing relevant code snips along the way. We're not Python natives and acknowledge that there are probably more efficient and neater ways of achieving the same end goal! As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. The script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! Make sure you amend your data path to point to your data file (instructions below). We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo . Setting up your environment \u00b6 You're going to need to make sure you have Python and pip installed to get this code to run. If you're just starting out with Python, you may have to add Python to your environment variables . The is generally easiest to do by checking the box when you're installing Python choosing to 'add to PATH'. The alternative approach to the above is to use a Jupyter notebook which has the environment already set up - this might be the easier option for people new to programming. We're using some pretty new Python features, so it might be worth checking your version and updating if you're keen to follow along. To install betfairlightweight open a command prompt, or a terminal in your text editor of choice and input pip install betfairlightweight then return. Data input \u00b6 We started with the historic data parsing example from liampauling 's Github repo. Our first issue was that the example provided was expecting to take in an individual market file. We wanted to be able to accept data in a tar archive, a zipped folder, or a directory of individual bz2 files. Here's the code we used for handling the different file formats. # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None and then used it like this: # the path directories to the data sets # accepts tar files, zipped files or # directory with bz2 file(s) market_paths = [ './2020_12_DecRacingPro.zip' , './PRO' , './2021_01_JanRacingPro.tar' ] ... for file_obj in load_markets ( market_paths ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): This means we can pass in the tar and/or zipped file in its compressed form and/or directory with individual bz2 files in it and not worry about extracting the file contents, or having to handle the logic of iterating over the inner nested file structure. File paths The program will look at the file path you pass in relative to the location of the script you're running. So it will start by looking in the same folder it's saved in and then follow your navigation instructions from there, using / to indicate a folder and ../ to navigate up a level in the folder structure. If our example the data files sit in the same folder as the script ( ./PRO ). If it were in a folder at the same level as the folder that our script is in then we'd need to navigate 'up' a level (using ../ ) and then into the folder housing the data, i.e. '../dataFolder/PRO' and if the data were in a different folder within the same folder as our script file we'd use './dataFolder/PRO' etc. Type definitions \u00b6 If you're used to working in strongly typed languages, especially those with type definitions, you might find it a bit frustrating to try and figure out where you can access the different data types, for example market name or runner BSP. There are some things you can do to make this a bit easier, other than digging into the betfairlightweight source code, which was where we started. If you want to look at the definitions from the source code: MarketBook, RunnerBook MarketDefinitionRunner, MarketDefinition There are some Python extensions you can use in your ide that go some way to helping here. # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) Market summary data \u00b6 The raw files show the data at 50ms (PRO) or 1 second (ADVANCED) intervals. Too produce our csv we will need to look at the state of the market before the market goes in play, and then the state at the end of the market, and calculate from that what the pre play and in play figures are. This is the data we're going to include in our output csv. Column Definition market_id unique market identifier event_date scheduled start date/time (UTC) country event country code track track name market_name market name selection_id unique runner identifier selection_name runner name result win/loss/removed bsp Betfair starting price pp_min pre play min price traded pp_max pre play max price traded pp_wap pre play weighted average price pp_ltp pre play last traded price pp_volume pre play matched volume ip_min in play min price traded ip_max in play max price traded ip_wap in play weighted average price ip_ltp in play last traded price ip_volume in play matched volume betfairlightweight exposes snapshots of the market that include all the price data we need. To allow us to compute pre play and in play figures there are three market snapshots we need to find. These are the final view before the market turns in play, the market at the end of the race once it's no longer open but the price ladder hasn't yet been cleared, and the final closed snapshot that shows winner/loser status etc. We can then use the deltas between these market views to calculate the pre play and in play summary statistics. We iterate over these market snapshots and when we find the first market showing as in play we go back to the previous update, and use this as our pre play view. After this we keep iterating until we find the last time that the market status shows as 'open' and then use the data from the following update for the final pricing data (i.e. the first market view once the market was suspended at the end of the race). The winner/loser statuses come from the final market view. def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) We needed to write a function to parse the price data (pre play and in play) and pull out the values we're interested in. We used a reduce function to go over each matched price point, and calculate the four necessary values. To calculate weighted average price we multiplied price by size for each price point, and added them together. Once they're summed, we divided that figure by the total matched value. The matched volume is simply the sum of all matched stakes. The min price and max price are the lowest and highest values where money has matched on the runner. Reduce functions I gather from some actual Python gurus in our community that while reduce functions are very common in other languages (i.e. the ones I normally work in!), apparently they're not very Pythonic... if you're super keen, feel free to rewrite this section into a list/dict comprehension or another more Pythonic solution! # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> ( float , float , float , float ): if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) Our volume figures don't include BSP bets yet, so to account for that we're looking at the back_stake_taken and lay_liability_taken values on the SP object from the post play market snapshot, then finding whichever the smaller of those two values is and saving it that so we can add it to the traded_volume field in a later step. We use the smaller value of back_stake_taken or ( lay_liability_taken /(BSP - 1)) (i.e. backer's stake for SP lay bets) as any difference between the two values will have matched against non-BSP money and therefore is already accounted for in our matched volume. preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if type ( r . sp . actual_sp ) is float else 0 ) - 1 ) ) ) for r in postplay_market . runners ] For our csv, we have columns for runner id, runner name, winning status and BSP, so we'll store these values too. The runner name is a bit harder to get, as we need to match up the runner definition with the same selection_id as the market_book object we're currently looking at. # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] Not all markets go in play, and therefore won't have any values for the in play portion of the csv, so we need to make sure we can handle this case. We don't have in play figures separate to pre play; we have a snapshot before the market went in play, and then the view at the end of the market, so we need to use the difference between these two sets of figures to figure out what happened in play. We have two ladders, one post play and one pre play. We go through every price point in the post play ladder, and remove any volume that's showing in the pre play ladder at the corresponding price point. This leaves us with the volumes matched while the market was in play. One corner case we had to catch is that our resulting list might have prices with 0 volume, which trip up our min and max values, which doesn't use volume in its calculations. To catch this we filter out any items from the ladder with a volume of 0. Note: there are some markets included in the data files that are effective empty and don't contain any price data. We're disregarding these markets and printing out an error message to the log ( market has no price data ). # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] Writing to CSV \u00b6 We defined the columns we want for our csv pretty early in the code. # record prices to a file with open ( \"output.csv\" , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) We then assign the values for each column. # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Filtering markets \u00b6 Currently we're going through every file provided in the raw data folders, which in our case included markets from different countries, all different market types and both gallops and harness races. To save filtering these markets manually later in Excel, and also to avoid processing additional data we don't need and slowing the process down further, we decided to add a market filter so we only kept the markets we were interested in. We filtered on three things: event country code (i.e. AU, NZ, GB etc) market type (i.e. win, place etc) race type (i.e. gallops or harness) Using this logic, we are only keeping Australian win markets for gallops races. # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) Filtering out harness markets was the trickiest part of the process, as there's no neat way of separating harness meetings from gallops. To do this we had to parse the market name and look for the words 'trot' and 'pace', and treat the market as harness if we found either. To make it a little tidier we wrote a function to split the market name into its component parts. # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) We declare an evaluate_market flag and set it to none, and then in our loop the first time we evaluate the market we run the filter and skip any markets that don't meet our criteria. eval_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) Helper functions \u00b6 There are a couple of helper functions we wrote along the way to make the rest of the code easier to handle. As string Takes in a number and returns a text representation of it, rounding to two decimal places. # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' Min value greater than 0 Returns the smaller of two numbers, where the smaller isn't 0. # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) Final thoughts \u00b6 betfairlightweight provides a ready made package that makes it easier to work with the JSON data and a pretty easy way to convert the data into a csv format, allowing you to then do your data wrangling in Excel if that's where you're more comfortable. Our intention is that you don't need a heap of Python experience to be able to work through this tutorial; as long as you're prepared to get the Python environment set up and learn some basic programming skills, the hope is that you'll be able to customise your own csv file and maybe even extend on what we've covered and produced here. We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space Complete code \u00b6 Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Please note: the script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! You'll also see errors logged to the out file or terminal screen depending on your set up. Download from Github import logging from typing import List , Set , Dict , Tuple , Optional from unittest.mock import patch from itertools import zip_longest import functools import os import tarfile import zipfile import bz2 import glob # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) # the path directories to the data sets # accepts tar files, zipped files or # directory with bz2 file(s) market_paths = [ '../_data/2020_12_DecRacingPro.zip' , '../_data/PRO' , '../_data/2021_01_JanRacingPro.tar' ] # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> ( float , float , float , float ): if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # setup logging logging . basicConfig ( level = logging . FATAL ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = betfairlightweight . StreamListener ( max_latency = None ) # record prices to a file with open ( \"output.csv\" , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for file_obj in load_markets ( market_paths ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) # no price data for market if postplay_market is None : print ( 'market has no price data' ) continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if type ( r . sp . actual_sp ) is float else 0 ) - 1 ) ) ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Disclaimer \u00b6 Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"JSON to CSV tutorial"},{"location":"historicData/jsonToCsvTutorial/#json-to-csv-tutorial-making-a-market-summary","text":"The historic pricing data available on the Betfair Historic Data site is an excellent resource, including almost every market offered on the Exchange back to 2016. We do appreciate though that the JSON format of the data sets can make it challenging to find value in the data, especially if you're not confident in working with large data sets. In this tutorial we're going to step through the process of using the Python betfairlightweight library to take in a compressed tar folder, process the historic JSON files, and convert the data into a simple csv output, including basic market summary data for each runner split into pre play and in play values. We're also going to include a filter function, to allow us to filter out markets we're not interested in. The idea of this tutorial is to share a way of using existing libraries to make working with the JSON data sets easier, and hopefully the provide a foundation that you can build your own code base and data sets from. We'll be focusing on horse racing data; what we want to produce is a csv output that includes one row per runner for each market we're interested in, along with summary pre-play and in-play data for the runner. We'll step through the issues we encountered and how we went about solving the various challenges, including sharing relevant code snips along the way. We're not Python natives and acknowledge that there are probably more efficient and neater ways of achieving the same end goal! As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. The script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! Make sure you amend your data path to point to your data file (instructions below). We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo .","title":"JSON to CSV tutorial: making a market summary"},{"location":"historicData/jsonToCsvTutorial/#setting-up-your-environment","text":"You're going to need to make sure you have Python and pip installed to get this code to run. If you're just starting out with Python, you may have to add Python to your environment variables . The is generally easiest to do by checking the box when you're installing Python choosing to 'add to PATH'. The alternative approach to the above is to use a Jupyter notebook which has the environment already set up - this might be the easier option for people new to programming. We're using some pretty new Python features, so it might be worth checking your version and updating if you're keen to follow along. To install betfairlightweight open a command prompt, or a terminal in your text editor of choice and input pip install betfairlightweight then return.","title":"Setting up your environment"},{"location":"historicData/jsonToCsvTutorial/#data-input","text":"We started with the historic data parsing example from liampauling 's Github repo. Our first issue was that the example provided was expecting to take in an individual market file. We wanted to be able to accept data in a tar archive, a zipped folder, or a directory of individual bz2 files. Here's the code we used for handling the different file formats. # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None and then used it like this: # the path directories to the data sets # accepts tar files, zipped files or # directory with bz2 file(s) market_paths = [ './2020_12_DecRacingPro.zip' , './PRO' , './2021_01_JanRacingPro.tar' ] ... for file_obj in load_markets ( market_paths ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): This means we can pass in the tar and/or zipped file in its compressed form and/or directory with individual bz2 files in it and not worry about extracting the file contents, or having to handle the logic of iterating over the inner nested file structure. File paths The program will look at the file path you pass in relative to the location of the script you're running. So it will start by looking in the same folder it's saved in and then follow your navigation instructions from there, using / to indicate a folder and ../ to navigate up a level in the folder structure. If our example the data files sit in the same folder as the script ( ./PRO ). If it were in a folder at the same level as the folder that our script is in then we'd need to navigate 'up' a level (using ../ ) and then into the folder housing the data, i.e. '../dataFolder/PRO' and if the data were in a different folder within the same folder as our script file we'd use './dataFolder/PRO' etc.","title":"Data input"},{"location":"historicData/jsonToCsvTutorial/#type-definitions","text":"If you're used to working in strongly typed languages, especially those with type definitions, you might find it a bit frustrating to try and figure out where you can access the different data types, for example market name or runner BSP. There are some things you can do to make this a bit easier, other than digging into the betfairlightweight source code, which was where we started. If you want to look at the definitions from the source code: MarketBook, RunnerBook MarketDefinitionRunner, MarketDefinition There are some Python extensions you can use in your ide that go some way to helping here. # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook )","title":"Type definitions"},{"location":"historicData/jsonToCsvTutorial/#market-summary-data","text":"The raw files show the data at 50ms (PRO) or 1 second (ADVANCED) intervals. Too produce our csv we will need to look at the state of the market before the market goes in play, and then the state at the end of the market, and calculate from that what the pre play and in play figures are. This is the data we're going to include in our output csv. Column Definition market_id unique market identifier event_date scheduled start date/time (UTC) country event country code track track name market_name market name selection_id unique runner identifier selection_name runner name result win/loss/removed bsp Betfair starting price pp_min pre play min price traded pp_max pre play max price traded pp_wap pre play weighted average price pp_ltp pre play last traded price pp_volume pre play matched volume ip_min in play min price traded ip_max in play max price traded ip_wap in play weighted average price ip_ltp in play last traded price ip_volume in play matched volume betfairlightweight exposes snapshots of the market that include all the price data we need. To allow us to compute pre play and in play figures there are three market snapshots we need to find. These are the final view before the market turns in play, the market at the end of the race once it's no longer open but the price ladder hasn't yet been cleared, and the final closed snapshot that shows winner/loser status etc. We can then use the deltas between these market views to calculate the pre play and in play summary statistics. We iterate over these market snapshots and when we find the first market showing as in play we go back to the previous update, and use this as our pre play view. After this we keep iterating until we find the last time that the market status shows as 'open' and then use the data from the following update for the final pricing data (i.e. the first market view once the market was suspended at the end of the race). The winner/loser statuses come from the final market view. def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) We needed to write a function to parse the price data (pre play and in play) and pull out the values we're interested in. We used a reduce function to go over each matched price point, and calculate the four necessary values. To calculate weighted average price we multiplied price by size for each price point, and added them together. Once they're summed, we divided that figure by the total matched value. The matched volume is simply the sum of all matched stakes. The min price and max price are the lowest and highest values where money has matched on the runner. Reduce functions I gather from some actual Python gurus in our community that while reduce functions are very common in other languages (i.e. the ones I normally work in!), apparently they're not very Pythonic... if you're super keen, feel free to rewrite this section into a list/dict comprehension or another more Pythonic solution! # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> ( float , float , float , float ): if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) Our volume figures don't include BSP bets yet, so to account for that we're looking at the back_stake_taken and lay_liability_taken values on the SP object from the post play market snapshot, then finding whichever the smaller of those two values is and saving it that so we can add it to the traded_volume field in a later step. We use the smaller value of back_stake_taken or ( lay_liability_taken /(BSP - 1)) (i.e. backer's stake for SP lay bets) as any difference between the two values will have matched against non-BSP money and therefore is already accounted for in our matched volume. preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if type ( r . sp . actual_sp ) is float else 0 ) - 1 ) ) ) for r in postplay_market . runners ] For our csv, we have columns for runner id, runner name, winning status and BSP, so we'll store these values too. The runner name is a bit harder to get, as we need to match up the runner definition with the same selection_id as the market_book object we're currently looking at. # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] Not all markets go in play, and therefore won't have any values for the in play portion of the csv, so we need to make sure we can handle this case. We don't have in play figures separate to pre play; we have a snapshot before the market went in play, and then the view at the end of the market, so we need to use the difference between these two sets of figures to figure out what happened in play. We have two ladders, one post play and one pre play. We go through every price point in the post play ladder, and remove any volume that's showing in the pre play ladder at the corresponding price point. This leaves us with the volumes matched while the market was in play. One corner case we had to catch is that our resulting list might have prices with 0 volume, which trip up our min and max values, which doesn't use volume in its calculations. To catch this we filter out any items from the ladder with a volume of 0. Note: there are some markets included in the data files that are effective empty and don't contain any price data. We're disregarding these markets and printing out an error message to the log ( market has no price data ). # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ]","title":"Market summary data"},{"location":"historicData/jsonToCsvTutorial/#writing-to-csv","text":"We defined the columns we want for our csv pretty early in the code. # record prices to a file with open ( \"output.csv\" , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) We then assign the values for each column. # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) )","title":"Writing to CSV"},{"location":"historicData/jsonToCsvTutorial/#filtering-markets","text":"Currently we're going through every file provided in the raw data folders, which in our case included markets from different countries, all different market types and both gallops and harness races. To save filtering these markets manually later in Excel, and also to avoid processing additional data we don't need and slowing the process down further, we decided to add a market filter so we only kept the markets we were interested in. We filtered on three things: event country code (i.e. AU, NZ, GB etc) market type (i.e. win, place etc) race type (i.e. gallops or harness) Using this logic, we are only keeping Australian win markets for gallops races. # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) Filtering out harness markets was the trickiest part of the process, as there's no neat way of separating harness meetings from gallops. To do this we had to parse the market name and look for the words 'trot' and 'pace', and treat the market as harness if we found either. To make it a little tidier we wrote a function to split the market name into its component parts. # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) We declare an evaluate_market flag and set it to none, and then in our loop the first time we evaluate the market we run the filter and skip any markets that don't meet our criteria. eval_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None )","title":"Filtering markets"},{"location":"historicData/jsonToCsvTutorial/#helper-functions","text":"There are a couple of helper functions we wrote along the way to make the rest of the code easier to handle. As string Takes in a number and returns a text representation of it, rounding to two decimal places. # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' Min value greater than 0 Returns the smaller of two numbers, where the smaller isn't 0. # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b )","title":"Helper functions"},{"location":"historicData/jsonToCsvTutorial/#final-thoughts","text":"betfairlightweight provides a ready made package that makes it easier to work with the JSON data and a pretty easy way to convert the data into a csv format, allowing you to then do your data wrangling in Excel if that's where you're more comfortable. Our intention is that you don't need a heap of Python experience to be able to work through this tutorial; as long as you're prepared to get the Python environment set up and learn some basic programming skills, the hope is that you'll be able to customise your own csv file and maybe even extend on what we've covered and produced here. We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space","title":"Final thoughts"},{"location":"historicData/jsonToCsvTutorial/#complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Please note: the script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! You'll also see errors logged to the out file or terminal screen depending on your set up. Download from Github import logging from typing import List , Set , Dict , Tuple , Optional from unittest.mock import patch from itertools import zip_longest import functools import os import tarfile import zipfile import bz2 import glob # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) # the path directories to the data sets # accepts tar files, zipped files or # directory with bz2 file(s) market_paths = [ '../_data/2020_12_DecRacingPro.zip' , '../_data/PRO' , '../_data/2021_01_JanRacingPro.tar' ] # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> ( float , float , float , float ): if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # setup logging logging . basicConfig ( level = logging . FATAL ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = betfairlightweight . StreamListener ( max_latency = None ) # record prices to a file with open ( \"output.csv\" , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for file_obj in load_markets ( market_paths ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) # no price data for market if postplay_market is None : print ( 'market has no price data' ) continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if type ( r . sp . actual_sp ) is float else 0 ) - 1 ) ) ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) )","title":"Complete code"},{"location":"historicData/jsonToCsvTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"historicData/usingHistoricDataSite/","text":"Using the Historic Data site \u00b6 The Betfair Historic Data site includes complete historic data for nearly all markets offered on the Exchange since 2016, when the new APING was launched. The data available includes prices, volume traded, winning status, average weighted price, BSP, and a variety of other details that are valuable for modelling and strategy development. We know that the process of downloading and extracting these data files can be a bit intimidating the first time round, so here's a walk through of one way to go about it to help make it more accessible. Data tiers There are three tiers of historic data available on this site. You can download samples of each tier of data here . The biggest differece is between the free and paid data. The free data includes a lot of information about the market, but no volume, and only last traded price per minute, not a full price ladder. The two paid tiers include the same data, just at different frequencies. If your strategy isn't particularly price sensitive and doesn't need volume as a variable then you'll probably be fine wtih the free tier, however if you need to see a more granular view of the market then you should probably consider the paid advanced or pro tiers. A full catalogue of the values included in each data tier is available here . Basic Advanced Pro 1 minute intervals last traded price no volume 1 second intervals price ladder (top 3) volume API tick intervals (50ms) price ladder (full) volume Purchasing the data \u00b6 Start by going to the Betfair Historic Data site and log in using your Betfair account. Note: if you have less than 100 Betfair points you may have problems downloading data. On the Home page select the data set you want to download. Free data You need to 'purchase' the data set you want to download, even if it's from the free tier You can only 'purchase' each time period of data once. For example, if you had previously 'purchased' all Greyhound data for January 2018, then tried to download Greyhound data for January to March 2018 you would receive an error, and would need to purchase the data for February to March instead. Once you 'purchase' your choice of data it's recommended that you go to the My Data page, and choose the subset of data to then download. Downloading the data \u00b6 On the My Data page you can filter the purchased data to the actual markets you're interested in. You can filter by Sport, Date range, EventId, Event Name, Market Type, Country & File Type (M = market, E = Event), which will cut down the size of the data you need to download. For example, if you wanted the win market for Australian and New Zealand greyhound races you'd use these filters. File type The file type filter has two options that you can choose from: E = Event - includes event level data, i.e. Geelong greyhounds on x date M = Market - includes market level data, i.e. the win market for Geelong greyhounds race 3 on x date The site can be pretty slow to download from, and you'll generally have a better experience if you download the data a bit at a time, say month by month. Alternatively if you're going to download a lot of data it might be worth having a look at the historic data API, that can automate the download process and speed it up significantly. There's a guide available here , and some sample code the help get you started. Unzipping the files \u00b6 You'll need to download a program to unzip the TAR files. Here we'll be using 7Zip , which is free, open source and generally well respected. Once you've downloaded it make sure you also install it onto the computer you'll be using to open the data files. Locate the data.tar file in your computer's file explorer program. Right click on the file, select '7-Zip' from the menu then choose 'Extract files...'. In the model that pops up change the path mode to 'No pathnames'. You can also change the name and/or path of the folder you want the files extracted to if you want to. You now have a collection of .bz2 files. The final step is to select all the files, right click, select '7-Zip' from the menu then choose 'Extract here'. This will then extract all the individual zipped files which you can then either open in a text editor - you can use something basic like Notepad (installed on basically all computers by default) or a more complete program like Visual Studio Code (my go to), Vim or Notepad++ - or you can parse over the using a program to do the work for you. We'll explore how to parse the data another time. If you're opening the files with a text editor you might need to right click, choose 'open with' and select your preferred program. What's it for? \u00b6 The data available on the Historic Data site is extensive, and can be a really valuable tool or input. For example, you can include some of the columns as variables in a predictive model, compare BSP odds against win rates, or determine the average length of time it takes for 2 year old horses to run 1200m at Geelong. Quality data underpins the vast majority of successful betting strategies, so becoming comfortable working with the data available to you is a really important part of both the modelling and automation processes. Extra resources \u00b6 Here are some other useful resources that can help you work with this Historic Data: Historic Data FAQs Data Specification API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API","title":"Downloading from the Historic Data site"},{"location":"historicData/usingHistoricDataSite/#using-the-historic-data-site","text":"The Betfair Historic Data site includes complete historic data for nearly all markets offered on the Exchange since 2016, when the new APING was launched. The data available includes prices, volume traded, winning status, average weighted price, BSP, and a variety of other details that are valuable for modelling and strategy development. We know that the process of downloading and extracting these data files can be a bit intimidating the first time round, so here's a walk through of one way to go about it to help make it more accessible. Data tiers There are three tiers of historic data available on this site. You can download samples of each tier of data here . The biggest differece is between the free and paid data. The free data includes a lot of information about the market, but no volume, and only last traded price per minute, not a full price ladder. The two paid tiers include the same data, just at different frequencies. If your strategy isn't particularly price sensitive and doesn't need volume as a variable then you'll probably be fine wtih the free tier, however if you need to see a more granular view of the market then you should probably consider the paid advanced or pro tiers. A full catalogue of the values included in each data tier is available here . Basic Advanced Pro 1 minute intervals last traded price no volume 1 second intervals price ladder (top 3) volume API tick intervals (50ms) price ladder (full) volume","title":"Using the Historic Data site"},{"location":"historicData/usingHistoricDataSite/#purchasing-the-data","text":"Start by going to the Betfair Historic Data site and log in using your Betfair account. Note: if you have less than 100 Betfair points you may have problems downloading data. On the Home page select the data set you want to download. Free data You need to 'purchase' the data set you want to download, even if it's from the free tier You can only 'purchase' each time period of data once. For example, if you had previously 'purchased' all Greyhound data for January 2018, then tried to download Greyhound data for January to March 2018 you would receive an error, and would need to purchase the data for February to March instead. Once you 'purchase' your choice of data it's recommended that you go to the My Data page, and choose the subset of data to then download.","title":"Purchasing the data"},{"location":"historicData/usingHistoricDataSite/#downloading-the-data","text":"On the My Data page you can filter the purchased data to the actual markets you're interested in. You can filter by Sport, Date range, EventId, Event Name, Market Type, Country & File Type (M = market, E = Event), which will cut down the size of the data you need to download. For example, if you wanted the win market for Australian and New Zealand greyhound races you'd use these filters. File type The file type filter has two options that you can choose from: E = Event - includes event level data, i.e. Geelong greyhounds on x date M = Market - includes market level data, i.e. the win market for Geelong greyhounds race 3 on x date The site can be pretty slow to download from, and you'll generally have a better experience if you download the data a bit at a time, say month by month. Alternatively if you're going to download a lot of data it might be worth having a look at the historic data API, that can automate the download process and speed it up significantly. There's a guide available here , and some sample code the help get you started.","title":"Downloading the data"},{"location":"historicData/usingHistoricDataSite/#unzipping-the-files","text":"You'll need to download a program to unzip the TAR files. Here we'll be using 7Zip , which is free, open source and generally well respected. Once you've downloaded it make sure you also install it onto the computer you'll be using to open the data files. Locate the data.tar file in your computer's file explorer program. Right click on the file, select '7-Zip' from the menu then choose 'Extract files...'. In the model that pops up change the path mode to 'No pathnames'. You can also change the name and/or path of the folder you want the files extracted to if you want to. You now have a collection of .bz2 files. The final step is to select all the files, right click, select '7-Zip' from the menu then choose 'Extract here'. This will then extract all the individual zipped files which you can then either open in a text editor - you can use something basic like Notepad (installed on basically all computers by default) or a more complete program like Visual Studio Code (my go to), Vim or Notepad++ - or you can parse over the using a program to do the work for you. We'll explore how to parse the data another time. If you're opening the files with a text editor you might need to right click, choose 'open with' and select your preferred program.","title":"Unzipping the files"},{"location":"historicData/usingHistoricDataSite/#whats-it-for","text":"The data available on the Historic Data site is extensive, and can be a really valuable tool or input. For example, you can include some of the columns as variables in a predictive model, compare BSP odds against win rates, or determine the average length of time it takes for 2 year old horses to run 1200m at Geelong. Quality data underpins the vast majority of successful betting strategies, so becoming comfortable working with the data available to you is a really important part of both the modelling and automation processes.","title":"What's it for?"},{"location":"historicData/usingHistoricDataSite/#extra-resources","text":"Here are some other useful resources that can help you work with this Historic Data: Historic Data FAQs Data Specification API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API","title":"Extra resources"},{"location":"modelling/AFLmodellingPython/","text":"AFL Modelling Walkthrough \u00b6 01. Data Cleaning \u00b6 These tutorials will walk you through how to construct your own basic AFL model, using publicly available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through the basics of cleaning this dataset and how we have done it. If you want to get straight to feature creation or modelling, feel free to jump ahead! # Import libraries import pandas as pd import numpy as np import re pd . set_option ( 'display.max_columns' , None ) We will first explore the DataFrames, and then create functions to wrangle them and clean them into more consistent sets of data. # Read/clean each DataFrame match_results = pd . read_csv ( \"data/afl_match_results.csv\" ) odds = pd . read_csv ( \"data/afl_odds.csv\" ) player_stats = pd . read_csv ( \"data/afl_player_stats.csv\" ) odds . tail ( 3 ) trunc event_name path selection_name odds 4179 2018-09-01 Match Odds VFL/Richmond Reserves v Williamstown Williamstown 2.3878 4180 2018-09-01 Match Odds WAFL/South Fremantle v West Perth South Fremantle 1.5024 4181 2018-09-01 Match Odds WAFL/South Fremantle v West Perth West Perth 2.7382 match_results . tail ( 3 ) Game Date Round Home.Team Home.Goals Home.Behinds Home.Points Away.Team Away.Goals Away.Behinds Away.Points Venue Margin Season Round.Type Round.Number 15395 15396 2018-08-26 R23 Brisbane Lions 11 6 72 West Coast 14 14 98 Gabba -26 2018 Regular 23 15396 15397 2018-08-26 R23 Melbourne 15 12 102 GWS 8 9 57 M.C.G. 45 2018 Regular 23 15397 15398 2018-08-26 R23 St Kilda 14 10 94 North Melbourne 17 15 117 Docklands -23 2018 Regular 23 player_stats . tail ( 3 ) AF B BO CCL CG CL CM CP D DE Date ED FA FF G GA HB HO I50 ITC K M MG MI5 Match_id One.Percenters Opposition Player R50 Round SC SCL SI Season Status T T5 TO TOG Team UP Venue 89317 38 1 0 0.0 0 0 1 2 9 55.6 25/08/2018 5 0 0 0 0 3 0 0 2.0 6 3 132.0 2 9711 0 Fremantle Christopher Mayne 1 Round 23 35 0.0 2.0 2018 Away 1 0.0 1.0 57 Collingwood 7 Optus Stadium 89318 38 0 0 0.0 3 0 0 3 9 55.6 25/08/2018 5 0 1 0 0 3 0 0 4.0 6 3 172.0 0 9711 2 Fremantle Nathan Murphy 5 Round 23 29 0.0 0.0 2018 Away 1 0.0 3.0 70 Collingwood 6 Optus Stadium 89319 56 1 0 0.0 1 0 0 3 8 62.5 25/08/2018 5 0 0 2 0 2 0 0 2.0 6 3 180.0 3 9711 2 Fremantle Jaidyn Stephenson 0 Round 23 56 0.0 4.0 2018 Away 3 1.0 2.0 87 Collingwood 5 Optus Stadium Have a look at the structure of the DataFrames. Notice that for the odds DataFrame, each game is split between two rows, whilst for the match_results each game is on one row. We will have to get around this by splitting the games up onto two rows, as this will allow our feature transformation functions to be applied more easily later on. For the player_stats DataFrame we will aggregate these stats into each game on separate rows. First, we will write functions to make the odds data look a bit nicer, with only a team column, a date column and a 'home_game' column which takes the values 0 or 1 depending on if it was a home game for that team. To do this we will use the regex module to extract the team names from the path column, as well as the to_datetime function from pandas. We will also replace all the inconsistent team names with consistent team names. def get_cleaned_odds ( df = None ): # If a df hasn't been specified as a parameter, read the odds df if df is None : df = pd . read_csv ( \"data/afl_odds.csv\" ) # Get a dictionary of team names we want to change and their new values team_name_mapping = { 'Adelaide Crows' : 'Adelaide' , 'Brisbane Lions' : 'Brisbane' , 'Carlton Blues' : 'Carlton' , 'Collingwood Magpies' : 'Collingwood' , 'Essendon Bombers' : 'Essendon' , 'Fremantle Dockers' : 'Fremantle' , 'GWS Giants' : 'GWS' , 'Geelong Cats' : 'Geelong' , 'Gold Coast Suns' : 'Gold Coast' , 'Greater Western Sydney' : 'GWS' , 'Greater Western Sydney Giants' : 'GWS' , 'Hawthorn Hawks' : 'Hawthorn' , 'Melbourne Demons' : 'Melbourne' , 'North Melbourne Kangaroos' : 'North Melbourne' , 'Port Adelaide Magpies' : 'Port Adelaide' , 'Port Adelaide Power' : 'Port Adelaide' , 'P Adelaide' : 'Port Adelaide' , 'Richmond Tigers' : 'Richmond' , 'St Kilda Saints' : 'St Kilda' , 'Sydney Swans' : 'Sydney' , 'West Coast Eagles' : 'West Coast' , 'Wetsern Bulldogs' : 'Western Bulldogs' , 'Western Bullbogs' : 'Western Bulldogs' } # Add columns df = ( df . assign ( date = lambda df : pd . to_datetime ( df . trunc ), # Create a datetime column home_team = lambda df : df . path . str . extract ( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 1 ] . str . strip (), away_team = lambda df : df . path . str . extract ( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 2 ] . str . strip ()) . drop ( columns = [ 'path' , 'trunc' , 'event_name' ]) # Drop irrelevant columns . rename ( columns = { 'selection_name' : 'team' }) # Rename columns . replace ( team_name_mapping ) . sort_values ( by = 'date' ) . reset_index ( drop = True ) . assign ( home_game = lambda df : df . apply ( lambda row : 1 if row . home_team == row . team else 0 , axis = 'columns' )) . drop ( columns = [ 'home_team' , 'away_team' ])) return df # Apply the wrangling and cleaning function odds = get_cleaned_odds ( odds ) odds . tail () team odds date home_game 4177 South Fremantle 1.5024 2018-09-01 1 4178 Port Melbourne 2.8000 2018-09-01 0 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 We now have a DataFrame that looks nice and easy to join with our other DataFrames. Now let's lean up the match_details DataFrame. # Define a function which cleans the match results df, and separates each teams' stats onto individual rows def get_cleaned_match_results ( df = None ): # If a df hasn't been specified as a parameter, read the match_results df if df is None : df = pd . read_csv ( \"data/afl_match_results.csv\" ) # Create column lists to loop through - these are the columns we want in home and away dfs home_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' , 'Margin' , 'Venue' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' ] away_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' , 'Margin' , 'Venue' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' ] mapping = [ 'game' , 'date' , 'round' , 'team' , 'goals' , 'behinds' , 'points' , 'margin' , 'venue' , 'opponent' , 'opponent_goals' , 'opponent_behinds' , 'opponent_points' ] team_name_mapping = { 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' } # Create a df with only home games df_home = ( df [ home_columns ] . rename ( columns = { old_col : new_col for old_col , new_col in zip ( home_columns , mapping )}) . assign ( home_game = 1 )) # Create a df with only away games df_away = ( df [ away_columns ] . rename ( columns = { old_col : new_col for old_col , new_col in zip ( away_columns , mapping )}) . assign ( home_game = 0 , margin = lambda df : df . margin * - 1 )) # Append these dfs together new_df = ( df_home . append ( df_away ) . sort_values ( by = 'game' ) # Sort by game ID . reset_index ( drop = True ) # Reset index . assign ( date = lambda df : pd . to_datetime ( df . date )) # Create a datetime column . replace ( team_name_mapping )) # Rename team names to be consistent with other dfs return new_df match_results = get_cleaned_match_results ( match_results ) match_results . head () game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 3 2 1897-05-08 1 St Kilda 2 4 16 -25 Victoria Park Collingwood 5 11 41 0 4 3 1897-05-08 1 Geelong 3 6 24 -23 Corio Oval Essendon 7 5 47 1 Now we have both the odds DataFrame and match_results DataFrame ready for feature creation! Finally, we will aggregate the player_stats DataFrame stats for each game rather than individual player stats. For this DataFrame we have regular stats, such as disposals, marks etc. and Advanced Stats, such as Tackles Inside 50 and Metres Gained. However these advanced stats are only available from 2015, so we will not be using them in this tutorial - as there isn't enough data from 2015 to train our models. Let's now aggregate the player_stats DataFrame. def get_cleaned_aggregate_player_stats ( df = None ): # If a df hasn't been specified as a parameter, read the player_stats df if df is None : df = pd . read_csv ( \"data/afl_player_stats.csv\" ) agg_stats = ( df . rename ( columns = { # Rename columns to lowercase 'Season' : 'season' , 'Round' : 'round' , 'Team' : 'team' , 'Opposition' : 'opponent' , 'Date' : 'date' }) . groupby ( by = [ 'date' , 'season' , 'team' , 'opponent' ], as_index = False ) # Groupby to aggregate the stats for each game . sum () . drop ( columns = [ 'DE' , 'TOG' , 'Match_id' ]) # Drop columns . assign ( date = lambda df : pd . to_datetime ( df . date , format = \" %d /%m/%Y\" )) # Create a datetime object . sort_values ( by = 'date' ) . reset_index ( drop = True )) return agg_stats agg_stats = get_cleaned_aggregate_player_stats ( player_stats ) agg_stats . tail () date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3621 2018-08-26 2018 Brisbane West Coast 1652 5 0 14.0 49 37 8 132 394 302 20 18 11 9 167 48 49 59.0 227 104 5571.0 6 48 39 1645 23.0 86.0 62 13.0 69.0 256 3622 2018-08-26 2018 West Coast Brisbane 1548 11 5 13.0 49 42 9 141 360 262 18 20 14 8 137 39 56 70.0 223 95 5809.0 12 39 34 1655 29.0 94.0 55 6.0 59.0 217 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 We now have a three fully prepared DataFrames which are almost ready to be analysed and for a model to be built on! Let's have a look at how they look and then merge them together into our final DataFrame. odds . tail ( 3 ) team odds date home_game 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 match_results . tail ( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 30793 15397 2018-08-26 23 Melbourne 15 12 102 45 M.C.G. GWS 8 9 57 1 30794 15398 2018-08-26 23 St Kilda 14 10 94 -23 Docklands North Melbourne 17 15 117 1 30795 15398 2018-08-26 23 North Melbourne 17 15 117 23 Docklands St Kilda 14 10 94 0 agg_stats . tail ( 3 ) date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 merged_df = ( odds [ odds . team . isin ( agg_stats . team . unique ())] . pipe ( pd . merge , match_results , on = [ 'date' , 'team' , 'home_game' ]) . pipe ( pd . merge , agg_stats , on = [ 'date' , 'team' , 'opponent' ]) . sort_values ( by = [ 'game' ])) merged_df . tail ( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3199 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3195 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3200 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Great! We now have a clean looking datset with each row representing one team in a game. Let's now eliminate the outliers from a dataset. We know that Essendon had a doping scandal which resulted in their entire team being banned for a year in 2016, so let's remove all of their 2016 games. To do this we will filter based on the team and season, and then invert this with ~. # Define a function which eliminates outliers def outlier_eliminator ( df ): # Eliminate Essendon 2016 games essendon_filter_criteria = ~ ((( df [ 'team' ] == 'Essendon' ) & ( df [ 'season' ] == 2016 )) | (( df [ 'opponent' ] == 'Essendon' ) & ( df [ 'season' ] == 2016 ))) df = df [ essendon_filter_criteria ] . reset_index ( drop = True ) return df afl_data = outlier_eliminator ( merged_df ) afl_data . tail ( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Finally, let's mark all of the columns that we are going to use in feature creation with the string 'f_' at the start of their column name so that we can easily filter for these columns. non_feature_cols = [ 'team' , 'date' , 'home_game' , 'game' , 'round' , 'venue' , 'opponent' , 'season' ] afl_data = afl_data . rename ( columns = { col : 'f_' + col for col in afl_data if col not in non_feature_cols }) afl_data . tail ( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Our data is now fully ready to be explored and for features to be created. 02. Feature Creation \u00b6 These tutorials will walk you through how to construct your own basic AFL model. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through creating features from our dataset, which was cleaned in the first tutorial. Feature engineering is an integral part of the Data Science process. Creative and smart features can be the difference between an average performing model and a model profitable which beats the market odds. Grabbing Our Dataset \u00b6 First, we will import our required modules, as well as the prepare_afl_data function which we created in our afl_data_cleaning script. This essentially cleans all the data for us so that we're ready to explore the data and make some features. # Import modules from afl_data_cleaning_v2 import * import afl_data_cleaning_v2 import pandas as pd pd . set_option ( 'display.max_columns' , None ) import warnings warnings . filterwarnings ( 'ignore' ) import numpy as np # Use the prepare_afl_data function to prepare the data for us; this function condenses what we walked through in the previous tutorial afl_data = prepare_afl_data () afl_data . tail ( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Creating A Feature DataFrame \u00b6 Let's create a feature DataFrame and merge all of our features into this DataFrame as we go. features = afl_data [[ 'date' , 'game' , 'team' , 'opponent' , 'venue' , 'home_game' ]] . copy () What Each Column Refers To \u00b6 Below is a DataFrame which outlines what each column refers to. column_abbreviations = pd . read_csv ( \"data/afl_data_columns_mapping.csv\" ) column_abbreviations Feature Abbreviated Feature 0 GA Goal Assists 1 CP Contested Possessions 2 UP Uncontested Possessions 3 ED Effective Disposals 4 CM Contested Marks 5 MI5 Marks Inside 50 6 One.Percenters One Percenters 7 BO Bounces 8 K Kicks 9 HB Handballs 10 D Disposals 11 M Marks 12 G Goals 13 B Behinds 14 T Tackles 15 HO Hitouts 16 I50 Inside 50s 17 CL Clearances 18 CG Clangers 19 R50 Rebound 50s 20 FF Frees For 21 FA Frees Against 22 AF AFL Fantasy Points 23 SC Supercoach Points 24 CCL Centre Clearances 25 SCL Stoppage Clearances 26 SI Score Involvements 27 MG Metres Gained 28 TO Turnovers 29 ITC Intercepts 30 T5 Tackles Inside 50 Feature Creation \u00b6 Now let's think about what features we can create. We have a enormous amount of stats to sift through. To start, let's create some simple features based on our domain knowledge of Aussie Rules. Creating Expontentially Weighted Rolling Averages as Features \u00b6 Next, we will create rolling averages of statistics such as Tackles, which we will use as features. It is fair to assume that a team's performance in a certain stat may have predictive power to the overall result. And in general, if a team consistently performs well in this stat, this may have predictive power to the result of their future games. We can't simply train a model on stats from the game which we are trying to predict (i.e. data that we don't have before the game begins), as this will leak the result. We need to train our model on past data. One way of doing this is to train our model on average stats over a certain amount of games. If a team is averaging high in this stat, this may give insight into if they are a strong team. Similarly, if the team is averaging poorly in this stat (relative to the team they are playing), this may have predictive power and give rise to a predicted loss. To do this we will create a function which calculates the rolling averages, known as create_exp_weighted_avgs, which takes our cleaned DataFrame as an input, as well as the alpha which, when higher, weights recent performances more than old performances. To read more about expontentially weighted moving averages, please read the documentation here . First, we will grab all the columns which we want to create EMAs for, and then use our function to create the average for that column. We will create a new DataFrame and add these columns to this new DataFrame. # Define a function which returns a DataFrame with the expontential moving average for each numeric stat def create_exp_weighted_avgs ( df , span ): # Create a copy of the df with only the game id and the team - we will add cols to this df ema_features = df [[ 'game' , 'team' ]] . copy () feature_names = [ col for col in df . columns if col . startswith ( 'f_' )] # Get a list of columns we will iterate over for feature_name in feature_names : feature_ema = ( df . groupby ( 'team' )[ feature_name ] . transform ( lambda row : ( row . ewm ( span = span ) . mean () . shift ( 1 )))) ema_features [ feature_name ] = feature_ema return ema_features features_rolling_averages = create_exp_weighted_avgs ( afl_data , span = 10 ) features_rolling_averages . tail () game team f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3152 15396 West Coast 2.094236 12.809630 10.047145 86.904928 8.888770 11.435452 9.403444 78.016158 3193.612782 16.472115 11.958482 23.379562 100.095244 68.252001 27.688669 284.463270 719.884644 525.878017 36.762440 44.867118 25.618202 17.522871 270.478779 88.139376 105.698031 148.005305 449.405865 201.198907 11581.929999 20.048124 95.018480 74.180967 3314.157893 44.872398 177.894442 126.985101 20.565549 138.876613 438.848376 3153 15397 GWS 1.805565 13.100372 13.179329 91.781563 18.527618 10.371198 11.026754 73.253945 3165.127358 19.875913 12.947209 25.114002 105.856671 80.609640 23.374884 303.160047 741.439198 534.520295 42.597317 38.160889 26.208715 18.688880 300.188301 81.540693 106.989070 143.032506 441.250897 173.050118 12091.630837 21.106142 103.077097 80.201059 3419.245919 55.495610 219.879895 138.202470 25.313148 135.966798 438.466439 3154 15397 Melbourne 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.787800 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 3155 15398 North Melbourne 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.541130 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3156 15398 St Kilda 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.498760 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 As you can see our function worked perfectly! Now we have a full DataFrame of exponentially weighted moving averages. Note that as these rolling averages have been shifted by 1 to ensure no data leakage, the first round of the data will have all NA values. We can drop these later. Let's add these averages to our features DataFrame features = pd . merge ( features , features_rolling_averages , on = [ 'game' , 'team' ]) Creating a 'Form Between the Teams' Feature \u00b6 It is well known in Aussie Rules that often some teams perform better against certain teams than others. If we isolate our features to pure stats based on previous games not between the teams playing, or elo ratings, we won't account for any relationships between certain teams. An example is the Kennett Curse , where Geelong won 11 consecutive games against Hawthorn, despite being similarly matched teams. Let's create a feature which calculates how many games a team has won against their opposition over a given window of games. To do this, we will need to use historical data that dates back well before our current DataFrame starts at. Otherwise we will be using a lot of our games to calculate form, meaning we will have to drop these rows before feeding it into an algorithm. So let's use our prepare_match_results function which we defined in the afl_data_cleaning tutorial to grab a clean DataFrame of all match results since 1897. We can then calculate the form and join this to our current DataFrame. match_results = afl_data_cleaning_v2 . get_cleaned_match_results () match_results . head ( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 form_btwn_teams = match_results [[ 'game' , 'team' , 'opponent' , 'margin' ]] . copy () form_btwn_teams [ 'f_form_margin_btwn_teams' ] = ( match_results . groupby ([ 'team' , 'opponent' ])[ 'margin' ] . transform ( lambda row : row . rolling ( 5 ) . mean () . shift ()) . fillna ( 0 )) form_btwn_teams [ 'f_form_past_5_btwn_teams' ] = \\ ( match_results . assign ( win = lambda df : df . apply ( lambda row : 1 if row . margin > 0 else 0 , axis = 'columns' )) . groupby ([ 'team' , 'opponent' ])[ 'win' ] . transform ( lambda row : row . rolling ( 5 ) . mean () . shift () * 5 ) . fillna ( 0 )) form_btwn_teams . tail ( 3 ) game team opponent margin f_form_margin_btwn_teams f_form_past_5_btwn_teams 30793 15397 Melbourne GWS 45 -23.2 2.0 30794 15398 St Kilda North Melbourne -23 -3.2 2.0 30795 15398 North Melbourne St Kilda 23 3.2 3.0 # Merge to our features df features = pd . merge ( features , form_btwn_teams . drop ( columns = [ 'margin' ]), on = [ 'game' , 'team' , 'opponent' ]) Creating Efficiency Features \u00b6 Disposal Efficiency \u00b6 Disposal efficiency is pivotal in Aussie Rules football. If you are dispose of the ball effectively you are much more likely to score and much less likely to concede goals than if you dispose of it ineffectively. Let's create a disposal efficiency feature by dividing Effective Disposals by Disposals. Inside 50/Rebound 50 Efficiency \u00b6 Similarly, one could hypothesise that teams who keep the footy in their Inside 50 regularly will be more likely to score, whilst teams who are effective at getting the ball out of their Defensive 50 will be less likely to concede. Let's use this logic to create Inside 50 Efficiency and Rebound 50 Efficiency features. The formula used will be: Inside 50 Efficiency = R50_Opponents / I50 (lower is better). Rebound 50 Efficiency = R50 / I50_Opponents (higher is better). Using these formulas, I50 Efficiency = R50 Efficiency_Opponent. So we will just need to create the formulas for I50 efficiency. To create these features we will need the opposition's Inside 50s/Rebound 50s. So we will split out data into two DataFrames, create a new DataFrame by joining these two DataFrames on the Game, calculate our efficiency features, then join our features with our main features DataFrame. # Get each match on single rows single_row_df = ( afl_data [[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' , ]] . query ( 'home_game == 1' ) . rename ( columns = { 'team' : 'home_team' , 'f_I50' : 'f_I50_home' , 'f_R50' : 'f_R50_home' , 'f_D' : 'f_D_home' , 'f_ED' : 'f_ED_home' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , afl_data [[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' ]] . query ( 'home_game == 0' ) . rename ( columns = { 'team' : 'away_team' , 'f_I50' : 'f_I50_away' , 'f_R50' : 'f_R50_away' , 'f_D' : 'f_D_away' , 'f_ED' : 'f_ED_away' }) . drop ( columns = 'home_game' ), on = 'game' )) single_row_df . head () game home_team f_I50_home f_R50_home f_D_home f_ED_home away_team f_I50_away f_R50_away f_D_away f_ED_away 0 13764 Carlton 69 21 373 268 Richmond 37 50 316 226 1 13765 Geelong 54 40 428 310 St Kilda 52 45 334 246 2 13766 Collingwood 70 38 398 289 Port Adelaide 50 44 331 232 3 13767 Adelaide 59 38 366 264 Hawthorn 54 38 372 264 4 13768 Brisbane 50 39 343 227 Fremantle 57 30 351 250 single_row_df = single_row_df . assign ( f_I50_efficiency_home = lambda df : df . f_R50_away / df . f_I50_home , f_I50_efficiency_away = lambda df : df . f_R50_home / df . f_I50_away ) feature_efficiency_cols = [ 'f_I50_efficiency_home' , 'f_I50_efficiency_away' ] # Now let's create an Expontentially Weighted Moving Average for these features - we will need to reshape our DataFrame to do this efficiency_features_multi_row = ( single_row_df [[ 'game' , 'home_team' ] + feature_efficiency_cols ] . rename ( columns = { 'home_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency' , 'f_I50_efficiency_away' : 'f_I50_efficiency_opponent' , }) . append (( single_row_df [[ 'game' , 'away_team' ] + feature_efficiency_cols ] . rename ( columns = { 'away_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency_opponent' , 'f_I50_efficiency_away' : 'f_I50_efficiency' , })), sort = True ) . sort_values ( by = 'game' ) . reset_index ( drop = True )) efficiency_features = efficiency_features_multi_row [[ 'game' , 'team' ]] . copy () feature_efficiency_cols = [ 'f_I50_efficiency' , 'f_I50_efficiency_opponent' ] for feature in feature_efficiency_cols : efficiency_features [ feature ] = ( efficiency_features_multi_row . groupby ( 'team' )[ feature ] . transform ( lambda row : row . ewm ( span = 10 ) . mean () . shift ( 1 ))) # Get feature efficiency df back onto single rows efficiency_features = pd . merge ( efficiency_features , afl_data [[ 'game' , 'team' , 'home_game' ]], on = [ 'game' , 'team' ]) efficiency_features_single_row = ( efficiency_features . query ( 'home_game == 1' ) . rename ( columns = { 'team' : 'home_team' , 'f_I50_efficiency' : 'f_I50_efficiency_home' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_home' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , ( efficiency_features . query ( 'home_game == 0' ) . rename ( columns = { 'team' : 'away_team' , 'f_I50_efficiency' : 'f_I50_efficiency_away' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_away' }) . drop ( columns = 'home_game' )), on = 'game' )) efficiency_features_single_row . tail ( 5 ) game home_team f_I50_efficiency_home f_R50_efficiency_home away_team f_I50_efficiency_away f_R50_efficiency_away 1580 15394 Carlton 0.730668 0.675002 Adelaide 0.691614 0.677128 1581 15395 Sydney 0.699994 0.778280 Hawthorn 0.699158 0.673409 1582 15396 Brisbane 0.683604 0.691730 West Coast 0.696822 0.709605 1583 15397 Melbourne 0.667240 0.692632 GWS 0.684525 0.753783 1584 15398 St Kilda 0.730843 0.635819 North Melbourne 0.697018 0.654991 We will merge these features back to our features df later, when the features data frame is on a single row as well. Creating an Elo Feature \u00b6 Another feature which we could create is an Elo feature. If you don't know what Elo is, go ahead and read our article on it here . We have also written a guide on using elo to model the 2018 FIFA World Cup here . Essentially, Elo ratings increase if you win. The amount the rating increases is based on how strong the opponent is relative to the team who won. Weak teams get more points for beating stronger teams than they do for beating weaker teams, and vice versa for losses (teams lose points for losses). Mathematically, Elo ratings can also assign a probability for winning or losing based on the two Elo Ratings of the teams playing. So let's get into it. We will first define a function which calculates the elo for each team and applies these elos to our DataFrame. # Define a function which finds the elo for each team in each game and returns a dictionary with the game ID as a key and the # elos as the key's value, in a list. It also outputs the probabilities and a dictionary of the final elos for each team def elo_applier ( df , k_factor ): # Initialise a dictionary with default elos for each team elo_dict = { team : 1500 for team in df [ 'team' ] . unique ()} elos , elo_probs = {}, {} # Get a home and away dataframe so that we can get the teams on the same row home_df = df . loc [ df . home_game == 1 , [ 'team' , 'game' , 'f_margin' , 'home_game' ]] . rename ( columns = { 'team' : 'home_team' }) away_df = df . loc [ df . home_game == 0 , [ 'team' , 'game' ]] . rename ( columns = { 'team' : 'away_team' }) df = ( pd . merge ( home_df , away_df , on = 'game' ) . sort_values ( by = 'game' ) . drop_duplicates ( subset = 'game' , keep = 'first' ) . reset_index ( drop = True )) # Loop over the rows in the DataFrame for index , row in df . iterrows (): # Get the Game ID game_id = row [ 'game' ] # Get the margin margin = row [ 'f_margin' ] # If the game already has the elos for the home and away team in the elos dictionary, go to the next game if game_id in elos . keys (): continue # Get the team and opposition home_team = row [ 'home_team' ] away_team = row [ 'away_team' ] # Get the team and opposition elo score home_team_elo = elo_dict [ home_team ] away_team_elo = elo_dict [ away_team ] # Calculated the probability of winning for the team and opposition prob_win_home = 1 / ( 1 + 10 ** (( away_team_elo - home_team_elo ) / 400 )) prob_win_away = 1 - prob_win_home # Add the elos and probabilities our elos dictionary and elo_probs dictionary based on the Game ID elos [ game_id ] = [ home_team_elo , away_team_elo ] elo_probs [ game_id ] = [ prob_win_home , prob_win_away ] # Calculate the new elos of each team if margin > 0 : # Home team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 1 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 0 - prob_win_away ) elif margin < 0 : # Away team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 1 - prob_win_away ) elif margin == 0 : # Drawn game' update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0.5 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 0.5 - prob_win_away ) # Update elos in elo dictionary elo_dict [ home_team ] = new_home_team_elo elo_dict [ away_team ] = new_away_team_elo return elos , elo_probs , elo_dict # Use the elo applier function to get the elos and elo probabilities for each game - we will map these later elos , probs , elo_dict = elo_applier ( afl_data , 30 ) Great! now we have both rolling averages for stats as a feature, and the elo of the teams! Let's have a quick look at the current elo standings with a k-factor of 30, out of curiosity. for team in sorted ( elo_dict , key = elo_dict . get )[:: - 1 ]: print ( team , elo_dict [ team ]) Richmond 1695.2241513840117 Sydney 1645.548990879842 Hawthorn 1632.5266709780622 West Coast 1625.871701773721 Geelong 1625.423154644809 GWS 1597.4158602131877 Adelaide 1591.1704934545442 Collingwood 1560.370309216614 Melbourne 1558.5666572771509 Essendon 1529.0198398117086 Port Adelaide 1524.8882517820093 North Melbourne 1465.5637511922569 Western Bulldogs 1452.2110697845148 Fremantle 1393.142087030804 St Kilda 1360.9120149937303 Brisbane 1276.2923772139352 Gold Coast 1239.174528704772 Carlton 1226.6780896643265 This looks extremely similar to the currently AFL ladder, so this is a good sign for elo being an effective predictor of winning. Merging Our Features Into One Features DataFrame \u00b6 Now we need to reshape our features df so that we have all of the statistics for both teams in a game on a single row. We can then merge our elo and efficiency features to this df. # Look at our current features df features . tail ( 3 ) date game team opponent venue home_game f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP f_form_margin_btwn_teams f_form_past_5_btwn_teams 3156 2018-08-26 15397 Melbourne GWS M.C.G. 1 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.78780 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 -23.2 2.0 3157 2018-08-26 15398 North Melbourne St Kilda Docklands 0 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.54113 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3.2 3.0 3158 2018-08-26 15398 St Kilda North Melbourne Docklands 1 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.49876 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 -3.2 2.0 one_line_cols = [ 'game' , 'team' , 'home_game' ] + [ col for col in features if col . startswith ( 'f_' )] # Get all features onto individual rows for each match features_one_line = ( features . loc [ features . home_game == 1 , one_line_cols ] . rename ( columns = { 'team' : 'home_team' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , ( features . loc [ features . home_game == 0 , one_line_cols ] . drop ( columns = 'home_game' ) . rename ( columns = { 'team' : 'away_team' }) . rename ( columns = { col : col + '_away' for col in features . columns if col . startswith ( 'f_' )})), on = 'game' ) . drop ( columns = [ 'f_form_margin_btwn_teams_away' , 'f_form_past_5_btwn_teams_away' ])) # Add our created features - elo, efficiency etc. features_one_line = ( features_one_line . assign ( f_elo_home = lambda df : df . game . map ( elos ) . apply ( lambda x : x [ 0 ]), f_elo_away = lambda df : df . game . map ( elos ) . apply ( lambda x : x [ 1 ])) . pipe ( pd . merge , efficiency_features_single_row , on = [ 'game' , 'home_team' , 'away_team' ]) . pipe ( pd . merge , afl_data . loc [ afl_data . home_game == 1 , [ 'game' , 'date' , 'round' , 'venue' ]], on = [ 'game' ]) . dropna () . reset_index ( drop = True ) . assign ( season = lambda df : df . date . apply ( lambda row : row . year ))) ordered_cols = [ col for col in features_one_line if col [: 2 ] != 'f_' ] + [ col for col in features_one_line if col . startswith ( 'f_' )] feature_df = features_one_line [ ordered_cols ] Finally, let's reduce the dimensionality of the features df by subtracting the home features from the away features. This will reduce the huge amount of columns we have and make our data more manageable. To do this, we will need a list of columns which we are subtracting from each other. We will then loop over each of these columns to create our new differential columns. We will then add in the implied probability from the odds of the home and away team, as our current odds feature is simply an exponential moving average over the past n games. # Create differential df - this df is the home features - the away features diff_cols = [ col for col in feature_df . columns if col + '_away' in feature_df . columns and col != 'f_odds' and col . startswith ( 'f_' )] non_diff_cols = [ col for col in feature_df . columns if col not in diff_cols and col [: - 5 ] not in diff_cols ] diff_df = feature_df [ non_diff_cols ] . copy () for col in diff_cols : diff_df [ col + '_diff' ] = feature_df [ col ] - feature_df [ col + '_away' ] # Add current odds in to diff_df odds = get_cleaned_odds () home_odds = ( odds [ odds . home_game == 1 ] . assign ( f_current_odds_prob = lambda df : 1 / df . odds ) . rename ( columns = { 'team' : 'home_team' }) . drop ( columns = [ 'home_game' , 'odds' ])) away_odds = ( odds [ odds . home_game == 0 ] . assign ( f_current_odds_prob_away = lambda df : 1 / df . odds ) . rename ( columns = { 'team' : 'away_team' }) . drop ( columns = [ 'home_game' , 'odds' ])) diff_df = ( diff_df . pipe ( pd . merge , home_odds , on = [ 'date' , 'home_team' ]) . pipe ( pd . merge , away_odds , on = [ 'date' , 'away_team' ])) diff_df . tail () game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1626 15394 Carlton Adelaide 2018-08-25 23 Docklands 2018 6.467328 -26.2 1.0 2.066016 1230.072138 1587.776445 0.730668 0.675002 0.691614 0.677128 -3.498547 -5.527193 -26.518474 -34.473769 1.289715 0.217006 7.955295 -341.342677 -9.317269 3.088569 -2.600593 15.192839 -12.518345 -4.136673 -41.855717 -72.258378 -51.998775 9.499447 8.670917 -6.973088 -4.740623 -26.964945 -13.147675 -23.928700 -28.940883 -45.293433 -15.183406 -1900.784014 -0.362402 -1.314627 4.116133 -294.813511 -9.917793 -34.724925 -5.462844 -9.367141 -19.623785 -38.188082 0.187709 0.816860 1627 15395 Sydney Hawthorn 2018-08-25 23 S.C.G. 2018 2.128611 1.0 2.0 1.777290 1662.568452 1615.507209 0.699994 0.778280 0.699158 0.673409 -1.756730 -0.874690 -11.415069 -15.575319 0.014390 4.073909 4.160250 -174.005092 -0.942357 -4.078635 -4.192916 7.814496 -2.225780 6.215760 15.042979 -34.894261 -50.615255 4.214158 0.683548 -3.535594 -3.168608 -12.068691 -30.493980 -9.867332 2.588103 -22.825570 -5.604199 253.086090 -2.697132 -22.612327 25.340623 -90.812188 1.967104 -31.047879 0.007606 -6.880120 11.415593 -49.957313 0.440180 0.561924 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 Wrapping it Up \u00b6 We now have a fairly decent amount of features. Some other features which could be added include whether the game is in a major Capital city outisde of Mebourne (i.e. Sydney, Adelaide or Peth), how many 'Elite' players are playing (which could be judged by average SuperCoach scores over 110, for example), as well as your own metrics for attacking and defending. Note that all of our features have columns starting with 'f_' so in the section, we will grab this feature dataframe and use these features to sport predicting the matches. 03. Modelling \u00b6 These tutorials will walk you through how to construct your own basic AFL model, using publically available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through modelling our AFL data to create predictions. We will train a variety of quick and easy models to get a feel of what works and what doesn't. We will then tune our hyperparameters so that we are ready to make week by week predictions. Grabbing Our Dataset \u00b6 First, we will import our required modules, as well as the prepare_afl_features function which we created in our afl_feature_creation script. This essentially creates some basic features for us so that we can get started on the modelling component. # Import libraries from afl_data_cleaning_v2 import * import datetime import pandas as pd import numpy as np from sklearn import svm , tree , linear_model , neighbors , naive_bayes , ensemble , discriminant_analysis , gaussian_process # from xgboost import XGBClassifier from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV , train_test_split from sklearn.linear_model import LogisticRegressionCV from sklearn.feature_selection import RFECV import seaborn as sns from sklearn.preprocessing import OneHotEncoder , LabelEncoder , StandardScaler from sklearn import feature_selection from sklearn import metrics from sklearn.linear_model import LogisticRegression , RidgeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB import warnings warnings . filterwarnings ( 'ignore' ) import afl_feature_creation_v2 import afl_data_cleaning_v2 # Grab our feature DataFrame which we created in the previous tutorial feature_df = afl_feature_creation_v2 . prepare_afl_features () afl_data = afl_data_cleaning_v2 . prepare_afl_data () feature_df . tail ( 3 ) game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 # Get the result and merge to the feature_df match_results = ( pd . read_csv ( \"data/afl_match_results.csv\" ) . rename ( columns = { 'Game' : 'game' }) . assign ( result = lambda df : df . apply ( lambda row : 1 if row [ 'Home.Points' ] > row [ 'Away.Points' ] else 0 , axis = 1 ))) # Merge result column to feature_df feature_df = pd . merge ( feature_df , match_results [[ 'game' , 'result' ]], on = 'game' ) Creating a Training and Testing Set \u00b6 So that we don't train our data on the data that we will later test our model on, we will create separate train and test sets. For this exercise we will use the 2018 season to test how our model performs, whilst the rest of the data can be used to train the model. # Create our test and train sets from our afl DataFrame; drop the columns which leak the result, duplicates, and the advanced # stats which don't have data until 2015 feature_columns = [ col for col in feature_df if col . startswith ( 'f_' )] # Create our test set test_x = feature_df . loc [ feature_df . season == 2018 , [ 'game' ] + feature_columns ] test_y = feature_df . loc [ feature_df . season == 2018 , 'result' ] # Create our train set X = feature_df . loc [ feature_df . season != 2018 , [ 'game' ] + feature_columns ] y = feature_df . loc [ feature_df . season != 2018 , 'result' ] # Scale features scaler = StandardScaler () X [ feature_columns ] = scaler . fit_transform ( X [ feature_columns ]) test_x [ feature_columns ] = scaler . transform ( test_x [ feature_columns ]) Using Cross Validation to Find The Best Algorithms \u00b6 Now that we have our training set, we can run through a list of popular classifiers to determine which classifier is best for modelling our data. To do this we will create a function which uses Kfold cross-validation to find the 'best' algorithms, based on how accurate the algorithms' predictions are. This function will take in a list of classifiers, which we will define below, as well as the training set and it's outcome, and output a DataFrame with the mean and std of the accuracy of each algorithm. Let's jump into it! # Create a list of standard classifiers classifiers = [ #Ensemble Methods ensemble . AdaBoostClassifier (), ensemble . BaggingClassifier (), ensemble . ExtraTreesClassifier (), ensemble . GradientBoostingClassifier (), ensemble . RandomForestClassifier (), #Gaussian Processes gaussian_process . GaussianProcessClassifier (), #GLM linear_model . LogisticRegressionCV (), #Navies Bayes naive_bayes . BernoulliNB (), naive_bayes . GaussianNB (), #SVM svm . SVC ( probability = True ), svm . NuSVC ( probability = True ), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis (), discriminant_analysis . QuadraticDiscriminantAnalysis (), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # XGBClassifier() ] # Define a functiom which finds the best algorithms for our modelling task def find_best_algorithms ( classifier_list , X , y ): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold ( n_splits = 5 ) # Grab the cross validation scores for each algorithm cv_results = [ cross_val_score ( classifier , X , y , scoring = \"neg_log_loss\" , cv = kfold ) for classifier in classifier_list ] cv_means = [ cv_result . mean () * - 1 for cv_result in cv_results ] cv_std = [ cv_result . std () for cv_result in cv_results ] algorithm_names = [ alg . __class__ . __name__ for alg in classifiers ] # Create a DataFrame of all the CV results cv_results = pd . DataFrame ({ \"Mean Log Loss\" : cv_means , \"Log Loss Std\" : cv_std , \"Algorithm\" : algorithm_names }) return cv_results . sort_values ( by = 'Mean Log Loss' ) . reset_index ( drop = True ) best_algos = find_best_algorithms ( classifiers , X , y ) best_algos Mean Log Loss Log Loss Std Algorithm 0 0.539131 3.640578e-02 LogisticRegressionCV 1 0.551241 5.775685e-02 LinearDiscriminantAnalysis 2 0.630994 8.257481e-02 GradientBoostingClassifier 3 0.670041 9.205780e-03 AdaBoostClassifier 4 0.693147 2.360121e-08 GaussianProcessClassifier 5 0.712537 2.770864e-02 SVC 6 0.712896 2.440755e-02 NuSVC 7 0.836191 2.094224e-01 ExtraTreesClassifier 8 0.874307 1.558144e-01 RandomForestClassifier 9 1.288174 3.953037e-01 BaggingClassifier 10 1.884019 4.769589e-01 QuadraticDiscriminantAnalysis 11 2.652161 6.886897e-01 BernoulliNB 12 3.299651 6.427551e-01 GaussianNB # Try a logistic regression model and see how it performs in terms of accuracy kfold = StratifiedKFold ( n_splits = 5 ) cv_scores = cross_val_score ( linear_model . LogisticRegressionCV (), X , y , scoring = 'accuracy' , cv = kfold ) cv_scores . mean () 0.7452268937025035 Choosing Our Algorithms \u00b6 As we can see from above, there are some pretty poor algorithms for predicting the winner. On the other hand, whilst attaining an accuracy of 74.5% (at the time of writing) may seem like a decent result; we must first establish a baseline to judge our performance on. In this case, we will have two baselines; the proportion of games won by the home team and what the odds predict. If we can beat the odds we have created a very powerful model. Note that a baseline for the log loss can also be both the odds log loss and randomly guessing. Randomly guessing between two teams attains a log loss of log(2) = 0.69, so we have beaten this result. Once we establish our baseline, we will choose the top algorithms from above and tune their hyperparameters, as well as automatically selecting the best features to be used in our model. Defining Our Baseline \u00b6 As stated above, we must define our baseline so that we have a measure to beat. We will use the proportion of games won by the home team, as well as the proportion of favourites who won, based off the odds. To establish this baseline we will use our feature_df, as this has no dropped rows. # Find the percentage chance of winning at home in each season. afl_data = afl_data_cleaning_v2 . prepare_afl_data () afl_data [ 'home_win' ] = afl_data . apply ( lambda x : 1 if x [ 'f_margin' ] > 0 else 0 , axis = 1 ) home_games = afl_data [ afl_data [ 'home_game' ] == 1 ] home_games [[ \"home_win\" , 'season' ]] . groupby ([ 'season' ]) . mean () season home_win 2011 0.561856 2012 0.563725 2013 0.561576 2014 0.574257 2015 0.539604 2016 0.606742 2017 0.604061 2018 0.540404 # Find the proportion of favourites who have won # Define a function which finds if the odds correctly guessed the response def find_odds_prediction ( a_row ): if a_row [ 'f_odds' ] <= a_row [ 'f_odds_away' ] and a_row [ 'home_win' ] == 1 : return 1 elif a_row [ 'f_odds_away' ] < a_row [ 'f_odds' ] and a_row [ 'home_win' ] == 0 : return 1 else : return 0 # Define a function which splits our DataFrame so each game is on one row instead of two def get_df_on_one_line ( df ): cols_to_drop = [ 'date' , 'home_game' , 'opponent' , 'f_opponent_behinds' , 'f_opponent_goals' , 'f_opponent_points' , 'f_points' , 'round' , 'venue' , 'season' ] home_df = df [ df [ 'home_game' ] == 1 ] . rename ( columns = { 'team' : 'home_team' }) away_df = df [ df [ 'home_game' ] == 0 ] . rename ( columns = { 'team' : 'away_team' }) away_df = away_df . drop ( columns = cols_to_drop ) # Rename away_df columns away_df_renamed = away_df . rename ( columns = { col : col + '_away' for col in away_df . columns if col != 'game' }) merged_df = pd . merge ( home_df , away_df_renamed , on = 'game' ) merged_df [ 'home_win' ] = merged_df . f_margin . apply ( lambda x : 1 if x > 0 else 0 ) return merged_df afl_data_one_line = get_df_on_one_line ( afl_data ) afl_data_one_line [ 'odds_prediction' ] = afl_data_one_line . apply ( find_odds_prediction , axis = 1 ) print ( 'The overall mean accuracy of choosing the favourite based on the odds is {} %' . format ( round ( afl_data_one_line [ 'odds_prediction' ] . mean () * 100 , 2 ))) afl_data_one_line [[ \"odds_prediction\" , 'season' ]] . groupby ([ 'season' ]) . mean () The overall mean accuracy of choosing the favourite based on the odds is 73.15% season odds_prediction 2011 0.784615 2012 0.774510 2013 0.748768 2014 0.727723 2015 0.727723 2016 0.713483 2017 0.659898 2018 0.712121 ## Get a baseline log loss score from the odds afl_data_one_line [ 'odds_home_prob' ] = 1 / afl_data_one_line . f_odds afl_data_one_line [ 'odds_away_prob' ] = 1 / afl_data_one_line . f_odds_away metrics . log_loss ( afl_data_one_line . home_win , afl_data_one_line [[ 'odds_away_prob' , 'odds_home_prob' ]]) 0.5375306549682837 We can see that the odds are MUCH more accurate than just choosing the home team to win. We can also see that the mean accuracy of choosing the favourite is around 73%. That means that this is the score we need to beat. Similarly, the log loss of the odds is around 0.5385, whilst our model scores around 0.539 (at the time of writing), without hyperparamter optimisation. Let's choose only the algorithms with log losses below 0.67 chosen_algorithms = best_algos . loc [ best_algos [ 'Mean Log Loss' ] < 0.67 , 'Algorithm' ] . tolist () chosen_algorithms [ 'LogisticRegressionCV' , 'LinearDiscriminantAnalysis' , 'GradientBoostingClassifier' ] Using Grid Search To Tune Hyperparameters \u00b6 Now that we have our best models, we can use Grid Search to optimise our hyperparameters. Grid search basically involves searching through a range of different algorithm hyperparameters, and choosing those which result in the best score from some metrics, which in our case is accuracy. Let's do this for the algorithms which have hyperparameters which can be tuned. Note that if you are running this on your own computer it may take up to 10 minutes. # Define a function which optimises the hyperparameters of our chosen algorithms def optimise_hyperparameters ( train_x , train_y , algorithms , parameters ): kfold = StratifiedKFold ( n_splits = 5 ) best_estimators = [] for alg , params in zip ( algorithms , parameters ): gs = GridSearchCV ( alg , param_grid = params , cv = kfold , scoring = 'neg_log_loss' , verbose = 1 ) gs . fit ( train_x , train_y ) best_estimators . append ( gs . best_estimator_ ) return best_estimators # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.001 , 0.01 , 0.05 , 0.2 , 0.5 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } # Add our algorithms and parameters to lists to be used in our function alg_list = [ LogisticRegression ()] param_list = [ lr_grid ] # Find the best estimators, then add our other estimators which don't need optimisation best_estimators = optimise_hyperparameters ( X , y , alg_list , param_list ) Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=1)]: Done 90 out of 90 | elapsed: 5.2s finished lr_best_params = best_estimators [ 0 ] . get_params () lr_best_params { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } kfold = StratifiedKFold ( n_splits = 10 ) cv_scores = cross_val_score ( linear_model . LogisticRegression ( ** lr_best_params ), X , y , scoring = 'neg_log_loss' , cv = kfold ) cv_scores . mean () - 0.528741673153639 In the next iteration of this tutorial we will also optimise an XGB model and hopefully outperform our logistic regression model. Creating Predictions for the 2018 Season \u00b6 Now that we have an optimised logistic regression model, let's see how it performs on predicting the 2018 season. lr = LogisticRegression ( ** lr_best_params ) lr . fit ( X , y ) final_predictions = lr . predict ( test_x ) accuracy = ( final_predictions == test_y ) . mean () * 100 print ( \"Our accuracy in predicting the 2018 season is: {:.2f} %\" . format ( accuracy )) Our accuracy in predicting the 2018 season is: 67.68% Now let's have a look at all the games which we incorrectly predicted. game_ids = test_x [( final_predictions != test_y )] . game afl_data_one_line . loc [ afl_data_one_line . game . isin ( game_ids ), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] date home_team opponent f_odds f_odds_away f_margin 1386 2018-03-24 Gold Coast North Melbourne 2.0161 1.9784 16 1388 2018-03-25 Melbourne Geelong 1.7737 2.2755 -3 1391 2018-03-30 North Melbourne St Kilda 3.5769 1.3867 52 1392 2018-03-31 Carlton Gold Coast 1.5992 2.6620 -34 1396 2018-04-01 Western Bulldogs West Coast 1.8044 2.2445 -51 1397 2018-04-01 Sydney Port Adelaide 1.4949 3.0060 -23 1398 2018-04-02 Geelong Hawthorn 1.7597 2.3024 -1 1406 2018-04-08 Western Bulldogs Essendon 3.8560 1.3538 21 1408 2018-04-13 Adelaide Collingwood 1.2048 5.9197 -48 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1415 2018-04-15 Hawthorn Melbourne 2.2855 1.7772 67 1417 2018-04-20 Sydney Adelaide 1.2640 4.6929 -10 1420 2018-04-21 Port Adelaide Geelong 1.5053 2.9515 -34 1422 2018-04-22 North Melbourne Hawthorn 2.6170 1.6132 28 1423 2018-04-22 Brisbane Gold Coast 1.7464 2.3277 -5 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1427 2018-04-28 Geelong Sydney 1.5019 2.9833 -17 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 1439 2018-05-05 Sydney North Melbourne 1.2777 4.5690 -2 1444 2018-05-11 Hawthorn Sydney 1.6283 2.5818 -8 1445 2018-05-12 GWS West Coast 1.5425 2.8292 -25 1446 2018-05-12 Carlton Essendon 3.1742 1.4570 13 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1456 2018-05-19 Essendon Geelong 5.6530 1.2104 34 1460 2018-05-20 Brisbane Hawthorn 3.2891 1.4318 56 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1466 2018-05-26 GWS Essendon 1.4364 3.2652 -35 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 ... ... ... ... ... ... ... 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 1485 2018-06-11 Melbourne Collingwood 1.6034 2.6450 -42 1492 2018-06-21 West Coast Essendon 1.3694 3.6843 -28 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1499 2018-06-29 Western Bulldogs Geelong 6.2067 1.1889 2 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1504 2018-07-01 Melbourne St Kilda 1.1405 7.7934 -2 1505 2018-07-01 Essendon North Melbourne 2.0993 1.9022 17 1506 2018-07-01 Fremantle Brisbane 1.2914 4.3743 -55 1507 2018-07-05 Sydney Geelong 1.7807 2.2675 -12 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1516 2018-07-12 Adelaide Geelong 2.0517 1.9444 15 1518 2018-07-14 Hawthorn Brisbane 1.2281 5.4105 -33 1521 2018-07-14 GWS Richmond 2.7257 1.5765 2 1522 2018-07-15 Collingwood West Coast 1.5600 2.7815 -35 1523 2018-07-15 North Melbourne Sydney 1.9263 2.0647 -6 1524 2018-07-15 Fremantle Port Adelaide 5.9110 1.2047 9 1527 2018-07-21 Sydney Gold Coast 1.0342 27.8520 -24 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 1533 2018-07-22 Port Adelaide GWS 1.6480 2.5452 -22 1538 2018-07-28 Gold Coast Carlton 1.3933 3.5296 -35 1546 2018-08-04 Adelaide Port Adelaide 2.0950 1.9135 3 1548 2018-08-04 St Kilda Western Bulldogs 1.6120 2.6368 -35 1555 2018-08-11 Port Adelaide West Coast 1.4187 3.3505 -4 1558 2018-08-12 North Melbourne Western Bulldogs 1.3175 4.1239 -7 1559 2018-08-12 Melbourne Sydney 1.3627 3.7445 -9 1564 2018-08-18 GWS Sydney 1.8478 2.1672 -20 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 Very interesting! Most of the games we got wrong were upsets. Let's have a look at the games we incorrectly predicted that weren't upsets. ( afl_data_one_line . loc [ afl_data_one_line . game . isin ( game_ids ), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] . assign ( home_favourite = lambda df : df . apply ( lambda row : 1 if row . f_odds < row . f_odds_away else 0 , axis = 1 )) . assign ( upset = lambda df : df . apply ( lambda row : 1 if row . home_favourite == 1 and row . f_margin < 0 else ( 1 if row . home_favourite == 0 and row . f_margin > 0 else 0 ), axis = 1 )) . query ( 'upset == 0' )) date home_team opponent f_odds f_odds_away f_margin home_favourite upset 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1 0 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1 0 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 0 0 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 0 0 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 0 0 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1 0 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1 0 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 0 0 1479 2018-06-08 Port Adelaide Richmond 1.7422 2.3420 14 1 0 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 0 0 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1 0 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1 0 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 0 0 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1 0 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 0 0 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 0 0 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 0 0 Let's now look at our model's log loss for the 2018 season compared to the odds. predictions_probs = lr . predict_proba ( test_x ) metrics . log_loss ( test_y , predictions_probs ) 0.584824211055384 test_x_unscaled = feature_df . loc [ feature_df . season == 2018 , [ 'game' ] + feature_columns ] metrics . log_loss ( test_y , test_x_unscaled [[ 'f_current_odds_prob_away' , 'f_current_odds_prob' ]]) 0.5545776633924343 So whilst our model performs decently, it doesn't beat the odds in terms of log loss. That's okay, it's still a decent start. In future iterations we can implement other algorithms and create new features which may improve performance. Next Steps \u00b6 Now that we have a model up and running, the next steps are to implement the model on a week to week basis. 04. Weekly Predictions \u00b6 Now that we have explored different algorithms for modelling, we can implement our chosen model and predict this week's AFL games! All you need to do is run the afl_modelling script each Thursday or Friday to predict the following week's games. # Import Modules from afl_feature_creation_v2 import prepare_afl_features import afl_data_cleaning_v2 import afl_feature_creation_v2 import afl_modelling_v2 import datetime import pandas as pd import numpy as np pd . set_option ( 'display.max_columns' , None ) from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler import warnings warnings . filterwarnings ( 'ignore' ) Creating The Features For This Weekend's Games \u00b6 To actually predict this weekend's games, we need to create the same features that we have created in the previous tutorials for the games that will be played this weekend. This includes all the rolling averages, efficiency features, elo features etc. So the majority of this tutorial will be using previously defined functions to create features for the following weekend's games. Create Next Week's DataFrame \u00b6 Let's first get our cleaned afl_data dataset, as well as the odds for next weekend and the 2018 fixture. # Grab the cleaned AFL dataset and the column order afl_data = afl_data_cleaning_v2 . prepare_afl_data () ordered_cols = afl_data . columns # Define a function which grabs the odds for each game for the following weekend def get_next_week_odds ( path ): # Get next week's odds next_week_odds = pd . read_csv ( path ) next_week_odds = next_week_odds . rename ( columns = { \"team_1\" : \"home_team\" , \"team_2\" : \"away_team\" , \"team_1_odds\" : \"odds\" , \"team_2_odds\" : \"odds_away\" }) return next_week_odds # Import the fixture # Define a function which gets the fixture and cleans it up def get_fixture ( path ): # Get the afl fixture fixture = pd . read_csv ( path ) # Replace team names and reformat fixture = fixture . replace ({ 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' }) fixture [ 'Date' ] = pd . to_datetime ( fixture [ 'Date' ]) . dt . date . astype ( str ) fixture = fixture . rename ( columns = { \"Home.Team\" : \"home_team\" , \"Away.Team\" : \"away_team\" }) return fixture next_week_odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) fixture = get_fixture ( \"data/afl_fixture_2018.csv\" ) fixture . tail () Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG next_week_odds home_team away_team odds odds_away 0 West Coast Collingwood 2.34 1.75 Now that we have these DataFrames, we will define a function which combines the fixture and next week's odds to create a single DataFrame for the games over the next 7 days. To use this function we will need Game IDs for next week. So we will create another function which creates Game IDs by using the Game ID from the last game played and adding 1 to it. # Define a function which creates game IDs for this week's footy games def create_next_weeks_game_ids ( afl_data ): odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) # Get last week's Game ID last_afl_data_game = afl_data [ 'game' ] . iloc [ - 1 ] # Create Game IDs for next week game_ids = [( i + 1 ) + last_afl_data_game for i in range ( odds . shape [ 0 ])] return game_ids # Define a function which creates this week's footy game DataFrame def get_next_week_df ( afl_data ): # Get the fixture and the odds for next week's footy games fixture = get_fixture ( \"data/afl_fixture_2018.csv\" ) next_week_odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) next_week_odds [ 'game' ] = create_next_weeks_game_ids ( afl_data ) # Get today's date and next week's date and create a DataFrame for next week's games # todays_date = datetime.datetime.today().strftime('%Y-%m-%d') # date_in_7_days = (datetime.datetime.today() + datetime.timedelta(days=7)).strftime('%Y-%m-%d') todays_date = '2018-09-27' date_in_7_days = '2018-10-04' fixture = fixture [( fixture [ 'Date' ] >= todays_date ) & ( fixture [ 'Date' ] < date_in_7_days )] . drop ( columns = [ 'Season.Game' ]) next_week_df = pd . merge ( fixture , next_week_odds , on = [ 'home_team' , 'away_team' ]) # Split the DataFrame onto two rows for each game h_df = ( next_week_df [[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds' , 'Season' , 'Round' , 'Venue' ]] . rename ( columns = { 'home_team' : 'team' , 'away_team' : 'opponent' }) . assign ( home_game = 1 )) a_df = ( next_week_df [[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds_away' , 'Season' , 'Round' , 'Venue' ]] . rename ( columns = { 'odds_away' : 'odds' , 'home_team' : 'opponent' , 'away_team' : 'team' }) . assign ( home_game = 0 )) next_week = a_df . append ( h_df ) . sort_values ( by = 'game' ) . rename ( columns = { 'Date' : 'date' , 'Season' : 'season' , 'Round' : 'round' , 'Venue' : 'venue' }) next_week [ 'date' ] = pd . to_datetime ( next_week . date ) next_week [ 'round' ] = afl_data [ 'round' ] . iloc [ - 1 ] + 1 return next_week next_week_df = get_next_week_df ( afl_data ) game_ids_next_round = create_next_weeks_game_ids ( afl_data ) next_week_df date round season venue game home_game odds opponent team 0 2018-09-29 27 2018 MCG 15407 0 1.75 West Coast Collingwood 0 2018-09-29 27 2018 MCG 15407 1 2.34 Collingwood West Coast fixture . tail () Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG Create Each Feature \u00b6 Now let's append next week's DataFrame to our afl_data, match_results and odds DataFrames and then create all the features we used in the AFL Feature Creation Tutorial . We need to append the games and then feed them into our function so that we can create features for upcoming games. # Append next week's games to our afl_data DataFrame afl_data = afl_data . append ( next_week_df ) . reset_index ( drop = True ) # Append next week's games to match results (we need to do this for our feature creation to run) match_results = afl_data_cleaning_v2 . get_cleaned_match_results () . append ( next_week_df ) # Append next week's games to odds odds = ( afl_data_cleaning_v2 . get_cleaned_odds () . pipe ( lambda df : df . append ( next_week_df [ df . columns ])) . reset_index ( drop = True )) features_df = afl_feature_creation_v2 . prepare_afl_features ( afl_data = afl_data , match_results = match_results , odds = odds ) features_df . tail () game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_GA1_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_Unnamed: 0_diff f_behinds_diff f_goals_diff f_margin_diff f_opponent_behinds_diff f_opponent_goals_diff f_opponent_points_diff f_points_diff f_current_odds_prob f_current_odds_prob_away 1065 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.966936 -23.2 2.0 1.813998 1523.456734 1609.444874 0.653525 0.680168 0.704767 0.749812 140.535514 0.605144 -9.771981 5.892176 7.172376 6.614609 -1.365211 30.766262 21.998618 0.067228 -1.404730 -3.166732 6.933998 6.675576 0.000000 38.708158 24.587333 12.008987 10.482382 -16.709540 -15.415060 289.188486 6.350287 -2.263536 -20.966818 50.388632 0.723637 15.537783 22.912269 2.065039 10.215523 -6.689429 3259.163465 -0.136383 3.553795 16.563721 -2.353514 1.162696 4.622664 21.186385 0.661551 0.340379 1066 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.089084 -3.2 2.0 2.577161 1397.237139 1499.366007 0.725980 0.655749 0.723949 0.677174 51.799992 3.399035 6.067393 -2.189489 -10.475859 1.154766 -8.883840 -21.810962 33.058382 40.618410 2.286314 -0.345734 -3.778445 -2.182673 0.000000 19.816372 -21.562916 2.678384 -14.777698 13.242010 12.065594 -82.381996 -2.176564 2.335825 -4.952336 45.719406 3.344217 -2.095613 -3.929084 -3.182381 -12.832197 57.226776 -20221.371526 1.968709 -1.897958 -15.177001 1.067099 0.781811 5.757963 -9.419038 0.284269 0.717566 1067 15404 Collingwood GWS 2018-09-15 25 M.C.G. 2018 1.882301 12.6 3.0 2.018344 1546.000498 1590.806454 0.693185 0.706222 0.718446 0.727961 205.916671 -1.642954 -2.980828 -0.266023 8.547225 -3.751909 -0.664977 10.563513 48.175985 43.531908 -5.836979 5.388668 4.395675 2.555152 0.000000 51.588962 11.558254 4.276481 11.284445 -3.412977 -2.206815 -234.577304 2.637758 -10.537765 -11.127876 125.607377 -3.485896 3.532031 15.102292 -2.500685 8.187543 38.053445 12500.525732 -1.006173 2.520135 18.634835 -2.159882 -0.393386 -4.520198 14.114637 0.608495 0.393856 1068 15406 West Coast Melbourne 2018-09-22 26 Perth Stadium 2018 2.013572 21.2 3.0 1.884148 1577.888606 1542.095154 0.688877 0.708941 0.649180 0.698319 -118.135184 -3.005709 2.453190 -5.103869 -14.368949 -12.245458 2.771411 -45.364271 -60.210182 -24.049523 -2.791277 6.115918 -5.041030 -5.335746 0.000000 -78.816902 -18.784547 -13.957754 -5.527613 18.606721 25.366778 -910.988860 -5.515812 -9.483590 8.914093 -131.380758 -7.142529 -49.484957 -13.718798 -4.862994 -9.834616 -23.673638 -3178.282073 -1.785349 -2.569957 -20.008787 0.476202 0.387915 2.803694 -17.205093 0.543774 0.457875 1069 15407 West Coast Collingwood 2018-09-29 27 MCG 2018 1.981832 17.2 3.0 1.838864 1591.348723 1562.924273 0.679011 0.724125 0.711352 0.709346 159.522670 0.893421 -0.475725 3.391070 -5.088751 5.875388 5.352234 7.729063 -7.358202 -4.719968 6.113565 4.822252 2.871241 2.690270 3.636364 -64.238180 -0.631102 2.078832 6.005613 56.879978 34.373271 1016.491933 1.199751 2.454685 12.197047 219.666562 2.484363 0.379162 2.566991 0.639666 2.258377 -23.841529 -368920.360240 -0.646160 0.892051 3.040850 1.589568 0.012622 1.665299 4.706148 0.427350 0.571429 Create Predictions For the Upcoming Round \u00b6 Now that we have our features, we can use our model that we created in part 3 to predict the next round. First we need to filter our features_df into a training df and a df with next round's features/matches. Then we can use the model created in the last tutorial to create predictions. For simplicity, I have hardcoded the parameters we used in the last tutorial. # Get the train df by only taking the games IDs which aren't in the next week df train_df = features_df [ ~ features_df . game . isin ( next_week_df . game )] # Get the result and merge to the feature_df match_results = ( pd . read_csv ( \"data/afl_match_results.csv\" ) . rename ( columns = { 'Game' : 'game' }) . assign ( result = lambda df : df . apply ( lambda row : 1 if row [ 'Home.Points' ] > row [ 'Away.Points' ] else 0 , axis = 1 ))) train_df = pd . merge ( train_df , match_results [[ 'game' , 'result' ]], on = 'game' ) train_x = train_df . drop ( columns = [ 'result' ]) train_y = train_df . result next_round_x = features_df [ features_df . game . isin ( next_week_df . game )] # Fit out logistic regression model - note that our predictions come out in the order of [away_team_prob, home_team_prob] lr_best_params = { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } feature_cols = [ col for col in train_df if col . startswith ( 'f_' )] # Scale features scaler = StandardScaler () train_x [ feature_cols ] = scaler . fit_transform ( train_x [ feature_cols ]) next_round_x [ feature_cols ] = scaler . transform ( next_round_x [ feature_cols ]) lr = LogisticRegression ( ** lr_best_params ) lr . fit ( train_x [ feature_cols ], train_y ) prediction_probs = lr . predict_proba ( next_round_x [ feature_cols ]) modelled_home_odds = [ 1 / i [ 1 ] for i in prediction_probs ] modelled_away_odds = [ 1 / i [ 0 ] for i in prediction_probs ] # Create a predictions df preds_df = ( next_round_x [[ 'date' , 'home_team' , 'away_team' , 'venue' , 'game' ]] . copy () . assign ( modelled_home_odds = modelled_home_odds , modelled_away_odds = modelled_away_odds ) . pipe ( pd . merge , next_week_odds , on = [ 'home_team' , 'away_team' ]) . pipe ( pd . merge , features_df [[ 'game' , 'f_elo_home' , 'f_elo_away' ]], on = 'game' ) . drop ( columns = 'game' ) ) preds_df date home_team away_team venue modelled_home_odds modelled_away_odds odds odds_away f_elo_home f_elo_away 0 2018-09-29 West Coast Collingwood MCG 2.326826 1.753679 2.34 1.75 1591.348723 1562.924273 Alternatively, if you want to generate predictions using a script which uses all the above code, just run the following: print ( afl_modelling_v2 . create_predictions ()) date home_team away_team venue modelled_home_odds \\ 0 2018-09-29 West Coast Collingwood MCG 2.326826 modelled_away_odds odds odds_away f_elo_home f_elo_away 0 1.753679 2.34 1.75 1591.348723 1562.924273 Conclusion \u00b6 Congratulations! You have created AFL predictions for this week. If you are beginner to this, don't be overwhelmed. The process gets easier each time you do it. And it is super rewarding. In future iterations we will update this tutorial to predict actual odds, and then integrate this with Betfair's API so that you can create an automated betting strategy using Machine Learning to create your predictions! Disclaimer \u00b6 Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"AFL modelling walkthrough in Python"},{"location":"modelling/AFLmodellingPython/#afl-modelling-walkthrough","text":"","title":"AFL Modelling Walkthrough"},{"location":"modelling/AFLmodellingPython/#01-data-cleaning","text":"These tutorials will walk you through how to construct your own basic AFL model, using publicly available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through the basics of cleaning this dataset and how we have done it. If you want to get straight to feature creation or modelling, feel free to jump ahead! # Import libraries import pandas as pd import numpy as np import re pd . set_option ( 'display.max_columns' , None ) We will first explore the DataFrames, and then create functions to wrangle them and clean them into more consistent sets of data. # Read/clean each DataFrame match_results = pd . read_csv ( \"data/afl_match_results.csv\" ) odds = pd . read_csv ( \"data/afl_odds.csv\" ) player_stats = pd . read_csv ( \"data/afl_player_stats.csv\" ) odds . tail ( 3 ) trunc event_name path selection_name odds 4179 2018-09-01 Match Odds VFL/Richmond Reserves v Williamstown Williamstown 2.3878 4180 2018-09-01 Match Odds WAFL/South Fremantle v West Perth South Fremantle 1.5024 4181 2018-09-01 Match Odds WAFL/South Fremantle v West Perth West Perth 2.7382 match_results . tail ( 3 ) Game Date Round Home.Team Home.Goals Home.Behinds Home.Points Away.Team Away.Goals Away.Behinds Away.Points Venue Margin Season Round.Type Round.Number 15395 15396 2018-08-26 R23 Brisbane Lions 11 6 72 West Coast 14 14 98 Gabba -26 2018 Regular 23 15396 15397 2018-08-26 R23 Melbourne 15 12 102 GWS 8 9 57 M.C.G. 45 2018 Regular 23 15397 15398 2018-08-26 R23 St Kilda 14 10 94 North Melbourne 17 15 117 Docklands -23 2018 Regular 23 player_stats . tail ( 3 ) AF B BO CCL CG CL CM CP D DE Date ED FA FF G GA HB HO I50 ITC K M MG MI5 Match_id One.Percenters Opposition Player R50 Round SC SCL SI Season Status T T5 TO TOG Team UP Venue 89317 38 1 0 0.0 0 0 1 2 9 55.6 25/08/2018 5 0 0 0 0 3 0 0 2.0 6 3 132.0 2 9711 0 Fremantle Christopher Mayne 1 Round 23 35 0.0 2.0 2018 Away 1 0.0 1.0 57 Collingwood 7 Optus Stadium 89318 38 0 0 0.0 3 0 0 3 9 55.6 25/08/2018 5 0 1 0 0 3 0 0 4.0 6 3 172.0 0 9711 2 Fremantle Nathan Murphy 5 Round 23 29 0.0 0.0 2018 Away 1 0.0 3.0 70 Collingwood 6 Optus Stadium 89319 56 1 0 0.0 1 0 0 3 8 62.5 25/08/2018 5 0 0 2 0 2 0 0 2.0 6 3 180.0 3 9711 2 Fremantle Jaidyn Stephenson 0 Round 23 56 0.0 4.0 2018 Away 3 1.0 2.0 87 Collingwood 5 Optus Stadium Have a look at the structure of the DataFrames. Notice that for the odds DataFrame, each game is split between two rows, whilst for the match_results each game is on one row. We will have to get around this by splitting the games up onto two rows, as this will allow our feature transformation functions to be applied more easily later on. For the player_stats DataFrame we will aggregate these stats into each game on separate rows. First, we will write functions to make the odds data look a bit nicer, with only a team column, a date column and a 'home_game' column which takes the values 0 or 1 depending on if it was a home game for that team. To do this we will use the regex module to extract the team names from the path column, as well as the to_datetime function from pandas. We will also replace all the inconsistent team names with consistent team names. def get_cleaned_odds ( df = None ): # If a df hasn't been specified as a parameter, read the odds df if df is None : df = pd . read_csv ( \"data/afl_odds.csv\" ) # Get a dictionary of team names we want to change and their new values team_name_mapping = { 'Adelaide Crows' : 'Adelaide' , 'Brisbane Lions' : 'Brisbane' , 'Carlton Blues' : 'Carlton' , 'Collingwood Magpies' : 'Collingwood' , 'Essendon Bombers' : 'Essendon' , 'Fremantle Dockers' : 'Fremantle' , 'GWS Giants' : 'GWS' , 'Geelong Cats' : 'Geelong' , 'Gold Coast Suns' : 'Gold Coast' , 'Greater Western Sydney' : 'GWS' , 'Greater Western Sydney Giants' : 'GWS' , 'Hawthorn Hawks' : 'Hawthorn' , 'Melbourne Demons' : 'Melbourne' , 'North Melbourne Kangaroos' : 'North Melbourne' , 'Port Adelaide Magpies' : 'Port Adelaide' , 'Port Adelaide Power' : 'Port Adelaide' , 'P Adelaide' : 'Port Adelaide' , 'Richmond Tigers' : 'Richmond' , 'St Kilda Saints' : 'St Kilda' , 'Sydney Swans' : 'Sydney' , 'West Coast Eagles' : 'West Coast' , 'Wetsern Bulldogs' : 'Western Bulldogs' , 'Western Bullbogs' : 'Western Bulldogs' } # Add columns df = ( df . assign ( date = lambda df : pd . to_datetime ( df . trunc ), # Create a datetime column home_team = lambda df : df . path . str . extract ( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 1 ] . str . strip (), away_team = lambda df : df . path . str . extract ( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 2 ] . str . strip ()) . drop ( columns = [ 'path' , 'trunc' , 'event_name' ]) # Drop irrelevant columns . rename ( columns = { 'selection_name' : 'team' }) # Rename columns . replace ( team_name_mapping ) . sort_values ( by = 'date' ) . reset_index ( drop = True ) . assign ( home_game = lambda df : df . apply ( lambda row : 1 if row . home_team == row . team else 0 , axis = 'columns' )) . drop ( columns = [ 'home_team' , 'away_team' ])) return df # Apply the wrangling and cleaning function odds = get_cleaned_odds ( odds ) odds . tail () team odds date home_game 4177 South Fremantle 1.5024 2018-09-01 1 4178 Port Melbourne 2.8000 2018-09-01 0 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 We now have a DataFrame that looks nice and easy to join with our other DataFrames. Now let's lean up the match_details DataFrame. # Define a function which cleans the match results df, and separates each teams' stats onto individual rows def get_cleaned_match_results ( df = None ): # If a df hasn't been specified as a parameter, read the match_results df if df is None : df = pd . read_csv ( \"data/afl_match_results.csv\" ) # Create column lists to loop through - these are the columns we want in home and away dfs home_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' , 'Margin' , 'Venue' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' ] away_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' , 'Margin' , 'Venue' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' ] mapping = [ 'game' , 'date' , 'round' , 'team' , 'goals' , 'behinds' , 'points' , 'margin' , 'venue' , 'opponent' , 'opponent_goals' , 'opponent_behinds' , 'opponent_points' ] team_name_mapping = { 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' } # Create a df with only home games df_home = ( df [ home_columns ] . rename ( columns = { old_col : new_col for old_col , new_col in zip ( home_columns , mapping )}) . assign ( home_game = 1 )) # Create a df with only away games df_away = ( df [ away_columns ] . rename ( columns = { old_col : new_col for old_col , new_col in zip ( away_columns , mapping )}) . assign ( home_game = 0 , margin = lambda df : df . margin * - 1 )) # Append these dfs together new_df = ( df_home . append ( df_away ) . sort_values ( by = 'game' ) # Sort by game ID . reset_index ( drop = True ) # Reset index . assign ( date = lambda df : pd . to_datetime ( df . date )) # Create a datetime column . replace ( team_name_mapping )) # Rename team names to be consistent with other dfs return new_df match_results = get_cleaned_match_results ( match_results ) match_results . head () game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 3 2 1897-05-08 1 St Kilda 2 4 16 -25 Victoria Park Collingwood 5 11 41 0 4 3 1897-05-08 1 Geelong 3 6 24 -23 Corio Oval Essendon 7 5 47 1 Now we have both the odds DataFrame and match_results DataFrame ready for feature creation! Finally, we will aggregate the player_stats DataFrame stats for each game rather than individual player stats. For this DataFrame we have regular stats, such as disposals, marks etc. and Advanced Stats, such as Tackles Inside 50 and Metres Gained. However these advanced stats are only available from 2015, so we will not be using them in this tutorial - as there isn't enough data from 2015 to train our models. Let's now aggregate the player_stats DataFrame. def get_cleaned_aggregate_player_stats ( df = None ): # If a df hasn't been specified as a parameter, read the player_stats df if df is None : df = pd . read_csv ( \"data/afl_player_stats.csv\" ) agg_stats = ( df . rename ( columns = { # Rename columns to lowercase 'Season' : 'season' , 'Round' : 'round' , 'Team' : 'team' , 'Opposition' : 'opponent' , 'Date' : 'date' }) . groupby ( by = [ 'date' , 'season' , 'team' , 'opponent' ], as_index = False ) # Groupby to aggregate the stats for each game . sum () . drop ( columns = [ 'DE' , 'TOG' , 'Match_id' ]) # Drop columns . assign ( date = lambda df : pd . to_datetime ( df . date , format = \" %d /%m/%Y\" )) # Create a datetime object . sort_values ( by = 'date' ) . reset_index ( drop = True )) return agg_stats agg_stats = get_cleaned_aggregate_player_stats ( player_stats ) agg_stats . tail () date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3621 2018-08-26 2018 Brisbane West Coast 1652 5 0 14.0 49 37 8 132 394 302 20 18 11 9 167 48 49 59.0 227 104 5571.0 6 48 39 1645 23.0 86.0 62 13.0 69.0 256 3622 2018-08-26 2018 West Coast Brisbane 1548 11 5 13.0 49 42 9 141 360 262 18 20 14 8 137 39 56 70.0 223 95 5809.0 12 39 34 1655 29.0 94.0 55 6.0 59.0 217 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 We now have a three fully prepared DataFrames which are almost ready to be analysed and for a model to be built on! Let's have a look at how they look and then merge them together into our final DataFrame. odds . tail ( 3 ) team odds date home_game 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 match_results . tail ( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 30793 15397 2018-08-26 23 Melbourne 15 12 102 45 M.C.G. GWS 8 9 57 1 30794 15398 2018-08-26 23 St Kilda 14 10 94 -23 Docklands North Melbourne 17 15 117 1 30795 15398 2018-08-26 23 North Melbourne 17 15 117 23 Docklands St Kilda 14 10 94 0 agg_stats . tail ( 3 ) date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 merged_df = ( odds [ odds . team . isin ( agg_stats . team . unique ())] . pipe ( pd . merge , match_results , on = [ 'date' , 'team' , 'home_game' ]) . pipe ( pd . merge , agg_stats , on = [ 'date' , 'team' , 'opponent' ]) . sort_values ( by = [ 'game' ])) merged_df . tail ( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3199 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3195 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3200 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Great! We now have a clean looking datset with each row representing one team in a game. Let's now eliminate the outliers from a dataset. We know that Essendon had a doping scandal which resulted in their entire team being banned for a year in 2016, so let's remove all of their 2016 games. To do this we will filter based on the team and season, and then invert this with ~. # Define a function which eliminates outliers def outlier_eliminator ( df ): # Eliminate Essendon 2016 games essendon_filter_criteria = ~ ((( df [ 'team' ] == 'Essendon' ) & ( df [ 'season' ] == 2016 )) | (( df [ 'opponent' ] == 'Essendon' ) & ( df [ 'season' ] == 2016 ))) df = df [ essendon_filter_criteria ] . reset_index ( drop = True ) return df afl_data = outlier_eliminator ( merged_df ) afl_data . tail ( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Finally, let's mark all of the columns that we are going to use in feature creation with the string 'f_' at the start of their column name so that we can easily filter for these columns. non_feature_cols = [ 'team' , 'date' , 'home_game' , 'game' , 'round' , 'venue' , 'opponent' , 'season' ] afl_data = afl_data . rename ( columns = { col : 'f_' + col for col in afl_data if col not in non_feature_cols }) afl_data . tail ( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Our data is now fully ready to be explored and for features to be created.","title":"01. Data Cleaning"},{"location":"modelling/AFLmodellingPython/#02-feature-creation","text":"These tutorials will walk you through how to construct your own basic AFL model. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through creating features from our dataset, which was cleaned in the first tutorial. Feature engineering is an integral part of the Data Science process. Creative and smart features can be the difference between an average performing model and a model profitable which beats the market odds.","title":"02. Feature Creation"},{"location":"modelling/AFLmodellingPython/#grabbing-our-dataset","text":"First, we will import our required modules, as well as the prepare_afl_data function which we created in our afl_data_cleaning script. This essentially cleans all the data for us so that we're ready to explore the data and make some features. # Import modules from afl_data_cleaning_v2 import * import afl_data_cleaning_v2 import pandas as pd pd . set_option ( 'display.max_columns' , None ) import warnings warnings . filterwarnings ( 'ignore' ) import numpy as np # Use the prepare_afl_data function to prepare the data for us; this function condenses what we walked through in the previous tutorial afl_data = prepare_afl_data () afl_data . tail ( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269","title":"Grabbing Our Dataset"},{"location":"modelling/AFLmodellingPython/#creating-a-feature-dataframe","text":"Let's create a feature DataFrame and merge all of our features into this DataFrame as we go. features = afl_data [[ 'date' , 'game' , 'team' , 'opponent' , 'venue' , 'home_game' ]] . copy ()","title":"Creating A Feature DataFrame"},{"location":"modelling/AFLmodellingPython/#what-each-column-refers-to","text":"Below is a DataFrame which outlines what each column refers to. column_abbreviations = pd . read_csv ( \"data/afl_data_columns_mapping.csv\" ) column_abbreviations Feature Abbreviated Feature 0 GA Goal Assists 1 CP Contested Possessions 2 UP Uncontested Possessions 3 ED Effective Disposals 4 CM Contested Marks 5 MI5 Marks Inside 50 6 One.Percenters One Percenters 7 BO Bounces 8 K Kicks 9 HB Handballs 10 D Disposals 11 M Marks 12 G Goals 13 B Behinds 14 T Tackles 15 HO Hitouts 16 I50 Inside 50s 17 CL Clearances 18 CG Clangers 19 R50 Rebound 50s 20 FF Frees For 21 FA Frees Against 22 AF AFL Fantasy Points 23 SC Supercoach Points 24 CCL Centre Clearances 25 SCL Stoppage Clearances 26 SI Score Involvements 27 MG Metres Gained 28 TO Turnovers 29 ITC Intercepts 30 T5 Tackles Inside 50","title":"What Each Column Refers To"},{"location":"modelling/AFLmodellingPython/#feature-creation","text":"Now let's think about what features we can create. We have a enormous amount of stats to sift through. To start, let's create some simple features based on our domain knowledge of Aussie Rules.","title":"Feature Creation"},{"location":"modelling/AFLmodellingPython/#creating-expontentially-weighted-rolling-averages-as-features","text":"Next, we will create rolling averages of statistics such as Tackles, which we will use as features. It is fair to assume that a team's performance in a certain stat may have predictive power to the overall result. And in general, if a team consistently performs well in this stat, this may have predictive power to the result of their future games. We can't simply train a model on stats from the game which we are trying to predict (i.e. data that we don't have before the game begins), as this will leak the result. We need to train our model on past data. One way of doing this is to train our model on average stats over a certain amount of games. If a team is averaging high in this stat, this may give insight into if they are a strong team. Similarly, if the team is averaging poorly in this stat (relative to the team they are playing), this may have predictive power and give rise to a predicted loss. To do this we will create a function which calculates the rolling averages, known as create_exp_weighted_avgs, which takes our cleaned DataFrame as an input, as well as the alpha which, when higher, weights recent performances more than old performances. To read more about expontentially weighted moving averages, please read the documentation here . First, we will grab all the columns which we want to create EMAs for, and then use our function to create the average for that column. We will create a new DataFrame and add these columns to this new DataFrame. # Define a function which returns a DataFrame with the expontential moving average for each numeric stat def create_exp_weighted_avgs ( df , span ): # Create a copy of the df with only the game id and the team - we will add cols to this df ema_features = df [[ 'game' , 'team' ]] . copy () feature_names = [ col for col in df . columns if col . startswith ( 'f_' )] # Get a list of columns we will iterate over for feature_name in feature_names : feature_ema = ( df . groupby ( 'team' )[ feature_name ] . transform ( lambda row : ( row . ewm ( span = span ) . mean () . shift ( 1 )))) ema_features [ feature_name ] = feature_ema return ema_features features_rolling_averages = create_exp_weighted_avgs ( afl_data , span = 10 ) features_rolling_averages . tail () game team f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3152 15396 West Coast 2.094236 12.809630 10.047145 86.904928 8.888770 11.435452 9.403444 78.016158 3193.612782 16.472115 11.958482 23.379562 100.095244 68.252001 27.688669 284.463270 719.884644 525.878017 36.762440 44.867118 25.618202 17.522871 270.478779 88.139376 105.698031 148.005305 449.405865 201.198907 11581.929999 20.048124 95.018480 74.180967 3314.157893 44.872398 177.894442 126.985101 20.565549 138.876613 438.848376 3153 15397 GWS 1.805565 13.100372 13.179329 91.781563 18.527618 10.371198 11.026754 73.253945 3165.127358 19.875913 12.947209 25.114002 105.856671 80.609640 23.374884 303.160047 741.439198 534.520295 42.597317 38.160889 26.208715 18.688880 300.188301 81.540693 106.989070 143.032506 441.250897 173.050118 12091.630837 21.106142 103.077097 80.201059 3419.245919 55.495610 219.879895 138.202470 25.313148 135.966798 438.466439 3154 15397 Melbourne 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.787800 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 3155 15398 North Melbourne 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.541130 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3156 15398 St Kilda 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.498760 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 As you can see our function worked perfectly! Now we have a full DataFrame of exponentially weighted moving averages. Note that as these rolling averages have been shifted by 1 to ensure no data leakage, the first round of the data will have all NA values. We can drop these later. Let's add these averages to our features DataFrame features = pd . merge ( features , features_rolling_averages , on = [ 'game' , 'team' ])","title":"Creating Expontentially Weighted Rolling Averages as Features"},{"location":"modelling/AFLmodellingPython/#creating-a-form-between-the-teams-feature","text":"It is well known in Aussie Rules that often some teams perform better against certain teams than others. If we isolate our features to pure stats based on previous games not between the teams playing, or elo ratings, we won't account for any relationships between certain teams. An example is the Kennett Curse , where Geelong won 11 consecutive games against Hawthorn, despite being similarly matched teams. Let's create a feature which calculates how many games a team has won against their opposition over a given window of games. To do this, we will need to use historical data that dates back well before our current DataFrame starts at. Otherwise we will be using a lot of our games to calculate form, meaning we will have to drop these rows before feeding it into an algorithm. So let's use our prepare_match_results function which we defined in the afl_data_cleaning tutorial to grab a clean DataFrame of all match results since 1897. We can then calculate the form and join this to our current DataFrame. match_results = afl_data_cleaning_v2 . get_cleaned_match_results () match_results . head ( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 form_btwn_teams = match_results [[ 'game' , 'team' , 'opponent' , 'margin' ]] . copy () form_btwn_teams [ 'f_form_margin_btwn_teams' ] = ( match_results . groupby ([ 'team' , 'opponent' ])[ 'margin' ] . transform ( lambda row : row . rolling ( 5 ) . mean () . shift ()) . fillna ( 0 )) form_btwn_teams [ 'f_form_past_5_btwn_teams' ] = \\ ( match_results . assign ( win = lambda df : df . apply ( lambda row : 1 if row . margin > 0 else 0 , axis = 'columns' )) . groupby ([ 'team' , 'opponent' ])[ 'win' ] . transform ( lambda row : row . rolling ( 5 ) . mean () . shift () * 5 ) . fillna ( 0 )) form_btwn_teams . tail ( 3 ) game team opponent margin f_form_margin_btwn_teams f_form_past_5_btwn_teams 30793 15397 Melbourne GWS 45 -23.2 2.0 30794 15398 St Kilda North Melbourne -23 -3.2 2.0 30795 15398 North Melbourne St Kilda 23 3.2 3.0 # Merge to our features df features = pd . merge ( features , form_btwn_teams . drop ( columns = [ 'margin' ]), on = [ 'game' , 'team' , 'opponent' ])","title":"Creating a 'Form Between the Teams' Feature"},{"location":"modelling/AFLmodellingPython/#creating-efficiency-features","text":"","title":"Creating Efficiency Features"},{"location":"modelling/AFLmodellingPython/#disposal-efficiency","text":"Disposal efficiency is pivotal in Aussie Rules football. If you are dispose of the ball effectively you are much more likely to score and much less likely to concede goals than if you dispose of it ineffectively. Let's create a disposal efficiency feature by dividing Effective Disposals by Disposals.","title":"Disposal Efficiency"},{"location":"modelling/AFLmodellingPython/#inside-50rebound-50-efficiency","text":"Similarly, one could hypothesise that teams who keep the footy in their Inside 50 regularly will be more likely to score, whilst teams who are effective at getting the ball out of their Defensive 50 will be less likely to concede. Let's use this logic to create Inside 50 Efficiency and Rebound 50 Efficiency features. The formula used will be: Inside 50 Efficiency = R50_Opponents / I50 (lower is better). Rebound 50 Efficiency = R50 / I50_Opponents (higher is better). Using these formulas, I50 Efficiency = R50 Efficiency_Opponent. So we will just need to create the formulas for I50 efficiency. To create these features we will need the opposition's Inside 50s/Rebound 50s. So we will split out data into two DataFrames, create a new DataFrame by joining these two DataFrames on the Game, calculate our efficiency features, then join our features with our main features DataFrame. # Get each match on single rows single_row_df = ( afl_data [[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' , ]] . query ( 'home_game == 1' ) . rename ( columns = { 'team' : 'home_team' , 'f_I50' : 'f_I50_home' , 'f_R50' : 'f_R50_home' , 'f_D' : 'f_D_home' , 'f_ED' : 'f_ED_home' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , afl_data [[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' ]] . query ( 'home_game == 0' ) . rename ( columns = { 'team' : 'away_team' , 'f_I50' : 'f_I50_away' , 'f_R50' : 'f_R50_away' , 'f_D' : 'f_D_away' , 'f_ED' : 'f_ED_away' }) . drop ( columns = 'home_game' ), on = 'game' )) single_row_df . head () game home_team f_I50_home f_R50_home f_D_home f_ED_home away_team f_I50_away f_R50_away f_D_away f_ED_away 0 13764 Carlton 69 21 373 268 Richmond 37 50 316 226 1 13765 Geelong 54 40 428 310 St Kilda 52 45 334 246 2 13766 Collingwood 70 38 398 289 Port Adelaide 50 44 331 232 3 13767 Adelaide 59 38 366 264 Hawthorn 54 38 372 264 4 13768 Brisbane 50 39 343 227 Fremantle 57 30 351 250 single_row_df = single_row_df . assign ( f_I50_efficiency_home = lambda df : df . f_R50_away / df . f_I50_home , f_I50_efficiency_away = lambda df : df . f_R50_home / df . f_I50_away ) feature_efficiency_cols = [ 'f_I50_efficiency_home' , 'f_I50_efficiency_away' ] # Now let's create an Expontentially Weighted Moving Average for these features - we will need to reshape our DataFrame to do this efficiency_features_multi_row = ( single_row_df [[ 'game' , 'home_team' ] + feature_efficiency_cols ] . rename ( columns = { 'home_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency' , 'f_I50_efficiency_away' : 'f_I50_efficiency_opponent' , }) . append (( single_row_df [[ 'game' , 'away_team' ] + feature_efficiency_cols ] . rename ( columns = { 'away_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency_opponent' , 'f_I50_efficiency_away' : 'f_I50_efficiency' , })), sort = True ) . sort_values ( by = 'game' ) . reset_index ( drop = True )) efficiency_features = efficiency_features_multi_row [[ 'game' , 'team' ]] . copy () feature_efficiency_cols = [ 'f_I50_efficiency' , 'f_I50_efficiency_opponent' ] for feature in feature_efficiency_cols : efficiency_features [ feature ] = ( efficiency_features_multi_row . groupby ( 'team' )[ feature ] . transform ( lambda row : row . ewm ( span = 10 ) . mean () . shift ( 1 ))) # Get feature efficiency df back onto single rows efficiency_features = pd . merge ( efficiency_features , afl_data [[ 'game' , 'team' , 'home_game' ]], on = [ 'game' , 'team' ]) efficiency_features_single_row = ( efficiency_features . query ( 'home_game == 1' ) . rename ( columns = { 'team' : 'home_team' , 'f_I50_efficiency' : 'f_I50_efficiency_home' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_home' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , ( efficiency_features . query ( 'home_game == 0' ) . rename ( columns = { 'team' : 'away_team' , 'f_I50_efficiency' : 'f_I50_efficiency_away' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_away' }) . drop ( columns = 'home_game' )), on = 'game' )) efficiency_features_single_row . tail ( 5 ) game home_team f_I50_efficiency_home f_R50_efficiency_home away_team f_I50_efficiency_away f_R50_efficiency_away 1580 15394 Carlton 0.730668 0.675002 Adelaide 0.691614 0.677128 1581 15395 Sydney 0.699994 0.778280 Hawthorn 0.699158 0.673409 1582 15396 Brisbane 0.683604 0.691730 West Coast 0.696822 0.709605 1583 15397 Melbourne 0.667240 0.692632 GWS 0.684525 0.753783 1584 15398 St Kilda 0.730843 0.635819 North Melbourne 0.697018 0.654991 We will merge these features back to our features df later, when the features data frame is on a single row as well.","title":"Inside 50/Rebound 50 Efficiency"},{"location":"modelling/AFLmodellingPython/#creating-an-elo-feature","text":"Another feature which we could create is an Elo feature. If you don't know what Elo is, go ahead and read our article on it here . We have also written a guide on using elo to model the 2018 FIFA World Cup here . Essentially, Elo ratings increase if you win. The amount the rating increases is based on how strong the opponent is relative to the team who won. Weak teams get more points for beating stronger teams than they do for beating weaker teams, and vice versa for losses (teams lose points for losses). Mathematically, Elo ratings can also assign a probability for winning or losing based on the two Elo Ratings of the teams playing. So let's get into it. We will first define a function which calculates the elo for each team and applies these elos to our DataFrame. # Define a function which finds the elo for each team in each game and returns a dictionary with the game ID as a key and the # elos as the key's value, in a list. It also outputs the probabilities and a dictionary of the final elos for each team def elo_applier ( df , k_factor ): # Initialise a dictionary with default elos for each team elo_dict = { team : 1500 for team in df [ 'team' ] . unique ()} elos , elo_probs = {}, {} # Get a home and away dataframe so that we can get the teams on the same row home_df = df . loc [ df . home_game == 1 , [ 'team' , 'game' , 'f_margin' , 'home_game' ]] . rename ( columns = { 'team' : 'home_team' }) away_df = df . loc [ df . home_game == 0 , [ 'team' , 'game' ]] . rename ( columns = { 'team' : 'away_team' }) df = ( pd . merge ( home_df , away_df , on = 'game' ) . sort_values ( by = 'game' ) . drop_duplicates ( subset = 'game' , keep = 'first' ) . reset_index ( drop = True )) # Loop over the rows in the DataFrame for index , row in df . iterrows (): # Get the Game ID game_id = row [ 'game' ] # Get the margin margin = row [ 'f_margin' ] # If the game already has the elos for the home and away team in the elos dictionary, go to the next game if game_id in elos . keys (): continue # Get the team and opposition home_team = row [ 'home_team' ] away_team = row [ 'away_team' ] # Get the team and opposition elo score home_team_elo = elo_dict [ home_team ] away_team_elo = elo_dict [ away_team ] # Calculated the probability of winning for the team and opposition prob_win_home = 1 / ( 1 + 10 ** (( away_team_elo - home_team_elo ) / 400 )) prob_win_away = 1 - prob_win_home # Add the elos and probabilities our elos dictionary and elo_probs dictionary based on the Game ID elos [ game_id ] = [ home_team_elo , away_team_elo ] elo_probs [ game_id ] = [ prob_win_home , prob_win_away ] # Calculate the new elos of each team if margin > 0 : # Home team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 1 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 0 - prob_win_away ) elif margin < 0 : # Away team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 1 - prob_win_away ) elif margin == 0 : # Drawn game' update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0.5 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 0.5 - prob_win_away ) # Update elos in elo dictionary elo_dict [ home_team ] = new_home_team_elo elo_dict [ away_team ] = new_away_team_elo return elos , elo_probs , elo_dict # Use the elo applier function to get the elos and elo probabilities for each game - we will map these later elos , probs , elo_dict = elo_applier ( afl_data , 30 ) Great! now we have both rolling averages for stats as a feature, and the elo of the teams! Let's have a quick look at the current elo standings with a k-factor of 30, out of curiosity. for team in sorted ( elo_dict , key = elo_dict . get )[:: - 1 ]: print ( team , elo_dict [ team ]) Richmond 1695.2241513840117 Sydney 1645.548990879842 Hawthorn 1632.5266709780622 West Coast 1625.871701773721 Geelong 1625.423154644809 GWS 1597.4158602131877 Adelaide 1591.1704934545442 Collingwood 1560.370309216614 Melbourne 1558.5666572771509 Essendon 1529.0198398117086 Port Adelaide 1524.8882517820093 North Melbourne 1465.5637511922569 Western Bulldogs 1452.2110697845148 Fremantle 1393.142087030804 St Kilda 1360.9120149937303 Brisbane 1276.2923772139352 Gold Coast 1239.174528704772 Carlton 1226.6780896643265 This looks extremely similar to the currently AFL ladder, so this is a good sign for elo being an effective predictor of winning.","title":"Creating an Elo Feature"},{"location":"modelling/AFLmodellingPython/#merging-our-features-into-one-features-dataframe","text":"Now we need to reshape our features df so that we have all of the statistics for both teams in a game on a single row. We can then merge our elo and efficiency features to this df. # Look at our current features df features . tail ( 3 ) date game team opponent venue home_game f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP f_form_margin_btwn_teams f_form_past_5_btwn_teams 3156 2018-08-26 15397 Melbourne GWS M.C.G. 1 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.78780 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 -23.2 2.0 3157 2018-08-26 15398 North Melbourne St Kilda Docklands 0 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.54113 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3.2 3.0 3158 2018-08-26 15398 St Kilda North Melbourne Docklands 1 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.49876 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 -3.2 2.0 one_line_cols = [ 'game' , 'team' , 'home_game' ] + [ col for col in features if col . startswith ( 'f_' )] # Get all features onto individual rows for each match features_one_line = ( features . loc [ features . home_game == 1 , one_line_cols ] . rename ( columns = { 'team' : 'home_team' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , ( features . loc [ features . home_game == 0 , one_line_cols ] . drop ( columns = 'home_game' ) . rename ( columns = { 'team' : 'away_team' }) . rename ( columns = { col : col + '_away' for col in features . columns if col . startswith ( 'f_' )})), on = 'game' ) . drop ( columns = [ 'f_form_margin_btwn_teams_away' , 'f_form_past_5_btwn_teams_away' ])) # Add our created features - elo, efficiency etc. features_one_line = ( features_one_line . assign ( f_elo_home = lambda df : df . game . map ( elos ) . apply ( lambda x : x [ 0 ]), f_elo_away = lambda df : df . game . map ( elos ) . apply ( lambda x : x [ 1 ])) . pipe ( pd . merge , efficiency_features_single_row , on = [ 'game' , 'home_team' , 'away_team' ]) . pipe ( pd . merge , afl_data . loc [ afl_data . home_game == 1 , [ 'game' , 'date' , 'round' , 'venue' ]], on = [ 'game' ]) . dropna () . reset_index ( drop = True ) . assign ( season = lambda df : df . date . apply ( lambda row : row . year ))) ordered_cols = [ col for col in features_one_line if col [: 2 ] != 'f_' ] + [ col for col in features_one_line if col . startswith ( 'f_' )] feature_df = features_one_line [ ordered_cols ] Finally, let's reduce the dimensionality of the features df by subtracting the home features from the away features. This will reduce the huge amount of columns we have and make our data more manageable. To do this, we will need a list of columns which we are subtracting from each other. We will then loop over each of these columns to create our new differential columns. We will then add in the implied probability from the odds of the home and away team, as our current odds feature is simply an exponential moving average over the past n games. # Create differential df - this df is the home features - the away features diff_cols = [ col for col in feature_df . columns if col + '_away' in feature_df . columns and col != 'f_odds' and col . startswith ( 'f_' )] non_diff_cols = [ col for col in feature_df . columns if col not in diff_cols and col [: - 5 ] not in diff_cols ] diff_df = feature_df [ non_diff_cols ] . copy () for col in diff_cols : diff_df [ col + '_diff' ] = feature_df [ col ] - feature_df [ col + '_away' ] # Add current odds in to diff_df odds = get_cleaned_odds () home_odds = ( odds [ odds . home_game == 1 ] . assign ( f_current_odds_prob = lambda df : 1 / df . odds ) . rename ( columns = { 'team' : 'home_team' }) . drop ( columns = [ 'home_game' , 'odds' ])) away_odds = ( odds [ odds . home_game == 0 ] . assign ( f_current_odds_prob_away = lambda df : 1 / df . odds ) . rename ( columns = { 'team' : 'away_team' }) . drop ( columns = [ 'home_game' , 'odds' ])) diff_df = ( diff_df . pipe ( pd . merge , home_odds , on = [ 'date' , 'home_team' ]) . pipe ( pd . merge , away_odds , on = [ 'date' , 'away_team' ])) diff_df . tail () game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1626 15394 Carlton Adelaide 2018-08-25 23 Docklands 2018 6.467328 -26.2 1.0 2.066016 1230.072138 1587.776445 0.730668 0.675002 0.691614 0.677128 -3.498547 -5.527193 -26.518474 -34.473769 1.289715 0.217006 7.955295 -341.342677 -9.317269 3.088569 -2.600593 15.192839 -12.518345 -4.136673 -41.855717 -72.258378 -51.998775 9.499447 8.670917 -6.973088 -4.740623 -26.964945 -13.147675 -23.928700 -28.940883 -45.293433 -15.183406 -1900.784014 -0.362402 -1.314627 4.116133 -294.813511 -9.917793 -34.724925 -5.462844 -9.367141 -19.623785 -38.188082 0.187709 0.816860 1627 15395 Sydney Hawthorn 2018-08-25 23 S.C.G. 2018 2.128611 1.0 2.0 1.777290 1662.568452 1615.507209 0.699994 0.778280 0.699158 0.673409 -1.756730 -0.874690 -11.415069 -15.575319 0.014390 4.073909 4.160250 -174.005092 -0.942357 -4.078635 -4.192916 7.814496 -2.225780 6.215760 15.042979 -34.894261 -50.615255 4.214158 0.683548 -3.535594 -3.168608 -12.068691 -30.493980 -9.867332 2.588103 -22.825570 -5.604199 253.086090 -2.697132 -22.612327 25.340623 -90.812188 1.967104 -31.047879 0.007606 -6.880120 11.415593 -49.957313 0.440180 0.561924 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566","title":"Merging Our Features Into One Features DataFrame"},{"location":"modelling/AFLmodellingPython/#wrapping-it-up","text":"We now have a fairly decent amount of features. Some other features which could be added include whether the game is in a major Capital city outisde of Mebourne (i.e. Sydney, Adelaide or Peth), how many 'Elite' players are playing (which could be judged by average SuperCoach scores over 110, for example), as well as your own metrics for attacking and defending. Note that all of our features have columns starting with 'f_' so in the section, we will grab this feature dataframe and use these features to sport predicting the matches.","title":"Wrapping it Up"},{"location":"modelling/AFLmodellingPython/#03-modelling","text":"These tutorials will walk you through how to construct your own basic AFL model, using publically available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through modelling our AFL data to create predictions. We will train a variety of quick and easy models to get a feel of what works and what doesn't. We will then tune our hyperparameters so that we are ready to make week by week predictions.","title":"03. Modelling"},{"location":"modelling/AFLmodellingPython/#grabbing-our-dataset_1","text":"First, we will import our required modules, as well as the prepare_afl_features function which we created in our afl_feature_creation script. This essentially creates some basic features for us so that we can get started on the modelling component. # Import libraries from afl_data_cleaning_v2 import * import datetime import pandas as pd import numpy as np from sklearn import svm , tree , linear_model , neighbors , naive_bayes , ensemble , discriminant_analysis , gaussian_process # from xgboost import XGBClassifier from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV , train_test_split from sklearn.linear_model import LogisticRegressionCV from sklearn.feature_selection import RFECV import seaborn as sns from sklearn.preprocessing import OneHotEncoder , LabelEncoder , StandardScaler from sklearn import feature_selection from sklearn import metrics from sklearn.linear_model import LogisticRegression , RidgeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB import warnings warnings . filterwarnings ( 'ignore' ) import afl_feature_creation_v2 import afl_data_cleaning_v2 # Grab our feature DataFrame which we created in the previous tutorial feature_df = afl_feature_creation_v2 . prepare_afl_features () afl_data = afl_data_cleaning_v2 . prepare_afl_data () feature_df . tail ( 3 ) game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 # Get the result and merge to the feature_df match_results = ( pd . read_csv ( \"data/afl_match_results.csv\" ) . rename ( columns = { 'Game' : 'game' }) . assign ( result = lambda df : df . apply ( lambda row : 1 if row [ 'Home.Points' ] > row [ 'Away.Points' ] else 0 , axis = 1 ))) # Merge result column to feature_df feature_df = pd . merge ( feature_df , match_results [[ 'game' , 'result' ]], on = 'game' )","title":"Grabbing Our Dataset"},{"location":"modelling/AFLmodellingPython/#creating-a-training-and-testing-set","text":"So that we don't train our data on the data that we will later test our model on, we will create separate train and test sets. For this exercise we will use the 2018 season to test how our model performs, whilst the rest of the data can be used to train the model. # Create our test and train sets from our afl DataFrame; drop the columns which leak the result, duplicates, and the advanced # stats which don't have data until 2015 feature_columns = [ col for col in feature_df if col . startswith ( 'f_' )] # Create our test set test_x = feature_df . loc [ feature_df . season == 2018 , [ 'game' ] + feature_columns ] test_y = feature_df . loc [ feature_df . season == 2018 , 'result' ] # Create our train set X = feature_df . loc [ feature_df . season != 2018 , [ 'game' ] + feature_columns ] y = feature_df . loc [ feature_df . season != 2018 , 'result' ] # Scale features scaler = StandardScaler () X [ feature_columns ] = scaler . fit_transform ( X [ feature_columns ]) test_x [ feature_columns ] = scaler . transform ( test_x [ feature_columns ])","title":"Creating a Training and Testing Set"},{"location":"modelling/AFLmodellingPython/#using-cross-validation-to-find-the-best-algorithms","text":"Now that we have our training set, we can run through a list of popular classifiers to determine which classifier is best for modelling our data. To do this we will create a function which uses Kfold cross-validation to find the 'best' algorithms, based on how accurate the algorithms' predictions are. This function will take in a list of classifiers, which we will define below, as well as the training set and it's outcome, and output a DataFrame with the mean and std of the accuracy of each algorithm. Let's jump into it! # Create a list of standard classifiers classifiers = [ #Ensemble Methods ensemble . AdaBoostClassifier (), ensemble . BaggingClassifier (), ensemble . ExtraTreesClassifier (), ensemble . GradientBoostingClassifier (), ensemble . RandomForestClassifier (), #Gaussian Processes gaussian_process . GaussianProcessClassifier (), #GLM linear_model . LogisticRegressionCV (), #Navies Bayes naive_bayes . BernoulliNB (), naive_bayes . GaussianNB (), #SVM svm . SVC ( probability = True ), svm . NuSVC ( probability = True ), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis (), discriminant_analysis . QuadraticDiscriminantAnalysis (), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # XGBClassifier() ] # Define a functiom which finds the best algorithms for our modelling task def find_best_algorithms ( classifier_list , X , y ): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold ( n_splits = 5 ) # Grab the cross validation scores for each algorithm cv_results = [ cross_val_score ( classifier , X , y , scoring = \"neg_log_loss\" , cv = kfold ) for classifier in classifier_list ] cv_means = [ cv_result . mean () * - 1 for cv_result in cv_results ] cv_std = [ cv_result . std () for cv_result in cv_results ] algorithm_names = [ alg . __class__ . __name__ for alg in classifiers ] # Create a DataFrame of all the CV results cv_results = pd . DataFrame ({ \"Mean Log Loss\" : cv_means , \"Log Loss Std\" : cv_std , \"Algorithm\" : algorithm_names }) return cv_results . sort_values ( by = 'Mean Log Loss' ) . reset_index ( drop = True ) best_algos = find_best_algorithms ( classifiers , X , y ) best_algos Mean Log Loss Log Loss Std Algorithm 0 0.539131 3.640578e-02 LogisticRegressionCV 1 0.551241 5.775685e-02 LinearDiscriminantAnalysis 2 0.630994 8.257481e-02 GradientBoostingClassifier 3 0.670041 9.205780e-03 AdaBoostClassifier 4 0.693147 2.360121e-08 GaussianProcessClassifier 5 0.712537 2.770864e-02 SVC 6 0.712896 2.440755e-02 NuSVC 7 0.836191 2.094224e-01 ExtraTreesClassifier 8 0.874307 1.558144e-01 RandomForestClassifier 9 1.288174 3.953037e-01 BaggingClassifier 10 1.884019 4.769589e-01 QuadraticDiscriminantAnalysis 11 2.652161 6.886897e-01 BernoulliNB 12 3.299651 6.427551e-01 GaussianNB # Try a logistic regression model and see how it performs in terms of accuracy kfold = StratifiedKFold ( n_splits = 5 ) cv_scores = cross_val_score ( linear_model . LogisticRegressionCV (), X , y , scoring = 'accuracy' , cv = kfold ) cv_scores . mean () 0.7452268937025035","title":"Using Cross Validation to Find The Best Algorithms"},{"location":"modelling/AFLmodellingPython/#choosing-our-algorithms","text":"As we can see from above, there are some pretty poor algorithms for predicting the winner. On the other hand, whilst attaining an accuracy of 74.5% (at the time of writing) may seem like a decent result; we must first establish a baseline to judge our performance on. In this case, we will have two baselines; the proportion of games won by the home team and what the odds predict. If we can beat the odds we have created a very powerful model. Note that a baseline for the log loss can also be both the odds log loss and randomly guessing. Randomly guessing between two teams attains a log loss of log(2) = 0.69, so we have beaten this result. Once we establish our baseline, we will choose the top algorithms from above and tune their hyperparameters, as well as automatically selecting the best features to be used in our model.","title":"Choosing Our Algorithms"},{"location":"modelling/AFLmodellingPython/#defining-our-baseline","text":"As stated above, we must define our baseline so that we have a measure to beat. We will use the proportion of games won by the home team, as well as the proportion of favourites who won, based off the odds. To establish this baseline we will use our feature_df, as this has no dropped rows. # Find the percentage chance of winning at home in each season. afl_data = afl_data_cleaning_v2 . prepare_afl_data () afl_data [ 'home_win' ] = afl_data . apply ( lambda x : 1 if x [ 'f_margin' ] > 0 else 0 , axis = 1 ) home_games = afl_data [ afl_data [ 'home_game' ] == 1 ] home_games [[ \"home_win\" , 'season' ]] . groupby ([ 'season' ]) . mean () season home_win 2011 0.561856 2012 0.563725 2013 0.561576 2014 0.574257 2015 0.539604 2016 0.606742 2017 0.604061 2018 0.540404 # Find the proportion of favourites who have won # Define a function which finds if the odds correctly guessed the response def find_odds_prediction ( a_row ): if a_row [ 'f_odds' ] <= a_row [ 'f_odds_away' ] and a_row [ 'home_win' ] == 1 : return 1 elif a_row [ 'f_odds_away' ] < a_row [ 'f_odds' ] and a_row [ 'home_win' ] == 0 : return 1 else : return 0 # Define a function which splits our DataFrame so each game is on one row instead of two def get_df_on_one_line ( df ): cols_to_drop = [ 'date' , 'home_game' , 'opponent' , 'f_opponent_behinds' , 'f_opponent_goals' , 'f_opponent_points' , 'f_points' , 'round' , 'venue' , 'season' ] home_df = df [ df [ 'home_game' ] == 1 ] . rename ( columns = { 'team' : 'home_team' }) away_df = df [ df [ 'home_game' ] == 0 ] . rename ( columns = { 'team' : 'away_team' }) away_df = away_df . drop ( columns = cols_to_drop ) # Rename away_df columns away_df_renamed = away_df . rename ( columns = { col : col + '_away' for col in away_df . columns if col != 'game' }) merged_df = pd . merge ( home_df , away_df_renamed , on = 'game' ) merged_df [ 'home_win' ] = merged_df . f_margin . apply ( lambda x : 1 if x > 0 else 0 ) return merged_df afl_data_one_line = get_df_on_one_line ( afl_data ) afl_data_one_line [ 'odds_prediction' ] = afl_data_one_line . apply ( find_odds_prediction , axis = 1 ) print ( 'The overall mean accuracy of choosing the favourite based on the odds is {} %' . format ( round ( afl_data_one_line [ 'odds_prediction' ] . mean () * 100 , 2 ))) afl_data_one_line [[ \"odds_prediction\" , 'season' ]] . groupby ([ 'season' ]) . mean () The overall mean accuracy of choosing the favourite based on the odds is 73.15% season odds_prediction 2011 0.784615 2012 0.774510 2013 0.748768 2014 0.727723 2015 0.727723 2016 0.713483 2017 0.659898 2018 0.712121 ## Get a baseline log loss score from the odds afl_data_one_line [ 'odds_home_prob' ] = 1 / afl_data_one_line . f_odds afl_data_one_line [ 'odds_away_prob' ] = 1 / afl_data_one_line . f_odds_away metrics . log_loss ( afl_data_one_line . home_win , afl_data_one_line [[ 'odds_away_prob' , 'odds_home_prob' ]]) 0.5375306549682837 We can see that the odds are MUCH more accurate than just choosing the home team to win. We can also see that the mean accuracy of choosing the favourite is around 73%. That means that this is the score we need to beat. Similarly, the log loss of the odds is around 0.5385, whilst our model scores around 0.539 (at the time of writing), without hyperparamter optimisation. Let's choose only the algorithms with log losses below 0.67 chosen_algorithms = best_algos . loc [ best_algos [ 'Mean Log Loss' ] < 0.67 , 'Algorithm' ] . tolist () chosen_algorithms [ 'LogisticRegressionCV' , 'LinearDiscriminantAnalysis' , 'GradientBoostingClassifier' ]","title":"Defining Our Baseline"},{"location":"modelling/AFLmodellingPython/#using-grid-search-to-tune-hyperparameters","text":"Now that we have our best models, we can use Grid Search to optimise our hyperparameters. Grid search basically involves searching through a range of different algorithm hyperparameters, and choosing those which result in the best score from some metrics, which in our case is accuracy. Let's do this for the algorithms which have hyperparameters which can be tuned. Note that if you are running this on your own computer it may take up to 10 minutes. # Define a function which optimises the hyperparameters of our chosen algorithms def optimise_hyperparameters ( train_x , train_y , algorithms , parameters ): kfold = StratifiedKFold ( n_splits = 5 ) best_estimators = [] for alg , params in zip ( algorithms , parameters ): gs = GridSearchCV ( alg , param_grid = params , cv = kfold , scoring = 'neg_log_loss' , verbose = 1 ) gs . fit ( train_x , train_y ) best_estimators . append ( gs . best_estimator_ ) return best_estimators # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.001 , 0.01 , 0.05 , 0.2 , 0.5 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } # Add our algorithms and parameters to lists to be used in our function alg_list = [ LogisticRegression ()] param_list = [ lr_grid ] # Find the best estimators, then add our other estimators which don't need optimisation best_estimators = optimise_hyperparameters ( X , y , alg_list , param_list ) Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=1)]: Done 90 out of 90 | elapsed: 5.2s finished lr_best_params = best_estimators [ 0 ] . get_params () lr_best_params { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } kfold = StratifiedKFold ( n_splits = 10 ) cv_scores = cross_val_score ( linear_model . LogisticRegression ( ** lr_best_params ), X , y , scoring = 'neg_log_loss' , cv = kfold ) cv_scores . mean () - 0.528741673153639 In the next iteration of this tutorial we will also optimise an XGB model and hopefully outperform our logistic regression model.","title":"Using Grid Search To Tune Hyperparameters"},{"location":"modelling/AFLmodellingPython/#creating-predictions-for-the-2018-season","text":"Now that we have an optimised logistic regression model, let's see how it performs on predicting the 2018 season. lr = LogisticRegression ( ** lr_best_params ) lr . fit ( X , y ) final_predictions = lr . predict ( test_x ) accuracy = ( final_predictions == test_y ) . mean () * 100 print ( \"Our accuracy in predicting the 2018 season is: {:.2f} %\" . format ( accuracy )) Our accuracy in predicting the 2018 season is: 67.68% Now let's have a look at all the games which we incorrectly predicted. game_ids = test_x [( final_predictions != test_y )] . game afl_data_one_line . loc [ afl_data_one_line . game . isin ( game_ids ), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] date home_team opponent f_odds f_odds_away f_margin 1386 2018-03-24 Gold Coast North Melbourne 2.0161 1.9784 16 1388 2018-03-25 Melbourne Geelong 1.7737 2.2755 -3 1391 2018-03-30 North Melbourne St Kilda 3.5769 1.3867 52 1392 2018-03-31 Carlton Gold Coast 1.5992 2.6620 -34 1396 2018-04-01 Western Bulldogs West Coast 1.8044 2.2445 -51 1397 2018-04-01 Sydney Port Adelaide 1.4949 3.0060 -23 1398 2018-04-02 Geelong Hawthorn 1.7597 2.3024 -1 1406 2018-04-08 Western Bulldogs Essendon 3.8560 1.3538 21 1408 2018-04-13 Adelaide Collingwood 1.2048 5.9197 -48 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1415 2018-04-15 Hawthorn Melbourne 2.2855 1.7772 67 1417 2018-04-20 Sydney Adelaide 1.2640 4.6929 -10 1420 2018-04-21 Port Adelaide Geelong 1.5053 2.9515 -34 1422 2018-04-22 North Melbourne Hawthorn 2.6170 1.6132 28 1423 2018-04-22 Brisbane Gold Coast 1.7464 2.3277 -5 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1427 2018-04-28 Geelong Sydney 1.5019 2.9833 -17 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 1439 2018-05-05 Sydney North Melbourne 1.2777 4.5690 -2 1444 2018-05-11 Hawthorn Sydney 1.6283 2.5818 -8 1445 2018-05-12 GWS West Coast 1.5425 2.8292 -25 1446 2018-05-12 Carlton Essendon 3.1742 1.4570 13 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1456 2018-05-19 Essendon Geelong 5.6530 1.2104 34 1460 2018-05-20 Brisbane Hawthorn 3.2891 1.4318 56 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1466 2018-05-26 GWS Essendon 1.4364 3.2652 -35 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 ... ... ... ... ... ... ... 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 1485 2018-06-11 Melbourne Collingwood 1.6034 2.6450 -42 1492 2018-06-21 West Coast Essendon 1.3694 3.6843 -28 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1499 2018-06-29 Western Bulldogs Geelong 6.2067 1.1889 2 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1504 2018-07-01 Melbourne St Kilda 1.1405 7.7934 -2 1505 2018-07-01 Essendon North Melbourne 2.0993 1.9022 17 1506 2018-07-01 Fremantle Brisbane 1.2914 4.3743 -55 1507 2018-07-05 Sydney Geelong 1.7807 2.2675 -12 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1516 2018-07-12 Adelaide Geelong 2.0517 1.9444 15 1518 2018-07-14 Hawthorn Brisbane 1.2281 5.4105 -33 1521 2018-07-14 GWS Richmond 2.7257 1.5765 2 1522 2018-07-15 Collingwood West Coast 1.5600 2.7815 -35 1523 2018-07-15 North Melbourne Sydney 1.9263 2.0647 -6 1524 2018-07-15 Fremantle Port Adelaide 5.9110 1.2047 9 1527 2018-07-21 Sydney Gold Coast 1.0342 27.8520 -24 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 1533 2018-07-22 Port Adelaide GWS 1.6480 2.5452 -22 1538 2018-07-28 Gold Coast Carlton 1.3933 3.5296 -35 1546 2018-08-04 Adelaide Port Adelaide 2.0950 1.9135 3 1548 2018-08-04 St Kilda Western Bulldogs 1.6120 2.6368 -35 1555 2018-08-11 Port Adelaide West Coast 1.4187 3.3505 -4 1558 2018-08-12 North Melbourne Western Bulldogs 1.3175 4.1239 -7 1559 2018-08-12 Melbourne Sydney 1.3627 3.7445 -9 1564 2018-08-18 GWS Sydney 1.8478 2.1672 -20 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 Very interesting! Most of the games we got wrong were upsets. Let's have a look at the games we incorrectly predicted that weren't upsets. ( afl_data_one_line . loc [ afl_data_one_line . game . isin ( game_ids ), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] . assign ( home_favourite = lambda df : df . apply ( lambda row : 1 if row . f_odds < row . f_odds_away else 0 , axis = 1 )) . assign ( upset = lambda df : df . apply ( lambda row : 1 if row . home_favourite == 1 and row . f_margin < 0 else ( 1 if row . home_favourite == 0 and row . f_margin > 0 else 0 ), axis = 1 )) . query ( 'upset == 0' )) date home_team opponent f_odds f_odds_away f_margin home_favourite upset 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1 0 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1 0 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 0 0 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 0 0 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 0 0 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1 0 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1 0 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 0 0 1479 2018-06-08 Port Adelaide Richmond 1.7422 2.3420 14 1 0 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 0 0 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1 0 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1 0 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 0 0 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1 0 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 0 0 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 0 0 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 0 0 Let's now look at our model's log loss for the 2018 season compared to the odds. predictions_probs = lr . predict_proba ( test_x ) metrics . log_loss ( test_y , predictions_probs ) 0.584824211055384 test_x_unscaled = feature_df . loc [ feature_df . season == 2018 , [ 'game' ] + feature_columns ] metrics . log_loss ( test_y , test_x_unscaled [[ 'f_current_odds_prob_away' , 'f_current_odds_prob' ]]) 0.5545776633924343 So whilst our model performs decently, it doesn't beat the odds in terms of log loss. That's okay, it's still a decent start. In future iterations we can implement other algorithms and create new features which may improve performance.","title":"Creating Predictions for the 2018 Season"},{"location":"modelling/AFLmodellingPython/#next-steps","text":"Now that we have a model up and running, the next steps are to implement the model on a week to week basis.","title":"Next Steps"},{"location":"modelling/AFLmodellingPython/#04-weekly-predictions","text":"Now that we have explored different algorithms for modelling, we can implement our chosen model and predict this week's AFL games! All you need to do is run the afl_modelling script each Thursday or Friday to predict the following week's games. # Import Modules from afl_feature_creation_v2 import prepare_afl_features import afl_data_cleaning_v2 import afl_feature_creation_v2 import afl_modelling_v2 import datetime import pandas as pd import numpy as np pd . set_option ( 'display.max_columns' , None ) from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler import warnings warnings . filterwarnings ( 'ignore' )","title":"04. Weekly Predictions"},{"location":"modelling/AFLmodellingPython/#creating-the-features-for-this-weekends-games","text":"To actually predict this weekend's games, we need to create the same features that we have created in the previous tutorials for the games that will be played this weekend. This includes all the rolling averages, efficiency features, elo features etc. So the majority of this tutorial will be using previously defined functions to create features for the following weekend's games.","title":"Creating The Features For This Weekend's Games"},{"location":"modelling/AFLmodellingPython/#create-next-weeks-dataframe","text":"Let's first get our cleaned afl_data dataset, as well as the odds for next weekend and the 2018 fixture. # Grab the cleaned AFL dataset and the column order afl_data = afl_data_cleaning_v2 . prepare_afl_data () ordered_cols = afl_data . columns # Define a function which grabs the odds for each game for the following weekend def get_next_week_odds ( path ): # Get next week's odds next_week_odds = pd . read_csv ( path ) next_week_odds = next_week_odds . rename ( columns = { \"team_1\" : \"home_team\" , \"team_2\" : \"away_team\" , \"team_1_odds\" : \"odds\" , \"team_2_odds\" : \"odds_away\" }) return next_week_odds # Import the fixture # Define a function which gets the fixture and cleans it up def get_fixture ( path ): # Get the afl fixture fixture = pd . read_csv ( path ) # Replace team names and reformat fixture = fixture . replace ({ 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' }) fixture [ 'Date' ] = pd . to_datetime ( fixture [ 'Date' ]) . dt . date . astype ( str ) fixture = fixture . rename ( columns = { \"Home.Team\" : \"home_team\" , \"Away.Team\" : \"away_team\" }) return fixture next_week_odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) fixture = get_fixture ( \"data/afl_fixture_2018.csv\" ) fixture . tail () Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG next_week_odds home_team away_team odds odds_away 0 West Coast Collingwood 2.34 1.75 Now that we have these DataFrames, we will define a function which combines the fixture and next week's odds to create a single DataFrame for the games over the next 7 days. To use this function we will need Game IDs for next week. So we will create another function which creates Game IDs by using the Game ID from the last game played and adding 1 to it. # Define a function which creates game IDs for this week's footy games def create_next_weeks_game_ids ( afl_data ): odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) # Get last week's Game ID last_afl_data_game = afl_data [ 'game' ] . iloc [ - 1 ] # Create Game IDs for next week game_ids = [( i + 1 ) + last_afl_data_game for i in range ( odds . shape [ 0 ])] return game_ids # Define a function which creates this week's footy game DataFrame def get_next_week_df ( afl_data ): # Get the fixture and the odds for next week's footy games fixture = get_fixture ( \"data/afl_fixture_2018.csv\" ) next_week_odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) next_week_odds [ 'game' ] = create_next_weeks_game_ids ( afl_data ) # Get today's date and next week's date and create a DataFrame for next week's games # todays_date = datetime.datetime.today().strftime('%Y-%m-%d') # date_in_7_days = (datetime.datetime.today() + datetime.timedelta(days=7)).strftime('%Y-%m-%d') todays_date = '2018-09-27' date_in_7_days = '2018-10-04' fixture = fixture [( fixture [ 'Date' ] >= todays_date ) & ( fixture [ 'Date' ] < date_in_7_days )] . drop ( columns = [ 'Season.Game' ]) next_week_df = pd . merge ( fixture , next_week_odds , on = [ 'home_team' , 'away_team' ]) # Split the DataFrame onto two rows for each game h_df = ( next_week_df [[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds' , 'Season' , 'Round' , 'Venue' ]] . rename ( columns = { 'home_team' : 'team' , 'away_team' : 'opponent' }) . assign ( home_game = 1 )) a_df = ( next_week_df [[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds_away' , 'Season' , 'Round' , 'Venue' ]] . rename ( columns = { 'odds_away' : 'odds' , 'home_team' : 'opponent' , 'away_team' : 'team' }) . assign ( home_game = 0 )) next_week = a_df . append ( h_df ) . sort_values ( by = 'game' ) . rename ( columns = { 'Date' : 'date' , 'Season' : 'season' , 'Round' : 'round' , 'Venue' : 'venue' }) next_week [ 'date' ] = pd . to_datetime ( next_week . date ) next_week [ 'round' ] = afl_data [ 'round' ] . iloc [ - 1 ] + 1 return next_week next_week_df = get_next_week_df ( afl_data ) game_ids_next_round = create_next_weeks_game_ids ( afl_data ) next_week_df date round season venue game home_game odds opponent team 0 2018-09-29 27 2018 MCG 15407 0 1.75 West Coast Collingwood 0 2018-09-29 27 2018 MCG 15407 1 2.34 Collingwood West Coast fixture . tail () Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG","title":"Create Next Week's DataFrame"},{"location":"modelling/AFLmodellingPython/#create-each-feature","text":"Now let's append next week's DataFrame to our afl_data, match_results and odds DataFrames and then create all the features we used in the AFL Feature Creation Tutorial . We need to append the games and then feed them into our function so that we can create features for upcoming games. # Append next week's games to our afl_data DataFrame afl_data = afl_data . append ( next_week_df ) . reset_index ( drop = True ) # Append next week's games to match results (we need to do this for our feature creation to run) match_results = afl_data_cleaning_v2 . get_cleaned_match_results () . append ( next_week_df ) # Append next week's games to odds odds = ( afl_data_cleaning_v2 . get_cleaned_odds () . pipe ( lambda df : df . append ( next_week_df [ df . columns ])) . reset_index ( drop = True )) features_df = afl_feature_creation_v2 . prepare_afl_features ( afl_data = afl_data , match_results = match_results , odds = odds ) features_df . tail () game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_GA1_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_Unnamed: 0_diff f_behinds_diff f_goals_diff f_margin_diff f_opponent_behinds_diff f_opponent_goals_diff f_opponent_points_diff f_points_diff f_current_odds_prob f_current_odds_prob_away 1065 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.966936 -23.2 2.0 1.813998 1523.456734 1609.444874 0.653525 0.680168 0.704767 0.749812 140.535514 0.605144 -9.771981 5.892176 7.172376 6.614609 -1.365211 30.766262 21.998618 0.067228 -1.404730 -3.166732 6.933998 6.675576 0.000000 38.708158 24.587333 12.008987 10.482382 -16.709540 -15.415060 289.188486 6.350287 -2.263536 -20.966818 50.388632 0.723637 15.537783 22.912269 2.065039 10.215523 -6.689429 3259.163465 -0.136383 3.553795 16.563721 -2.353514 1.162696 4.622664 21.186385 0.661551 0.340379 1066 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.089084 -3.2 2.0 2.577161 1397.237139 1499.366007 0.725980 0.655749 0.723949 0.677174 51.799992 3.399035 6.067393 -2.189489 -10.475859 1.154766 -8.883840 -21.810962 33.058382 40.618410 2.286314 -0.345734 -3.778445 -2.182673 0.000000 19.816372 -21.562916 2.678384 -14.777698 13.242010 12.065594 -82.381996 -2.176564 2.335825 -4.952336 45.719406 3.344217 -2.095613 -3.929084 -3.182381 -12.832197 57.226776 -20221.371526 1.968709 -1.897958 -15.177001 1.067099 0.781811 5.757963 -9.419038 0.284269 0.717566 1067 15404 Collingwood GWS 2018-09-15 25 M.C.G. 2018 1.882301 12.6 3.0 2.018344 1546.000498 1590.806454 0.693185 0.706222 0.718446 0.727961 205.916671 -1.642954 -2.980828 -0.266023 8.547225 -3.751909 -0.664977 10.563513 48.175985 43.531908 -5.836979 5.388668 4.395675 2.555152 0.000000 51.588962 11.558254 4.276481 11.284445 -3.412977 -2.206815 -234.577304 2.637758 -10.537765 -11.127876 125.607377 -3.485896 3.532031 15.102292 -2.500685 8.187543 38.053445 12500.525732 -1.006173 2.520135 18.634835 -2.159882 -0.393386 -4.520198 14.114637 0.608495 0.393856 1068 15406 West Coast Melbourne 2018-09-22 26 Perth Stadium 2018 2.013572 21.2 3.0 1.884148 1577.888606 1542.095154 0.688877 0.708941 0.649180 0.698319 -118.135184 -3.005709 2.453190 -5.103869 -14.368949 -12.245458 2.771411 -45.364271 -60.210182 -24.049523 -2.791277 6.115918 -5.041030 -5.335746 0.000000 -78.816902 -18.784547 -13.957754 -5.527613 18.606721 25.366778 -910.988860 -5.515812 -9.483590 8.914093 -131.380758 -7.142529 -49.484957 -13.718798 -4.862994 -9.834616 -23.673638 -3178.282073 -1.785349 -2.569957 -20.008787 0.476202 0.387915 2.803694 -17.205093 0.543774 0.457875 1069 15407 West Coast Collingwood 2018-09-29 27 MCG 2018 1.981832 17.2 3.0 1.838864 1591.348723 1562.924273 0.679011 0.724125 0.711352 0.709346 159.522670 0.893421 -0.475725 3.391070 -5.088751 5.875388 5.352234 7.729063 -7.358202 -4.719968 6.113565 4.822252 2.871241 2.690270 3.636364 -64.238180 -0.631102 2.078832 6.005613 56.879978 34.373271 1016.491933 1.199751 2.454685 12.197047 219.666562 2.484363 0.379162 2.566991 0.639666 2.258377 -23.841529 -368920.360240 -0.646160 0.892051 3.040850 1.589568 0.012622 1.665299 4.706148 0.427350 0.571429","title":"Create Each Feature"},{"location":"modelling/AFLmodellingPython/#create-predictions-for-the-upcoming-round","text":"Now that we have our features, we can use our model that we created in part 3 to predict the next round. First we need to filter our features_df into a training df and a df with next round's features/matches. Then we can use the model created in the last tutorial to create predictions. For simplicity, I have hardcoded the parameters we used in the last tutorial. # Get the train df by only taking the games IDs which aren't in the next week df train_df = features_df [ ~ features_df . game . isin ( next_week_df . game )] # Get the result and merge to the feature_df match_results = ( pd . read_csv ( \"data/afl_match_results.csv\" ) . rename ( columns = { 'Game' : 'game' }) . assign ( result = lambda df : df . apply ( lambda row : 1 if row [ 'Home.Points' ] > row [ 'Away.Points' ] else 0 , axis = 1 ))) train_df = pd . merge ( train_df , match_results [[ 'game' , 'result' ]], on = 'game' ) train_x = train_df . drop ( columns = [ 'result' ]) train_y = train_df . result next_round_x = features_df [ features_df . game . isin ( next_week_df . game )] # Fit out logistic regression model - note that our predictions come out in the order of [away_team_prob, home_team_prob] lr_best_params = { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } feature_cols = [ col for col in train_df if col . startswith ( 'f_' )] # Scale features scaler = StandardScaler () train_x [ feature_cols ] = scaler . fit_transform ( train_x [ feature_cols ]) next_round_x [ feature_cols ] = scaler . transform ( next_round_x [ feature_cols ]) lr = LogisticRegression ( ** lr_best_params ) lr . fit ( train_x [ feature_cols ], train_y ) prediction_probs = lr . predict_proba ( next_round_x [ feature_cols ]) modelled_home_odds = [ 1 / i [ 1 ] for i in prediction_probs ] modelled_away_odds = [ 1 / i [ 0 ] for i in prediction_probs ] # Create a predictions df preds_df = ( next_round_x [[ 'date' , 'home_team' , 'away_team' , 'venue' , 'game' ]] . copy () . assign ( modelled_home_odds = modelled_home_odds , modelled_away_odds = modelled_away_odds ) . pipe ( pd . merge , next_week_odds , on = [ 'home_team' , 'away_team' ]) . pipe ( pd . merge , features_df [[ 'game' , 'f_elo_home' , 'f_elo_away' ]], on = 'game' ) . drop ( columns = 'game' ) ) preds_df date home_team away_team venue modelled_home_odds modelled_away_odds odds odds_away f_elo_home f_elo_away 0 2018-09-29 West Coast Collingwood MCG 2.326826 1.753679 2.34 1.75 1591.348723 1562.924273 Alternatively, if you want to generate predictions using a script which uses all the above code, just run the following: print ( afl_modelling_v2 . create_predictions ()) date home_team away_team venue modelled_home_odds \\ 0 2018-09-29 West Coast Collingwood MCG 2.326826 modelled_away_odds odds odds_away f_elo_home f_elo_away 0 1.753679 2.34 1.75 1591.348723 1562.924273","title":"Create Predictions For the Upcoming Round"},{"location":"modelling/AFLmodellingPython/#conclusion","text":"Congratulations! You have created AFL predictions for this week. If you are beginner to this, don't be overwhelmed. The process gets easier each time you do it. And it is super rewarding. In future iterations we will update this tutorial to predict actual odds, and then integrate this with Betfair's API so that you can create an automated betting strategy using Machine Learning to create your predictions!","title":"Conclusion"},{"location":"modelling/AFLmodellingPython/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/AusOpenPythonTutorial/","text":"Australian Open Datathon R Tutorial \u00b6 Overview \u00b6 The Task \u00b6 This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodology and thought process, read this article. Intention \u00b6 This notebook will demonstrate how to: Process the raw data sets Produce simple features Run a predictive model on H2O Outputs the final predictions for the submissions Load the data and required packages import numpy as np import pandas as pd import os import gc import sys import warnings warnings.filterwarnings ( 'ignore' ) import h2o from h2o.automl import H2OAutoML pd.options.display.max_columns = 999 # We are loading both the mens and womens match csvs df_atp = pd.read_csv ( \"data/ATP_matches.csv\" ) df_wta = pd.read_csv ( \"data/WTA_matches.csv\" ) Data pre-processing \u00b6 Filter the matches to hard and indoor hard only due to the fact that Australian Open is on hard surface and we want the models to train specifically for hard surfaces matches Convert the columns in both datasets to the correct types. For example, we want to make sure the date columns are in the datetime format and numerical columns are either integer or floats. This will help reduce the memory in use and make the feature engineering process easier ### Include hard and indoor hard only df_atp = df_atp.loc [ df_atp.Court_Surface.isin ([ 'Hard' , 'Indoor Hard' ])] df_wta = df_wta.loc [ df_wta.Court_Surface.isin ([ 'Hard' , 'Indoor Hard' ])] ### Exclude qualifying rounds df_atp = df_atp.loc [ df_atp.Round_Description != 'Qualifying' ] df_wta = df_wta.loc [ df_wta.Round_Description != 'Qualifying' ] # Store the shape of the data for reference check later atp_shape = df_atp.shape wta_shape = df_wta.shape numeric_columns = [ 'Winner_Rank' , 'Loser_Rank' , 'Retirement_Ind' , 'Winner_Sets_Won' , 'Winner_Games_Won' , 'Winner_Aces' , 'Winner_DoubleFaults' , 'Winner_FirstServes_Won' , 'Winner_FirstServes_In' , 'Winner_SecondServes_Won' , 'Winner_SecondServes_In' , 'Winner_BreakPoints_Won' , 'Winner_BreakPoints' , 'Winner_ReturnPoints_Won' , 'Winner_ReturnPoints_Faced' , 'Winner_TotalPoints_Won' , 'Loser_Sets_Won' , 'Loser_Games_Won' , 'Loser_Aces' , 'Loser_DoubleFaults' , 'Loser_FirstServes_Won' , 'Loser_FirstServes_In' , 'Loser_SecondServes_Won' , 'Loser_SecondServes_In' , 'Loser_BreakPoints_Won' , 'Loser_BreakPoints' , 'Loser_ReturnPoints_Won' , 'Loser_ReturnPoints_Faced' , 'Loser_TotalPoints_Won' ] text_columns = [ 'Winner' , 'Loser' , 'Tournament' , 'Court_Surface' , 'Round_Description' ] date_columns = [ 'Tournament_Date' ] # we set the **erros** to coerce so any non-numerical values (text,special characters) will return an NA df_atp [ numeric_columns ] = df_atp [ numeric_columns ] .apply ( pd.to_numeric , errors = 'coerce' ) df_wta [ numeric_columns ] = df_wta [ numeric_columns ] .apply ( pd.to_numeric , errors = 'coerce' ) df_atp [ date_columns ] = df_atp [ date_columns ] .apply ( pd.to_datetime ) df_wta [ date_columns ] = df_wta [ date_columns ] .apply ( pd.to_datetime ) Feature Engineering \u00b6 The raw datasets are constructed in a way that each row will have the seperate stats for both the winner and loser of that match. However, we want to reshape the data so that each row we will only have one player randomly selected from the winner/loser columns and the features are the difference between opponents statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. In addition, for the features, we will take the rolling average of the player's most recent 15 matches before the particular tournament starts. For example, if the match is the second round of the Australian Open 2018, the features will be the last 15 matches before the first round of Australian Open 2018. The reason of not including the stats in the first round is that we would not have known the player's stats in the first round for the final submissions A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, difference in ELO rating etc. The target variable will be whether or not Player A wins (1=Player A wins and 0=lose). The steps we take are: Convert the raw data frames into long format: Create some new features Take the rolling average for each player and each match Since we will be only training our models on US Open and Australian Open, we will only be creating features for those matches. However, the rolling average will take into account any hard surface matches before those tournaments Calculate the difference of averages for each match in the data frames Convert the raw data frames into long format: \u00b6 # Before we split the data frame into winner and loser, we want to create a feature that captures the total number of games the match takes. # We have to do it before the split or we will lose this information df_atp [ 'Total_Games' ] = df_atp.Winner_Games_Won + df_atp.Loser_Games_Won df_wta [ 'Total_Games' ] = df_wta.Winner_Games_Won + df_wta.Loser_Games_Won # Get the column names for the winner and loser stats winner_cols = [ col for col in df_atp.columns if col.startswith ( 'Winner' )] loser_cols = [ col for col in df_atp.columns if col.startswith ( 'Loser' )] # create a winner data frame to store the winner stats and a loser data frame for the losers # In addition to the winner and loser columns, we are adding common columns as well (e.g. tournamnt dates) common_cols = [ 'Total_Games' , 'Tournament' , 'Tournament_Date' , 'Court_Surface' , 'Round_Description' ] df_winner_atp = df_atp [ winner_cols + common_cols ] df_loser_atp = df_atp [ loser_cols + common_cols ] df_winner_wta = df_wta [ winner_cols + common_cols ] df_loser_wta = df_wta [ loser_cols + common_cols ] # Create a new column to show whether the player has won or not. df_winner_atp [ \"won\" ] = 1 df_loser_atp [ \"won\" ] = 0 df_winner_wta [ \"won\" ] = 1 df_loser_wta [ \"won\" ] = 0 # Rename the columns for the winner and loser data frames so we can append them later on. # We will rename the Winner_ / Loser_ columns to Player_ new_column_names = [ col.replace ( 'Winner' , 'Player' ) for col in winner_cols ] df_winner_atp.columns = new_column_names + common_cols + [ 'won' ] # They all should be the same df_loser_atp.columns = df_winner_atp.columns df_winner_wta.columns = df_winner_atp.columns df_loser_wta.columns = df_winner_atp.columns # append the winner and loser data frames df_long_atp = df_winner_atp.append ( df_loser_atp ) df_long_wta = df_winner_wta.append ( df_loser_wta ) So now our data frames are in long format and should looks like this df_long_atp.head () Player Player_Rank Player_Sets_Won Player_Games_Won Player_Aces Player_DoubleFaults Player_FirstServes_Won Player_FirstServes_In Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Total_Games Tournament Tournament_Date Court_Surface Round_Description won Edouard Roger-Vasselin 106.0 2.0 12 5.0 2.0 22 30 12 19 4.0 7.0 25.0 59.0 59 19 Chennai 2012-01-02 Hard First Round 1 Dudi Sela 83.0 2.0 12 2.0 0.0 14 17 11 16 6.0 14.0 36.0 58.0 61 13 Chennai 2012-01-02 Hard First Round 1 Go Soeda 120.0 2.0 19 6.0 1.0 48 64 19 39 5.0 11.0 42.0 105.0 109 33 Chennai 2012-01-02 Hard First Round 1 Yuki Bhambri 345.0 2.0 12 1.0 2.0 22 29 9 17 5.0 13.0 34.0 62.0 65 17 Chennai 2012-01-02 Hard First Round 1 Yuichi Sugita 235.0 2.0 12 3.0 1.0 37 51 11 27 3.0 7.0 22.0 54.0 70 19 Chennai 2012-01-02 Hard First Round 1 Create some new features \u00b6 Thinking about the dynamics of tennis, we know that players often will matches by \u201cbreaking\u201d the opponent\u2019s serve (i.e. winning a game when the opponent is serving). This is especially important in tennis. Let\u2019s create a feature called Player_BreakPoints_Per_Game, which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let\u2019s also create a feature called Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \u201cholding\u201d serve is important (i.e. winning a game when you are serving). Let\u2019s create a feature called Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let\u2019s create a feature called Player_Game_Win_Percentage which is the propotion of games that a player wins. So the four new features we will create are: Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage # Here, we will define a function so we can apply it to both atp and wta data frames def get_new_features ( df ) : # Input: # df: data frame to get the data from # Return: the df with the new features # Point Win ratio when serving df [ 'Player_Serve_Win_Ratio' ] = ( df.Player_FirstServes_Won + df.Player_SecondServes_Won - df.Player_DoubleFaults ) \\ / ( df.Player_FirstServes_In + df.Player_SecondServes_In + df.Player_DoubleFaults ) # Point win ratio when returning df [ 'Player_Return_Win_Ratio' ] = df.Player_ReturnPoints_Won / df.Player_ReturnPoints_Faced # Breakpoints per receiving game df [ 'Player_BreakPoints_Per_Return_Game' ] = df.Player_BreakPoints / df.Total_Games df [ 'Player_Game_Win_Percentage' ] = df.Player_Games_Won / df.Total_Games return df # Apply the function we just created to the long data frames df_long_atp = get_new_features ( df_long_atp ) df_long_wta = get_new_features ( df_long_wta ) # The long table should have exactly twice of the rows of the original data assert df_long_atp.shape [ 0 ] == atp_shape [ 0 ] * 2 assert df_long_wta.shape [ 0 ] == wta_shape [ 0 ] * 2 Take the rolling average for each player and each match \u00b6 To train our models, we cannot simply use the player stats for that current match. In fact, we wont be able to use any stats from the same tournament. The logic behind this is that when we try to predict the results in 2019, we would not know the stats of any of the matches in the Australian Open 2019 tournament. As a result, we will use the players' past performance. Here, we will do a rolling average of the most recent 15 matches before the tournament. To do the above, we will follow the steps below: List all the tournament dates for US and Australian Opens Loop through the dates from point 1, for each date, we filter the data to only include matches before that date and take the most recent 15 games Take the average of those 15 games # the two tournaments we will be using for training and thus the feature generation tournaments = [ 'U.S. Open, New York' , 'Australian Open, Melbourne' ] # Store the dates for the loops tournament_dates_atp = df_atp.loc [ df_atp.Tournament.isin ( tournaments )] .groupby ([ 'Tournament' , 'Tournament_Date' ]) \\ .size () .reset_index ()[[ 'Tournament' , 'Tournament_Date' ]] tournament_dates_wta = df_wta.loc [ df_wta.Tournament.isin ( tournaments )] .groupby ([ 'Tournament' , 'Tournament_Date' ]) \\ .size () .reset_index ()[[ 'Tournament' , 'Tournament_Date' ]] # We are adding one more date for the final prediction tournament_dates_atp.loc [ -1 ] = [ 'Australian Open, Melbourne' , pd.to_datetime ( '2019-01-15' )] tournament_dates_wta.loc [ -1 ] = [ 'Australian Open, Melbourne' , pd.to_datetime ( '2019-01-15' )] Following are the dates for each tournament tournament_dates_atp Tournament Tournament_Date Australian Open, Melbourne 2012-01-16 Australian Open, Melbourne 2013-01-1 Australian Open, Melbourne 2014-01-1 Australian Open, Melbourne 2015-01-1 Australian Open, Melbourne 2016-01-1 Australian Open, Melbourne 2017-01-1 Australian Open, Melbourne 2018-01-1 U.S. Open, New York 2012-08-2 U.S. Open, New York 2013-08-2 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 tournament_dates_wta Tournament Tournament_Date Australian Open, Melbourne 2014-01-13 Australian Open, Melbourne 2015-01-19 Australian Open, Melbourne 2016-01-18 Australian Open, Melbourne 2017-01-16 Australian Open, Melbourne 2018-01-15 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 They look fine but it is interesting that for men's, we have two more years of data from 2012 to 2013 # Let's define a function to calculate the rolling averages def get_rolling_features ( df , date_df = None , rolling_cols = None , last_cols = None ) : # Input: # df: data frame to get the data from # date_df: data frame that has the start dates for each tournament # rolling_cols: columns to get the rolling averages # last_cols: columns to get the last value (most recent) # Return: the df with the new features # Sort the data by player and dates so the most recent matches are at the bottom df = df.sort_values ([ 'Player' , 'Tournament_Date' , 'Tournament' ], ascending = True ) # For each tournament, get the rolling averages of that player's past matches before the tournament start date for index , tournament_date in enumerate ( date_df.Tournament_Date ) : # create a temp df to store the interim results df_temp = df.loc [ df.Tournament_Date < tournament_date ] # for ranks, we only take the last one. (comment this out if want to take avg of rank) df_temp_last = df_temp.groupby ( 'Player' )[ last_cols ] .last () .reset_index () # take the most recent 15 matches for the rolling average df_temp = df_temp.groupby ( 'Player' )[ rolling_cols ] .rolling ( 15 , min_periods = 1 ) .mean () .reset_index () df_temp = df_temp.groupby ( 'Player' ) .tail ( 1 ) # take the last row of the above df_temp = df_temp.merge ( df_temp_last , on = 'Player' , how = 'left' ) if index == 0 : df_result = df_temp df_result [ 'tournament_date_index' ] = tournament_date # so we know which tournament this feature is for else : df_temp [ 'tournament_date_index' ] = tournament_date df_result = df_result.append ( df_temp ) df_result.drop ( 'level_1' , axis = 1 , inplace = True ) return df_result # columns we are applying the rolling averages on rolling_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' ] # columns we are taking the most recent values on # For the player rank, we think we can just use the latest rank (before the tournament starts) # as it should refect the most recent performance of the player last_cols = [ 'Player_Rank' ] # Apply the rolling average function to the long data frames (it will take a few mins to run) df_rolling_atp = get_rolling_features ( df_long_atp , tournament_dates_atp , rolling_cols , last_cols = last_cols ) df_rolling_wta = get_rolling_features ( df_long_wta , tournament_dates_wta , rolling_cols , last_cols = last_cols ) df_rolling_atp.head ( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Adrian Mannarino 0.623408 0.353397 0.257859 0.447246 87.0 2012-01-16 Albert Montanes 0.507246 0.195652 0.000000 0.294118 50.0 2012-01-16 df_rolling_wta.head ( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Agnieszka Radwanska 0.413333 0.475410 0.350000 0.350000 5.0 2014-01-13 Ajla Tomljanovic 0.468457 0.407319 0.242253 0.462634 75.0 2014-01-13 Calculate the difference of averages for each match in the data frames \u00b6 In the original data frames, the first column is always the winner and followed by the loser. Same for the player stats. Thus, we cannot simply calculate the difference between winner and loser and create a target variable indicating player 1 will win or not because it will always be the winner in this case (target always = 1). As a result, we need to pick a player randomly so the player might or might not be the winner In addition, instead of using both the features for player 1 and 2, we will take the difference of averages between the randomised player 1 and 2. The main benefit is that it will reduce the number of features to half Steps: We will create a random number for each player which only return 0 or 1 If it is zero, we will assign the winner to player 1 and loser to player 2 We will join the features to the player 1 and 2. The join will be on the player names and the tournament date (tournament_index in the feature data frames) For players who do not have any history, we will fill the stats by zeros and rank by 999 # Randomise the match_wide dataset so the first player is not always the winner # set a seed so the random number is reproducable np.random.seed ( 2 ) # randomise a number 0/1 with 50% chance each # if 0 then take the winner, 1 then take loser df_atp [ 'random_number' ] = np.random.randint ( 2 , size = len ( df_atp )) df_atp [ 'randomised_player_1' ] = np.where ( df_atp [ 'random_number' ] == 0 , df_atp [ 'Winner' ], df_atp [ 'Loser' ]) df_atp [ 'randomised_player_2' ] = np.where ( df_atp [ 'random_number' ] == 0 , df_atp [ 'Loser' ], df_atp [ 'Winner' ]) df_wta [ 'random_number' ] = np.random.randint ( 2 , size = len ( df_wta )) df_wta [ 'randomised_player_1' ] = np.where ( df_wta [ 'random_number' ] == 0 , df_wta [ 'Winner' ], df_wta [ 'Loser' ]) df_wta [ 'randomised_player_2' ] = np.where ( df_wta [ 'random_number' ] == 0 , df_wta [ 'Loser' ], df_wta [ 'Winner' ]) # set the target (win/loss) based on the new randomise number df_atp [ 'player_1_win' ] = np.where ( df_atp [ 'random_number' ] == 0 , 1 , 0 ) df_wta [ 'player_1_win' ] = np.where ( df_wta [ 'random_number' ] == 0 , 1 , 0 ) print ( 'After shuffling, the win rate for player 1 for the mens is {}%' .format ( df_atp [ 'player_1_win' ] .mean () * 100 )) print ( 'After shuffling, the win rate for player 1 for the womens is {}%' .format ( df_wta [ 'player_1_win' ] .mean () * 100 )) After shuffling, the win rate for player 1 for the mens is 49.64798919857267% After shuffling, the win rate for player 1 for the womens is 49.697671426733564% The win rates are close enough to 50%. So we are good to go # To get our data frames ready for model training, we will exclude other tournaments from the data now because we have gotten the rolling averages from them and # for training, we only need US and Australian Open matches df_atp = df_atp.loc [ df_atp.Tournament.isin ( tournaments )] df_wta = df_wta.loc [ df_wta.Tournament.isin ( tournaments )] # now we can remove other stats columns because we will be using the differences cols_to_keep = [ 'Winner' , 'Loser' , 'Tournament' , 'Tournament_Date' , 'player_1_win' , 'randomised_player_1' , 'randomised_player_2' ] df_atp = df_atp [ cols_to_keep ] df_wta = df_wta [ cols_to_keep ] # Here, we are joining the rolling average data frames to the individual matches. # We need to do it twice. One for player 1 and one for player 2 # Get the rolling features for player 1 df_atp = df_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_wta = df_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # we will use '_p1' to denote player 1 and '_p2' for player 2 df_atp = df_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_wta = df_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' .format ( df_atp.loc [ df_atp.Player_p1.isna (), 'randomised_player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament' .format ( df_atp.loc [ df_atp.Player_p2.isna (), 'randomised_player_2' ] .nunique ())) 59 player_1s do Not have previous match history before the tournament 56 player_2s do Not have previous match history before the tournament # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' .format ( df_wta.loc [ df_wta.Player_p1.isna (), 'randomised_player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament' .format ( df_wta.loc [ df_wta.Player_p2.isna (), 'randomised_player_2' ] .nunique ())) 41 player_1s do Not have previous match history before the tournament 37 player_2s do Not have previous match history before the tournament # Most of the missing are for the early years which makes sense as we dont have enough history for them df_wta.loc [ df_wta.Player_p1.isna (), 'Tournament_Date' ] .value_counts () 2014-01-13 29 2014-08-25 7 2015-08-31 5 2015-01-19 3 2017-08-28 3 2018-01-15 3 2018-08-27 3 Name: Tournament_Date, dtype: int64 df_atp.loc [ df_atp.Player_p1.isna (), 'Tournament_Date' ] .value_counts () 2012-01-16 29 2012-08-27 9 2014-01-13 5 2013-08-26 5 2016-01-18 5 2013-01-14 4 2014-08-25 3 2018-01-15 3 2017-08-28 3 2018-08-27 2 2016-08-29 2 2015-01-19 1 Name: Tournament_Date, dtype: int64 Now we have gotten the rolling averages for both player 1 and 2. What we need to do next is to simply calculate their difference. To calculate the difference, we need to: Split the data frames into two new data frames: Player 1 and Player 2 Take the difference between the two data frames def get_player_difference ( df , diff_cols = None ) : # Input: # df: data frame to get the data from # diff_cols: columns we take the difference on. For example is diff_cols = win rate. This function will calculate the # difference of the win rates between player 1 and player 2 # Return: the df with the new features p1_cols = [ i + '_p1' for i in diff_cols ] # column names for player 1 stats p2_cols = [ i + '_p2' for i in diff_cols ] # column names for player 2 stats # For any missing values, we will fill them by zeros except the ranking where we will use 999 df [ 'Player_Rank_p1' ] = df [ 'Player_Rank_p1' ] .fillna ( 999 ) df [ p1_cols ] = df [ p1_cols ] .fillna ( 0 ) df [ 'Player_Rank_p2' ] = df [ 'Player_Rank_p2' ] .fillna ( 999 ) df [ p2_cols ] = df [ p2_cols ] .fillna ( 0 ) new_column_name = [ i + '_diff' for i in diff_cols ] # Take the difference df_p1 = df [ p1_cols ] df_p2 = df [ p2_cols ] df_p1.columns = new_column_name df_p2.columns = new_column_name df_diff = df_p1 - df_p2 df_diff.columns = new_column_name # drop the p1 and p2 columns because We have the differences now df.drop ( p1_cols + p2_cols , axis = 1 , inplace = True ) # Concat the df_diff and raw_df df = pd.concat ([ df , df_diff ], axis = 1 ) return df , new_column_name diff_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' , 'Player_Rank' ] # Apply the function and get the difference between player 1 and 2 df_atp , _ = get_player_difference ( df_atp , diff_cols = diff_cols ) df_wta , _ = get_player_difference ( df_wta , diff_cols = diff_cols ) # Make a copy of the data frames in case we need to come back to check the values df_atp_final = df_atp.copy () df_wta_final = df_wta.copy () df_atp_final.head () Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Juan Martin del Potro Adrian Mannarino Australian Open, Melbourne 2012-01-16 1 Juan Martin del Potro Adrian Mannarino Juan Martin del Potro 2012-01-16 Adrian Mannarino 2012-01-16 0.035030 -0.021271 -0.025975 0.103479 -76.0 Pere Riba Albert Montanes Australian Open, Melbourne 2012-01-16 1 Pere Riba Albert Montanes Pere Riba 2012-01-16 Albert Montanes 2012-01-16 -0.156369 0.008893 0.066667 -0.094118 39.0 Tomas Berdych Albert Ramos-Vinolas Australian Open, Melbourne 2012-01-16 0 Albert Ramos-Vinolas Tomas Berdych Albert Ramos-Vinolas 2012-01-16 NaN NaT 0.498027 0.380092 0.414815 0.394444 -934.0 Rafael Nadal Alex Kuznetsov Australian Open, Melbourne 2012-01-16 0 Alex Kuznetsov Rafael Nadal NaN NaT Rafael Nadal 2012-01-16 -0.670139 -0.423057 -0.445623 -0.574767 997.0 Roger Federer Alexander Kudryavtsev Australian Open, Melbourne 2012-01-16 0 Alexander Kudryavtsev Roger Federer NaN NaT Roger Federer 2012-01-16 -0.721415 -0.449516 -0.360255 -0.668090 996.0 Modelling \u00b6 We will trian two models here, one for mens and one for womens. For training, we will use all available data from the second year (too many missing values in the first year) up until 2017. For validation, we will test the model on the 2018 Australian Open data This setup allows us to 'mimic' the final prediction (using historical matches to predict 2019 results) df_train_atp = df_atp_final.loc [( df_atp_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & ( df_atp_final.Tournament_Date > '2012-01-16' )] # excluding first year df_valid_atp = df_atp_final.loc [ df_atp_final.Tournament_Date == '2018-01-15' ] # Australian Open 2018 only df_train_wta = df_wta_final.loc [( df_wta_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & ( df_wta_final.Tournament_Date > '2014-01-13' )] # excluding first year df_train_atp.head () Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Daniel Brands Adrian Ungur U.S. Open, New York 2012-08-27 0 Adrian Ungur Daniel Brands NaN NaT Daniel Brands 2012-08-27 -0.535211 -0.300000 -0.043478 -0.434783 870.0 Richard Gasquet Albert Montanes U.S. Open, New York 2012-08-27 1 Richard Gasquet Albert Montanes Richard Gasquet 2012-08-27 Albert Montanes 2012-08-27 0.080003 0.077451 0.180847 0.131108 -37.0 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 1 Martin Klizan Alejandro Falla Martin Klizan 2012-08-27 Alejandro Falla 2012-08-27 0.077117 -0.044716 -0.087362 0.068180 -2.0 Andy Murray Alex Bogomolov Jr. U.S. Open, New York 2012-08-27 1 Andy Murray Alex Bogomolov Jr. Andy Murray 2012-08-27 Alex Bogomolov Jr. 2012-08-27 0.039641 0.031701 0.094722 0.059010 -69.0 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 1 Tommy Robredo Andreas Seppi Tommy Robredo 2012-08-27 Andreas Seppi 2012-08-27 -0.026814 0.006442 -0.009930 -0.067780 151.0 # target variable target = 'player_1_win' # features being fed into the models feats = [ 'Player_Serve_Win_Ratio_diff' , 'Player_Return_Win_Ratio_diff' , 'Player_BreakPoints_Per_Return_Game_diff' , 'Player_Game_Win_Percentage_diff' , 'Player_Rank_diff' ] print ( feats ) H2O model for ATP \u00b6 h2o.init () # Convert to an h2o frame df_train_atp_h2o = h2o.H2OFrame ( df_train_atp ) df_valid_atp_h2o = h2o.H2OFrame ( df_valid_atp ) # For binary classification, response should be a factor df_train_atp_h2o [ target ] = df_train_atp_h2o [ target ] .asfactor () df_valid_atp_h2o [ target ] = df_valid_atp_h2o [ target ] .asfactor () # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_atp = h2o.automl.H2OAutoML ( max_runtime_secs = 300 , max_models = 100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True , seed = 183 ) aml_atp.train ( x = feats , y = target , training_frame = df_train_atp_h2o , validation_frame = df_valid_atp_h2o ) # View the AutoML Leaderboard lb = aml_atp.leaderboard lb.head () model_id auc logloss mean_per_class_error rmse mse GBM_5_AutoML_20181221_094949 0.790281 0.554852 0.281363 0.431379 0.186088 GBM_grid_1_AutoML_20181221_094949_model_15 0.789329 0.556804 0.29856 0.431931 0.186564 GBM_grid_1_AutoML_20181221_094949_model_7 0.788013 0.557808 0.295899 0.432968 0.187461 StackedEnsemble_BestOfFamily_AutoML_20181221_094949 0.788131 0.558028 0.285321 0.432849 0.187358 GBM_grid_1_AutoML_20181221_094949_model_20 0.785633 0.561094 0.283932 0.43479 0.189043 StackedEnsemble_AllModels_AutoML_20181221_094949 0.784411 0.561587 0.293244 0.434667 0.188935 GBM_grid_1_AutoML_20181221_094949_model_25 0.785311 0.561783 0.291912 0.434888 0.189127 GBM_grid_1_AutoML_20181221_094949_model_17 0.774832 0.570883 0.295836 0.439375 0.193051 DeepLearning_1_AutoML_20181221_094949 0.779388 0.572823 0.311737 0.438479 0.192264 GBM_grid_1_AutoML_20181221_094949_model_14 0.7718 0.578867 0.285835 0.441373 0.19481 H2O model for WTA # Convert to an h2o frame df_train_wta_h2o = h2o.H2OFrame ( df_train_wta ) df_valid_wta_h2o = h2o.H2OFrame ( df_valid_wta ) # For binary classification, response should be a factor df_train_wta_h2o [ target ] = df_train_wta_h2o [ target ] .asfactor () df_valid_wta_h2o [ target ] = df_valid_wta_h2o [ target ] .asfactor () # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_wta = h2o.automl.H2OAutoML ( max_runtime_secs = 300 , max_models = 100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True , seed = 183 ) aml_wta.train ( x = feats , y = target , training_frame = df_train_wta_h2o , validation_frame = df_valid_wta_h2o ) # View the AutoML Leaderboard lb = aml_wta.leaderboard lb.head () model_id auc logloss mean_per_class_error rmse mse StackedEnsemble_AllModels_AutoML_20181221_095400 0.726046 0.60827 0.321222 0.457117 0.208956 StackedEnsemble_BestOfFamily_AutoML_20181221_095400 0.724911 0.609329 0.337847 0.457659 0.209452 DeepLearning_grid_1_AutoML_20181221_095400_model_3 0.729152 0.612669 0.315971 0.45641 0.20831 GBM_grid_1_AutoML_20181221_095400_model_7 0.721204 0.615763 0.336848 0.460885 0.212415 GBM_5_AutoML_20181221_095400 0.719252 0.616535 0.319179 0.461055 0.212572 GBM_grid_1_AutoML_20181221_095400_model_15 0.715921 0.619263 0.318673 0.462215 0.213643 GLM_grid_1_AutoML_20181221_095400_model_1 0.726048 0.622989 0.366124 0.463099 0.214461 GBM_grid_1_AutoML_20181221_095400_model_17 0.709261 0.624902 0.34876 0.465628 0.216809 GBM_grid_1_AutoML_20181221_095400_model_18 0.70946 0.625704 0.393556 0.466147 0.217293 DeepLearning_grid_1_AutoML_20181221_095400_model_2 0.713419 0.628008 0.311334 0.463638 0.21496 Use the models to predict and make submissions \u00b6 Now let's use the models we just created to make the submissions df_predict_atp = pd.read_csv ( \"data/men_dummy_submission_file.csv\" ) df_predict_wta = pd.read_csv ( \"data/women_dummy_submission_file.csv\" , encoding = 'latin1' ) # for womens, there are some names need a different encoding df_predict_wta.head ( 2 ) player_1 player_2 player_1_win_probability Simona Halep Angelique Kerber 0.5 Simona Halep Caroline Wozniacki 0.5 Get the features for the predict df \u00b6 We need to join the features to the 2019 players # Before we join the features by the names and the dates, we need to convert any non-english characters to english first translationTable = str.maketrans ( \"\u00e9\u00e0\u00e8\u00f9\u00e2\u00ea\u00ee\u00f4\u00fb\u00e7\u00f1\u00e1\" , \"eaeuaeioucna\" ) df_predict_atp [ 'player_1' ] = df_predict_atp.player_1.apply ( lambda x : x.translate ( translationTable )) df_predict_atp [ 'player_2' ] = df_predict_atp.player_2.apply ( lambda x : x.translate ( translationTable )) df_predict_wta [ 'player_1' ] = df_predict_wta.player_1.apply ( lambda x : x.translate ( translationTable )) df_predict_wta [ 'player_2' ] = df_predict_wta.player_2.apply ( lambda x : x.translate ( translationTable )) # Also we need to convert the names into lower cases df_predict_atp [ 'player_1' ] = df_predict_atp [ 'player_1' ] .str.lower () df_predict_atp [ 'player_2' ] = df_predict_atp [ 'player_2' ] .str.lower () df_predict_wta [ 'player_1' ] = df_predict_wta [ 'player_1' ] .str.lower () df_predict_wta [ 'player_2' ] = df_predict_wta [ 'player_2' ] .str.lower () df_rolling_atp [ 'Player' ] = df_rolling_atp [ 'Player' ] .str.lower () df_rolling_wta [ 'Player' ] = df_rolling_wta [ 'Player' ] .str.lower () # Lastly, some players have slightly difference names in the submission data and the match data. So we are editing them here manually df_predict_atp.loc [ df_predict_atp.player_1 == 'jaume munar' , 'player_1' ] = 'jaume antoni munar clar' df_predict_atp.loc [ df_predict_atp.player_2 == 'jaume munar' , 'player_2' ] = 'jaume antoni munar clar' df_predict_wta.loc [ df_predict_wta.player_1 == 'daria kasatkina' , 'player_1' ] = 'darya kasatkina' df_predict_wta.loc [ df_predict_wta.player_2 == 'daria kasatkina' , 'player_2' ] = 'darya kasatkina' df_predict_wta.loc [ df_predict_wta.player_1 == 'lesia tsurenko' , 'player_1' ] = 'lesya tsurenko' df_predict_wta.loc [ df_predict_wta.player_2 == 'lesia tsurenko' , 'player_2' ] = 'lesya tsurenko' df_predict_wta.loc [ df_predict_wta.player_1 == 'danielle collins' , 'player_1' ] = 'danielle rose collins' df_predict_wta.loc [ df_predict_wta.player_2 == 'danielle collins' , 'player_2' ] = 'danielle rose collins' df_predict_wta.loc [ df_predict_wta.player_1 == 'anna karolina schmiedlova' , 'player_1' ] = 'anna schmiedlova' df_predict_wta.loc [ df_predict_wta.player_2 == 'anna karolina schmiedlova' , 'player_2' ] = 'anna schmiedlova' df_predict_wta.loc [ df_predict_wta.player_1 == 'georgina garcia perez' , 'player_1' ] = 'georgina garcia-perez' df_predict_wta.loc [ df_predict_wta.player_2 == 'georgina garcia perez' , 'player_2' ] = 'georgina garcia-perez' # create and tournament date column and set it to 2019 so we can join the lastest features df_predict_atp [ 'Tournament_Date' ] = pd.to_datetime ( '2019-01-15' ) df_predict_wta [ 'Tournament_Date' ] = pd.to_datetime ( '2019-01-15' ) # Get the rolling features for player 1 df_predict_atp = df_predict_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_predict_wta = df_predict_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # For duplicate columns, we will use '_p1' to denote player 1 and '_p2' for player 2 df_predict_atp = df_predict_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_predict_wta = df_predict_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_predict_atp.head ( 2 ) player_1 player_2 player_1_win_probability Tournament_Date Player_p1 Player_Serve_Win_Ratio_p1 Player_Return_Win_Ratio_p1 Player_BreakPoints_Per_Return_Game_p1 Player_Game_Win_Percentage_p1 Player_Rank_p1 tournament_date_index_p1 Player_p2 Player_Serve_Win_Ratio_p2 Player_Return_Win_Ratio_p2 Player_BreakPoints_Per_Return_Game_p2 Player_Game_Win_Percentage_p2 Player_Rank_p2 tournament_date_index_p2 novak djokovic rafael nadal 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 rafael nadal 0.622425 0.401028 0.334270 0.570833 1.0 2019-01-15 novak djokovic roger federer 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 roger federer 0.620070 0.389781 0.269224 0.564244 3.0 2019-01-15 # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the mens' .format ( df_predict_atp.loc [ df_predict_atp.Player_p1.isna (), 'player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament in the mens' .format ( df_predict_atp.loc [ df_predict_atp.Player_p2.isna (), 'player_2' ] .nunique ())) 3 player_1s do Not have previous match history before the tournament in the mens 3 player_2s do Not have previous match history before the tournament in the mens # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the womens' .format ( df_predict_wta.loc [ df_predict_wta.Player_p1.isna (), 'player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament in the womens' .format ( df_predict_wta.loc [ df_predict_wta.Player_p2.isna (), 'player_2' ] .nunique ())) 0 player_1s do Not have previous match history before the tournament in the womens 0 player_2s do Not have previous match history before the tournament in the womens print ( df_predict_atp.loc [ df_predict_atp.Player_p1.isna (), 'player_1' ] .unique () .tolist ()) [ 'christian garin' , 'pedro sousa' , 'hugo dellien' ] print ( df_predict_wta.loc [ df_predict_wta.Player_p1.isna (), 'player_1' ] .unique () .tolist ()) [] We will do the differencing again for the prediction data frames exactly like what we did for training # Apply the function and get the difference between player 1 and 2 df_predict_atp , _ = get_player_difference ( df_predict_atp , diff_cols = diff_cols ) df_predict_wta , _ = get_player_difference ( df_predict_wta , diff_cols = diff_cols ) Make the prediction \u00b6 df_predict_atp_h2o = h2o.H2OFrame ( df_predict_atp [ feats ]) df_predict_wta_h2o = h2o.H2OFrame ( df_predict_wta [ feats ]) atp_preds = aml_atp.predict ( df_predict_atp_h2o )[ 'p1' ] .as_data_frame () wta_preds = aml_wta.predict ( df_predict_wta_h2o )[ 'p1' ] .as_data_frame () df_predict_atp [ 'player_1_win_probability' ] = atp_preds df_predict_wta [ 'player_1_win_probability' ] = wta_preds atp_submission = df_predict_atp [[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] wta_submission = df_predict_wta [[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] atp_submission.head () player_1 player_2 player_1_win_probability novak djokovic rafael nadal 0.571588 novak djokovic roger federer 0.662511 novak djokovic juan martin del potro 0.544306 novak djokovic alexander zverev 0.709483 novak djokovic kevin anderson 0.687195 wta_submission.head () player_1 player_2 player_1_win_probability simona halep angelique kerber 0.455224 simona halep caroline wozniacki 0.546276 simona halep elina svitolina 0.408014 simona halep naomi osaka 0.285125 simona halep sloane stephens 0.576643 Let's look at who has the highest win rate from our models atp_submission.groupby ( 'player_1' )[ 'player_1_win_probability' ] .mean () \\ .reset_index () .sort_values ( 'player_1_win_probability' , ascending = False ) .head () player_1 player_1_win_probability novak djokovic 0.846377 juan martin del potro 0.787337 karen khachanov 0.782963 rafael nadal 0.778707 roger federer 0.767337 wta_submission.groupby ( 'player_1' )[ 'player_1_win_probability' ] .mean () \\ .reset_index () .sort_values ( 'player_1_win_probability' , ascending = False ) .head () player_1 player_1_win_probability madison keys 0.750580 naomi osaka 0.749195 caroline wozniacki 0.722409 kiki bertens 0.713904 aryna sabalenka 0.707368 Now we can output the predictions as csvs atp_submission.to_csv ( 'submission/atp_submission_python.csv' , index = False ) wta_submission.to_csv ( 'submission/wta_submission_pthon.csv' , index = False ) atp_submission.shape (16256, 3) Disclaimer \u00b6 Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Aus Open Python Tutorial"},{"location":"modelling/AusOpenPythonTutorial/#australian-open-datathon-r-tutorial","text":"","title":"Australian Open Datathon R Tutorial"},{"location":"modelling/AusOpenPythonTutorial/#overview","text":"","title":"Overview"},{"location":"modelling/AusOpenPythonTutorial/#the-task","text":"This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodology and thought process, read this article.","title":"The Task"},{"location":"modelling/AusOpenPythonTutorial/#intention","text":"This notebook will demonstrate how to: Process the raw data sets Produce simple features Run a predictive model on H2O Outputs the final predictions for the submissions Load the data and required packages import numpy as np import pandas as pd import os import gc import sys import warnings warnings.filterwarnings ( 'ignore' ) import h2o from h2o.automl import H2OAutoML pd.options.display.max_columns = 999 # We are loading both the mens and womens match csvs df_atp = pd.read_csv ( \"data/ATP_matches.csv\" ) df_wta = pd.read_csv ( \"data/WTA_matches.csv\" )","title":"Intention"},{"location":"modelling/AusOpenPythonTutorial/#data-pre-processing","text":"Filter the matches to hard and indoor hard only due to the fact that Australian Open is on hard surface and we want the models to train specifically for hard surfaces matches Convert the columns in both datasets to the correct types. For example, we want to make sure the date columns are in the datetime format and numerical columns are either integer or floats. This will help reduce the memory in use and make the feature engineering process easier ### Include hard and indoor hard only df_atp = df_atp.loc [ df_atp.Court_Surface.isin ([ 'Hard' , 'Indoor Hard' ])] df_wta = df_wta.loc [ df_wta.Court_Surface.isin ([ 'Hard' , 'Indoor Hard' ])] ### Exclude qualifying rounds df_atp = df_atp.loc [ df_atp.Round_Description != 'Qualifying' ] df_wta = df_wta.loc [ df_wta.Round_Description != 'Qualifying' ] # Store the shape of the data for reference check later atp_shape = df_atp.shape wta_shape = df_wta.shape numeric_columns = [ 'Winner_Rank' , 'Loser_Rank' , 'Retirement_Ind' , 'Winner_Sets_Won' , 'Winner_Games_Won' , 'Winner_Aces' , 'Winner_DoubleFaults' , 'Winner_FirstServes_Won' , 'Winner_FirstServes_In' , 'Winner_SecondServes_Won' , 'Winner_SecondServes_In' , 'Winner_BreakPoints_Won' , 'Winner_BreakPoints' , 'Winner_ReturnPoints_Won' , 'Winner_ReturnPoints_Faced' , 'Winner_TotalPoints_Won' , 'Loser_Sets_Won' , 'Loser_Games_Won' , 'Loser_Aces' , 'Loser_DoubleFaults' , 'Loser_FirstServes_Won' , 'Loser_FirstServes_In' , 'Loser_SecondServes_Won' , 'Loser_SecondServes_In' , 'Loser_BreakPoints_Won' , 'Loser_BreakPoints' , 'Loser_ReturnPoints_Won' , 'Loser_ReturnPoints_Faced' , 'Loser_TotalPoints_Won' ] text_columns = [ 'Winner' , 'Loser' , 'Tournament' , 'Court_Surface' , 'Round_Description' ] date_columns = [ 'Tournament_Date' ] # we set the **erros** to coerce so any non-numerical values (text,special characters) will return an NA df_atp [ numeric_columns ] = df_atp [ numeric_columns ] .apply ( pd.to_numeric , errors = 'coerce' ) df_wta [ numeric_columns ] = df_wta [ numeric_columns ] .apply ( pd.to_numeric , errors = 'coerce' ) df_atp [ date_columns ] = df_atp [ date_columns ] .apply ( pd.to_datetime ) df_wta [ date_columns ] = df_wta [ date_columns ] .apply ( pd.to_datetime )","title":"Data pre-processing"},{"location":"modelling/AusOpenPythonTutorial/#feature-engineering","text":"The raw datasets are constructed in a way that each row will have the seperate stats for both the winner and loser of that match. However, we want to reshape the data so that each row we will only have one player randomly selected from the winner/loser columns and the features are the difference between opponents statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. In addition, for the features, we will take the rolling average of the player's most recent 15 matches before the particular tournament starts. For example, if the match is the second round of the Australian Open 2018, the features will be the last 15 matches before the first round of Australian Open 2018. The reason of not including the stats in the first round is that we would not have known the player's stats in the first round for the final submissions A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, difference in ELO rating etc. The target variable will be whether or not Player A wins (1=Player A wins and 0=lose). The steps we take are: Convert the raw data frames into long format: Create some new features Take the rolling average for each player and each match Since we will be only training our models on US Open and Australian Open, we will only be creating features for those matches. However, the rolling average will take into account any hard surface matches before those tournaments Calculate the difference of averages for each match in the data frames","title":"Feature Engineering"},{"location":"modelling/AusOpenPythonTutorial/#convert-the-raw-data-frames-into-long-format","text":"# Before we split the data frame into winner and loser, we want to create a feature that captures the total number of games the match takes. # We have to do it before the split or we will lose this information df_atp [ 'Total_Games' ] = df_atp.Winner_Games_Won + df_atp.Loser_Games_Won df_wta [ 'Total_Games' ] = df_wta.Winner_Games_Won + df_wta.Loser_Games_Won # Get the column names for the winner and loser stats winner_cols = [ col for col in df_atp.columns if col.startswith ( 'Winner' )] loser_cols = [ col for col in df_atp.columns if col.startswith ( 'Loser' )] # create a winner data frame to store the winner stats and a loser data frame for the losers # In addition to the winner and loser columns, we are adding common columns as well (e.g. tournamnt dates) common_cols = [ 'Total_Games' , 'Tournament' , 'Tournament_Date' , 'Court_Surface' , 'Round_Description' ] df_winner_atp = df_atp [ winner_cols + common_cols ] df_loser_atp = df_atp [ loser_cols + common_cols ] df_winner_wta = df_wta [ winner_cols + common_cols ] df_loser_wta = df_wta [ loser_cols + common_cols ] # Create a new column to show whether the player has won or not. df_winner_atp [ \"won\" ] = 1 df_loser_atp [ \"won\" ] = 0 df_winner_wta [ \"won\" ] = 1 df_loser_wta [ \"won\" ] = 0 # Rename the columns for the winner and loser data frames so we can append them later on. # We will rename the Winner_ / Loser_ columns to Player_ new_column_names = [ col.replace ( 'Winner' , 'Player' ) for col in winner_cols ] df_winner_atp.columns = new_column_names + common_cols + [ 'won' ] # They all should be the same df_loser_atp.columns = df_winner_atp.columns df_winner_wta.columns = df_winner_atp.columns df_loser_wta.columns = df_winner_atp.columns # append the winner and loser data frames df_long_atp = df_winner_atp.append ( df_loser_atp ) df_long_wta = df_winner_wta.append ( df_loser_wta ) So now our data frames are in long format and should looks like this df_long_atp.head () Player Player_Rank Player_Sets_Won Player_Games_Won Player_Aces Player_DoubleFaults Player_FirstServes_Won Player_FirstServes_In Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Total_Games Tournament Tournament_Date Court_Surface Round_Description won Edouard Roger-Vasselin 106.0 2.0 12 5.0 2.0 22 30 12 19 4.0 7.0 25.0 59.0 59 19 Chennai 2012-01-02 Hard First Round 1 Dudi Sela 83.0 2.0 12 2.0 0.0 14 17 11 16 6.0 14.0 36.0 58.0 61 13 Chennai 2012-01-02 Hard First Round 1 Go Soeda 120.0 2.0 19 6.0 1.0 48 64 19 39 5.0 11.0 42.0 105.0 109 33 Chennai 2012-01-02 Hard First Round 1 Yuki Bhambri 345.0 2.0 12 1.0 2.0 22 29 9 17 5.0 13.0 34.0 62.0 65 17 Chennai 2012-01-02 Hard First Round 1 Yuichi Sugita 235.0 2.0 12 3.0 1.0 37 51 11 27 3.0 7.0 22.0 54.0 70 19 Chennai 2012-01-02 Hard First Round 1","title":"Convert the raw data frames into long format:"},{"location":"modelling/AusOpenPythonTutorial/#create-some-new-features","text":"Thinking about the dynamics of tennis, we know that players often will matches by \u201cbreaking\u201d the opponent\u2019s serve (i.e. winning a game when the opponent is serving). This is especially important in tennis. Let\u2019s create a feature called Player_BreakPoints_Per_Game, which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let\u2019s also create a feature called Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \u201cholding\u201d serve is important (i.e. winning a game when you are serving). Let\u2019s create a feature called Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let\u2019s create a feature called Player_Game_Win_Percentage which is the propotion of games that a player wins. So the four new features we will create are: Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage # Here, we will define a function so we can apply it to both atp and wta data frames def get_new_features ( df ) : # Input: # df: data frame to get the data from # Return: the df with the new features # Point Win ratio when serving df [ 'Player_Serve_Win_Ratio' ] = ( df.Player_FirstServes_Won + df.Player_SecondServes_Won - df.Player_DoubleFaults ) \\ / ( df.Player_FirstServes_In + df.Player_SecondServes_In + df.Player_DoubleFaults ) # Point win ratio when returning df [ 'Player_Return_Win_Ratio' ] = df.Player_ReturnPoints_Won / df.Player_ReturnPoints_Faced # Breakpoints per receiving game df [ 'Player_BreakPoints_Per_Return_Game' ] = df.Player_BreakPoints / df.Total_Games df [ 'Player_Game_Win_Percentage' ] = df.Player_Games_Won / df.Total_Games return df # Apply the function we just created to the long data frames df_long_atp = get_new_features ( df_long_atp ) df_long_wta = get_new_features ( df_long_wta ) # The long table should have exactly twice of the rows of the original data assert df_long_atp.shape [ 0 ] == atp_shape [ 0 ] * 2 assert df_long_wta.shape [ 0 ] == wta_shape [ 0 ] * 2","title":"Create some new features"},{"location":"modelling/AusOpenPythonTutorial/#take-the-rolling-average-for-each-player-and-each-match","text":"To train our models, we cannot simply use the player stats for that current match. In fact, we wont be able to use any stats from the same tournament. The logic behind this is that when we try to predict the results in 2019, we would not know the stats of any of the matches in the Australian Open 2019 tournament. As a result, we will use the players' past performance. Here, we will do a rolling average of the most recent 15 matches before the tournament. To do the above, we will follow the steps below: List all the tournament dates for US and Australian Opens Loop through the dates from point 1, for each date, we filter the data to only include matches before that date and take the most recent 15 games Take the average of those 15 games # the two tournaments we will be using for training and thus the feature generation tournaments = [ 'U.S. Open, New York' , 'Australian Open, Melbourne' ] # Store the dates for the loops tournament_dates_atp = df_atp.loc [ df_atp.Tournament.isin ( tournaments )] .groupby ([ 'Tournament' , 'Tournament_Date' ]) \\ .size () .reset_index ()[[ 'Tournament' , 'Tournament_Date' ]] tournament_dates_wta = df_wta.loc [ df_wta.Tournament.isin ( tournaments )] .groupby ([ 'Tournament' , 'Tournament_Date' ]) \\ .size () .reset_index ()[[ 'Tournament' , 'Tournament_Date' ]] # We are adding one more date for the final prediction tournament_dates_atp.loc [ -1 ] = [ 'Australian Open, Melbourne' , pd.to_datetime ( '2019-01-15' )] tournament_dates_wta.loc [ -1 ] = [ 'Australian Open, Melbourne' , pd.to_datetime ( '2019-01-15' )] Following are the dates for each tournament tournament_dates_atp Tournament Tournament_Date Australian Open, Melbourne 2012-01-16 Australian Open, Melbourne 2013-01-1 Australian Open, Melbourne 2014-01-1 Australian Open, Melbourne 2015-01-1 Australian Open, Melbourne 2016-01-1 Australian Open, Melbourne 2017-01-1 Australian Open, Melbourne 2018-01-1 U.S. Open, New York 2012-08-2 U.S. Open, New York 2013-08-2 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 tournament_dates_wta Tournament Tournament_Date Australian Open, Melbourne 2014-01-13 Australian Open, Melbourne 2015-01-19 Australian Open, Melbourne 2016-01-18 Australian Open, Melbourne 2017-01-16 Australian Open, Melbourne 2018-01-15 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 They look fine but it is interesting that for men's, we have two more years of data from 2012 to 2013 # Let's define a function to calculate the rolling averages def get_rolling_features ( df , date_df = None , rolling_cols = None , last_cols = None ) : # Input: # df: data frame to get the data from # date_df: data frame that has the start dates for each tournament # rolling_cols: columns to get the rolling averages # last_cols: columns to get the last value (most recent) # Return: the df with the new features # Sort the data by player and dates so the most recent matches are at the bottom df = df.sort_values ([ 'Player' , 'Tournament_Date' , 'Tournament' ], ascending = True ) # For each tournament, get the rolling averages of that player's past matches before the tournament start date for index , tournament_date in enumerate ( date_df.Tournament_Date ) : # create a temp df to store the interim results df_temp = df.loc [ df.Tournament_Date < tournament_date ] # for ranks, we only take the last one. (comment this out if want to take avg of rank) df_temp_last = df_temp.groupby ( 'Player' )[ last_cols ] .last () .reset_index () # take the most recent 15 matches for the rolling average df_temp = df_temp.groupby ( 'Player' )[ rolling_cols ] .rolling ( 15 , min_periods = 1 ) .mean () .reset_index () df_temp = df_temp.groupby ( 'Player' ) .tail ( 1 ) # take the last row of the above df_temp = df_temp.merge ( df_temp_last , on = 'Player' , how = 'left' ) if index == 0 : df_result = df_temp df_result [ 'tournament_date_index' ] = tournament_date # so we know which tournament this feature is for else : df_temp [ 'tournament_date_index' ] = tournament_date df_result = df_result.append ( df_temp ) df_result.drop ( 'level_1' , axis = 1 , inplace = True ) return df_result # columns we are applying the rolling averages on rolling_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' ] # columns we are taking the most recent values on # For the player rank, we think we can just use the latest rank (before the tournament starts) # as it should refect the most recent performance of the player last_cols = [ 'Player_Rank' ] # Apply the rolling average function to the long data frames (it will take a few mins to run) df_rolling_atp = get_rolling_features ( df_long_atp , tournament_dates_atp , rolling_cols , last_cols = last_cols ) df_rolling_wta = get_rolling_features ( df_long_wta , tournament_dates_wta , rolling_cols , last_cols = last_cols ) df_rolling_atp.head ( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Adrian Mannarino 0.623408 0.353397 0.257859 0.447246 87.0 2012-01-16 Albert Montanes 0.507246 0.195652 0.000000 0.294118 50.0 2012-01-16 df_rolling_wta.head ( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Agnieszka Radwanska 0.413333 0.475410 0.350000 0.350000 5.0 2014-01-13 Ajla Tomljanovic 0.468457 0.407319 0.242253 0.462634 75.0 2014-01-13","title":"Take the rolling average for each player and each match"},{"location":"modelling/AusOpenPythonTutorial/#calculate-the-difference-of-averages-for-each-match-in-the-data-frames","text":"In the original data frames, the first column is always the winner and followed by the loser. Same for the player stats. Thus, we cannot simply calculate the difference between winner and loser and create a target variable indicating player 1 will win or not because it will always be the winner in this case (target always = 1). As a result, we need to pick a player randomly so the player might or might not be the winner In addition, instead of using both the features for player 1 and 2, we will take the difference of averages between the randomised player 1 and 2. The main benefit is that it will reduce the number of features to half Steps: We will create a random number for each player which only return 0 or 1 If it is zero, we will assign the winner to player 1 and loser to player 2 We will join the features to the player 1 and 2. The join will be on the player names and the tournament date (tournament_index in the feature data frames) For players who do not have any history, we will fill the stats by zeros and rank by 999 # Randomise the match_wide dataset so the first player is not always the winner # set a seed so the random number is reproducable np.random.seed ( 2 ) # randomise a number 0/1 with 50% chance each # if 0 then take the winner, 1 then take loser df_atp [ 'random_number' ] = np.random.randint ( 2 , size = len ( df_atp )) df_atp [ 'randomised_player_1' ] = np.where ( df_atp [ 'random_number' ] == 0 , df_atp [ 'Winner' ], df_atp [ 'Loser' ]) df_atp [ 'randomised_player_2' ] = np.where ( df_atp [ 'random_number' ] == 0 , df_atp [ 'Loser' ], df_atp [ 'Winner' ]) df_wta [ 'random_number' ] = np.random.randint ( 2 , size = len ( df_wta )) df_wta [ 'randomised_player_1' ] = np.where ( df_wta [ 'random_number' ] == 0 , df_wta [ 'Winner' ], df_wta [ 'Loser' ]) df_wta [ 'randomised_player_2' ] = np.where ( df_wta [ 'random_number' ] == 0 , df_wta [ 'Loser' ], df_wta [ 'Winner' ]) # set the target (win/loss) based on the new randomise number df_atp [ 'player_1_win' ] = np.where ( df_atp [ 'random_number' ] == 0 , 1 , 0 ) df_wta [ 'player_1_win' ] = np.where ( df_wta [ 'random_number' ] == 0 , 1 , 0 ) print ( 'After shuffling, the win rate for player 1 for the mens is {}%' .format ( df_atp [ 'player_1_win' ] .mean () * 100 )) print ( 'After shuffling, the win rate for player 1 for the womens is {}%' .format ( df_wta [ 'player_1_win' ] .mean () * 100 )) After shuffling, the win rate for player 1 for the mens is 49.64798919857267% After shuffling, the win rate for player 1 for the womens is 49.697671426733564% The win rates are close enough to 50%. So we are good to go # To get our data frames ready for model training, we will exclude other tournaments from the data now because we have gotten the rolling averages from them and # for training, we only need US and Australian Open matches df_atp = df_atp.loc [ df_atp.Tournament.isin ( tournaments )] df_wta = df_wta.loc [ df_wta.Tournament.isin ( tournaments )] # now we can remove other stats columns because we will be using the differences cols_to_keep = [ 'Winner' , 'Loser' , 'Tournament' , 'Tournament_Date' , 'player_1_win' , 'randomised_player_1' , 'randomised_player_2' ] df_atp = df_atp [ cols_to_keep ] df_wta = df_wta [ cols_to_keep ] # Here, we are joining the rolling average data frames to the individual matches. # We need to do it twice. One for player 1 and one for player 2 # Get the rolling features for player 1 df_atp = df_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_wta = df_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # we will use '_p1' to denote player 1 and '_p2' for player 2 df_atp = df_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_wta = df_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' .format ( df_atp.loc [ df_atp.Player_p1.isna (), 'randomised_player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament' .format ( df_atp.loc [ df_atp.Player_p2.isna (), 'randomised_player_2' ] .nunique ())) 59 player_1s do Not have previous match history before the tournament 56 player_2s do Not have previous match history before the tournament # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' .format ( df_wta.loc [ df_wta.Player_p1.isna (), 'randomised_player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament' .format ( df_wta.loc [ df_wta.Player_p2.isna (), 'randomised_player_2' ] .nunique ())) 41 player_1s do Not have previous match history before the tournament 37 player_2s do Not have previous match history before the tournament # Most of the missing are for the early years which makes sense as we dont have enough history for them df_wta.loc [ df_wta.Player_p1.isna (), 'Tournament_Date' ] .value_counts () 2014-01-13 29 2014-08-25 7 2015-08-31 5 2015-01-19 3 2017-08-28 3 2018-01-15 3 2018-08-27 3 Name: Tournament_Date, dtype: int64 df_atp.loc [ df_atp.Player_p1.isna (), 'Tournament_Date' ] .value_counts () 2012-01-16 29 2012-08-27 9 2014-01-13 5 2013-08-26 5 2016-01-18 5 2013-01-14 4 2014-08-25 3 2018-01-15 3 2017-08-28 3 2018-08-27 2 2016-08-29 2 2015-01-19 1 Name: Tournament_Date, dtype: int64 Now we have gotten the rolling averages for both player 1 and 2. What we need to do next is to simply calculate their difference. To calculate the difference, we need to: Split the data frames into two new data frames: Player 1 and Player 2 Take the difference between the two data frames def get_player_difference ( df , diff_cols = None ) : # Input: # df: data frame to get the data from # diff_cols: columns we take the difference on. For example is diff_cols = win rate. This function will calculate the # difference of the win rates between player 1 and player 2 # Return: the df with the new features p1_cols = [ i + '_p1' for i in diff_cols ] # column names for player 1 stats p2_cols = [ i + '_p2' for i in diff_cols ] # column names for player 2 stats # For any missing values, we will fill them by zeros except the ranking where we will use 999 df [ 'Player_Rank_p1' ] = df [ 'Player_Rank_p1' ] .fillna ( 999 ) df [ p1_cols ] = df [ p1_cols ] .fillna ( 0 ) df [ 'Player_Rank_p2' ] = df [ 'Player_Rank_p2' ] .fillna ( 999 ) df [ p2_cols ] = df [ p2_cols ] .fillna ( 0 ) new_column_name = [ i + '_diff' for i in diff_cols ] # Take the difference df_p1 = df [ p1_cols ] df_p2 = df [ p2_cols ] df_p1.columns = new_column_name df_p2.columns = new_column_name df_diff = df_p1 - df_p2 df_diff.columns = new_column_name # drop the p1 and p2 columns because We have the differences now df.drop ( p1_cols + p2_cols , axis = 1 , inplace = True ) # Concat the df_diff and raw_df df = pd.concat ([ df , df_diff ], axis = 1 ) return df , new_column_name diff_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' , 'Player_Rank' ] # Apply the function and get the difference between player 1 and 2 df_atp , _ = get_player_difference ( df_atp , diff_cols = diff_cols ) df_wta , _ = get_player_difference ( df_wta , diff_cols = diff_cols ) # Make a copy of the data frames in case we need to come back to check the values df_atp_final = df_atp.copy () df_wta_final = df_wta.copy () df_atp_final.head () Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Juan Martin del Potro Adrian Mannarino Australian Open, Melbourne 2012-01-16 1 Juan Martin del Potro Adrian Mannarino Juan Martin del Potro 2012-01-16 Adrian Mannarino 2012-01-16 0.035030 -0.021271 -0.025975 0.103479 -76.0 Pere Riba Albert Montanes Australian Open, Melbourne 2012-01-16 1 Pere Riba Albert Montanes Pere Riba 2012-01-16 Albert Montanes 2012-01-16 -0.156369 0.008893 0.066667 -0.094118 39.0 Tomas Berdych Albert Ramos-Vinolas Australian Open, Melbourne 2012-01-16 0 Albert Ramos-Vinolas Tomas Berdych Albert Ramos-Vinolas 2012-01-16 NaN NaT 0.498027 0.380092 0.414815 0.394444 -934.0 Rafael Nadal Alex Kuznetsov Australian Open, Melbourne 2012-01-16 0 Alex Kuznetsov Rafael Nadal NaN NaT Rafael Nadal 2012-01-16 -0.670139 -0.423057 -0.445623 -0.574767 997.0 Roger Federer Alexander Kudryavtsev Australian Open, Melbourne 2012-01-16 0 Alexander Kudryavtsev Roger Federer NaN NaT Roger Federer 2012-01-16 -0.721415 -0.449516 -0.360255 -0.668090 996.0","title":"Calculate the difference of averages for each match in the data frames"},{"location":"modelling/AusOpenPythonTutorial/#modelling","text":"We will trian two models here, one for mens and one for womens. For training, we will use all available data from the second year (too many missing values in the first year) up until 2017. For validation, we will test the model on the 2018 Australian Open data This setup allows us to 'mimic' the final prediction (using historical matches to predict 2019 results) df_train_atp = df_atp_final.loc [( df_atp_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & ( df_atp_final.Tournament_Date > '2012-01-16' )] # excluding first year df_valid_atp = df_atp_final.loc [ df_atp_final.Tournament_Date == '2018-01-15' ] # Australian Open 2018 only df_train_wta = df_wta_final.loc [( df_wta_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & ( df_wta_final.Tournament_Date > '2014-01-13' )] # excluding first year df_train_atp.head () Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Daniel Brands Adrian Ungur U.S. Open, New York 2012-08-27 0 Adrian Ungur Daniel Brands NaN NaT Daniel Brands 2012-08-27 -0.535211 -0.300000 -0.043478 -0.434783 870.0 Richard Gasquet Albert Montanes U.S. Open, New York 2012-08-27 1 Richard Gasquet Albert Montanes Richard Gasquet 2012-08-27 Albert Montanes 2012-08-27 0.080003 0.077451 0.180847 0.131108 -37.0 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 1 Martin Klizan Alejandro Falla Martin Klizan 2012-08-27 Alejandro Falla 2012-08-27 0.077117 -0.044716 -0.087362 0.068180 -2.0 Andy Murray Alex Bogomolov Jr. U.S. Open, New York 2012-08-27 1 Andy Murray Alex Bogomolov Jr. Andy Murray 2012-08-27 Alex Bogomolov Jr. 2012-08-27 0.039641 0.031701 0.094722 0.059010 -69.0 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 1 Tommy Robredo Andreas Seppi Tommy Robredo 2012-08-27 Andreas Seppi 2012-08-27 -0.026814 0.006442 -0.009930 -0.067780 151.0 # target variable target = 'player_1_win' # features being fed into the models feats = [ 'Player_Serve_Win_Ratio_diff' , 'Player_Return_Win_Ratio_diff' , 'Player_BreakPoints_Per_Return_Game_diff' , 'Player_Game_Win_Percentage_diff' , 'Player_Rank_diff' ] print ( feats )","title":"Modelling"},{"location":"modelling/AusOpenPythonTutorial/#h2o-model-for-atp","text":"h2o.init () # Convert to an h2o frame df_train_atp_h2o = h2o.H2OFrame ( df_train_atp ) df_valid_atp_h2o = h2o.H2OFrame ( df_valid_atp ) # For binary classification, response should be a factor df_train_atp_h2o [ target ] = df_train_atp_h2o [ target ] .asfactor () df_valid_atp_h2o [ target ] = df_valid_atp_h2o [ target ] .asfactor () # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_atp = h2o.automl.H2OAutoML ( max_runtime_secs = 300 , max_models = 100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True , seed = 183 ) aml_atp.train ( x = feats , y = target , training_frame = df_train_atp_h2o , validation_frame = df_valid_atp_h2o ) # View the AutoML Leaderboard lb = aml_atp.leaderboard lb.head () model_id auc logloss mean_per_class_error rmse mse GBM_5_AutoML_20181221_094949 0.790281 0.554852 0.281363 0.431379 0.186088 GBM_grid_1_AutoML_20181221_094949_model_15 0.789329 0.556804 0.29856 0.431931 0.186564 GBM_grid_1_AutoML_20181221_094949_model_7 0.788013 0.557808 0.295899 0.432968 0.187461 StackedEnsemble_BestOfFamily_AutoML_20181221_094949 0.788131 0.558028 0.285321 0.432849 0.187358 GBM_grid_1_AutoML_20181221_094949_model_20 0.785633 0.561094 0.283932 0.43479 0.189043 StackedEnsemble_AllModels_AutoML_20181221_094949 0.784411 0.561587 0.293244 0.434667 0.188935 GBM_grid_1_AutoML_20181221_094949_model_25 0.785311 0.561783 0.291912 0.434888 0.189127 GBM_grid_1_AutoML_20181221_094949_model_17 0.774832 0.570883 0.295836 0.439375 0.193051 DeepLearning_1_AutoML_20181221_094949 0.779388 0.572823 0.311737 0.438479 0.192264 GBM_grid_1_AutoML_20181221_094949_model_14 0.7718 0.578867 0.285835 0.441373 0.19481 H2O model for WTA # Convert to an h2o frame df_train_wta_h2o = h2o.H2OFrame ( df_train_wta ) df_valid_wta_h2o = h2o.H2OFrame ( df_valid_wta ) # For binary classification, response should be a factor df_train_wta_h2o [ target ] = df_train_wta_h2o [ target ] .asfactor () df_valid_wta_h2o [ target ] = df_valid_wta_h2o [ target ] .asfactor () # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_wta = h2o.automl.H2OAutoML ( max_runtime_secs = 300 , max_models = 100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True , seed = 183 ) aml_wta.train ( x = feats , y = target , training_frame = df_train_wta_h2o , validation_frame = df_valid_wta_h2o ) # View the AutoML Leaderboard lb = aml_wta.leaderboard lb.head () model_id auc logloss mean_per_class_error rmse mse StackedEnsemble_AllModels_AutoML_20181221_095400 0.726046 0.60827 0.321222 0.457117 0.208956 StackedEnsemble_BestOfFamily_AutoML_20181221_095400 0.724911 0.609329 0.337847 0.457659 0.209452 DeepLearning_grid_1_AutoML_20181221_095400_model_3 0.729152 0.612669 0.315971 0.45641 0.20831 GBM_grid_1_AutoML_20181221_095400_model_7 0.721204 0.615763 0.336848 0.460885 0.212415 GBM_5_AutoML_20181221_095400 0.719252 0.616535 0.319179 0.461055 0.212572 GBM_grid_1_AutoML_20181221_095400_model_15 0.715921 0.619263 0.318673 0.462215 0.213643 GLM_grid_1_AutoML_20181221_095400_model_1 0.726048 0.622989 0.366124 0.463099 0.214461 GBM_grid_1_AutoML_20181221_095400_model_17 0.709261 0.624902 0.34876 0.465628 0.216809 GBM_grid_1_AutoML_20181221_095400_model_18 0.70946 0.625704 0.393556 0.466147 0.217293 DeepLearning_grid_1_AutoML_20181221_095400_model_2 0.713419 0.628008 0.311334 0.463638 0.21496","title":"H2O model for ATP"},{"location":"modelling/AusOpenPythonTutorial/#use-the-models-to-predict-and-make-submissions","text":"Now let's use the models we just created to make the submissions df_predict_atp = pd.read_csv ( \"data/men_dummy_submission_file.csv\" ) df_predict_wta = pd.read_csv ( \"data/women_dummy_submission_file.csv\" , encoding = 'latin1' ) # for womens, there are some names need a different encoding df_predict_wta.head ( 2 ) player_1 player_2 player_1_win_probability Simona Halep Angelique Kerber 0.5 Simona Halep Caroline Wozniacki 0.5","title":"Use the models to predict and make submissions"},{"location":"modelling/AusOpenPythonTutorial/#get-the-features-for-the-predict-df","text":"We need to join the features to the 2019 players # Before we join the features by the names and the dates, we need to convert any non-english characters to english first translationTable = str.maketrans ( \"\u00e9\u00e0\u00e8\u00f9\u00e2\u00ea\u00ee\u00f4\u00fb\u00e7\u00f1\u00e1\" , \"eaeuaeioucna\" ) df_predict_atp [ 'player_1' ] = df_predict_atp.player_1.apply ( lambda x : x.translate ( translationTable )) df_predict_atp [ 'player_2' ] = df_predict_atp.player_2.apply ( lambda x : x.translate ( translationTable )) df_predict_wta [ 'player_1' ] = df_predict_wta.player_1.apply ( lambda x : x.translate ( translationTable )) df_predict_wta [ 'player_2' ] = df_predict_wta.player_2.apply ( lambda x : x.translate ( translationTable )) # Also we need to convert the names into lower cases df_predict_atp [ 'player_1' ] = df_predict_atp [ 'player_1' ] .str.lower () df_predict_atp [ 'player_2' ] = df_predict_atp [ 'player_2' ] .str.lower () df_predict_wta [ 'player_1' ] = df_predict_wta [ 'player_1' ] .str.lower () df_predict_wta [ 'player_2' ] = df_predict_wta [ 'player_2' ] .str.lower () df_rolling_atp [ 'Player' ] = df_rolling_atp [ 'Player' ] .str.lower () df_rolling_wta [ 'Player' ] = df_rolling_wta [ 'Player' ] .str.lower () # Lastly, some players have slightly difference names in the submission data and the match data. So we are editing them here manually df_predict_atp.loc [ df_predict_atp.player_1 == 'jaume munar' , 'player_1' ] = 'jaume antoni munar clar' df_predict_atp.loc [ df_predict_atp.player_2 == 'jaume munar' , 'player_2' ] = 'jaume antoni munar clar' df_predict_wta.loc [ df_predict_wta.player_1 == 'daria kasatkina' , 'player_1' ] = 'darya kasatkina' df_predict_wta.loc [ df_predict_wta.player_2 == 'daria kasatkina' , 'player_2' ] = 'darya kasatkina' df_predict_wta.loc [ df_predict_wta.player_1 == 'lesia tsurenko' , 'player_1' ] = 'lesya tsurenko' df_predict_wta.loc [ df_predict_wta.player_2 == 'lesia tsurenko' , 'player_2' ] = 'lesya tsurenko' df_predict_wta.loc [ df_predict_wta.player_1 == 'danielle collins' , 'player_1' ] = 'danielle rose collins' df_predict_wta.loc [ df_predict_wta.player_2 == 'danielle collins' , 'player_2' ] = 'danielle rose collins' df_predict_wta.loc [ df_predict_wta.player_1 == 'anna karolina schmiedlova' , 'player_1' ] = 'anna schmiedlova' df_predict_wta.loc [ df_predict_wta.player_2 == 'anna karolina schmiedlova' , 'player_2' ] = 'anna schmiedlova' df_predict_wta.loc [ df_predict_wta.player_1 == 'georgina garcia perez' , 'player_1' ] = 'georgina garcia-perez' df_predict_wta.loc [ df_predict_wta.player_2 == 'georgina garcia perez' , 'player_2' ] = 'georgina garcia-perez' # create and tournament date column and set it to 2019 so we can join the lastest features df_predict_atp [ 'Tournament_Date' ] = pd.to_datetime ( '2019-01-15' ) df_predict_wta [ 'Tournament_Date' ] = pd.to_datetime ( '2019-01-15' ) # Get the rolling features for player 1 df_predict_atp = df_predict_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_predict_wta = df_predict_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # For duplicate columns, we will use '_p1' to denote player 1 and '_p2' for player 2 df_predict_atp = df_predict_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_predict_wta = df_predict_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_predict_atp.head ( 2 ) player_1 player_2 player_1_win_probability Tournament_Date Player_p1 Player_Serve_Win_Ratio_p1 Player_Return_Win_Ratio_p1 Player_BreakPoints_Per_Return_Game_p1 Player_Game_Win_Percentage_p1 Player_Rank_p1 tournament_date_index_p1 Player_p2 Player_Serve_Win_Ratio_p2 Player_Return_Win_Ratio_p2 Player_BreakPoints_Per_Return_Game_p2 Player_Game_Win_Percentage_p2 Player_Rank_p2 tournament_date_index_p2 novak djokovic rafael nadal 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 rafael nadal 0.622425 0.401028 0.334270 0.570833 1.0 2019-01-15 novak djokovic roger federer 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 roger federer 0.620070 0.389781 0.269224 0.564244 3.0 2019-01-15 # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the mens' .format ( df_predict_atp.loc [ df_predict_atp.Player_p1.isna (), 'player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament in the mens' .format ( df_predict_atp.loc [ df_predict_atp.Player_p2.isna (), 'player_2' ] .nunique ())) 3 player_1s do Not have previous match history before the tournament in the mens 3 player_2s do Not have previous match history before the tournament in the mens # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the womens' .format ( df_predict_wta.loc [ df_predict_wta.Player_p1.isna (), 'player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament in the womens' .format ( df_predict_wta.loc [ df_predict_wta.Player_p2.isna (), 'player_2' ] .nunique ())) 0 player_1s do Not have previous match history before the tournament in the womens 0 player_2s do Not have previous match history before the tournament in the womens print ( df_predict_atp.loc [ df_predict_atp.Player_p1.isna (), 'player_1' ] .unique () .tolist ()) [ 'christian garin' , 'pedro sousa' , 'hugo dellien' ] print ( df_predict_wta.loc [ df_predict_wta.Player_p1.isna (), 'player_1' ] .unique () .tolist ()) [] We will do the differencing again for the prediction data frames exactly like what we did for training # Apply the function and get the difference between player 1 and 2 df_predict_atp , _ = get_player_difference ( df_predict_atp , diff_cols = diff_cols ) df_predict_wta , _ = get_player_difference ( df_predict_wta , diff_cols = diff_cols )","title":"Get the features for the predict df"},{"location":"modelling/AusOpenPythonTutorial/#make-the-prediction","text":"df_predict_atp_h2o = h2o.H2OFrame ( df_predict_atp [ feats ]) df_predict_wta_h2o = h2o.H2OFrame ( df_predict_wta [ feats ]) atp_preds = aml_atp.predict ( df_predict_atp_h2o )[ 'p1' ] .as_data_frame () wta_preds = aml_wta.predict ( df_predict_wta_h2o )[ 'p1' ] .as_data_frame () df_predict_atp [ 'player_1_win_probability' ] = atp_preds df_predict_wta [ 'player_1_win_probability' ] = wta_preds atp_submission = df_predict_atp [[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] wta_submission = df_predict_wta [[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] atp_submission.head () player_1 player_2 player_1_win_probability novak djokovic rafael nadal 0.571588 novak djokovic roger federer 0.662511 novak djokovic juan martin del potro 0.544306 novak djokovic alexander zverev 0.709483 novak djokovic kevin anderson 0.687195 wta_submission.head () player_1 player_2 player_1_win_probability simona halep angelique kerber 0.455224 simona halep caroline wozniacki 0.546276 simona halep elina svitolina 0.408014 simona halep naomi osaka 0.285125 simona halep sloane stephens 0.576643 Let's look at who has the highest win rate from our models atp_submission.groupby ( 'player_1' )[ 'player_1_win_probability' ] .mean () \\ .reset_index () .sort_values ( 'player_1_win_probability' , ascending = False ) .head () player_1 player_1_win_probability novak djokovic 0.846377 juan martin del potro 0.787337 karen khachanov 0.782963 rafael nadal 0.778707 roger federer 0.767337 wta_submission.groupby ( 'player_1' )[ 'player_1_win_probability' ] .mean () \\ .reset_index () .sort_values ( 'player_1_win_probability' , ascending = False ) .head () player_1 player_1_win_probability madison keys 0.750580 naomi osaka 0.749195 caroline wozniacki 0.722409 kiki bertens 0.713904 aryna sabalenka 0.707368 Now we can output the predictions as csvs atp_submission.to_csv ( 'submission/atp_submission_python.csv' , index = False ) wta_submission.to_csv ( 'submission/wta_submission_pthon.csv' , index = False ) atp_submission.shape (16256, 3)","title":"Make the prediction"},{"location":"modelling/AusOpenPythonTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/AusOpenRTutorial/","text":"Australian Open Datathon R Tutorial \u00b6 Overview \u00b6 The Task \u00b6 This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodology and thought process, read this article. Exploring the Data \u00b6 First we need to get an idea of what the data looks like. Let's read the men's data in and get an idea of what it looks like. Note that you will need to install all the packages listed below unless you already have them. Note that for this tutorial I will be using dplyr , if you are not familiar with the syntax I encourage you to read up on the basics . # Import libraries library ( dplyr ) library ( readr ) library ( tidyr ) library ( RcppRoll ) library ( tidyselect ) library ( lubridate ) library ( stringr ) library ( zoo ) library ( purrr ) library ( h2o ) library ( DT ) mens = readr :: read_csv ( 'data/ATP_matches.csv' , na = \".\" ) # NAs are indicated by . mens %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Winner Loser Tournament Tournament_Date Court_Surface Round_Description Winner_Rank Loser_Rank Retirement_Ind Winner_Sets_Won ... Loser_DoubleFaults Loser_FirstServes_Won Loser_FirstServes_In Loser_SecondServes_Won Loser_SecondServes_In Loser_BreakPoints_Won Loser_BreakPoints Loser_ReturnPoints_Won Loser_ReturnPoints_Faced Loser_TotalPoints_Won Edouard Roger-Vasselin Eric Prodon Chennai 2-Jan-12 Hard First Round 106 97 0 2 ... 3 21 33 13 26 1 3 15 49 49 Dudi Sela Fabio Fognini Chennai 2-Jan-12 Hard First Round 83 48 0 2 ... 4 17 32 5 26 0 1 8 33 30 Go Soeda Frederico Gil Chennai 2-Jan-12 Hard First Round 120 102 0 2 ... 2 45 70 18 35 2 4 36 103 99 Yuki Bhambri Karol Beck Chennai 2-Jan-12 Hard First Round 345 101 0 2 ... 1 15 33 13 29 2 3 15 46 43 Yuichi Sugita Olivier Rochus Chennai 2-Jan-12 Hard First Round 235 67 0 2 ... 0 19 32 13 22 1 7 30 78 62 Benoit Paire Pere Riba Chennai 2-Jan-12 Hard First Round 95 89 0 2 ... 5 13 20 12 32 0 1 9 44 34 Victor Hanescu Sam Querrey Chennai 2-Jan-12 Hard First Round 90 93 0 2 ... 8 29 41 7 24 1 3 17 57 53 Yen-Hsun Lu Thiemo de Bakker Chennai 2-Jan-12 Hard First Round 82 223 0 2 ... 0 20 32 10 24 1 1 19 57 49 Andreas Beck Vasek Pospisil Chennai 2-Jan-12 Hard First Round 98 119 0 2 ... 3 39 57 16 38 1 5 24 74 79 Ivan Dodig Vishnu Vardhan Chennai 2-Jan-12 Hard First Round 36 313 0 2 ... 5 41 59 13 27 2 8 34 101 88 David Goffin Xavier Malisse Chennai 2-Jan-12 Hard First Round 174 49 0 2 ... 1 31 43 19 34 1 4 27 85 77 David Goffin Andreas Beck Chennai 2-Jan-12 Hard Second Round 174 98 0 2 ... 0 43 71 14 27 2 8 27 82 84 Dudi Sela Benoit Paire Chennai 2-Jan-12 Hard Second Round 83 95 0 2 ... 5 40 58 21 46 1 7 26 87 87 Stan Wawrinka Edouard Roger-Vasselin Chennai 2-Jan-12 Hard Second Round 17 106 0 2 ... 0 43 70 16 34 4 6 28 82 87 Go Soeda Ivan Dodig Chennai 2-Jan-12 Hard Second Round 120 36 0 2 ... 2 31 41 11 28 1 4 23 73 65 Milos Raonic Victor Hanescu Chennai 2-Jan-12 Hard Second Round 31 90 0 2 ... 1 25 38 5 14 0 4 15 56 45 Yuichi Sugita Yen-Hsun Lu Chennai 2-Jan-12 Hard Second Round 235 82 0 2 ... 4 34 45 12 34 2 9 38 93 84 Janko Tipsarevic Yuki Bhambri Chennai 2-Jan-12 Hard Second Round 9 345 0 2 ... 2 12 22 9 17 0 1 8 41 29 Janko Tipsarevic David Goffin Chennai 2-Jan-12 Hard Quarter-finals 9 174 0 2 ... 5 34 51 19 40 1 2 18 67 71 Milos Raonic Dudi Sela Chennai 2-Jan-12 Hard Quarter-finals 31 83 0 2 ... 2 23 31 19 28 0 3 16 69 58 Go Soeda Stan Wawrinka Chennai 2-Jan-12 Hard Quarter-finals 120 17 0 2 ... 4 18 34 13 31 3 7 31 74 62 Nicolas Almagro Yuichi Sugita Chennai 2-Jan-12 Hard Quarter-finals 10 235 0 2 ... 1 36 65 30 40 3 12 45 123 111 Janko Tipsarevic Go Soeda Chennai 2-Jan-12 Hard Semi-finals 9 120 0 2 ... 1 21 33 10 28 1 1 10 44 41 Milos Raonic Nicolas Almagro Chennai 2-Jan-12 Hard Semi-finals 31 10 0 2 ... 0 31 45 8 15 0 3 12 54 51 Milos Raonic Janko Tipsarevic Chennai 2-Jan-12 Hard Finals 31 9 0 2 ... 2 59 83 34 55 0 4 25 113 118 Igor Andreev Adrian Mannarino Brisbane 2-Jan-12 Hard First Round 115 87 0 2 ... 3 24 35 13 25 1 3 21 70 58 Alexandr Dolgopolov Alejandro Falla Brisbane 2-Jan-12 Hard First Round 15 74 0 2 ... 3 16 33 12 25 3 7 33 75 61 Tatsuma Ito Benjamin Mitchell Brisbane 2-Jan-12 Hard First Round 122 227 0 2 ... 6 30 44 7 24 0 2 13 52 50 Kei Nishikori Cedrik-Marcel Stebe Brisbane 2-Jan-12 Hard First Round 25 81 0 2 ... 2 27 49 23 41 3 6 28 75 78 Denis Istomin Florian Mayer Brisbane 2-Jan-12 Hard First Round 73 23 1 1 ... 1 28 38 11 17 0 2 15 56 54 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Malek Jaziri Fernando Verdasco Paris 29-Oct-18 Indoor Hard Second Round 55 27 0 2 ... 6 46 60 16 35 3 13 39 104 101 Alexander Zverev Frances Tiafoe Paris 29-Oct-18 Indoor Hard Second Round 5 44 0 2 ... 4 26 40 16 36 2 10 27 72 69 Dominic Thiem Gilles Simon Paris 29-Oct-18 Indoor Hard Second Round 8 31 0 2 ... 1 13 26 12 25 2 2 23 59 48 Novak Djokovic Joao Sousa Paris 29-Oct-18 Indoor Hard Second Round 2 48 0 2 ... 2 25 35 6 22 1 10 27 74 58 Karen Khachanov Matthew Ebden Paris 29-Oct-18 Indoor Hard Second Round 18 39 1 2 ... 6 8 18 5 20 1 2 10 30 23 John Isner Mikhail Kukushkin Paris 29-Oct-18 Indoor Hard Second Round 9 54 0 2 ... 1 54 80 24 39 0 1 13 90 91 Kevin Anderson Nikoloz Basilashvili Paris 29-Oct-18 Indoor Hard Second Round 6 22 0 2 ... 7 43 54 30 49 0 3 26 106 99 Marin Cilic Philipp Kohlschreiber Paris 29-Oct-18 Indoor Hard Second Round 7 43 0 2 ... 1 19 34 12 20 1 1 17 55 48 Jack Sock Richard Gasquet Paris 29-Oct-18 Indoor Hard Second Round 23 28 0 2 ... 4 18 33 16 29 0 4 19 59 53 Grigor Dimitrov Roberto Bautista Agut Paris 29-Oct-18 Indoor Hard Second Round 10 25 0 2 ... 0 34 48 11 20 2 4 27 76 72 Damir Dzumhur Stefanos Tsitsipas Paris 29-Oct-18 Indoor Hard Second Round 52 16 0 2 ... 3 14 26 15 30 2 2 17 52 46 Dominic Thiem Borna Coric Paris 29-Oct-18 Indoor Hard Third Round 8 13 0 2 ... 1 39 57 16 38 2 2 27 88 82 Novak Djokovic Damir Dzumhur Paris 29-Oct-18 Indoor Hard Third Round 2 52 1 2 ... 4 15 28 7 18 0 0 8 28 30 Alexander Zverev Diego Schwartzman Paris 29-Oct-18 Indoor Hard Third Round 5 19 0 2 ... 2 22 37 12 24 0 4 18 58 52 Roger Federer Fabio Fognini Paris 29-Oct-18 Indoor Hard Third Round 3 14 0 2 ... 6 22 32 15 37 1 5 16 54 53 Marin Cilic Grigor Dimitrov Paris 29-Oct-18 Indoor Hard Third Round 7 10 0 2 ... 1 37 55 14 32 1 5 22 71 73 Karen Khachanov John Isner Paris 29-Oct-18 Indoor Hard Third Round 18 9 0 2 ... 4 67 80 19 38 0 0 17 100 103 Kei Nishikori Kevin Anderson Paris 29-Oct-18 Indoor Hard Third Round 11 6 0 2 ... 1 26 33 11 19 0 0 11 51 48 Jack Sock Malek Jaziri Paris 29-Oct-18 Indoor Hard Third Round 23 55 0 2 ... 6 13 21 10 24 0 0 9 41 32 Karen Khachanov Alexander Zverev Paris 29-Oct-18 Indoor Hard Quarter-finals 18 5 0 2 ... 7 26 47 4 21 1 3 10 36 40 Dominic Thiem Jack Sock Paris 29-Oct-18 Indoor Hard Quarter-finals 8 23 0 2 ... 5 44 59 19 37 2 10 34 97 97 Roger Federer Kei Nishikori Paris 29-Oct-18 Indoor Hard Quarter-finals 3 11 0 2 ... 0 21 37 16 26 0 1 12 56 49 Novak Djokovic Marin Cilic Paris 29-Oct-18 Indoor Hard Quarter-finals 2 7 0 2 ... 0 38 55 11 28 2 5 29 85 78 Karen Khachanov Dominic Thiem Paris 29-Oct-18 Indoor Hard Semi-finals 18 8 0 2 ... 0 19 29 8 26 1 3 15 47 42 Novak Djokovic Roger Federer Paris 29-Oct-18 Indoor Hard Semi-finals 2 3 0 2 ... 2 69 93 25 46 1 2 29 113 123 Karen Khachanov Novak Djokovic Paris 29-Oct-18 Indoor Hard Finals 18 2 0 2 ... 1 30 43 14 28 1 5 20 66 64 Jaume Antoni Munar Clar Frances Tiafoe Milan 5-Nov-18 Indoor Hard NA 76 40 0 3 ... 3 21 29 6 17 0 2 5 46 32 Frances Tiafoe Hubert Hurkacz Milan 5-Nov-18 Indoor Hard NA 40 85 0 3 ... 4 35 48 10 19 1 7 22 78 67 Hubert Hurkacz Jaume Antoni Munar Clar Milan 5-Nov-18 Indoor Hard NA 85 76 0 3 ... 1 43 63 15 35 3 9 29 80 87 Andrey Rublev Liam Caruana Milan 5-Nov-18 Indoor Hard NA 68 NA 0 3 ... 1 28 39 4 14 1 3 18 57 50 As we can see, we have a Winner column, a Loser column, as well as other columns detailing the match details, and other columns which have the stats for that match. As we have a Winner column, if we use the current data structure to train a model we will leak the result. The model will simply learn that the actual winner comes from the Winner column, rather than learning from other features that we can create, such as First Serve % . To avoid this problem, let's reshape the data from wide to long, then shuffle the data. For this, we will define a function, split_winner_loser_columns , which splits the raw data frame into two data frames, appends them together, and then shuffles the data. Let's also remove all Grass and Clay matches from our data, as we will be modelling the Australian Open which is a hardcourt surface. Additionally, we will add a few columns, such as Match_Id and Total_Games . These will be useful later. split_winner_loser_columns <- function ( df ) { # This function splits the raw data into two data frames and appends them together then shuffles them # This output is a data frame with only one player's stats on each row (i.e. in long format) # Grab a df with only the Winner's stats winner = df %>% select ( - contains ( \"Loser\" )) %>% # Select only the Winner columns + extra game info columns as a df rename_at ( # Rename all columns containing \"Winner\" to \"Player\" vars ( contains ( \"Winner\" )), ~ str_replace ( . , \"Winner\" , \"Player\" ) ) %>% mutate ( Winner = 1 ) # Create a target column # Repeat the process with the loser's stats loser = df %>% select ( - contains ( \"Winner\" )) %>% rename_at ( vars ( contains ( \"Loser\" )), ~ str_replace ( . , \"Loser\" , \"Player\" ) ) %>% mutate ( Winner = 0 ) set.seed ( 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) # Create a df that appends both the Winner and loser df together combined_df = winner %>% rbind ( loser ) %>% # Append the loser df to the Winner df slice ( sample ( 1 : n ())) %>% # Randomise row order arrange ( Match_Id ) %>% # Arrange by Match_Id return () } # Read in men and womens data; randomise the data to avoid result leakage mens = readr :: read_csv ( 'data/ATP_matches.csv' , na = \".\" ) %>% filter ( Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% # Filter to only use hardcourt games mutate ( Match_Id = row_number (), # Add a match ID column to be used as a key Tournament_Date = dmy ( Tournament_Date ), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won ) %>% # Add a total games played column split_winner_loser_columns () # Change the data frame from wide to long mens %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Player Tournament Tournament_Date Court_Surface Round_Description Player_Rank Retirement_Ind Player_Sets_Won Player_Games_Won Player_Aces ... Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Match_Id Total_Games Winner Eric Prodon Chennai 2012-01-02 Hard First Round 97 0 0 7 2 ... 13 26 1 3 15 49 49 1 19 0 Edouard Roger-Vasselin Chennai 2012-01-02 Hard First Round 106 0 2 12 5 ... 12 19 4 7 25 59 59 1 19 1 Dudi Sela Chennai 2012-01-02 Hard First Round 83 0 2 12 2 ... 11 16 6 14 36 58 61 2 13 1 Fabio Fognini Chennai 2012-01-02 Hard First Round 48 0 0 1 1 ... 5 26 0 1 8 33 30 2 13 0 Frederico Gil Chennai 2012-01-02 Hard First Round 102 0 1 14 5 ... 18 35 2 4 36 103 99 3 33 0 Go Soeda Chennai 2012-01-02 Hard First Round 120 0 2 19 6 ... 19 39 5 11 42 105 109 3 33 1 Feature Creation \u00b6 Now that we have a fairly good understanding of what the data looks like, let's add some features. To do this we will define a function. Ideally we want to add features which will provide predictive power to our model. Thinking about the dynamics of tennis, we know that players often will matches by \"breaking\" the opponent's serve (i.e. winning a game when the opponent is serving). This is especially important in mens tennis. Let's create a feature called F_Player_BreakPoints_Per_Game , which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let's also create a feature called F_Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \"holding\" serve is important (i.e. winning a game when you are serving). Let's create a feature called F_Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let's create a feature called F_Player_Game_Win_Percentage which is the propotion of games that a player wins. add_ratio_features <- function ( df ) { # This function adds ratio features to a long df df %>% mutate ( # Point Win ratio when serving F_Player_Serve_Win_Ratio = ( Player_FirstServes_Won + Player_SecondServes_Won - Player_DoubleFaults ) / ( Player_FirstServes_In + Player_SecondServes_In + Player_DoubleFaults ), # Point win ratio when returning F_Player_Return_Win_Ratio = Player_ReturnPoints_Won / Player_ReturnPoints_Faced , # Breakpoints per receiving game F_Player_BreakPoints_Per_Game = Player_BreakPoints / Total_Games , F_Player_Game_Win_Percentage = Player_Games_Won / Total_Games ) %>% mutate_at ( vars ( colnames ( . ), - contains ( \"Rank\" ), - Tournament_Date ), # Replace all NAs with0 apart from Rank, Date ~ ifelse ( is.na ( . ), 0 , . ) ) %>% return () } mens = mens %>% add_ratio_features () # Add features Now that we have added our features, we need to create rolling averages for them. We cannot simply use current match statistics, as they will leak the result to the model. Instead, we need to use past match statistics to predict future matches. Here we will use a rolling mean with a window of 15. If the player hasn't played 15 games, we will instead use a cumulative mean. We will also lag the result so as to not leak the result. This next chunk of code simply takes all the columns starting with F_ and calculates these means. mens = mens %>% group_by ( Player ) %>% # Group by player mutate_at ( # Create a rolling mean with window 15 for each player. vars ( starts_with ( \"F_\" )), # If the player hasn't played 15 games, use a cumulative mean ~ coalesce ( rollmean ( . , k = 15 , align = \"right\" , fill = NA_real_ ), cummean ( . )) %>% lag () ) %>% ungroup () Creating a Training Feature Matrix \u00b6 In predictive modelling language - features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options We can train the model on every tennis match in the data set, or We can only train the model on Australian Open matches. Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. We have decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface - hard court. However, we also need to train our model in the same way that will be used to predict the 2019 Australian Open. When predicting the 2 nd round, we won't have data from the 1 st round. So we will need to build our training feature matrix with this in mind. We should extract features for a player from past games at the start of the tournament and apply them to every matchup that that player plays. To do this, we will create a function, extract_latest_features_for_tournament , which maps over our feature data frame for the dates in the first round of a tournament and grabs features. First, we need the Australian Open and US Open results - let's grab these and then apply our function. # Get Australian Open and US Open Results aus_us_open_results = mens %>% filter (( Tournament == \"Australian Open, Melbourne\" | Tournament == \"U.S. Open, New York\" ) & Round_Description != \"Qualifying\" & Tournament_Date != \"2012-01-16\" ) %>% # Filter out qualifiers select ( Match_Id , Player , Tournament , Tournament_Date , Round_Description , Winner ) # Create a function which extracts features for each tournament extract_latest_features_for_tournament = function ( df , dte ) { df %>% # Filter for the 1st round filter ( Tournament_Date == dte , Round_Description == \"First Round\" , Tournament_Date != \"2012-01-16\" ) %>% group_by ( Player ) %>% # Group by player select_at ( vars ( Match_Id , starts_with ( \"F_\" ), Player_Rank ) # Grab the players' features ) %>% rename ( F_Player_Rank = Player_Rank ) %>% ungroup () %>% mutate ( Feature_Date = dte ) %>% select ( Player , Feature_Date , everything ()) } # Create a feature matrix in long format feature_matrix_long = aus_us_open_results %>% distinct ( Tournament_Date ) %>% # Pull all Tournament Dates pull () %>% map_dfr ( ~ extract_latest_features_for_tournament ( mens , . ) # Get the features ) %>% filter ( Feature_Date != \"2012-01-16\" ) %>% # Filter out the first Aus Open mutate_at ( # Replace NAs with the mean vars ( starts_with ( \"F_\" )), ~ ifelse ( is.na ( . ), mean ( . , na.rm = TRUE ), . ) ) Now that we have a feature matrix in long format, we need to convert it to wide format so that the features are on the same row. To do this we will define a function gather_df , which converts the data frame from long to wide. Let's also join the results to the matrix and convert the Winner column to a factor. Finally, we will take the difference of player1 and player2's features, so as to reduce the dimensionality of the model. gather_df <- function ( df ) { # This function puts the df back into its original format of each row containing stats for both players df %>% arrange ( Match_Id ) %>% filter ( row_number () %% 2 != 0 ) %>% # Filter for every 2nd row, starting at the 1st index. e.g. 1, 3, 5 rename_at ( # Rename columns to player_1 vars ( contains ( \"Player\" )), ~ str_replace ( . , \"Player\" , \"player_1\" ) ) %>% inner_join ( df %>% filter ( row_number () %% 2 == 0 ) %>% rename_at ( vars ( contains ( \"Player\" )), # Rename columns to player_2 ~ str_replace ( . , \"Player\" , \"player_2\" ) ) %>% select ( Match_Id , contains ( \"Player\" )), by = c ( 'Match_Id' ) ) %>% select ( Match_Id , player_1 , player_2 , Winner , everything ()) %>% return () } # Joining results to features feature_matrix_wide = aus_us_open_results %>% inner_join ( feature_matrix_long %>% select ( - Match_Id ), by = c ( \"Player\" , \"Tournament_Date\" = \"Feature_Date\" )) %>% gather_df () %>% mutate ( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio , F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio , F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage , F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game , F_Rank_Diff = ( F_player_1_Rank - F_player_2_Rank ), Winner = as.factor ( Winner ) ) %>% select ( Match_Id , player_1 , player_2 , Tournament , Tournament_Date , Round_Description , Winner , contains ( \"Diff\" )) train = feature_matrix_wide train %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Match_Id player_1 player_2 Tournament Tournament_Date Round_Description Winner F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff 1139 Adrian Ungur Daniel Brands U.S. Open, New York 2012-08-27 First Round 0 0.03279412 -0.014757229 0.002877458 0.073938088 -13 1140 Albert Montanes Richard Gasquet U.S. Open, New York 2012-08-27 First Round 0 -0.08000322 -0.077451342 -0.131108056 -0.180846832 97 1141 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 First Round 1 0.07711693 -0.044715517 0.068179841 -0.087361962 1 1142 Alex Bogomolov Jr. Andy Murray U.S. Open, New York 2012-08-27 First Round 0 -0.03964074 -0.031700826 -0.059010072 -0.094721700 69 1143 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 First Round 1 -0.02681392 0.006442134 -0.067779660 -0.009930089 151 1144 Ryan Harrison Benjamin Becker U.S. Open, New York 2012-08-27 First Round 1 0.04251983 0.018604623 0.026486753 -0.003548973 -24 Creating the Feature Matrix for the 2019 Australian Open \u00b6 Now that we have our training set, train , we need to create a feature matrix to create predictions on. To do this, we need to generate features again. We could simply append a player list to our raw data frame, create a mock date and then use the extract_latest_features_for_tournament function that we used before. Instead, we're going to create a lookup table for each unique player in the 2019 Australian Open. We will need to get their last 15 games and then find the mean for each feature so that our features are the same. Let's first explore what the dummy submission file looks like, then use it to get the unique players. read_csv ( 'data/men_dummy_submission_file.csv' ) %>% glimpse () As we can see, the dummy submission file contains every potential match up for the Open. This will be updated a few days before the Open starts with the actual players playing. Let's now create the lookup feature table. # Get a vector of unique players in this years' open using the dummy submission file unique_players = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% pull ( player_1 ) %>% unique () # Get the last 15 games played for each unique player and find their features lookup_feature_table = read_csv ( 'data/ATP_matches.csv' , na = \".\" ) %>% filter ( Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% mutate ( Match_Id = row_number (), # Add a match ID column to be used as a key Tournament_Date = dmy ( Tournament_Date ), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won ) %>% # Add a total games played column # clean_missing_data() %>% # Clean missing data split_winner_loser_columns () %>% # Change the data frame from wide to long add_ratio_features () %>% filter ( Player %in% unique_players ) %>% group_by ( Player ) %>% top_n ( 15 , Match_Id ) %>% summarise ( F_Player_Serve_Win_Ratio = mean ( F_Player_Serve_Win_Ratio ), F_Player_Return_Win_Ratio = mean ( F_Player_Return_Win_Ratio ), F_Player_BreakPoints_Per_Game = mean ( F_Player_BreakPoints_Per_Game ), F_Player_Game_Win_Percentage = mean ( F_Player_Game_Win_Percentage ), F_Player_Rank = last ( Player_Rank ) ) Now let's create features for every single combination. To do this we'll join our lookup_feature_table to the player_1 and player_2 columns in the dummy_submission_file . # Create feature matrix for the Australian Open for all player 1s features_player_1 = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% select ( player_1 ) %>% inner_join ( lookup_feature_table , by = c ( \"player_1\" = \"Player\" )) %>% rename ( F_player_1_Serve_Win_Ratio = F_Player_Serve_Win_Ratio , F_player_1_Return_Win_Ratio = F_Player_Return_Win_Ratio , F_player_1_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game , F_player_1_Game_Win_Percentage = F_Player_Game_Win_Percentage , F_player_1_Rank = F_Player_Rank ) # Create feature matrix for the Australian Open for all player 2s features_player_2 = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% select ( player_2 ) %>% inner_join ( lookup_feature_table , by = c ( \"player_2\" = \"Player\" )) %>% rename ( F_player_2_Serve_Win_Ratio = F_Player_Serve_Win_Ratio , F_player_2_Return_Win_Ratio = F_Player_Return_Win_Ratio , F_player_2_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game , F_player_2_Game_Win_Percentage = F_Player_Game_Win_Percentage , F_player_2_Rank = F_Player_Rank ) # Join the two dfs together and subtract features to create Difference features aus_open_2019_features = features_player_1 %>% bind_cols ( features_player_2 ) %>% select ( player_1 , player_2 , everything ()) %>% mutate ( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio , F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio , F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage , F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game , F_Rank_Diff = ( F_player_1_Rank - F_player_2_Rank ) ) %>% select ( player_1 , player_2 , contains ( \"Diff\" )) aus_open_2019_features %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) player_1 player_2 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff Novak Djokovic Rafael Nadal 0.06347805 0.02503802 0.07002382 0.08951024 1 Novak Djokovic Roger Federer 0.06583364 0.03628491 0.07661295 0.15455628 -1 Novak Djokovic Juan Martin del Potro 0.01067079 0.03436023 0.06382353 0.11259979 -2 Novak Djokovic Alexander Zverev 0.11117863 0.03125651 0.11055585 0.08661036 -3 Novak Djokovic Kevin Anderson 0.02132375 0.10449337 0.11184503 0.23684083 -4 Novak Djokovic Marin Cilic 0.08410746 0.02434916 0.07653035 0.08355134 -5 Generating 2019 Australian Open Predictions \u00b6 Now that we have our features, we can finally train our model and generate predictions for the 2019 Australian Open. Due to its simplicity, we will use h2o's Auto Machine Learning function h2o.automl . This will train a heap of different models and optimise the hyperparameters, as well as creating stacked ensembles automatically for us. We will use optimising by log loss. First, we must create h2o frames for our training and feature data frames. Then we will run h2o.automl . Note that we can set the max_runtime_secs parameter. As this is a notebook, I have set it for 30 seconds - but I suggest you give it 10 minutes to create the best model. We can then create our predictions and assign them back to our aus_open_2019_features data frame. Finally, we will group_by player and find the best player, on average. ## Setup H2O h2o.init ( ip = \"localhost\" , port = 54321 , enable_assertions = TRUE , nthreads = 2 , max_mem_size = \"24g\" ) ## Sending file to h2o train_h2o = feature_matrix_wide %>% select ( contains ( \"Diff\" ), Winner ) %>% as.h2o ( destination_frame = \"train_h2o\" ) aus_open_2019_features_h2o = aus_open_2019_features %>% select ( contains ( \"Diff\" )) %>% as.h2o ( destination_frame = \"aus_open_2019_features_h2o\" ) ## Running Auto ML mens_model = h2o.automl ( y = \"Winner\" , training_frame = train_h2o , max_runtime_secs = 30 , max_models = 100 , stopping_metric = \"logloss\" , sort_metric = \"logloss\" , balance_classes = TRUE , seed = 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) ## Predictions on test frame predictions = h2o.predict ( mens_model @ leader , aus_open_2019_features_h2o ) %>% as.data.frame () aus_open_2019_features $ prob_player_1 = predictions $ p1 aus_open_2019_features $ prob_player_2 = predictions $ p0 h2o.shutdown ( prompt = FALSE ) Now let's find the best player by taking the mean of the prediction probability by player. aus_open_2019_features %>% select ( player_1 , starts_with ( \"F_\" ), prob_player_1 ) %>% group_by ( player_1 ) %>% summarise_all ( mean ) %>% arrange ( desc ( prob_player_1 )) %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) player_1 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff prob_player_1 Novak Djokovic 0.1109364627 0.076150615 0.1483970690 0.17144300 NA 0.8616486 Karen Khachanov 0.0960639298 0.061436164 0.1059967623 0.04544955 NA 0.8339594 Juan Martin del Potro 0.1003931993 0.042025222 0.0847985439 0.05943767 NA 0.8218308 Rafael Nadal 0.0480432305 0.051531252 0.0790179917 0.08181694 NA 0.8032543 Gilles Simon 0.0646937767 0.084843307 0.0901401318 0.08675350 NA 0.7985995 Roger Federer 0.0452014997 0.040992497 0.0725719954 0.01817046 NA 0.7962289 Kei Nishikori 0.0777155934 0.018720226 0.0800648870 0.02740276 NA 0.7843631 Marin Cilic 0.0285413602 0.053017465 0.0736687072 0.08883055 NA 0.7804876 Tomas Berdych 0.0471654691 0.047289449 0.0737401748 0.10584114 NA 0.7739211 Daniil Medvedev 0.0275430665 0.031121856 0.0721948279 0.01803757 NA 0.7543269 Stefanos Tsitsipas 0.0470382377 0.023825850 0.0577628626 0.02105227 NA 0.7511674 Dominic Thiem 0.0258904189 0.032481624 0.0483707080 0.05857158 NA 0.7451547 Alexander Zverev 0.0006199716 0.044811275 0.0380134371 0.08423392 NA 0.7374897 Kyle Edmund 0.0558006240 0.011963627 0.0478850676 0.05142186 NA 0.7304873 Pablo Carreno Busta 0.0321878318 0.029862068 0.0413674481 -0.00229784 NA 0.7302043 Borna Coric 0.0762084129 -0.010097922 0.0413621283 -0.01924267 NA 0.7268124 Kevin Anderson 0.0907358428 -0.027171681 0.0381421997 -0.06362578 NA 0.7260799 David Goffin -0.0034821911 0.037247336 0.0162572061 0.05603565 NA 0.7155908 Fernando Verdasco 0.0229261365 0.032884054 0.0521212576 0.04668854 NA 0.7120831 Roberto Bautista Agut 0.0047641170 0.049939608 0.0218975349 0.07331023 NA 0.7009891 Milos Raonic 0.0849726089 -0.028732182 0.0385944327 -0.08009382 NA 0.6986865 Fabio Fognini -0.0394792678 0.047935185 0.0226546894 0.06213496 NA 0.6982031 Hyeon Chung 0.0042489153 0.047722133 0.0158096386 0.04823304 NA 0.6958943 Jack Sock -0.0099659903 0.026454984 0.0186547428 0.02307214 NA 0.6757770 Diego Schwartzman -0.0317130675 0.032098381 0.0006215006 0.05621187 NA 0.6631067 John Millman 0.0016290285 0.042676556 0.0119857356 0.06228135 NA 0.6603912 Nikoloz Basilashvili -0.0099968609 0.005561102 0.0473876170 0.03661962 NA 0.6602628 John Isner 0.1346946527 -0.070556940 0.0161348609 -0.11425009 NA 0.6598097 Gael Monfils -0.0074254934 0.024286746 0.0295568649 0.04007519 NA 0.6449506 Richard Gasquet 0.0296009556 -0.011382437 0.0013138324 -0.03972967 NA 0.6442043 ... ... ... ... ... ... ... Laslo Djere -0.042300822 -0.0150684095 -0.064667709 -0.0349151578 NA 0.3606923 David Ferrer -0.036179509 0.0532782117 0.012751020 0.0914824480 NA 0.3488057 Bradley Klahn -0.001248083 -0.0444982448 -0.025987040 -0.1181295700 NA 0.3487806 Marcel Granollers -0.031011830 -0.0094056152 -0.049853664 0.0136841358 NA 0.3460035 Ricardas Berankis -0.022557215 -0.0103782963 -0.047937290 -0.0468488990 NA 0.3454980 Radu Albot -0.040829057 0.0076150564 -0.034891704 0.0443672533 NA 0.3420615 Jordan Thompson -0.068554906 0.0261969117 -0.044349181 0.0206636045 NA 0.3358572 Thomas Fabbiano -0.060583307 0.0275756029 -0.025883493 0.0709707306 NA 0.3319778 Roberto Carballes Baena -0.054016396 -0.0091521177 -0.019093050 0.0347187874 NA 0.3312105 Paolo Lorenzi -0.038613500 -0.0212206827 -0.052602703 0.0199474025 NA 0.3299791 Guido Andreozzi -0.038614385 -0.0133763922 0.029549861 0.0636745661 NA 0.3288762 Peter Polansky 0.007461636 -0.0163389196 -0.024034159 -0.0442144260 NA 0.3216756 Ernests Gulbis -0.062827089 -0.0134699552 -0.027633425 -0.0518663252 NA 0.3123511 Thiago Monteiro 0.001235931 -0.0288349103 -0.043831840 -0.0654744344 NA 0.3122069 Casper Ruud 0.016838968 -0.0178511679 0.015234507 0.0219131874 NA 0.3119321 Marco Trungelliti -0.022148774 -0.0005658242 0.048542554 0.1243537739 NA 0.3092636 Jiri Vesely -0.050204009 -0.0351868278 -0.042887646 -0.0160467165 NA 0.3089287 Guillermo Garcia-Lopez -0.090076100 -0.0108663630 -0.048712763 -0.0124446402 NA 0.3080898 Michael Mmoh -0.063802934 -0.0079053251 -0.011112236 -0.0332042032 NA 0.2822330 Jason Kubler -0.124758873 -0.0202756806 -0.013998570 0.1020895301 NA 0.2814246 Ruben Bemelmans -0.029036164 -0.0138846550 -0.032256254 -0.0363563402 NA 0.2772185 Bjorn Fratangelo -0.014149222 0.0033574304 -0.019931504 -0.0360199607 NA 0.2652527 Pablo Andujar -0.042869833 -0.0488261697 -0.070057834 -0.0164918910 NA 0.2647100 Christian Garin -0.046150875 0.0235799476 -0.006209664 0.0736304057 NA 0.2631607 Ivo Karlovic 0.071597162 -0.1093833837 0.001410787 -0.1237762218 NA 0.2500242 Juan Ignacio Londero -0.026454456 -0.0715665271 -0.016749898 -0.0363353678 NA 0.2351747 Ramkumar Ramanathan -0.005371622 -0.0606138479 -0.041631884 -0.0005573405 NA 0.2272977 Reilly Opelka 0.025704824 -0.0607219257 -0.015474944 -0.0720809006 NA 0.2262993 Carlos Berlocq -0.063580460 0.0074576369 -0.054277974 -0.0165235079 NA 0.2112275 Pedro Sousa -0.197333352 -0.0734557562 -0.161962722 -0.1023311674 NA 0.1502313 Disclaimer \u00b6 Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Aus Open R Tutorial"},{"location":"modelling/AusOpenRTutorial/#australian-open-datathon-r-tutorial","text":"","title":"Australian Open Datathon R Tutorial"},{"location":"modelling/AusOpenRTutorial/#overview","text":"","title":"Overview"},{"location":"modelling/AusOpenRTutorial/#the-task","text":"This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodology and thought process, read this article.","title":"The Task"},{"location":"modelling/AusOpenRTutorial/#exploring-the-data","text":"First we need to get an idea of what the data looks like. Let's read the men's data in and get an idea of what it looks like. Note that you will need to install all the packages listed below unless you already have them. Note that for this tutorial I will be using dplyr , if you are not familiar with the syntax I encourage you to read up on the basics . # Import libraries library ( dplyr ) library ( readr ) library ( tidyr ) library ( RcppRoll ) library ( tidyselect ) library ( lubridate ) library ( stringr ) library ( zoo ) library ( purrr ) library ( h2o ) library ( DT ) mens = readr :: read_csv ( 'data/ATP_matches.csv' , na = \".\" ) # NAs are indicated by . mens %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Winner Loser Tournament Tournament_Date Court_Surface Round_Description Winner_Rank Loser_Rank Retirement_Ind Winner_Sets_Won ... Loser_DoubleFaults Loser_FirstServes_Won Loser_FirstServes_In Loser_SecondServes_Won Loser_SecondServes_In Loser_BreakPoints_Won Loser_BreakPoints Loser_ReturnPoints_Won Loser_ReturnPoints_Faced Loser_TotalPoints_Won Edouard Roger-Vasselin Eric Prodon Chennai 2-Jan-12 Hard First Round 106 97 0 2 ... 3 21 33 13 26 1 3 15 49 49 Dudi Sela Fabio Fognini Chennai 2-Jan-12 Hard First Round 83 48 0 2 ... 4 17 32 5 26 0 1 8 33 30 Go Soeda Frederico Gil Chennai 2-Jan-12 Hard First Round 120 102 0 2 ... 2 45 70 18 35 2 4 36 103 99 Yuki Bhambri Karol Beck Chennai 2-Jan-12 Hard First Round 345 101 0 2 ... 1 15 33 13 29 2 3 15 46 43 Yuichi Sugita Olivier Rochus Chennai 2-Jan-12 Hard First Round 235 67 0 2 ... 0 19 32 13 22 1 7 30 78 62 Benoit Paire Pere Riba Chennai 2-Jan-12 Hard First Round 95 89 0 2 ... 5 13 20 12 32 0 1 9 44 34 Victor Hanescu Sam Querrey Chennai 2-Jan-12 Hard First Round 90 93 0 2 ... 8 29 41 7 24 1 3 17 57 53 Yen-Hsun Lu Thiemo de Bakker Chennai 2-Jan-12 Hard First Round 82 223 0 2 ... 0 20 32 10 24 1 1 19 57 49 Andreas Beck Vasek Pospisil Chennai 2-Jan-12 Hard First Round 98 119 0 2 ... 3 39 57 16 38 1 5 24 74 79 Ivan Dodig Vishnu Vardhan Chennai 2-Jan-12 Hard First Round 36 313 0 2 ... 5 41 59 13 27 2 8 34 101 88 David Goffin Xavier Malisse Chennai 2-Jan-12 Hard First Round 174 49 0 2 ... 1 31 43 19 34 1 4 27 85 77 David Goffin Andreas Beck Chennai 2-Jan-12 Hard Second Round 174 98 0 2 ... 0 43 71 14 27 2 8 27 82 84 Dudi Sela Benoit Paire Chennai 2-Jan-12 Hard Second Round 83 95 0 2 ... 5 40 58 21 46 1 7 26 87 87 Stan Wawrinka Edouard Roger-Vasselin Chennai 2-Jan-12 Hard Second Round 17 106 0 2 ... 0 43 70 16 34 4 6 28 82 87 Go Soeda Ivan Dodig Chennai 2-Jan-12 Hard Second Round 120 36 0 2 ... 2 31 41 11 28 1 4 23 73 65 Milos Raonic Victor Hanescu Chennai 2-Jan-12 Hard Second Round 31 90 0 2 ... 1 25 38 5 14 0 4 15 56 45 Yuichi Sugita Yen-Hsun Lu Chennai 2-Jan-12 Hard Second Round 235 82 0 2 ... 4 34 45 12 34 2 9 38 93 84 Janko Tipsarevic Yuki Bhambri Chennai 2-Jan-12 Hard Second Round 9 345 0 2 ... 2 12 22 9 17 0 1 8 41 29 Janko Tipsarevic David Goffin Chennai 2-Jan-12 Hard Quarter-finals 9 174 0 2 ... 5 34 51 19 40 1 2 18 67 71 Milos Raonic Dudi Sela Chennai 2-Jan-12 Hard Quarter-finals 31 83 0 2 ... 2 23 31 19 28 0 3 16 69 58 Go Soeda Stan Wawrinka Chennai 2-Jan-12 Hard Quarter-finals 120 17 0 2 ... 4 18 34 13 31 3 7 31 74 62 Nicolas Almagro Yuichi Sugita Chennai 2-Jan-12 Hard Quarter-finals 10 235 0 2 ... 1 36 65 30 40 3 12 45 123 111 Janko Tipsarevic Go Soeda Chennai 2-Jan-12 Hard Semi-finals 9 120 0 2 ... 1 21 33 10 28 1 1 10 44 41 Milos Raonic Nicolas Almagro Chennai 2-Jan-12 Hard Semi-finals 31 10 0 2 ... 0 31 45 8 15 0 3 12 54 51 Milos Raonic Janko Tipsarevic Chennai 2-Jan-12 Hard Finals 31 9 0 2 ... 2 59 83 34 55 0 4 25 113 118 Igor Andreev Adrian Mannarino Brisbane 2-Jan-12 Hard First Round 115 87 0 2 ... 3 24 35 13 25 1 3 21 70 58 Alexandr Dolgopolov Alejandro Falla Brisbane 2-Jan-12 Hard First Round 15 74 0 2 ... 3 16 33 12 25 3 7 33 75 61 Tatsuma Ito Benjamin Mitchell Brisbane 2-Jan-12 Hard First Round 122 227 0 2 ... 6 30 44 7 24 0 2 13 52 50 Kei Nishikori Cedrik-Marcel Stebe Brisbane 2-Jan-12 Hard First Round 25 81 0 2 ... 2 27 49 23 41 3 6 28 75 78 Denis Istomin Florian Mayer Brisbane 2-Jan-12 Hard First Round 73 23 1 1 ... 1 28 38 11 17 0 2 15 56 54 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Malek Jaziri Fernando Verdasco Paris 29-Oct-18 Indoor Hard Second Round 55 27 0 2 ... 6 46 60 16 35 3 13 39 104 101 Alexander Zverev Frances Tiafoe Paris 29-Oct-18 Indoor Hard Second Round 5 44 0 2 ... 4 26 40 16 36 2 10 27 72 69 Dominic Thiem Gilles Simon Paris 29-Oct-18 Indoor Hard Second Round 8 31 0 2 ... 1 13 26 12 25 2 2 23 59 48 Novak Djokovic Joao Sousa Paris 29-Oct-18 Indoor Hard Second Round 2 48 0 2 ... 2 25 35 6 22 1 10 27 74 58 Karen Khachanov Matthew Ebden Paris 29-Oct-18 Indoor Hard Second Round 18 39 1 2 ... 6 8 18 5 20 1 2 10 30 23 John Isner Mikhail Kukushkin Paris 29-Oct-18 Indoor Hard Second Round 9 54 0 2 ... 1 54 80 24 39 0 1 13 90 91 Kevin Anderson Nikoloz Basilashvili Paris 29-Oct-18 Indoor Hard Second Round 6 22 0 2 ... 7 43 54 30 49 0 3 26 106 99 Marin Cilic Philipp Kohlschreiber Paris 29-Oct-18 Indoor Hard Second Round 7 43 0 2 ... 1 19 34 12 20 1 1 17 55 48 Jack Sock Richard Gasquet Paris 29-Oct-18 Indoor Hard Second Round 23 28 0 2 ... 4 18 33 16 29 0 4 19 59 53 Grigor Dimitrov Roberto Bautista Agut Paris 29-Oct-18 Indoor Hard Second Round 10 25 0 2 ... 0 34 48 11 20 2 4 27 76 72 Damir Dzumhur Stefanos Tsitsipas Paris 29-Oct-18 Indoor Hard Second Round 52 16 0 2 ... 3 14 26 15 30 2 2 17 52 46 Dominic Thiem Borna Coric Paris 29-Oct-18 Indoor Hard Third Round 8 13 0 2 ... 1 39 57 16 38 2 2 27 88 82 Novak Djokovic Damir Dzumhur Paris 29-Oct-18 Indoor Hard Third Round 2 52 1 2 ... 4 15 28 7 18 0 0 8 28 30 Alexander Zverev Diego Schwartzman Paris 29-Oct-18 Indoor Hard Third Round 5 19 0 2 ... 2 22 37 12 24 0 4 18 58 52 Roger Federer Fabio Fognini Paris 29-Oct-18 Indoor Hard Third Round 3 14 0 2 ... 6 22 32 15 37 1 5 16 54 53 Marin Cilic Grigor Dimitrov Paris 29-Oct-18 Indoor Hard Third Round 7 10 0 2 ... 1 37 55 14 32 1 5 22 71 73 Karen Khachanov John Isner Paris 29-Oct-18 Indoor Hard Third Round 18 9 0 2 ... 4 67 80 19 38 0 0 17 100 103 Kei Nishikori Kevin Anderson Paris 29-Oct-18 Indoor Hard Third Round 11 6 0 2 ... 1 26 33 11 19 0 0 11 51 48 Jack Sock Malek Jaziri Paris 29-Oct-18 Indoor Hard Third Round 23 55 0 2 ... 6 13 21 10 24 0 0 9 41 32 Karen Khachanov Alexander Zverev Paris 29-Oct-18 Indoor Hard Quarter-finals 18 5 0 2 ... 7 26 47 4 21 1 3 10 36 40 Dominic Thiem Jack Sock Paris 29-Oct-18 Indoor Hard Quarter-finals 8 23 0 2 ... 5 44 59 19 37 2 10 34 97 97 Roger Federer Kei Nishikori Paris 29-Oct-18 Indoor Hard Quarter-finals 3 11 0 2 ... 0 21 37 16 26 0 1 12 56 49 Novak Djokovic Marin Cilic Paris 29-Oct-18 Indoor Hard Quarter-finals 2 7 0 2 ... 0 38 55 11 28 2 5 29 85 78 Karen Khachanov Dominic Thiem Paris 29-Oct-18 Indoor Hard Semi-finals 18 8 0 2 ... 0 19 29 8 26 1 3 15 47 42 Novak Djokovic Roger Federer Paris 29-Oct-18 Indoor Hard Semi-finals 2 3 0 2 ... 2 69 93 25 46 1 2 29 113 123 Karen Khachanov Novak Djokovic Paris 29-Oct-18 Indoor Hard Finals 18 2 0 2 ... 1 30 43 14 28 1 5 20 66 64 Jaume Antoni Munar Clar Frances Tiafoe Milan 5-Nov-18 Indoor Hard NA 76 40 0 3 ... 3 21 29 6 17 0 2 5 46 32 Frances Tiafoe Hubert Hurkacz Milan 5-Nov-18 Indoor Hard NA 40 85 0 3 ... 4 35 48 10 19 1 7 22 78 67 Hubert Hurkacz Jaume Antoni Munar Clar Milan 5-Nov-18 Indoor Hard NA 85 76 0 3 ... 1 43 63 15 35 3 9 29 80 87 Andrey Rublev Liam Caruana Milan 5-Nov-18 Indoor Hard NA 68 NA 0 3 ... 1 28 39 4 14 1 3 18 57 50 As we can see, we have a Winner column, a Loser column, as well as other columns detailing the match details, and other columns which have the stats for that match. As we have a Winner column, if we use the current data structure to train a model we will leak the result. The model will simply learn that the actual winner comes from the Winner column, rather than learning from other features that we can create, such as First Serve % . To avoid this problem, let's reshape the data from wide to long, then shuffle the data. For this, we will define a function, split_winner_loser_columns , which splits the raw data frame into two data frames, appends them together, and then shuffles the data. Let's also remove all Grass and Clay matches from our data, as we will be modelling the Australian Open which is a hardcourt surface. Additionally, we will add a few columns, such as Match_Id and Total_Games . These will be useful later. split_winner_loser_columns <- function ( df ) { # This function splits the raw data into two data frames and appends them together then shuffles them # This output is a data frame with only one player's stats on each row (i.e. in long format) # Grab a df with only the Winner's stats winner = df %>% select ( - contains ( \"Loser\" )) %>% # Select only the Winner columns + extra game info columns as a df rename_at ( # Rename all columns containing \"Winner\" to \"Player\" vars ( contains ( \"Winner\" )), ~ str_replace ( . , \"Winner\" , \"Player\" ) ) %>% mutate ( Winner = 1 ) # Create a target column # Repeat the process with the loser's stats loser = df %>% select ( - contains ( \"Winner\" )) %>% rename_at ( vars ( contains ( \"Loser\" )), ~ str_replace ( . , \"Loser\" , \"Player\" ) ) %>% mutate ( Winner = 0 ) set.seed ( 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) # Create a df that appends both the Winner and loser df together combined_df = winner %>% rbind ( loser ) %>% # Append the loser df to the Winner df slice ( sample ( 1 : n ())) %>% # Randomise row order arrange ( Match_Id ) %>% # Arrange by Match_Id return () } # Read in men and womens data; randomise the data to avoid result leakage mens = readr :: read_csv ( 'data/ATP_matches.csv' , na = \".\" ) %>% filter ( Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% # Filter to only use hardcourt games mutate ( Match_Id = row_number (), # Add a match ID column to be used as a key Tournament_Date = dmy ( Tournament_Date ), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won ) %>% # Add a total games played column split_winner_loser_columns () # Change the data frame from wide to long mens %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Player Tournament Tournament_Date Court_Surface Round_Description Player_Rank Retirement_Ind Player_Sets_Won Player_Games_Won Player_Aces ... Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Match_Id Total_Games Winner Eric Prodon Chennai 2012-01-02 Hard First Round 97 0 0 7 2 ... 13 26 1 3 15 49 49 1 19 0 Edouard Roger-Vasselin Chennai 2012-01-02 Hard First Round 106 0 2 12 5 ... 12 19 4 7 25 59 59 1 19 1 Dudi Sela Chennai 2012-01-02 Hard First Round 83 0 2 12 2 ... 11 16 6 14 36 58 61 2 13 1 Fabio Fognini Chennai 2012-01-02 Hard First Round 48 0 0 1 1 ... 5 26 0 1 8 33 30 2 13 0 Frederico Gil Chennai 2012-01-02 Hard First Round 102 0 1 14 5 ... 18 35 2 4 36 103 99 3 33 0 Go Soeda Chennai 2012-01-02 Hard First Round 120 0 2 19 6 ... 19 39 5 11 42 105 109 3 33 1","title":"Exploring the Data"},{"location":"modelling/AusOpenRTutorial/#feature-creation","text":"Now that we have a fairly good understanding of what the data looks like, let's add some features. To do this we will define a function. Ideally we want to add features which will provide predictive power to our model. Thinking about the dynamics of tennis, we know that players often will matches by \"breaking\" the opponent's serve (i.e. winning a game when the opponent is serving). This is especially important in mens tennis. Let's create a feature called F_Player_BreakPoints_Per_Game , which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let's also create a feature called F_Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \"holding\" serve is important (i.e. winning a game when you are serving). Let's create a feature called F_Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let's create a feature called F_Player_Game_Win_Percentage which is the propotion of games that a player wins. add_ratio_features <- function ( df ) { # This function adds ratio features to a long df df %>% mutate ( # Point Win ratio when serving F_Player_Serve_Win_Ratio = ( Player_FirstServes_Won + Player_SecondServes_Won - Player_DoubleFaults ) / ( Player_FirstServes_In + Player_SecondServes_In + Player_DoubleFaults ), # Point win ratio when returning F_Player_Return_Win_Ratio = Player_ReturnPoints_Won / Player_ReturnPoints_Faced , # Breakpoints per receiving game F_Player_BreakPoints_Per_Game = Player_BreakPoints / Total_Games , F_Player_Game_Win_Percentage = Player_Games_Won / Total_Games ) %>% mutate_at ( vars ( colnames ( . ), - contains ( \"Rank\" ), - Tournament_Date ), # Replace all NAs with0 apart from Rank, Date ~ ifelse ( is.na ( . ), 0 , . ) ) %>% return () } mens = mens %>% add_ratio_features () # Add features Now that we have added our features, we need to create rolling averages for them. We cannot simply use current match statistics, as they will leak the result to the model. Instead, we need to use past match statistics to predict future matches. Here we will use a rolling mean with a window of 15. If the player hasn't played 15 games, we will instead use a cumulative mean. We will also lag the result so as to not leak the result. This next chunk of code simply takes all the columns starting with F_ and calculates these means. mens = mens %>% group_by ( Player ) %>% # Group by player mutate_at ( # Create a rolling mean with window 15 for each player. vars ( starts_with ( \"F_\" )), # If the player hasn't played 15 games, use a cumulative mean ~ coalesce ( rollmean ( . , k = 15 , align = \"right\" , fill = NA_real_ ), cummean ( . )) %>% lag () ) %>% ungroup ()","title":"Feature Creation"},{"location":"modelling/AusOpenRTutorial/#creating-a-training-feature-matrix","text":"In predictive modelling language - features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options We can train the model on every tennis match in the data set, or We can only train the model on Australian Open matches. Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. We have decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface - hard court. However, we also need to train our model in the same way that will be used to predict the 2019 Australian Open. When predicting the 2 nd round, we won't have data from the 1 st round. So we will need to build our training feature matrix with this in mind. We should extract features for a player from past games at the start of the tournament and apply them to every matchup that that player plays. To do this, we will create a function, extract_latest_features_for_tournament , which maps over our feature data frame for the dates in the first round of a tournament and grabs features. First, we need the Australian Open and US Open results - let's grab these and then apply our function. # Get Australian Open and US Open Results aus_us_open_results = mens %>% filter (( Tournament == \"Australian Open, Melbourne\" | Tournament == \"U.S. Open, New York\" ) & Round_Description != \"Qualifying\" & Tournament_Date != \"2012-01-16\" ) %>% # Filter out qualifiers select ( Match_Id , Player , Tournament , Tournament_Date , Round_Description , Winner ) # Create a function which extracts features for each tournament extract_latest_features_for_tournament = function ( df , dte ) { df %>% # Filter for the 1st round filter ( Tournament_Date == dte , Round_Description == \"First Round\" , Tournament_Date != \"2012-01-16\" ) %>% group_by ( Player ) %>% # Group by player select_at ( vars ( Match_Id , starts_with ( \"F_\" ), Player_Rank ) # Grab the players' features ) %>% rename ( F_Player_Rank = Player_Rank ) %>% ungroup () %>% mutate ( Feature_Date = dte ) %>% select ( Player , Feature_Date , everything ()) } # Create a feature matrix in long format feature_matrix_long = aus_us_open_results %>% distinct ( Tournament_Date ) %>% # Pull all Tournament Dates pull () %>% map_dfr ( ~ extract_latest_features_for_tournament ( mens , . ) # Get the features ) %>% filter ( Feature_Date != \"2012-01-16\" ) %>% # Filter out the first Aus Open mutate_at ( # Replace NAs with the mean vars ( starts_with ( \"F_\" )), ~ ifelse ( is.na ( . ), mean ( . , na.rm = TRUE ), . ) ) Now that we have a feature matrix in long format, we need to convert it to wide format so that the features are on the same row. To do this we will define a function gather_df , which converts the data frame from long to wide. Let's also join the results to the matrix and convert the Winner column to a factor. Finally, we will take the difference of player1 and player2's features, so as to reduce the dimensionality of the model. gather_df <- function ( df ) { # This function puts the df back into its original format of each row containing stats for both players df %>% arrange ( Match_Id ) %>% filter ( row_number () %% 2 != 0 ) %>% # Filter for every 2nd row, starting at the 1st index. e.g. 1, 3, 5 rename_at ( # Rename columns to player_1 vars ( contains ( \"Player\" )), ~ str_replace ( . , \"Player\" , \"player_1\" ) ) %>% inner_join ( df %>% filter ( row_number () %% 2 == 0 ) %>% rename_at ( vars ( contains ( \"Player\" )), # Rename columns to player_2 ~ str_replace ( . , \"Player\" , \"player_2\" ) ) %>% select ( Match_Id , contains ( \"Player\" )), by = c ( 'Match_Id' ) ) %>% select ( Match_Id , player_1 , player_2 , Winner , everything ()) %>% return () } # Joining results to features feature_matrix_wide = aus_us_open_results %>% inner_join ( feature_matrix_long %>% select ( - Match_Id ), by = c ( \"Player\" , \"Tournament_Date\" = \"Feature_Date\" )) %>% gather_df () %>% mutate ( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio , F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio , F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage , F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game , F_Rank_Diff = ( F_player_1_Rank - F_player_2_Rank ), Winner = as.factor ( Winner ) ) %>% select ( Match_Id , player_1 , player_2 , Tournament , Tournament_Date , Round_Description , Winner , contains ( \"Diff\" )) train = feature_matrix_wide train %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Match_Id player_1 player_2 Tournament Tournament_Date Round_Description Winner F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff 1139 Adrian Ungur Daniel Brands U.S. Open, New York 2012-08-27 First Round 0 0.03279412 -0.014757229 0.002877458 0.073938088 -13 1140 Albert Montanes Richard Gasquet U.S. Open, New York 2012-08-27 First Round 0 -0.08000322 -0.077451342 -0.131108056 -0.180846832 97 1141 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 First Round 1 0.07711693 -0.044715517 0.068179841 -0.087361962 1 1142 Alex Bogomolov Jr. Andy Murray U.S. Open, New York 2012-08-27 First Round 0 -0.03964074 -0.031700826 -0.059010072 -0.094721700 69 1143 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 First Round 1 -0.02681392 0.006442134 -0.067779660 -0.009930089 151 1144 Ryan Harrison Benjamin Becker U.S. Open, New York 2012-08-27 First Round 1 0.04251983 0.018604623 0.026486753 -0.003548973 -24","title":"Creating a Training Feature Matrix"},{"location":"modelling/AusOpenRTutorial/#creating-the-feature-matrix-for-the-2019-australian-open","text":"Now that we have our training set, train , we need to create a feature matrix to create predictions on. To do this, we need to generate features again. We could simply append a player list to our raw data frame, create a mock date and then use the extract_latest_features_for_tournament function that we used before. Instead, we're going to create a lookup table for each unique player in the 2019 Australian Open. We will need to get their last 15 games and then find the mean for each feature so that our features are the same. Let's first explore what the dummy submission file looks like, then use it to get the unique players. read_csv ( 'data/men_dummy_submission_file.csv' ) %>% glimpse () As we can see, the dummy submission file contains every potential match up for the Open. This will be updated a few days before the Open starts with the actual players playing. Let's now create the lookup feature table. # Get a vector of unique players in this years' open using the dummy submission file unique_players = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% pull ( player_1 ) %>% unique () # Get the last 15 games played for each unique player and find their features lookup_feature_table = read_csv ( 'data/ATP_matches.csv' , na = \".\" ) %>% filter ( Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% mutate ( Match_Id = row_number (), # Add a match ID column to be used as a key Tournament_Date = dmy ( Tournament_Date ), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won ) %>% # Add a total games played column # clean_missing_data() %>% # Clean missing data split_winner_loser_columns () %>% # Change the data frame from wide to long add_ratio_features () %>% filter ( Player %in% unique_players ) %>% group_by ( Player ) %>% top_n ( 15 , Match_Id ) %>% summarise ( F_Player_Serve_Win_Ratio = mean ( F_Player_Serve_Win_Ratio ), F_Player_Return_Win_Ratio = mean ( F_Player_Return_Win_Ratio ), F_Player_BreakPoints_Per_Game = mean ( F_Player_BreakPoints_Per_Game ), F_Player_Game_Win_Percentage = mean ( F_Player_Game_Win_Percentage ), F_Player_Rank = last ( Player_Rank ) ) Now let's create features for every single combination. To do this we'll join our lookup_feature_table to the player_1 and player_2 columns in the dummy_submission_file . # Create feature matrix for the Australian Open for all player 1s features_player_1 = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% select ( player_1 ) %>% inner_join ( lookup_feature_table , by = c ( \"player_1\" = \"Player\" )) %>% rename ( F_player_1_Serve_Win_Ratio = F_Player_Serve_Win_Ratio , F_player_1_Return_Win_Ratio = F_Player_Return_Win_Ratio , F_player_1_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game , F_player_1_Game_Win_Percentage = F_Player_Game_Win_Percentage , F_player_1_Rank = F_Player_Rank ) # Create feature matrix for the Australian Open for all player 2s features_player_2 = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% select ( player_2 ) %>% inner_join ( lookup_feature_table , by = c ( \"player_2\" = \"Player\" )) %>% rename ( F_player_2_Serve_Win_Ratio = F_Player_Serve_Win_Ratio , F_player_2_Return_Win_Ratio = F_Player_Return_Win_Ratio , F_player_2_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game , F_player_2_Game_Win_Percentage = F_Player_Game_Win_Percentage , F_player_2_Rank = F_Player_Rank ) # Join the two dfs together and subtract features to create Difference features aus_open_2019_features = features_player_1 %>% bind_cols ( features_player_2 ) %>% select ( player_1 , player_2 , everything ()) %>% mutate ( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio , F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio , F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage , F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game , F_Rank_Diff = ( F_player_1_Rank - F_player_2_Rank ) ) %>% select ( player_1 , player_2 , contains ( \"Diff\" )) aus_open_2019_features %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) player_1 player_2 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff Novak Djokovic Rafael Nadal 0.06347805 0.02503802 0.07002382 0.08951024 1 Novak Djokovic Roger Federer 0.06583364 0.03628491 0.07661295 0.15455628 -1 Novak Djokovic Juan Martin del Potro 0.01067079 0.03436023 0.06382353 0.11259979 -2 Novak Djokovic Alexander Zverev 0.11117863 0.03125651 0.11055585 0.08661036 -3 Novak Djokovic Kevin Anderson 0.02132375 0.10449337 0.11184503 0.23684083 -4 Novak Djokovic Marin Cilic 0.08410746 0.02434916 0.07653035 0.08355134 -5","title":"Creating the Feature Matrix for the 2019 Australian Open"},{"location":"modelling/AusOpenRTutorial/#generating-2019-australian-open-predictions","text":"Now that we have our features, we can finally train our model and generate predictions for the 2019 Australian Open. Due to its simplicity, we will use h2o's Auto Machine Learning function h2o.automl . This will train a heap of different models and optimise the hyperparameters, as well as creating stacked ensembles automatically for us. We will use optimising by log loss. First, we must create h2o frames for our training and feature data frames. Then we will run h2o.automl . Note that we can set the max_runtime_secs parameter. As this is a notebook, I have set it for 30 seconds - but I suggest you give it 10 minutes to create the best model. We can then create our predictions and assign them back to our aus_open_2019_features data frame. Finally, we will group_by player and find the best player, on average. ## Setup H2O h2o.init ( ip = \"localhost\" , port = 54321 , enable_assertions = TRUE , nthreads = 2 , max_mem_size = \"24g\" ) ## Sending file to h2o train_h2o = feature_matrix_wide %>% select ( contains ( \"Diff\" ), Winner ) %>% as.h2o ( destination_frame = \"train_h2o\" ) aus_open_2019_features_h2o = aus_open_2019_features %>% select ( contains ( \"Diff\" )) %>% as.h2o ( destination_frame = \"aus_open_2019_features_h2o\" ) ## Running Auto ML mens_model = h2o.automl ( y = \"Winner\" , training_frame = train_h2o , max_runtime_secs = 30 , max_models = 100 , stopping_metric = \"logloss\" , sort_metric = \"logloss\" , balance_classes = TRUE , seed = 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) ## Predictions on test frame predictions = h2o.predict ( mens_model @ leader , aus_open_2019_features_h2o ) %>% as.data.frame () aus_open_2019_features $ prob_player_1 = predictions $ p1 aus_open_2019_features $ prob_player_2 = predictions $ p0 h2o.shutdown ( prompt = FALSE ) Now let's find the best player by taking the mean of the prediction probability by player. aus_open_2019_features %>% select ( player_1 , starts_with ( \"F_\" ), prob_player_1 ) %>% group_by ( player_1 ) %>% summarise_all ( mean ) %>% arrange ( desc ( prob_player_1 )) %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) player_1 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff prob_player_1 Novak Djokovic 0.1109364627 0.076150615 0.1483970690 0.17144300 NA 0.8616486 Karen Khachanov 0.0960639298 0.061436164 0.1059967623 0.04544955 NA 0.8339594 Juan Martin del Potro 0.1003931993 0.042025222 0.0847985439 0.05943767 NA 0.8218308 Rafael Nadal 0.0480432305 0.051531252 0.0790179917 0.08181694 NA 0.8032543 Gilles Simon 0.0646937767 0.084843307 0.0901401318 0.08675350 NA 0.7985995 Roger Federer 0.0452014997 0.040992497 0.0725719954 0.01817046 NA 0.7962289 Kei Nishikori 0.0777155934 0.018720226 0.0800648870 0.02740276 NA 0.7843631 Marin Cilic 0.0285413602 0.053017465 0.0736687072 0.08883055 NA 0.7804876 Tomas Berdych 0.0471654691 0.047289449 0.0737401748 0.10584114 NA 0.7739211 Daniil Medvedev 0.0275430665 0.031121856 0.0721948279 0.01803757 NA 0.7543269 Stefanos Tsitsipas 0.0470382377 0.023825850 0.0577628626 0.02105227 NA 0.7511674 Dominic Thiem 0.0258904189 0.032481624 0.0483707080 0.05857158 NA 0.7451547 Alexander Zverev 0.0006199716 0.044811275 0.0380134371 0.08423392 NA 0.7374897 Kyle Edmund 0.0558006240 0.011963627 0.0478850676 0.05142186 NA 0.7304873 Pablo Carreno Busta 0.0321878318 0.029862068 0.0413674481 -0.00229784 NA 0.7302043 Borna Coric 0.0762084129 -0.010097922 0.0413621283 -0.01924267 NA 0.7268124 Kevin Anderson 0.0907358428 -0.027171681 0.0381421997 -0.06362578 NA 0.7260799 David Goffin -0.0034821911 0.037247336 0.0162572061 0.05603565 NA 0.7155908 Fernando Verdasco 0.0229261365 0.032884054 0.0521212576 0.04668854 NA 0.7120831 Roberto Bautista Agut 0.0047641170 0.049939608 0.0218975349 0.07331023 NA 0.7009891 Milos Raonic 0.0849726089 -0.028732182 0.0385944327 -0.08009382 NA 0.6986865 Fabio Fognini -0.0394792678 0.047935185 0.0226546894 0.06213496 NA 0.6982031 Hyeon Chung 0.0042489153 0.047722133 0.0158096386 0.04823304 NA 0.6958943 Jack Sock -0.0099659903 0.026454984 0.0186547428 0.02307214 NA 0.6757770 Diego Schwartzman -0.0317130675 0.032098381 0.0006215006 0.05621187 NA 0.6631067 John Millman 0.0016290285 0.042676556 0.0119857356 0.06228135 NA 0.6603912 Nikoloz Basilashvili -0.0099968609 0.005561102 0.0473876170 0.03661962 NA 0.6602628 John Isner 0.1346946527 -0.070556940 0.0161348609 -0.11425009 NA 0.6598097 Gael Monfils -0.0074254934 0.024286746 0.0295568649 0.04007519 NA 0.6449506 Richard Gasquet 0.0296009556 -0.011382437 0.0013138324 -0.03972967 NA 0.6442043 ... ... ... ... ... ... ... Laslo Djere -0.042300822 -0.0150684095 -0.064667709 -0.0349151578 NA 0.3606923 David Ferrer -0.036179509 0.0532782117 0.012751020 0.0914824480 NA 0.3488057 Bradley Klahn -0.001248083 -0.0444982448 -0.025987040 -0.1181295700 NA 0.3487806 Marcel Granollers -0.031011830 -0.0094056152 -0.049853664 0.0136841358 NA 0.3460035 Ricardas Berankis -0.022557215 -0.0103782963 -0.047937290 -0.0468488990 NA 0.3454980 Radu Albot -0.040829057 0.0076150564 -0.034891704 0.0443672533 NA 0.3420615 Jordan Thompson -0.068554906 0.0261969117 -0.044349181 0.0206636045 NA 0.3358572 Thomas Fabbiano -0.060583307 0.0275756029 -0.025883493 0.0709707306 NA 0.3319778 Roberto Carballes Baena -0.054016396 -0.0091521177 -0.019093050 0.0347187874 NA 0.3312105 Paolo Lorenzi -0.038613500 -0.0212206827 -0.052602703 0.0199474025 NA 0.3299791 Guido Andreozzi -0.038614385 -0.0133763922 0.029549861 0.0636745661 NA 0.3288762 Peter Polansky 0.007461636 -0.0163389196 -0.024034159 -0.0442144260 NA 0.3216756 Ernests Gulbis -0.062827089 -0.0134699552 -0.027633425 -0.0518663252 NA 0.3123511 Thiago Monteiro 0.001235931 -0.0288349103 -0.043831840 -0.0654744344 NA 0.3122069 Casper Ruud 0.016838968 -0.0178511679 0.015234507 0.0219131874 NA 0.3119321 Marco Trungelliti -0.022148774 -0.0005658242 0.048542554 0.1243537739 NA 0.3092636 Jiri Vesely -0.050204009 -0.0351868278 -0.042887646 -0.0160467165 NA 0.3089287 Guillermo Garcia-Lopez -0.090076100 -0.0108663630 -0.048712763 -0.0124446402 NA 0.3080898 Michael Mmoh -0.063802934 -0.0079053251 -0.011112236 -0.0332042032 NA 0.2822330 Jason Kubler -0.124758873 -0.0202756806 -0.013998570 0.1020895301 NA 0.2814246 Ruben Bemelmans -0.029036164 -0.0138846550 -0.032256254 -0.0363563402 NA 0.2772185 Bjorn Fratangelo -0.014149222 0.0033574304 -0.019931504 -0.0360199607 NA 0.2652527 Pablo Andujar -0.042869833 -0.0488261697 -0.070057834 -0.0164918910 NA 0.2647100 Christian Garin -0.046150875 0.0235799476 -0.006209664 0.0736304057 NA 0.2631607 Ivo Karlovic 0.071597162 -0.1093833837 0.001410787 -0.1237762218 NA 0.2500242 Juan Ignacio Londero -0.026454456 -0.0715665271 -0.016749898 -0.0363353678 NA 0.2351747 Ramkumar Ramanathan -0.005371622 -0.0606138479 -0.041631884 -0.0005573405 NA 0.2272977 Reilly Opelka 0.025704824 -0.0607219257 -0.015474944 -0.0720809006 NA 0.2262993 Carlos Berlocq -0.063580460 0.0074576369 -0.054277974 -0.0165235079 NA 0.2112275 Pedro Sousa -0.197333352 -0.0734557562 -0.161962722 -0.1023311674 NA 0.1502313","title":"Generating 2019 Australian Open Predictions"},{"location":"modelling/AusOpenRTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/EPLmlPython/","text":"EPL Machine Learning Walkthrough \u00b6 01. Data Acquisition & Exploration \u00b6 Welcome to the first part of this Machine Learning Walkthrough. This tutorial will be made of two parts; how we actually acquired our data (programmatically) and exploring the data to find potential features to use in the next tutorial . Data Acquisition \u00b6 We will be grabbing our data from football-data.co.uk , which has an enormous amount of soccer data dating back to the 90s. They also generously allow us to use it for free! However, the data is in separate CSVs based on the season. That means we would need to manually download 20 different files if we wanted the past 20 seasons. Rather than do this laborious and boring task, let's create a function which downloads the files for us, and appends them all into one big CSV. To do this, we will use BeautifulSoup, a Python library which helps to pull data from HTML and XML files. We will then define a function which collates all the data for us into one DataFrame. # Import Modules import pandas as pd import requests from bs4 import BeautifulSoup import datetime pd . set_option ( 'display.max_columns' , 100 ) import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline from data_preparation_functions import * def grab_epl_data (): # Connect to football-data.co.uk res = requests . get ( \"http://www.football-data.co.uk/englandm.php\" ) # Create a BeautifulSoup object soup = BeautifulSoup ( res . content , 'lxml' ) # Find the tables with the links to the data in them. table = soup . find_all ( 'table' , { 'align' : 'center' , 'cellspacing' : '0' , 'width' : '800' })[ 1 ] body = table . find_all ( 'td' , { 'valign' : 'top' })[ 1 ] # Grab the urls for the csv files links = [ link . get ( 'href' ) for link in body . find_all ( 'a' )] links_text = [ link_text . text for link_text in body . find_all ( 'a' )] data_urls = [] # Create a list of links prefix = 'http://www.football-data.co.uk/' for i , text in enumerate ( links_text ): if text == 'Premier League' : data_urls . append ( prefix + links [ i ]) # Get rid of last 11 uls as these don't include match stats and odds, and we # only want from 2005 onwards data_urls = data_urls [: - 12 ] df = pd . DataFrame () # Iterate over the urls for url in data_urls : # Get the season and make it a column season = url . split ( '/' )[ 4 ] print ( f \"Getting data for season { season } \" ) # Read the data from the url into a DataFrame temp_df = pd . read_csv ( url ) temp_df [ 'season' ] = season # Create helpful columns like Day, Month, Year, Date etc. so that our data is clean temp_df = ( temp_df . dropna ( axis = 'columns' , thresh = temp_df . shape [ 0 ] - 30 ) . assign ( Day = lambda df : df . Date . str . split ( '/' ) . str [ 0 ], Month = lambda df : df . Date . str . split ( '/' ) . str [ 1 ], Year = lambda df : df . Date . str . split ( '/' ) . str [ 2 ]) . assign ( Date = lambda df : df . Month + '/' + df . Day + '/' + df . Year ) . assign ( Date = lambda df : pd . to_datetime ( df . Date )) . dropna ()) # Append the temp_df to the main df df = df . append ( temp_df , sort = True ) # Drop all NAs df = df . dropna ( axis = 1 ) . dropna () . sort_values ( by = 'Date' ) print ( \"Finished grabbing data.\" ) return df df = grab_epl_data () # df.to_csv(\"data/epl_data.csv\", index=False) Getting data for season 1819 Getting data for season 1718 Getting data for season 1617 Getting data for season 1516 Getting data for season 1415 Getting data for season 1314 Getting data for season 1213 Getting data for season 1112 Getting data for season 1011 Getting data for season 0910 Getting data for season 0809 Getting data for season 0708 Getting data for season 0607 Getting data for season 0506 Finished grabbing data . Whenever we want to update our data (for example if we want the most recent Gameweek included), all we have to do is run that function and then save the data to a csv with the commented out line above. Data Exploration \u00b6 Now that we have our data, let's explore it. Let's first look at home team win rates since 2005 to see if there is a consistent trend. To get an idea of what our data looks like, we'll look at the tail of the dataset first. df . tail ( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season 28 3.0 11.0 0.0 9.0 3.0 2.0 Crystal Palace 3.00 3.25 2.60 2.95 3.1 2.55 42.0 20.0 -0.25 1.71 2.13 2.92 1.73 2.16 3.22 2.55 1.79 2.21 3.04 1.77 2.23 3.36 2.66 39.0 2018-08-26 26 E0 1.0 2.0 H 6.0 14.0 0.0 13.0 5.0 0.0 0.0 D 4.0 Watford 2.95 3.20 2.5 2.90 3.1 2.50 08 A Taylor 2.90 3.3 2.6 18 1819 27 5.0 8.0 0.0 15.0 3.0 1.0 Chelsea 1.66 4.00 5.75 1.67 3.8 5.25 42.0 22.0 1.00 1.92 1.88 1.67 2.18 1.71 3.90 5.25 2.01 1.95 1.71 2.28 1.76 4.17 5.75 40.0 2018-08-26 26 E0 2.0 1.0 A 4.0 16.0 0.0 6.0 2.0 0.0 0.0 D 3.0 Newcastle 1.70 3.75 5.0 1.67 3.8 5.25 08 P Tierney 1.67 4.0 5.5 18 1819 29 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.90 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.00 1.76 2.25 3.40 2.67 40.0 2018-08-27 27 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.6 2.75 3.2 2.55 08 C Pawson 2.90 3.3 2.6 18 1819 # Create Home Win, Draw Win and Away Win columns df = df . assign ( homeWin = lambda df : df . apply ( lambda row : 1 if row . FTHG > row . FTAG else 0 , axis = 'columns' ), draw = lambda df : df . apply ( lambda row : 1 if row . FTHG == row . FTAG else 0 , axis = 'columns' ), awayWin = lambda df : df . apply ( lambda row : 1 if row . FTHG < row . FTAG else 0 , axis = 'columns' )) Home Ground Advantage \u00b6 win_rates = \\ ( df . groupby ( 'season' ) . mean () . loc [:, [ 'homeWin' , 'draw' , 'awayWin' ]]) win_rates homeWin draw awayWin season 0506 0.505263 0.202632 0.292105 0607 0.477573 0.258575 0.263852 0708 0.463158 0.263158 0.273684 0809 0.453826 0.255937 0.290237 0910 0.507895 0.252632 0.239474 1011 0.471053 0.292105 0.236842 1112 0.450000 0.244737 0.305263 1213 0.433862 0.285714 0.280423 1314 0.472973 0.208108 0.318919 1415 0.453826 0.245383 0.300792 1516 0.414248 0.282322 0.303430 1617 0.492105 0.221053 0.286842 1718 0.455263 0.260526 0.284211 1819 0.466667 0.200000 0.333333 Findings \u00b6 As we can see, winrates across home team wins, draws and away team wins are very consistent. It seems that the home team wins around 46-47% of the time, the draw happens about 25% of the time, and the away team wins about 27% of the time. Let's plot this DataFrame so that we can see the trend more easily. # Set the style plt . style . use ( 'ggplot' ) fig = plt . figure () ax = fig . add_subplot ( 111 ) home_line = ax . plot ( win_rates . homeWin , label = 'Home Win Rate' ) away_line = ax . plot ( win_rates . awayWin , label = 'Away Win Rate' ) draw_line = ax . plot ( win_rates . draw , label = 'Draw Win Rate' ) ax . set_xlabel ( \"season\" ) ax . set_ylabel ( \"Win Rate\" ) plt . title ( \"Win Rates\" , fontsize = 16 ) # Add the legend locations home_legend = plt . legend ( handles = home_line , loc = 'upper right' , bbox_to_anchor = ( 1 , 1 )) ax = plt . gca () . add_artist ( home_legend ) away_legend = plt . legend ( handles = away_line , loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.4 )) ax = plt . gca () . add_artist ( away_legend ) draw_legend = plt . legend ( handles = draw_line , loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.06 )) As we can see, the winrates are relatively stable each season, except for in 14/15 when the home win rate drops dramatically. Out of interest, let's also have a look at which team has the best home ground advantage. Let's define HGA as home win rate - away win rate. And then plot some of the big clubs' HGA against each other. home_win_rates = \\ ( df . groupby ([ 'HomeTeam' ]) . homeWin . mean ()) away_win_rates = \\ ( df . groupby ([ 'AwayTeam' ]) . awayWin . mean ()) hga = ( home_win_rates - away_win_rates ) . reset_index () . rename ( columns = { 0 : 'HGA' }) . sort_values ( by = 'HGA' , ascending = False ) hga . head ( 10 ) HomeTeam HGA 15 Fulham 0.315573 7 Brighton 0.304762 20 Man City 0.244980 14 Everton 0.241935 30 Stoke 0.241131 10 Charlton 0.236842 0 Arsenal 0.236140 27 Reading 0.234962 33 Tottenham 0.220207 21 Man United 0.215620 So the club with the best HGA is Fulham - interesting. This is most likely because Fulham have won 100% of home games in 2018 so far which is skewing the mean. Let's see how the HGA for some of the big clubs based compare over seasons. big_clubs = [ 'Liverpool' , 'Man City' , 'Man United' , 'Chelsea' , 'Arsenal' ] home_win_rates_5 = df [ df . HomeTeam . isin ( big_clubs )] . groupby ([ 'HomeTeam' , 'season' ]) . homeWin . mean () away_win_rates_5 = df [ df . AwayTeam . isin ( big_clubs )] . groupby ([ 'AwayTeam' , 'season' ]) . awayWin . mean () hga_top_5 = home_win_rates_5 - away_win_rates_5 hga_top_5 . unstack ( level = 0 ) HomeTeam Arsenal Chelsea Liverpool Man City Man United season 0506 0.421053 0.368421 0.263158 0.263158 0.052632 0607 0.263158 0.000000 0.421053 -0.052632 0.105263 0708 0.210526 -0.052632 0.157895 0.368421 0.368421 0809 0.105263 -0.157895 -0.052632 0.578947 0.210526 0910 0.368421 0.368421 0.421053 0.315789 0.263158 1011 0.157895 0.368421 0.368421 0.263158 0.684211 1112 0.157895 0.315789 -0.105263 0.421053 0.105263 1213 0.052632 0.105263 0.105263 0.248538 0.201754 1314 0.143275 0.251462 0.307018 0.362573 -0.026316 1415 0.131579 0.210526 0.105263 0.210526 0.421053 1516 0.210526 -0.105263 0.000000 0.263158 0.263158 1617 0.263158 0.210526 0.105263 -0.052632 -0.105263 1718 0.578947 0.052632 0.157895 0.000000 0.263158 1819 0.500000 0.000000 0.000000 0.500000 0.500000 Now let's plot it. sns . lineplot ( x = 'season' , y = 'HGA' , hue = 'team' , data = hga_top_5 . reset_index () . rename ( columns = { 0 : 'HGA' , 'HomeTeam' : 'team' })) plt . legend ( loc = 'lower center' , ncol = 6 , bbox_to_anchor = ( 0.45 , - 0.2 )) plt . title ( \"HGA Among the top 5 clubs\" , fontsize = 14 ) plt . show () The results here seem to be quite erratic, although it seems that Arsenal consistently has a HGA above 0. Let's now look at the distributions of each of our columns. The odds columns are likely to be highly skewed, so we may have to account for this later. for col in df . select_dtypes ( 'number' ) . columns : sns . distplot ( df [ col ]) plt . title ( f \"Distribution for { col } \" ) plt . show () Exploring Referee Home Ground Bias \u00b6 What may be of interest is whether certain referees are correlated with the home team winning more often. Let's explore referee home ground bias for referees for the top 10 Referees based on games. print ( 'Overall Home Win Rate: {:.4} %' . format ( df . homeWin . mean () * 100 )) # Get the top 10 refs based on games top_10_refs = df . Referee . value_counts () . head ( 10 ) . index df [ df . Referee . isin ( top_10_refs )] . groupby ( 'Referee' ) . homeWin . mean () . sort_values ( ascending = False ) Overall Home Win Rate: 46.55% Referee L Mason 0.510373 C Foy 0.500000 M Clattenburg 0.480000 M Jones 0.475248 P Dowd 0.469880 M Atkinson 0.469565 M Oliver 0.466019 H Webb 0.456604 A Marriner 0.455516 M Dean 0.442049 Name: homeWin, dtype: float64 It seems that L Mason may be the most influenced by the home crowd. Whilst the overall home win rate is 46.5%, the home win rate when he is the Referee is 51%. However it should be noted that this doesn't mean that he causes the win through bias. It could just be that he referees the best clubs, so naturally their home win rate is high. Variable Correlation With Margin \u00b6 Let's now explore different variables' relationships with margin. First, we'll create a margin column, then we will pick a few different variables to look at the correlations amongst each other, using a correlation heatmap. df [ 'margin' ] = df [ 'FTHG' ] - df [ 'FTAG' ] stat_cols = [ 'AC' , 'AF' , 'AR' , 'AS' , 'AST' , 'AY' , 'HC' , 'HF' , 'HR' , 'HS' , 'HST' , 'HTR' , 'HY' , 'margin' ] stat_correlations = df [ stat_cols ] . corr () stat_correlations [ 'margin' ] . sort_values () AST - 0.345703 AS - 0.298665 HY - 0.153806 HR - 0.129393 AC - 0.073204 HF - 0.067469 AF 0.005474 AY 0.013746 HC 0.067433 AR 0.103528 HS 0.275847 HST 0.367591 margin 1.000000 Name : margin , dtype : float64 Unsurprisingly, Home Shots on Target correlate the most with Margin, and Away Reds is also high. What is surprising is that Home Yellows has quite a strong negative correlation with margin - this may be because players will play more aggresively when they are losing to try and get the lead back, and hence receive more yellow cards. Let's now look at the heatmap between variables. sns . heatmap ( stat_correlations , annot = True , annot_kws = { 'size' : 10 }) < matplotlib . axes . _subplots . AxesSubplot at 0x220a4227048 > Analysing Features \u00b6 What we are really interested in, is how our features (creating in the next tutorial), correlate with winning. We will skip ahead here and use a function to create our features for us, and then examine how the moving averages/different features correlate with winning. # Create a cleaned df of all of our data pre_features_df = create_df ( 'data/epl_data.csv' ) # Create our features features = create_feature_df ( pre_features_df ) Creating all games feature DataFrame C : \\ Users \\ wardj \\ Documents \\ Betfair Public Github \\ predictive - models \\ epl \\ data_preparation_functions . py : 419 : RuntimeWarning : invalid value encountered in double_scalars . pipe ( lambda df : ( df . eloAgainst * df [ goalsForOrAgainstCol ]) . sum () / df . eloAgainst . sum ())) Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . features = ( pre_features_df . assign ( margin = lambda df : df . FTHG - df . FTAG ) . loc [:, [ 'gameId' , 'margin' ]] . pipe ( pd . merge , features , on = [ 'gameId' ])) features . corr () . margin . sort_values ( ascending = False )[: 20 ] margin 1.000000 f_awayOdds 0.413893 f_totalMktH % 0.330420 f_defMktH % 0.325392 f_eloAgainstAway 0.317853 f_eloForHome 0.317853 f_midMktH % 0.316080 f_attMktH % 0.312262 f_sizeOfHandicapAway 0.301667 f_goalsForHome 0.298930 f_wtEloGoalsForHome 0.297157 f_shotsForHome 0.286239 f_cornersForHome 0.279917 f_gkMktH % 0.274732 f_homeWinPc38Away 0.271326 f_homeWinPc38Home 0.271326 f_wtEloGoalsAgainstAway 0.269663 f_goalsAgainstAway 0.258418 f_cornersAgainstAway 0.257148 f_drawOdds 0.256807 Name : margin , dtype : float64 As we can see away odds is most highly correlated to margin. This makes sense, as odds generally have most/all information included in the price. What is interesting is that elo seems to also be highly correlated, which is good news for our elo model that we made. Similarly, weighted goals and the the value of the defence relative to other teams ('defMktH%' etc.) is strongly correlated to margin. 02. Data Preparation & Feature Engineering \u00b6 Welcome to the second part of this Machine Learning Walkthrough. This tutorial will focus on data preparation and feature creation, before we dive into modelling in the next tutorial . Specifically, this tutorial will cover a few things: Data wrangling specifically for sport Feature creation - focussing on commonly used features in sports modelling, such as exponential moving averages Using functions to modularise the data preparation process Data Wrangling \u00b6 We will begin by utilising functions we have defined in our data_preparation_functions script to wrangle our data into a format that can be consumed by Machine Learning algorithms. A typical issue faced by aspect of modelling sport is the issue of Machine Learning algorithms requiring all features for the teams playing to be on the same row of a table, whereas when we actual calculate these features, we usually require the teams to be on separate rows as it makes it a lot easier to calculate typical features, such as expontentially weighted moving averages . We will explore this issue and show how we deal with issues like these. # Import libraries from data_preparation_functions import * from sklearn.metrics import log_loss from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold , cross_val_score import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) We have created some functions which prepare the data for you. For thoroughly commented explanation of how the functions work, read through the data_preparation_functions.py script along side this walkthrough. Essentially, each functions wrangles the data through a similar process. It first reads in the data from a csv file, then converts the columns to datatypes that we can work with, such as converting the Date column to a datetime data type. It then adds a Game ID column, so each game is easily identifiable and joined on. We then assign the DataFrame some other columns which may be useful, such as 'Year', 'Result' and 'homeWin'. Finally, we drop redundant column and return the DataFrame. Let us now create six different DataFrames, which we will use to create features. Later, we will join these features back into one main feature DataFrame. Create 6 distinct DataFrames \u00b6 # This table includes all of our data in one big DataFrame df = create_df ( 'data/epl_data.csv' ) df . head ( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.0 2.38 8 A Wiley 2.75 3.25 2.4 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.0 2.10 8 M Riley 3.10 3.25 2.2 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.2 3.75 8 G Poll 1.80 3.30 4.5 2005 0506 3 0 1 away # This includes only the typical soccer stats, like home corners, home shots on target etc. stats = create_stats_df ( 'data/epl_data.csv' ) stats . head ( 3 ) gameId HomeTeam AwayTeam FTHG FTAG HTHG HTAG HS AS HST AST HF AF HC AC HY AY HR AR 0 1 West Ham Blackburn 3.0 1.0 0.0 1.0 13.0 11.0 5.0 5.0 11.0 14.0 2.0 6.0 0.0 1.0 0.0 1.0 1 2 Aston Villa Bolton 2.0 2.0 2.0 2.0 3.0 13.0 2.0 6.0 14.0 16.0 7.0 8.0 0.0 2.0 0.0 0.0 2 3 Everton Man United 0.0 2.0 0.0 1.0 10.0 12.0 5.0 5.0 15.0 14.0 8.0 6.0 3.0 1.0 0.0 0.0 # This includes all of our betting related data, such as win/draw/lose odds, asian handicaps etc. betting = create_betting_df ( 'data/epl_data.csv' ) betting . head ( 3 ) B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Day Div IWA IWD IWH LBA LBD LBH Month VCA VCD VCH Year homeWin awayWin result HomeTeam AwayTeam gameId 0 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 13 E0 2.7 3.0 2.3 2.75 3.0 2.38 8 2.75 3.25 2.4 2005 1 0 home West Ham Blackburn 1 1 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 13 E0 3.1 3.0 2.1 3.20 3.0 2.10 8 3.10 3.25 2.2 2005 0 0 draw Aston Villa Bolton 2 2 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 13 E0 1.8 3.1 3.8 1.83 3.2 3.75 8 1.80 3.30 4.5 2005 0 1 away Everton Man United 3 # This includes all of the team information for each game. team_info = create_team_info_df ( 'data/epl_data.csv' ) team_info . head ( 3 ) gameId Date season HomeTeam AwayTeam FTR HTR Referee 0 1 2005-08-13 0506 West Ham Blackburn H A A Wiley 1 2 2005-08-13 0506 Aston Villa Bolton D D M Riley 2 3 2005-08-13 0506 Everton Man United A A G Poll # Whilst the other DataFrames date back to 2005, this DataFrame has data from 2001 to 2005. historic_games = create_historic_games_df ( 'data/historic_games_pre2005.csv' ) historic_games . head ( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin 0 2001-08-18 Charlton Everton 1 2 -1 20012002 0 1 2001-08-18 Derby Blackburn 2 1 -1 20012002 1 2 2001-08-18 Leeds Southampton 2 0 -1 20012002 1 # This is the historic_games DataFrame appended to the df DataFrame. all_games = create_all_games_df ( 'data/epl_data.csv' , 'data/historic_games_pre2005.csv' ) all_games . head ( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin awayWin homeWinPc5 homeWinPc38 awayWinPc5 awayWinPc38 gameIdHistoric 0 2001-08-18 Charlton Everton 1.0 2.0 -1 20012002 0 1 NaN NaN NaN NaN 1 1 2001-08-18 Derby Blackburn 2.0 1.0 -1 20012002 1 0 NaN NaN NaN NaN 2 2 2001-08-18 Leeds Southampton 2.0 0.0 -1 20012002 1 0 NaN NaN NaN NaN 3 Feature Creation \u00b6 Now that we have all of our pre-prepared DataFrames, and we know that the data is clean, we can move onto feature creation. As is common practice with sports modelling, we are going to start by creating expontentially weighted moving averages (EMA) as features. To get a better understanding of how EMAs work, read here . In short, an EMA is like a simple moving average, except it weights recent instances more than older instances based on an alpha parameter. The documentation for the pandas (emw method)[ https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html ] we will be using states that we can specify alpha in a number of ways. We will specify it in terms of span, where $\\alpha = 2 / (span+1), span \u2265 1 $. Let's first define a function which calculates the exponential moving average for each column in the stats DataFrame. We will then apply this function with other functions we have created, such as create_betting_features_ema, which creates moving averages of betting data. However, we must first change the structure of our data. Notice that currently each row has both the Home Team's data and the Away Team's data on a single row. This makes it difficult to calculate rolling averages, so we will restructure our DataFrames to ensure each row only contains single team's data. To do this, we will define a function, reate_multiline_df_stats. # Define a function which restructures our DataFrame def create_multiline_df_stats ( old_stats_df ): # Create a list of columns we want and their mappings to more interpretable names home_stats_cols = [ 'HomeTeam' , 'FTHG' , 'FTAG' , 'HTHG' , 'HTAG' , 'HS' , 'AS' , 'HST' , 'AST' , 'HF' , 'AF' , 'HC' , 'AC' , 'HY' , 'AY' , 'HR' , 'AR' ] away_stats_cols = [ 'AwayTeam' , 'FTAG' , 'FTHG' , 'HTAG' , 'HTHG' , 'AS' , 'HS' , 'AST' , 'HST' , 'AF' , 'HF' , 'AC' , 'HC' , 'AY' , 'HY' , 'AR' , 'HR' ] stats_cols_mapping = [ 'team' , 'goalsFor' , 'goalsAgainst' , 'halfTimeGoalsFor' , 'halfTimeGoalsAgainst' , 'shotsFor' , 'shotsAgainst' , 'shotsOnTargetFor' , 'shotsOnTargetAgainst' , 'freesFor' , 'freesAgainst' , 'cornersFor' , 'cornersAgainst' , 'yellowsFor' , 'yellowsAgainst' , 'redsFor' , 'redsAgainst' ] # Create a dictionary of the old column names to new column names home_mapping = { old_col : new_col for old_col , new_col in zip ( home_stats_cols , stats_cols_mapping )} away_mapping = { old_col : new_col for old_col , new_col in zip ( away_stats_cols , stats_cols_mapping )} # Put each team onto an individual row multi_line_stats = ( old_stats_df [[ 'gameId' ] + home_stats_cols ] # Filter for only the home team columns . rename ( columns = home_mapping ) # Rename the columns . assign ( homeGame = 1 ) # Assign homeGame=1 so that we can use a general function later . append (( old_stats_df [[ 'gameId' ] + away_stats_cols ]) # Append the away team columns . rename ( columns = away_mapping ) # Rename the away team columns . assign ( homeGame = 0 ), sort = True ) . sort_values ( by = 'gameId' ) # Sort the values . reset_index ( drop = True )) return multi_line_stats # Define a function which creates an EMA DataFrame from the stats DataFrame def create_stats_features_ema ( stats , span ): # Create a restructured DataFrames so that we can calculate EMA multi_line_stats = create_multiline_df_stats ( stats ) # Create a copy of the DataFrame ema_features = multi_line_stats [[ 'gameId' , 'team' , 'homeGame' ]] . copy () # Get the columns that we want to create EMA for feature_names = multi_line_stats . drop ( columns = [ 'gameId' , 'team' , 'homeGame' ]) . columns # Loop over the features for feature_name in feature_names : feature_ema = ( multi_line_stats . groupby ( 'team' )[ feature_name ] # Calculate the EMA . transform ( lambda row : row . ewm ( span = span , min_periods = 2 ) . mean () . shift ( 1 ))) # Shift the data down 1 so we don't leak data ema_features [ feature_name ] = feature_ema # Add the new feature to the DataFrame return ema_features # Apply the function stats_features = create_stats_features_ema ( stats , span = 5 ) stats_features . tail () gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9903 4952 Newcastle 1 4.301743 4.217300 11.789345 12.245066 0.797647 0.833658 0.644214 0.420832 2.323450e-10 3.333631e-01 11.335147 13.265955 3.211345 4.067990 1.848860 1.627140 9904 4953 Burnley 0 4.880132 5.165915 13.326703 8.800033 1.945502 0.667042 0.609440 0.529409 3.874405e-03 3.356120e-10 13.129631 10.642381 4.825874 3.970285 0.963527 0.847939 9905 4953 Fulham 1 4.550255 4.403060 10.188263 8.555589 2.531046 1.003553 0.860573 0.076949 1.002518e-04 8.670776e-03 17.463779 12.278877 8.334019 4.058213 0.980097 1.102974 9906 4954 Man United 1 3.832573 4.759683 11.640608 10.307946 1.397234 1.495032 1.034251 0.809280 6.683080e-05 1.320468e-05 8.963022 10.198642 3.216957 3.776900 1.040077 1.595650 9907 4954 Tottenham 0 3.042034 5.160211 8.991460 9.955635 1.332704 2.514789 0.573728 1.010491 4.522878e-08 1.354409e-05 12.543406 17.761004 3.757437 7.279845 1.478976 1.026601 As we can see, we now have averages for each team. Let's create a quick table to see the top 10 teams' goalsFor average EMAs since 2005. pd . DataFrame ( stats_features . groupby ( 'team' ) . goalsFor . mean () . sort_values ( ascending = False )[: 10 ]) goalsFor team Man United 1.895026 Chelsea 1.888892 Arsenal 1.876770 Man City 1.835863 Liverpool 1.771125 Tottenham 1.655063 Leicester 1.425309 Blackpool 1.390936 Everton 1.387110 Southampton 1.288349 Optimising Alpha \u00b6 It looks like Man United and Chelsea have been two of the best teams since 2005, based on goalsFor. Now that we have our stats features, we may be tempted to move on. However, we have arbitrarily chosen a span of 5. How do we know that this is the best value? We don't. Let's try and optimise this value. To do this, we will use a simple Logistic Regression model to create probabilistic predictions based on the stats features we created before. We will iterate a range of span values, from say, 3 to 15, and choose the value which produces a model with the lowest log loss, based on cross validation. To do this, we need to restructure our DataFrame back to how it was before. def restructure_stats_features ( stats_features ): non_features = [ 'homeGame' , 'team' , 'gameId' ] stats_features_restructured = ( stats_features . query ( 'homeGame == 1' ) . rename ( columns = { col : 'f_' + col + 'Home' for col in stats_features . columns if col not in non_features }) . rename ( columns = { 'team' : 'HomeTeam' }) . pipe ( pd . merge , ( stats_features . query ( 'homeGame == 0' ) . rename ( columns = { 'team' : 'AwayTeam' }) . rename ( columns = { col : 'f_' + col + 'Away' for col in stats_features . columns if col not in non_features })), on = [ 'gameId' ]) . pipe ( pd . merge , df [[ 'gameId' , 'result' ]], on = 'gameId' ) . dropna ()) return stats_features_restructured restructure_stats_features ( stats_features ) . head () gameId HomeTeam homeGame_x f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome AwayTeam homeGame_y f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway result 20 21 Birmingham 1 4.8 7.8 12.0 9.4 1.2 0.6 0.6 0.6 0.0 0.0 11.4 8.2 6.4 2.8 1.0 2.6 Middlesbrough 0 3.0 5.6 14.0 12.8 1.2 0.0 0.0 0.0 0.0 0.4 17.2 8.8 7.6 2.6 3.0 1.4 away 21 22 Portsmouth 1 2.6 4.6 21.8 16.6 2.0 0.6 1.0 0.0 0.0 0.0 8.0 10.4 3.6 4.0 3.2 1.8 Aston Villa 0 9.8 7.0 14.2 18.2 1.4 0.8 0.8 0.8 0.0 0.0 16.0 3.0 9.6 2.6 2.0 0.6 draw 22 23 Sunderland 1 5.0 5.0 11.6 18.0 1.8 0.4 1.0 0.4 0.4 0.6 14.6 6.0 5.2 3.2 1.2 2.6 Man City 0 7.8 3.6 8.6 12.4 0.6 1.2 0.6 0.6 0.0 0.0 10.6 11.4 2.4 6.8 3.0 1.4 away 23 24 Arsenal 1 3.0 7.4 17.0 18.6 0.6 0.8 0.0 0.0 0.4 0.0 6.2 11.4 4.0 6.6 1.6 1.8 Fulham 0 7.2 3.0 20.8 13.2 1.2 0.6 0.6 0.0 0.0 0.0 12.4 10.8 7.0 5.2 2.0 1.6 home 24 25 Blackburn 1 1.4 7.2 12.8 21.2 1.8 1.6 0.0 1.0 0.0 0.4 10.0 14.0 4.4 7.4 1.2 1.6 Tottenham 0 6.4 3.8 11.2 18.8 0.0 2.0 0.0 0.4 0.0 0.0 11.6 15.2 4.6 7.2 0.6 2.6 draw Now let's write a function that optimises our span based on log loss of the output of a Logistic Regression model. def optimise_alpha ( features ): le = LabelEncoder () y = le . fit_transform ( features . result ) # Encode the result from away, draw, home win to 0, 1, 2 X = features [[ col for col in features . columns if col . startswith ( 'f_' )]] # Only get the features - these all start with f_ lr = LogisticRegression () kfold = StratifiedKFold ( n_splits = 5 ) ave_cv_score = cross_val_score ( lr , X , y , scoring = 'neg_log_loss' , cv = kfold ) . mean () return ave_cv_score best_score = np . float ( 'inf' ) best_span = 0 cv_scores = [] # Iterate over a range of spans for span in range ( 1 , 120 , 3 ): stats_features = create_stats_features_ema ( stats , span = span ) restructured_stats_features = restructure_stats_features ( stats_features ) cv_score = optimise_alpha ( restructured_stats_features ) cv_scores . append ( cv_score ) if cv_score * - 1 < best_score : best_score = cv_score * - 1 best_span = span plt . style . use ( 'ggplot' ) plt . plot ( list ( range ( 1 , 120 , 3 )), ( pd . Series ( cv_scores ) *- 1 )) # Plot our results plt . title ( \"Optimising alpha\" ) plt . xlabel ( \"Span\" ) plt . ylabel ( \"Log Loss\" ) plt . show () print ( \"Our lowest log loss ( {:2f} ) occurred at a span of {} \" . format ( best_score , best_span )) Our lowest log loss (0.980835) occurred at a span of 55 The above method is just an example of how you can optimise hyparameters. Obviously this example has many limitations, such as attempting to optimise each statistic with the same alpha. However, for the rest of these tutorial series we will use this span value. Now let's create the rest of our features. For thorough explanations and the actual code behind some of the functions used, please refer to the data_preparation_functions.py script. Creating our Features DataFrame \u00b6 We will utilise pre-made functions to create all of our features in just a few lines of code. As part of this process we will create features which include margin weighted elo, an exponential average for asian handicap data, and odds as features. Our Elo function is essentially the same as the one we created in the AFL tutorial; if you would like to know more about Elo models please read this article. Note that the cell below may take a few minutes to run. # Create feature DataFrames features_all_games = create_all_games_features ( all_games ) C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive-models\\epl\\data_preparation_functions.py:419: RuntimeWarning: invalid value encountered in double_scalars .pipe(lambda df: (df.eloAgainst * df[goalsForOrAgainstCol]).sum() / df.eloAgainst.sum())) The features_all_games df includes elo for each team, as well as their win percentage at home and away over the past 5 and 38 games. For more information on how it was calculated, read through the data_preparation_functions script. features_all_games . head ( 3 ) Date awayWin awayWinPc38 awayWinPc5 eloAgainst eloFor gameId gameIdHistoric goalsAgainst goalsFor homeGame homeWin homeWinPc38 homeWinPc5 season team wtEloGoalsFor wtEloGoalsAgainst 0 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 2.0 1.0 1 0 NaN NaN 20012002 Charlton NaN NaN 1 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 1.0 2.0 0 0 NaN NaN 20012002 Everton NaN NaN 2 2001-08-18 0 NaN NaN 1500.0 1500.0 -1 2 1.0 2.0 1 1 NaN NaN 20012002 Derby NaN NaN The features_stats df includes all the expontential weighted averages for each stat in the stats df. # Create feature stats df features_stats = create_stats_features_ema ( stats , span = best_span ) features_stats . tail ( 3 ) gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9905 4953 Fulham 1 6.006967 5.045733 10.228997 9.965651 2.147069 1.093550 0.630485 0.364246 0.032937 0.043696 16.510067 11.718122 7.184386 4.645762 1.310424 1.389716 9906 4954 Man United 1 4.463018 5.461075 11.605712 10.870367 0.843222 1.586308 0.427065 0.730650 0.042588 0.027488 10.865754 13.003121 3.562675 4.626450 1.740735 1.712785 9907 4954 Tottenham 0 3.868619 6.362901 10.784145 10.140388 0.954928 2.100166 0.439129 0.799968 0.024351 0.026211 9.947515 16.460598 3.370010 6.136120 1.925005 1.364268 The features_odds df includes a moving average of some of the odds data. # Create feature_odds df features_odds = create_betting_features_ema ( betting , span = 10 ) features_odds . tail ( 3 ) gameId team avAsianHandicapOddsAgainst avAsianHandicapOddsFor avgreaterthan2.5 avlessthan2.5 sizeOfHandicap 9905 4953 Fulham 1.884552 1.985978 1.756776 2.128261 0.502253 9906 4954 Man United 1.871586 2.031787 1.900655 1.963478 -0.942445 9907 4954 Tottenham 1.947833 1.919607 1.629089 2.383593 -1.235630 The features market values has market values and the % of total market for each position. These values are in millions. # Create feature market values df features_market_values = create_market_values_features ( df ) # This creates a df with one game per row features_market_values . head ( 3 ) gameId Year HomeTeam AwayTeam defMktValH attMktValH gkMktValH totalMktValH midMktValH defMktValA attMktValA gkMktValA totalMktValA midMktValA attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% 0 1 2005 West Ham Blackburn 16.90 18.50 6.40 46.40 4.60 27.25 13.00 3.25 70.70 27.20 2.252911 1.583126 0.588168 3.477861 2.486940 4.010007 4.524247 2.297469 1.913986 2.916354 1 2 2005 Aston Villa Bolton 27.63 31.85 7.60 105.83 38.75 9.60 24.55 8.50 72.40 29.75 3.878659 2.989673 4.954673 3.803910 4.065926 1.412700 5.372543 6.008766 4.365456 2.986478 2 3 2005 Everton Man United 44.35 31.38 8.55 109.78 25.50 82.63 114.60 9.25 288.48 82.00 3.821423 13.955867 3.260494 10.484727 6.526378 12.159517 6.044111 6.538951 4.528392 11.899714 all_games_cols = [ 'Date' , 'gameId' , 'team' , 'season' , 'homeGame' , 'homeWinPc38' , 'homeWinPc5' , 'awayWinPc38' , 'awayWinPc5' , 'eloFor' , 'eloAgainst' , 'wtEloGoalsFor' , 'wtEloGoalsAgainst' ] # Join the features together features_multi_line = ( features_all_games [ all_games_cols ] . pipe ( pd . merge , features_stats . drop ( columns = 'homeGame' ), on = [ 'gameId' , 'team' ]) . pipe ( pd . merge , features_odds , on = [ 'gameId' , 'team' ])) # Put each instance on an individual row features_with_na = put_features_on_one_line ( features_multi_line ) market_val_feature_names = [ 'attMktH%' , 'attMktA%' , 'midMktH%' , 'midMktA%' , 'defMktH%' , 'defMktA%' , 'gkMktH%' , 'gkMktA%' , 'totalMktH%' , 'totalMktA%' ] # Merge our team values dataframe to features and result from df features_with_na = ( features_with_na . pipe ( pd . merge , ( features_market_values [ market_val_feature_names + [ 'gameId' ]]) . rename ({ col : 'f_' + col for col in market_val_feature_names }), on = 'gameId' ) . pipe ( pd . merge , df [[ 'HomeTeam' , 'AwayTeam' , 'gameId' , 'result' , 'B365A' , 'B365D' , 'B365H' ]], on = [ 'HomeTeam' , 'AwayTeam' , 'gameId' ])) # Drop NAs from calculating the rolling averages - don't drop Win Pc 38 and Win Pc 5 columns features = features_with_na . dropna ( subset = features_with_na . drop ( columns = [ col for col in features_with_na . columns if 'WinPc' in col ]) . columns ) # Fill NAs for the Win Pc columns features = features . fillna ( features . mean ()) features . head ( 3 ) Date gameId HomeTeam season homeGame f_homeWinPc38Home f_homeWinPc5Home f_awayWinPc38Home f_awayWinPc5Home f_eloForHome f_eloAgainstHome f_wtEloGoalsForHome f_wtEloGoalsAgainstHome f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome f_avAsianHandicapOddsAgainstHome f_avAsianHandicapOddsForHome f_avgreaterthan2.5Home f_avlessthan2.5Home f_sizeOfHandicapHome AwayTeam f_homeWinPc38Away f_homeWinPc5Away f_awayWinPc38Away f_awayWinPc5Away f_eloForAway f_eloAgainstAway f_wtEloGoalsForAway f_wtEloGoalsAgainstAway f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway f_avAsianHandicapOddsAgainstAway f_avAsianHandicapOddsForAway f_avgreaterthan2.5Away f_avlessthan2.5Away f_sizeOfHandicapAway attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% result B365A B365D B365H 20 2005-08-23 21 Birmingham 0506 1 0.394737 0.4 0.263158 0.2 1478.687038 1492.866048 1.061763 1.260223 4.981818 7.527273 12.000000 9.945455 1.018182 0.509091 0.509091 0.509091 0.000000 0.000000 11.945455 8.018182 6.490909 2.981818 1.000000 2.509091 1.9090 1.9455 2.0510 1.6735 -0.1375 Middlesbrough 0.394737 0.4 0.263158 0.2 1492.866048 1478.687038 1.12994 1.279873 2.545455 5.509091 13.545455 13.436364 1.018182 0.000000 0.000000 0.000000 0.0 0.490909 17.018182 8.072727 7.509091 2.509091 3.0 1.490909 1.9395 1.9095 2.0035 1.7155 0.3875 5.132983 5.260851 3.341048 4.289788 3.502318 4.168935 2.332815 3.216457 3.934396 4.522205 away 2.75 3.2 2.50 21 2005-08-23 22 Portsmouth 0506 1 0.447368 0.4 0.263158 0.4 1405.968416 1489.229314 1.147101 1.503051 2.509091 4.963636 21.981818 16.054545 2.000000 0.509091 1.000000 0.000000 0.000000 0.000000 8.454545 10.490909 3.963636 4.454545 3.018182 1.527273 1.8965 1.9690 2.0040 1.7005 0.2500 Aston Villa 0.447368 0.4 0.263158 0.4 1489.229314 1405.968416 1.17516 1.263229 9.527273 7.000000 14.472727 17.563636 1.490909 0.981818 0.981818 0.981818 0.0 0.000000 15.545455 3.000000 9.054545 2.509091 2.0 0.509091 1.8565 1.9770 1.8505 1.8485 0.7125 3.738614 3.878659 4.494368 4.954673 2.884262 4.065926 3.746642 5.372543 3.743410 4.365456 draw 2.75 3.2 2.50 22 2005-08-23 23 Sunderland 0506 1 0.236842 0.0 0.236842 0.4 1277.888970 1552.291880 0.650176 1.543716 5.000000 5.000000 12.418182 17.545455 1.981818 0.490909 1.000000 0.490909 0.490909 0.509091 14.509091 6.909091 5.018182 3.927273 1.018182 2.509091 1.8520 1.9915 1.8535 1.8500 0.7125 Man City 0.236842 0.0 0.236842 0.4 1552.291880 1277.888970 1.28875 1.287367 7.527273 3.509091 8.963636 12.490909 0.509091 1.018182 0.509091 0.509091 0.0 0.000000 10.963636 11.945455 2.490909 6.981818 3.0 1.490909 1.8150 2.0395 2.0060 1.7095 -0.2000 0.706318 3.750792 1.476812 1.070209 2.634096 4.455890 0.777605 4.913050 1.499427 3.151477 away 2.50 3.2 2.75 We now have a features DataFrame ready, with all the feature columns beginning with the \"f_\". In the next section, we will walk through the modelling process to try and find the best type of model to use. 03. Model Building & Hyperparameter Tuning \u00b6 Welcome to the third part of this Machine Learning Walkthrough. This tutorial will focus on the model building process, including how to tune hyperparameters. In the [next tutorial], we will create weekly predictions based on the model we have created here. Specifically, this tutorial will cover a few things: Choosing which Machine Learning algorithm to use from a variety of choices Hyperparameter Tuning Overfitting/Underfitting Choosing an Algorithm \u00b6 The best way to decide on specific algorithm to use, is to try them all! To do this, we will define a function which we first used in our AFL Predictions tutorial. This will iterate over a number of algorithms and give us a good indication of which algorithms are suited for this dataset and exercise. Let's first use grab the features we created in the last tutorial. This may take a minute or two to run. ## Import libraries from data_preparation_functions import * import pandas as pd import numpy as np import matplotlib as plt import seaborn as sns import warnings from sklearn import linear_model , tree , discriminant_analysis , naive_bayes , ensemble , gaussian_process from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV from sklearn.metrics import log_loss , confusion_matrix warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.max_columns' , 100 ) features = create_feature_df () Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . To start our modelling process, we need to make a training set, a test set and a holdout set. As we are using cross validation, we will make our training set all of the seasons up until 2017/18, and we will use the 2017/18 season as the test set. feature_list = [ col for col in features . columns if col . startswith ( \"f_\" )] betting_features = [] le = LabelEncoder () # Initiate a label encoder to transform the labels 'away', 'draw', 'home' to 0, 1, 2 # Grab all seasons except for 17/18 to use CV with all_x = features . loc [ features . season != '1718' , [ 'gameId' ] + feature_list ] all_y = features . loc [ features . season != '1718' , 'result' ] all_y = le . fit_transform ( all_y ) # Create our training vector as the seasons except 16/17 and 17/18 train_x = features . loc [ ~ features . season . isin ([ '1617' , '1718' ]), [ 'gameId' ] + feature_list ] train_y = le . transform ( features . loc [ ~ features . season . isin ([ '1617' , '1718' ]), 'result' ]) # Create our holdout vectors as the 16/17 season holdout_x = features . loc [ features . season == '1617' , [ 'gameId' ] + feature_list ] holdout_y = le . transform ( features . loc [ features . season == '1617' , 'result' ]) # Create our test vectors as the 17/18 season test_x = features . loc [ features . season == '1718' , [ 'gameId' ] + feature_list ] test_y = le . transform ( features . loc [ features . season == '1718' , 'result' ]) # Create a list of standard classifiers classifiers = [ #GLM linear_model . LogisticRegressionCV (), #Navies Bayes naive_bayes . BernoulliNB (), naive_bayes . GaussianNB (), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis (), discriminant_analysis . QuadraticDiscriminantAnalysis (), #Ensemble Methods ensemble . AdaBoostClassifier (), ensemble . BaggingClassifier (), ensemble . ExtraTreesClassifier (), ensemble . GradientBoostingClassifier (), ensemble . RandomForestClassifier (), #Gaussian Processes gaussian_process . GaussianProcessClassifier (), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # xgb.XGBClassifier() ] def find_best_algorithms ( classifier_list , X , y ): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold ( n_splits = 5 ) # Grab the cross validation scores for each algorithm cv_results = [ cross_val_score ( classifier , X , y , scoring = \"neg_log_loss\" , cv = kfold ) for classifier in classifier_list ] cv_means = [ cv_result . mean () * - 1 for cv_result in cv_results ] cv_std = [ cv_result . std () for cv_result in cv_results ] algorithm_names = [ alg . __class__ . __name__ for alg in classifiers ] # Create a DataFrame of all the CV results cv_results = pd . DataFrame ({ \"Mean Log Loss\" : cv_means , \"Log Loss Std\" : cv_std , \"Algorithm\" : algorithm_names }) . sort_values ( by = 'Mean Log Loss' ) return cv_results algorithm_results = find_best_algorithms ( classifiers , all_x , all_y ) algorithm_results Mean Log Loss Log Loss Std Algorithm 0 0.966540 0.020347 LogisticRegressionCV 3 0.986679 0.015601 LinearDiscriminantAnalysis 1 1.015197 0.017466 BernoulliNB 10 1.098612 0.000000 GaussianProcessClassifier 5 1.101281 0.044383 AdaBoostClassifier 8 1.137778 0.153391 GradientBoostingClassifier 7 2.093981 0.284831 ExtraTreesClassifier 9 2.095088 0.130367 RandomForestClassifier 6 2.120571 0.503132 BaggingClassifier 4 4.065796 1.370119 QuadraticDiscriminantAnalysis 2 5.284171 0.826991 GaussianNB We can see that LogisticRegression seems to perform the best out of all the algorithms, and some algorithms have a very high log loss. This is most likely due to overfitting. It would definitely be useful to condense our features down to reduce the dimensionality of the dataset. Hyperparameter Tuning \u00b6 For now, however, we will use logistic regression. Let's first try and tune a logistic regression model with cross validation. To do this, we will use grid search . Grid search essentially tries out each combination of values and finds the model with the lowest error metric, which in our case is log loss. 'C' in logistic regression determines the amount of regularization. Lower values increase regularization. # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.01 , 0.05 , 0.2 , 1 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } kfold = StratifiedKFold ( n_splits = 5 ) gs = GridSearchCV ( LogisticRegression (), param_grid = lr_grid , cv = kfold , scoring = 'neg_log_loss' ) gs . fit ( all_x , all_y ) print ( \"Best log loss: {} \" . format ( gs . best_score_ *- 1 )) best_lr_params = gs . best_params_ Best log loss : 0.9669551970849734 Defining a Baseline \u00b6 We should also define a baseline, as we don't really know if our log loss is good or bad. Randomly assigning a \u2153 chance to each selection yields a log loss of log3 = 1.09. However, what we are really interested in, is how our model performs relative to the odds. So let's find the log loss of the odds. # Finding the log loss of the odds log_loss ( all_y , 1 / all_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) 0.9590114943474463 This is good news: our algorithm almost beats the bookies in terms of log loss. It would be great if we could beat this result. Analysing the Errors Made \u00b6 Now that we have a logistic regression model tuned, let's see what type of errors it made. To do this we will look at the confusion matrix produced when we predict our holdout set. lr = LogisticRegression ( ** best_lr_params ) # Instantiate the model lr . fit ( train_x , train_y ) # Fit our model lr_predict = lr . predict ( holdout_x ) # Predict the holdout values # Create a confusion matrix c_matrix = ( pd . DataFrame ( confusion_matrix ( holdout_y , lr_predict ), columns = le . classes_ , index = le . classes_ ) . rename_axis ( 'Actual' ) . rename_axis ( 'Predicted' , axis = 'columns' )) c_matrix Predicted away draw home Actual away 77 0 32 draw 26 3 55 home 33 7 147 As we can see, when we predicted 'away' as the result, we correctly predicted 79 / 109 results, a hit rate of 70.6%. However, when we look at our draw hit rate, we only predicted 6 / 84 correctly, meaning we only had a hit rate of around 8.3%. For a more in depth analysis of our predictions, please skip to the Analysing Predictions & Staking Strategies section of the tutorial. Before we move on, however, let's use our model to predict the 17/18 season and compare how we went with the odds. # Get test predictions test_lr = LogisticRegression ( ** best_lr_params ) test_lr . fit ( all_x , all_y ) test_predictions_probs = lr . predict_proba ( test_x ) test_predictions = lr . predict ( test_x ) test_ll = log_loss ( test_y , test_predictions_probs ) test_accuracy = ( test_predictions == test_y ) . mean () print ( \"Our predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.2f} \" . format ( test_ll , test_accuracy )) Our predictions for the 2017/18 season have a log loss of: 0.95767 and an accuracy of: 0.56 # Get accuracy and log loss based on the odds odds_ll = log_loss ( test_y , 1 / test_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) odds_predictions = test_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]] . apply ( lambda row : row . idxmin ()[ 2 : 6 ], axis = 1 ) . values odds_accuracy = ( odds_predictions == le . inverse_transform ( test_y )) . mean () print ( \"Odds predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.3f} \" . format ( odds_ll , odds_accuracy )) Odds predictions for the 2017/18 season have a log loss of: 0.94635 and an accuracy of: 0.545 Results \u00b6 There we have it! The odds predicted 54.5% of EPL games correctly in the 2017/18 season, whilst our model predicted 54% correctly. This is a decent result for the first iteration of our model. In future iterations, we could wait a certain number of matches each season and calculate EMAs for on those first n games. This may help the issue of players switching clubs and teams becoming relatively stronger/weaker compared to previous seasons. 04. Weekly Predictions \u00b6 Welcome to the third part of this Machine Learning Walkthrough. This tutorial will be a walk through of creating weekly EPL predictions from the basic logistic regression model we built in the previous tutorial. We will then analyse our predictions and create staking strategies in the next tutorial. Specifically, this tutorial will cover a few things: Obtaining Weekly Odds / Game Info Using Betfair's API Data Wrangling This Week's Game Info Into Our Feature Set Obtaining Weekly Odds / Game Info Using Betfair's API \u00b6 The first thing we need to do to create weekly predictions is get both the games being played this week, as well as match odds from Betfair to be used as features. To make this process easier, I have created a csv file with the fixture for the 2018/19 season. Let's load that now. ## Import libraries import pandas as pd from weekly_prediction_functions import * from data_preparation_functions import * from sklearn.metrics import log_loss , confusion_matrix import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.max_columns' , 100 ) fixture = ( pd . read_csv ( 'data/fixture.csv' ) . assign ( Date = lambda df : pd . to_datetime ( df . Date ))) fixture . head () Date Time (AEST) HomeTeam AwayTeam Venue TV Year round season 0 2018-08-11 5:00 AM Man United Leicester Old Trafford, Manchester Optus, Fox Sports (delay) 2018 1 1819 1 2018-08-11 9:30 PM Newcastle Tottenham St.James\u2019 Park, Newcastle Optus, SBS 2018 1 1819 2 2018-08-12 12:00 AM Bournemouth Cardiff Vitality Stadium, Bournemouth Optus 2018 1 1819 3 2018-08-12 12:00 AM Fulham Crystal Palace Craven Cottage, London Optus 2018 1 1819 4 2018-08-12 12:00 AM Huddersfield Chelsea John Smith\u2019s Stadium, Huddersfield Optus, Fox Sports (delay) 2018 1 1819 Now we are going to connect to the API and retrieve game level information for the next week. To do this, we will use an R script. If you are not familiar with R, don't worry, it is relatively simple to read through. For this, we will run the script weekly_game_info_puller.R. Go ahead and run that script now. Note that for this step, you will require a Betfair API App Key. If you don't have one, visit this page and follow the instructions . I will upload an updated weekly file, so you can follow along regardless of if you have an App Key or not. Let's load that file in now. game_info = create_game_info_df ( \"data/weekly_game_info.csv\" ) game_info . head ( 3 ) AwayTeam HomeTeam awaySelectionId drawSelectionId homeSelectionId draw marketId marketStartTime totalMatched eventId eventName homeOdds drawOdds awayOdds competitionId Date localMarketStartTime 0 Arsenal Cardiff 1096 58805 79343 The Draw 1.146897152 2018-09-02 12:30:00 30123.595116 28852020 Cardiff v Arsenal 7.00 4.3 1.62 10932509 2018-09-02 Sun September 2, 10:30PM 1 Bournemouth Chelsea 1141 58805 55190 The Draw 1.146875421 2018-09-01 14:00:00 30821.329656 28851426 Chelsea v Bournemouth 1.32 6.8 12.00 10932509 2018-09-01 Sun September 2, 12:00AM 2 Fulham Brighton 56764 58805 18567 The Draw 1.146875746 2018-09-01 14:00:00 16594.833096 28851429 Brighton v Fulham 2.36 3.5 3.50 10932509 2018-09-01 Sun September 2, 12:00AM Finally, we will use the API to grab the weekly odds. This R script is also provided, but I have also included the weekly odds csv for convenience. odds = ( pd . read_csv ( 'data/weekly_epl_odds.csv' ) . replace ({ 'Man Utd' : 'Man United' , 'C Palace' : 'Crystal Palace' })) odds . head ( 3 ) HomeTeam AwayTeam f_homeOdds f_drawOdds f_awayOdds 0 Leicester Liverpool 7.80 5.1 1.48 1 Brighton Fulham 2.36 3.5 3.50 2 Everton Huddersfield 1.54 4.4 8.20 Data Wrangling This Week's Game Info Into Our Feature Set \u00b6 Now we have the arduous task of wrangling all of this info into a feature set that we can use to predict this week's games. Luckily our functions we created earlier should work if we just append the non-features to our main dataframe. df = create_df ( 'data/epl_data.csv' ) df . head () AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.50 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.90 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.00 2.38 8 A Wiley 2.75 3.25 2.40 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.30 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.40 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.00 2.10 8 M Riley 3.10 3.25 2.20 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.00 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.80 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.20 3.75 8 G Poll 1.80 3.30 4.50 2005 0506 3 0 1 away 3 6.0 13.0 0.0 7.0 4.0 2.0 Birmingham 2.87 3.25 2.37 2.80 3.20 2.30 56.0 21.0 0.00 1.69 2.04 2.87 2.05 1.81 3.16 2.31 1.77 2.24 3.05 2.11 1.85 3.30 2.60 36.0 2005-08-13 13 E0 0.0 0.0 D 6.0 12.0 0.0 15.0 7.0 0.0 0.0 D 1.0 Fulham 2.9 3.0 2.2 2.88 3.00 2.25 8 R Styles 2.80 3.25 2.35 2005 0506 4 0 0 draw 4 6.0 11.0 0.0 13.0 3.0 3.0 West Brom 5.00 3.40 1.72 4.80 3.45 1.65 55.0 23.0 -0.75 1.77 1.94 4.79 1.76 2.10 3.38 1.69 1.90 2.10 5.60 1.83 2.19 3.63 1.80 36.0 2005-08-13 13 E0 0.0 0.0 D 3.0 13.0 0.0 15.0 8.0 0.0 0.0 D 2.0 Man City 4.2 3.2 1.7 4.50 3.25 1.67 8 C Foy 5.00 3.25 1.75 2005 0506 5 0 0 draw Now we need to specify which game week we would like to predict. We will then filter the fixture for this game week and append this info to the main DataFrame round_to_predict = int ( input ( \"Which game week would you like to predict? Please input next week's Game Week \\n \" )) Which game week would you like to predict? Please input next week's Game Week 4 future_predictions = ( fixture . loc [ fixture [ 'round' ] == round_to_predict , [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]] . pipe ( pd . merge , odds , on = [ 'HomeTeam' , 'AwayTeam' ]) . rename ( columns = { 'f_homeOdds' : 'B365H' , 'f_awayOdds' : 'B365A' , 'f_drawOdds' : 'B365D' }) . assign ( season = lambda df : df . season . astype ( str ))) df_including_future_games = ( pd . read_csv ( 'data/epl_data.csv' , dtype = { 'season' : str }) . assign ( Date = lambda df : pd . to_datetime ( df . Date )) . pipe ( lambda df : df . dropna ( thresh = len ( df ) - 2 , axis = 1 )) # Drop cols with NAs . dropna ( axis = 0 ) # Drop rows with NAs . sort_values ( 'Date' ) . append ( future_predictions , sort = True ) . reset_index ( drop = True ) . assign ( gameId = lambda df : list ( df . index + 1 ), Year = lambda df : df . Date . apply ( lambda row : row . year ), homeWin = lambda df : df . apply ( lambda row : 1 if row . FTHG > row . FTAG else 0 , axis = 1 ), awayWin = lambda df : df . apply ( lambda row : 1 if row . FTAG > row . FTHG else 0 , axis = 1 ), result = lambda df : df . apply ( lambda row : 'home' if row . FTHG > row . FTAG else ( 'draw' if row . FTHG == row . FTAG else 'away' ), axis = 1 ))) df_including_future_games . tail ( 12 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 4952 4.0 8.0 0.0 12.0 2.0 1.0 Burnley 4.33 3.40 2.00 4.0 3.3 2.00 39.0 20.0 -0.25 1.65 2.22 4.14 2.22 1.69 3.36 1.98 1.72 2.31 4.5 2.32 1.74 3.57 2.04 36.0 2018-08-26 26.0 E0 2.0 4.0 H 6.0 11.0 0.0 25.0 12.0 2.0 3.0 H 2.0 Fulham 4.10 3.35 1.97 3.90 3.2 2.00 8.0 D Coote 4.33 3.4 2.0 2018 1819 4953 1 0 home 4953 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.9 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.0 1.76 2.25 3.40 2.67 40.0 2018-08-27 27.0 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.60 2.75 3.2 2.55 8.0 C Pawson 2.90 3.3 2.6 2018 1819 4954 0 1 away 4954 NaN NaN NaN NaN NaN NaN Liverpool 1.48 5.10 7.80 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-01 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Leicester NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4955 0 0 away 4955 NaN NaN NaN NaN NaN NaN Fulham 3.50 3.50 2.36 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Brighton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4956 0 0 away 4956 NaN NaN NaN NaN NaN NaN Man United 1.70 3.90 6.60 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Burnley NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4957 0 0 away 4957 NaN NaN NaN NaN NaN NaN Bournemouth 12.00 6.80 1.32 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Chelsea NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4958 0 0 away 4958 NaN NaN NaN NaN NaN NaN Southampton 4.50 3.55 2.04 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Crystal Palace NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4959 0 0 away 4959 NaN NaN NaN NaN NaN NaN Huddersfield 8.20 4.40 1.54 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Everton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4960 0 0 away 4960 NaN NaN NaN NaN NaN NaN Wolves 2.98 3.50 2.62 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN West Ham NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4961 0 0 away 4961 NaN NaN NaN NaN NaN NaN Newcastle 32.00 12.50 1.12 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Man City NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4962 0 0 away 4962 NaN NaN NaN NaN NaN NaN Arsenal 1.62 4.30 7.00 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Cardiff NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4963 0 0 away 4963 NaN NaN NaN NaN NaN NaN Tottenham 1.68 4.30 5.90 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-03 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Watford NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4964 0 0 away As we can see, what we have done is appended the Game information to our main DataFrame. The rest of the info is left as NAs, but this will be filled when we created our rolling average features. This is a 'hacky' type of way to complete this task, but works well as we can use the same functions that we created in the previous tutorials on this DataFrame. We now need to add the odds from our odds DataFrame, then we can just run our create features functions as usual. Predicting Next Gameweek's Results \u00b6 Now that we have our feature DataFrame, all we need to do is split the feature DataFrame up into a training set and next week's games, then use the model we tuned in the last tutorial to create predictions! features = create_feature_df ( df = df_including_future_games ) Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . # Create a feature DataFrame for this week's games. production_df = pd . merge ( future_predictions , features , on = [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]) # Create a training DataFrame training_df = features [ ~ features . gameId . isin ( production_df . gameId )] feature_names = [ col for col in training_df if col . startswith ( 'f_' )] le = LabelEncoder () train_y = le . fit_transform ( training_df . result ) train_x = training_df [ feature_names ] lr = LogisticRegression ( C = 0.01 , solver = 'liblinear' ) lr . fit ( train_x , train_y ) predicted_probs = lr . predict_proba ( production_df [ feature_names ]) predicted_odds = 1 / predicted_probs # Assign the modelled odds to our predictions df predictions_df = ( production_df . loc [:, [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'B365H' , 'B365D' , 'B365A' ]] . assign ( homeModelledOdds = [ i [ 2 ] for i in predicted_odds ], drawModelledOdds = [ i [ 1 ] for i in predicted_odds ], awayModelledOdds = [ i [ 0 ] for i in predicted_odds ]) . rename ( columns = { 'B365H' : 'BetfairHomeOdds' , 'B365D' : 'BetfairDrawOdds' , 'B365A' : 'BetfairAwayOdds' })) predictions_df Date HomeTeam AwayTeam BetfairHomeOdds BetfairDrawOdds BetfairAwayOdds homeModelledOdds drawModelledOdds awayModelledOdds 0 2018-09-01 Leicester Liverpool 7.80 5.10 1.48 5.747661 5.249857 1.573478 1 2018-09-02 Brighton Fulham 2.36 3.50 3.50 2.183193 3.803120 3.584057 2 2018-09-02 Burnley Man United 6.60 3.90 1.70 5.282620 4.497194 1.699700 3 2018-09-02 Chelsea Bournemouth 1.32 6.80 12.00 1.308366 6.079068 14.047070 4 2018-09-02 Crystal Palace Southampton 2.04 3.55 4.50 2.202871 4.213695 3.239122 5 2018-09-02 Everton Huddersfield 1.54 4.40 8.20 1.641222 3.759249 8.020055 6 2018-09-02 West Ham Wolves 2.62 3.50 2.98 1.999816 4.000456 4.000279 7 2018-09-02 Man City Newcastle 1.12 12.50 32.00 1.043103 29.427939 136.231983 8 2018-09-02 Cardiff Arsenal 7.00 4.30 1.62 6.256929 4.893445 1.572767 9 2018-09-03 Watford Tottenham 5.90 4.30 1.68 5.643663 4.338926 1.688224 Above are the predictions for this Gameweek's matches. In the next tutorial we will explore the errors our model has made, and work on creating a profitable betting strategy. Disclaimer \u00b6 Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"EPL ML walkthrough in Python"},{"location":"modelling/EPLmlPython/#epl-machine-learning-walkthrough","text":"","title":"EPL Machine Learning Walkthrough"},{"location":"modelling/EPLmlPython/#01-data-acquisition-exploration","text":"Welcome to the first part of this Machine Learning Walkthrough. This tutorial will be made of two parts; how we actually acquired our data (programmatically) and exploring the data to find potential features to use in the next tutorial .","title":"01. Data Acquisition &amp; Exploration"},{"location":"modelling/EPLmlPython/#data-acquisition","text":"We will be grabbing our data from football-data.co.uk , which has an enormous amount of soccer data dating back to the 90s. They also generously allow us to use it for free! However, the data is in separate CSVs based on the season. That means we would need to manually download 20 different files if we wanted the past 20 seasons. Rather than do this laborious and boring task, let's create a function which downloads the files for us, and appends them all into one big CSV. To do this, we will use BeautifulSoup, a Python library which helps to pull data from HTML and XML files. We will then define a function which collates all the data for us into one DataFrame. # Import Modules import pandas as pd import requests from bs4 import BeautifulSoup import datetime pd . set_option ( 'display.max_columns' , 100 ) import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline from data_preparation_functions import * def grab_epl_data (): # Connect to football-data.co.uk res = requests . get ( \"http://www.football-data.co.uk/englandm.php\" ) # Create a BeautifulSoup object soup = BeautifulSoup ( res . content , 'lxml' ) # Find the tables with the links to the data in them. table = soup . find_all ( 'table' , { 'align' : 'center' , 'cellspacing' : '0' , 'width' : '800' })[ 1 ] body = table . find_all ( 'td' , { 'valign' : 'top' })[ 1 ] # Grab the urls for the csv files links = [ link . get ( 'href' ) for link in body . find_all ( 'a' )] links_text = [ link_text . text for link_text in body . find_all ( 'a' )] data_urls = [] # Create a list of links prefix = 'http://www.football-data.co.uk/' for i , text in enumerate ( links_text ): if text == 'Premier League' : data_urls . append ( prefix + links [ i ]) # Get rid of last 11 uls as these don't include match stats and odds, and we # only want from 2005 onwards data_urls = data_urls [: - 12 ] df = pd . DataFrame () # Iterate over the urls for url in data_urls : # Get the season and make it a column season = url . split ( '/' )[ 4 ] print ( f \"Getting data for season { season } \" ) # Read the data from the url into a DataFrame temp_df = pd . read_csv ( url ) temp_df [ 'season' ] = season # Create helpful columns like Day, Month, Year, Date etc. so that our data is clean temp_df = ( temp_df . dropna ( axis = 'columns' , thresh = temp_df . shape [ 0 ] - 30 ) . assign ( Day = lambda df : df . Date . str . split ( '/' ) . str [ 0 ], Month = lambda df : df . Date . str . split ( '/' ) . str [ 1 ], Year = lambda df : df . Date . str . split ( '/' ) . str [ 2 ]) . assign ( Date = lambda df : df . Month + '/' + df . Day + '/' + df . Year ) . assign ( Date = lambda df : pd . to_datetime ( df . Date )) . dropna ()) # Append the temp_df to the main df df = df . append ( temp_df , sort = True ) # Drop all NAs df = df . dropna ( axis = 1 ) . dropna () . sort_values ( by = 'Date' ) print ( \"Finished grabbing data.\" ) return df df = grab_epl_data () # df.to_csv(\"data/epl_data.csv\", index=False) Getting data for season 1819 Getting data for season 1718 Getting data for season 1617 Getting data for season 1516 Getting data for season 1415 Getting data for season 1314 Getting data for season 1213 Getting data for season 1112 Getting data for season 1011 Getting data for season 0910 Getting data for season 0809 Getting data for season 0708 Getting data for season 0607 Getting data for season 0506 Finished grabbing data . Whenever we want to update our data (for example if we want the most recent Gameweek included), all we have to do is run that function and then save the data to a csv with the commented out line above.","title":"Data Acquisition"},{"location":"modelling/EPLmlPython/#data-exploration","text":"Now that we have our data, let's explore it. Let's first look at home team win rates since 2005 to see if there is a consistent trend. To get an idea of what our data looks like, we'll look at the tail of the dataset first. df . tail ( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season 28 3.0 11.0 0.0 9.0 3.0 2.0 Crystal Palace 3.00 3.25 2.60 2.95 3.1 2.55 42.0 20.0 -0.25 1.71 2.13 2.92 1.73 2.16 3.22 2.55 1.79 2.21 3.04 1.77 2.23 3.36 2.66 39.0 2018-08-26 26 E0 1.0 2.0 H 6.0 14.0 0.0 13.0 5.0 0.0 0.0 D 4.0 Watford 2.95 3.20 2.5 2.90 3.1 2.50 08 A Taylor 2.90 3.3 2.6 18 1819 27 5.0 8.0 0.0 15.0 3.0 1.0 Chelsea 1.66 4.00 5.75 1.67 3.8 5.25 42.0 22.0 1.00 1.92 1.88 1.67 2.18 1.71 3.90 5.25 2.01 1.95 1.71 2.28 1.76 4.17 5.75 40.0 2018-08-26 26 E0 2.0 1.0 A 4.0 16.0 0.0 6.0 2.0 0.0 0.0 D 3.0 Newcastle 1.70 3.75 5.0 1.67 3.8 5.25 08 P Tierney 1.67 4.0 5.5 18 1819 29 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.90 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.00 1.76 2.25 3.40 2.67 40.0 2018-08-27 27 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.6 2.75 3.2 2.55 08 C Pawson 2.90 3.3 2.6 18 1819 # Create Home Win, Draw Win and Away Win columns df = df . assign ( homeWin = lambda df : df . apply ( lambda row : 1 if row . FTHG > row . FTAG else 0 , axis = 'columns' ), draw = lambda df : df . apply ( lambda row : 1 if row . FTHG == row . FTAG else 0 , axis = 'columns' ), awayWin = lambda df : df . apply ( lambda row : 1 if row . FTHG < row . FTAG else 0 , axis = 'columns' ))","title":"Data Exploration"},{"location":"modelling/EPLmlPython/#home-ground-advantage","text":"win_rates = \\ ( df . groupby ( 'season' ) . mean () . loc [:, [ 'homeWin' , 'draw' , 'awayWin' ]]) win_rates homeWin draw awayWin season 0506 0.505263 0.202632 0.292105 0607 0.477573 0.258575 0.263852 0708 0.463158 0.263158 0.273684 0809 0.453826 0.255937 0.290237 0910 0.507895 0.252632 0.239474 1011 0.471053 0.292105 0.236842 1112 0.450000 0.244737 0.305263 1213 0.433862 0.285714 0.280423 1314 0.472973 0.208108 0.318919 1415 0.453826 0.245383 0.300792 1516 0.414248 0.282322 0.303430 1617 0.492105 0.221053 0.286842 1718 0.455263 0.260526 0.284211 1819 0.466667 0.200000 0.333333","title":"Home Ground Advantage"},{"location":"modelling/EPLmlPython/#findings","text":"As we can see, winrates across home team wins, draws and away team wins are very consistent. It seems that the home team wins around 46-47% of the time, the draw happens about 25% of the time, and the away team wins about 27% of the time. Let's plot this DataFrame so that we can see the trend more easily. # Set the style plt . style . use ( 'ggplot' ) fig = plt . figure () ax = fig . add_subplot ( 111 ) home_line = ax . plot ( win_rates . homeWin , label = 'Home Win Rate' ) away_line = ax . plot ( win_rates . awayWin , label = 'Away Win Rate' ) draw_line = ax . plot ( win_rates . draw , label = 'Draw Win Rate' ) ax . set_xlabel ( \"season\" ) ax . set_ylabel ( \"Win Rate\" ) plt . title ( \"Win Rates\" , fontsize = 16 ) # Add the legend locations home_legend = plt . legend ( handles = home_line , loc = 'upper right' , bbox_to_anchor = ( 1 , 1 )) ax = plt . gca () . add_artist ( home_legend ) away_legend = plt . legend ( handles = away_line , loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.4 )) ax = plt . gca () . add_artist ( away_legend ) draw_legend = plt . legend ( handles = draw_line , loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.06 )) As we can see, the winrates are relatively stable each season, except for in 14/15 when the home win rate drops dramatically. Out of interest, let's also have a look at which team has the best home ground advantage. Let's define HGA as home win rate - away win rate. And then plot some of the big clubs' HGA against each other. home_win_rates = \\ ( df . groupby ([ 'HomeTeam' ]) . homeWin . mean ()) away_win_rates = \\ ( df . groupby ([ 'AwayTeam' ]) . awayWin . mean ()) hga = ( home_win_rates - away_win_rates ) . reset_index () . rename ( columns = { 0 : 'HGA' }) . sort_values ( by = 'HGA' , ascending = False ) hga . head ( 10 ) HomeTeam HGA 15 Fulham 0.315573 7 Brighton 0.304762 20 Man City 0.244980 14 Everton 0.241935 30 Stoke 0.241131 10 Charlton 0.236842 0 Arsenal 0.236140 27 Reading 0.234962 33 Tottenham 0.220207 21 Man United 0.215620 So the club with the best HGA is Fulham - interesting. This is most likely because Fulham have won 100% of home games in 2018 so far which is skewing the mean. Let's see how the HGA for some of the big clubs based compare over seasons. big_clubs = [ 'Liverpool' , 'Man City' , 'Man United' , 'Chelsea' , 'Arsenal' ] home_win_rates_5 = df [ df . HomeTeam . isin ( big_clubs )] . groupby ([ 'HomeTeam' , 'season' ]) . homeWin . mean () away_win_rates_5 = df [ df . AwayTeam . isin ( big_clubs )] . groupby ([ 'AwayTeam' , 'season' ]) . awayWin . mean () hga_top_5 = home_win_rates_5 - away_win_rates_5 hga_top_5 . unstack ( level = 0 ) HomeTeam Arsenal Chelsea Liverpool Man City Man United season 0506 0.421053 0.368421 0.263158 0.263158 0.052632 0607 0.263158 0.000000 0.421053 -0.052632 0.105263 0708 0.210526 -0.052632 0.157895 0.368421 0.368421 0809 0.105263 -0.157895 -0.052632 0.578947 0.210526 0910 0.368421 0.368421 0.421053 0.315789 0.263158 1011 0.157895 0.368421 0.368421 0.263158 0.684211 1112 0.157895 0.315789 -0.105263 0.421053 0.105263 1213 0.052632 0.105263 0.105263 0.248538 0.201754 1314 0.143275 0.251462 0.307018 0.362573 -0.026316 1415 0.131579 0.210526 0.105263 0.210526 0.421053 1516 0.210526 -0.105263 0.000000 0.263158 0.263158 1617 0.263158 0.210526 0.105263 -0.052632 -0.105263 1718 0.578947 0.052632 0.157895 0.000000 0.263158 1819 0.500000 0.000000 0.000000 0.500000 0.500000 Now let's plot it. sns . lineplot ( x = 'season' , y = 'HGA' , hue = 'team' , data = hga_top_5 . reset_index () . rename ( columns = { 0 : 'HGA' , 'HomeTeam' : 'team' })) plt . legend ( loc = 'lower center' , ncol = 6 , bbox_to_anchor = ( 0.45 , - 0.2 )) plt . title ( \"HGA Among the top 5 clubs\" , fontsize = 14 ) plt . show () The results here seem to be quite erratic, although it seems that Arsenal consistently has a HGA above 0. Let's now look at the distributions of each of our columns. The odds columns are likely to be highly skewed, so we may have to account for this later. for col in df . select_dtypes ( 'number' ) . columns : sns . distplot ( df [ col ]) plt . title ( f \"Distribution for { col } \" ) plt . show ()","title":"Findings"},{"location":"modelling/EPLmlPython/#exploring-referee-home-ground-bias","text":"What may be of interest is whether certain referees are correlated with the home team winning more often. Let's explore referee home ground bias for referees for the top 10 Referees based on games. print ( 'Overall Home Win Rate: {:.4} %' . format ( df . homeWin . mean () * 100 )) # Get the top 10 refs based on games top_10_refs = df . Referee . value_counts () . head ( 10 ) . index df [ df . Referee . isin ( top_10_refs )] . groupby ( 'Referee' ) . homeWin . mean () . sort_values ( ascending = False ) Overall Home Win Rate: 46.55% Referee L Mason 0.510373 C Foy 0.500000 M Clattenburg 0.480000 M Jones 0.475248 P Dowd 0.469880 M Atkinson 0.469565 M Oliver 0.466019 H Webb 0.456604 A Marriner 0.455516 M Dean 0.442049 Name: homeWin, dtype: float64 It seems that L Mason may be the most influenced by the home crowd. Whilst the overall home win rate is 46.5%, the home win rate when he is the Referee is 51%. However it should be noted that this doesn't mean that he causes the win through bias. It could just be that he referees the best clubs, so naturally their home win rate is high.","title":"Exploring Referee Home Ground Bias"},{"location":"modelling/EPLmlPython/#variable-correlation-with-margin","text":"Let's now explore different variables' relationships with margin. First, we'll create a margin column, then we will pick a few different variables to look at the correlations amongst each other, using a correlation heatmap. df [ 'margin' ] = df [ 'FTHG' ] - df [ 'FTAG' ] stat_cols = [ 'AC' , 'AF' , 'AR' , 'AS' , 'AST' , 'AY' , 'HC' , 'HF' , 'HR' , 'HS' , 'HST' , 'HTR' , 'HY' , 'margin' ] stat_correlations = df [ stat_cols ] . corr () stat_correlations [ 'margin' ] . sort_values () AST - 0.345703 AS - 0.298665 HY - 0.153806 HR - 0.129393 AC - 0.073204 HF - 0.067469 AF 0.005474 AY 0.013746 HC 0.067433 AR 0.103528 HS 0.275847 HST 0.367591 margin 1.000000 Name : margin , dtype : float64 Unsurprisingly, Home Shots on Target correlate the most with Margin, and Away Reds is also high. What is surprising is that Home Yellows has quite a strong negative correlation with margin - this may be because players will play more aggresively when they are losing to try and get the lead back, and hence receive more yellow cards. Let's now look at the heatmap between variables. sns . heatmap ( stat_correlations , annot = True , annot_kws = { 'size' : 10 }) < matplotlib . axes . _subplots . AxesSubplot at 0x220a4227048 >","title":"Variable Correlation With Margin"},{"location":"modelling/EPLmlPython/#analysing-features","text":"What we are really interested in, is how our features (creating in the next tutorial), correlate with winning. We will skip ahead here and use a function to create our features for us, and then examine how the moving averages/different features correlate with winning. # Create a cleaned df of all of our data pre_features_df = create_df ( 'data/epl_data.csv' ) # Create our features features = create_feature_df ( pre_features_df ) Creating all games feature DataFrame C : \\ Users \\ wardj \\ Documents \\ Betfair Public Github \\ predictive - models \\ epl \\ data_preparation_functions . py : 419 : RuntimeWarning : invalid value encountered in double_scalars . pipe ( lambda df : ( df . eloAgainst * df [ goalsForOrAgainstCol ]) . sum () / df . eloAgainst . sum ())) Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . features = ( pre_features_df . assign ( margin = lambda df : df . FTHG - df . FTAG ) . loc [:, [ 'gameId' , 'margin' ]] . pipe ( pd . merge , features , on = [ 'gameId' ])) features . corr () . margin . sort_values ( ascending = False )[: 20 ] margin 1.000000 f_awayOdds 0.413893 f_totalMktH % 0.330420 f_defMktH % 0.325392 f_eloAgainstAway 0.317853 f_eloForHome 0.317853 f_midMktH % 0.316080 f_attMktH % 0.312262 f_sizeOfHandicapAway 0.301667 f_goalsForHome 0.298930 f_wtEloGoalsForHome 0.297157 f_shotsForHome 0.286239 f_cornersForHome 0.279917 f_gkMktH % 0.274732 f_homeWinPc38Away 0.271326 f_homeWinPc38Home 0.271326 f_wtEloGoalsAgainstAway 0.269663 f_goalsAgainstAway 0.258418 f_cornersAgainstAway 0.257148 f_drawOdds 0.256807 Name : margin , dtype : float64 As we can see away odds is most highly correlated to margin. This makes sense, as odds generally have most/all information included in the price. What is interesting is that elo seems to also be highly correlated, which is good news for our elo model that we made. Similarly, weighted goals and the the value of the defence relative to other teams ('defMktH%' etc.) is strongly correlated to margin.","title":"Analysing Features"},{"location":"modelling/EPLmlPython/#02-data-preparation-feature-engineering","text":"Welcome to the second part of this Machine Learning Walkthrough. This tutorial will focus on data preparation and feature creation, before we dive into modelling in the next tutorial . Specifically, this tutorial will cover a few things: Data wrangling specifically for sport Feature creation - focussing on commonly used features in sports modelling, such as exponential moving averages Using functions to modularise the data preparation process","title":"02. Data Preparation &amp; Feature Engineering"},{"location":"modelling/EPLmlPython/#data-wrangling","text":"We will begin by utilising functions we have defined in our data_preparation_functions script to wrangle our data into a format that can be consumed by Machine Learning algorithms. A typical issue faced by aspect of modelling sport is the issue of Machine Learning algorithms requiring all features for the teams playing to be on the same row of a table, whereas when we actual calculate these features, we usually require the teams to be on separate rows as it makes it a lot easier to calculate typical features, such as expontentially weighted moving averages . We will explore this issue and show how we deal with issues like these. # Import libraries from data_preparation_functions import * from sklearn.metrics import log_loss from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold , cross_val_score import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) We have created some functions which prepare the data for you. For thoroughly commented explanation of how the functions work, read through the data_preparation_functions.py script along side this walkthrough. Essentially, each functions wrangles the data through a similar process. It first reads in the data from a csv file, then converts the columns to datatypes that we can work with, such as converting the Date column to a datetime data type. It then adds a Game ID column, so each game is easily identifiable and joined on. We then assign the DataFrame some other columns which may be useful, such as 'Year', 'Result' and 'homeWin'. Finally, we drop redundant column and return the DataFrame. Let us now create six different DataFrames, which we will use to create features. Later, we will join these features back into one main feature DataFrame.","title":"Data Wrangling"},{"location":"modelling/EPLmlPython/#create-6-distinct-dataframes","text":"# This table includes all of our data in one big DataFrame df = create_df ( 'data/epl_data.csv' ) df . head ( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.0 2.38 8 A Wiley 2.75 3.25 2.4 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.0 2.10 8 M Riley 3.10 3.25 2.2 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.2 3.75 8 G Poll 1.80 3.30 4.5 2005 0506 3 0 1 away # This includes only the typical soccer stats, like home corners, home shots on target etc. stats = create_stats_df ( 'data/epl_data.csv' ) stats . head ( 3 ) gameId HomeTeam AwayTeam FTHG FTAG HTHG HTAG HS AS HST AST HF AF HC AC HY AY HR AR 0 1 West Ham Blackburn 3.0 1.0 0.0 1.0 13.0 11.0 5.0 5.0 11.0 14.0 2.0 6.0 0.0 1.0 0.0 1.0 1 2 Aston Villa Bolton 2.0 2.0 2.0 2.0 3.0 13.0 2.0 6.0 14.0 16.0 7.0 8.0 0.0 2.0 0.0 0.0 2 3 Everton Man United 0.0 2.0 0.0 1.0 10.0 12.0 5.0 5.0 15.0 14.0 8.0 6.0 3.0 1.0 0.0 0.0 # This includes all of our betting related data, such as win/draw/lose odds, asian handicaps etc. betting = create_betting_df ( 'data/epl_data.csv' ) betting . head ( 3 ) B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Day Div IWA IWD IWH LBA LBD LBH Month VCA VCD VCH Year homeWin awayWin result HomeTeam AwayTeam gameId 0 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 13 E0 2.7 3.0 2.3 2.75 3.0 2.38 8 2.75 3.25 2.4 2005 1 0 home West Ham Blackburn 1 1 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 13 E0 3.1 3.0 2.1 3.20 3.0 2.10 8 3.10 3.25 2.2 2005 0 0 draw Aston Villa Bolton 2 2 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 13 E0 1.8 3.1 3.8 1.83 3.2 3.75 8 1.80 3.30 4.5 2005 0 1 away Everton Man United 3 # This includes all of the team information for each game. team_info = create_team_info_df ( 'data/epl_data.csv' ) team_info . head ( 3 ) gameId Date season HomeTeam AwayTeam FTR HTR Referee 0 1 2005-08-13 0506 West Ham Blackburn H A A Wiley 1 2 2005-08-13 0506 Aston Villa Bolton D D M Riley 2 3 2005-08-13 0506 Everton Man United A A G Poll # Whilst the other DataFrames date back to 2005, this DataFrame has data from 2001 to 2005. historic_games = create_historic_games_df ( 'data/historic_games_pre2005.csv' ) historic_games . head ( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin 0 2001-08-18 Charlton Everton 1 2 -1 20012002 0 1 2001-08-18 Derby Blackburn 2 1 -1 20012002 1 2 2001-08-18 Leeds Southampton 2 0 -1 20012002 1 # This is the historic_games DataFrame appended to the df DataFrame. all_games = create_all_games_df ( 'data/epl_data.csv' , 'data/historic_games_pre2005.csv' ) all_games . head ( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin awayWin homeWinPc5 homeWinPc38 awayWinPc5 awayWinPc38 gameIdHistoric 0 2001-08-18 Charlton Everton 1.0 2.0 -1 20012002 0 1 NaN NaN NaN NaN 1 1 2001-08-18 Derby Blackburn 2.0 1.0 -1 20012002 1 0 NaN NaN NaN NaN 2 2 2001-08-18 Leeds Southampton 2.0 0.0 -1 20012002 1 0 NaN NaN NaN NaN 3","title":"Create 6 distinct DataFrames"},{"location":"modelling/EPLmlPython/#feature-creation","text":"Now that we have all of our pre-prepared DataFrames, and we know that the data is clean, we can move onto feature creation. As is common practice with sports modelling, we are going to start by creating expontentially weighted moving averages (EMA) as features. To get a better understanding of how EMAs work, read here . In short, an EMA is like a simple moving average, except it weights recent instances more than older instances based on an alpha parameter. The documentation for the pandas (emw method)[ https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html ] we will be using states that we can specify alpha in a number of ways. We will specify it in terms of span, where $\\alpha = 2 / (span+1), span \u2265 1 $. Let's first define a function which calculates the exponential moving average for each column in the stats DataFrame. We will then apply this function with other functions we have created, such as create_betting_features_ema, which creates moving averages of betting data. However, we must first change the structure of our data. Notice that currently each row has both the Home Team's data and the Away Team's data on a single row. This makes it difficult to calculate rolling averages, so we will restructure our DataFrames to ensure each row only contains single team's data. To do this, we will define a function, reate_multiline_df_stats. # Define a function which restructures our DataFrame def create_multiline_df_stats ( old_stats_df ): # Create a list of columns we want and their mappings to more interpretable names home_stats_cols = [ 'HomeTeam' , 'FTHG' , 'FTAG' , 'HTHG' , 'HTAG' , 'HS' , 'AS' , 'HST' , 'AST' , 'HF' , 'AF' , 'HC' , 'AC' , 'HY' , 'AY' , 'HR' , 'AR' ] away_stats_cols = [ 'AwayTeam' , 'FTAG' , 'FTHG' , 'HTAG' , 'HTHG' , 'AS' , 'HS' , 'AST' , 'HST' , 'AF' , 'HF' , 'AC' , 'HC' , 'AY' , 'HY' , 'AR' , 'HR' ] stats_cols_mapping = [ 'team' , 'goalsFor' , 'goalsAgainst' , 'halfTimeGoalsFor' , 'halfTimeGoalsAgainst' , 'shotsFor' , 'shotsAgainst' , 'shotsOnTargetFor' , 'shotsOnTargetAgainst' , 'freesFor' , 'freesAgainst' , 'cornersFor' , 'cornersAgainst' , 'yellowsFor' , 'yellowsAgainst' , 'redsFor' , 'redsAgainst' ] # Create a dictionary of the old column names to new column names home_mapping = { old_col : new_col for old_col , new_col in zip ( home_stats_cols , stats_cols_mapping )} away_mapping = { old_col : new_col for old_col , new_col in zip ( away_stats_cols , stats_cols_mapping )} # Put each team onto an individual row multi_line_stats = ( old_stats_df [[ 'gameId' ] + home_stats_cols ] # Filter for only the home team columns . rename ( columns = home_mapping ) # Rename the columns . assign ( homeGame = 1 ) # Assign homeGame=1 so that we can use a general function later . append (( old_stats_df [[ 'gameId' ] + away_stats_cols ]) # Append the away team columns . rename ( columns = away_mapping ) # Rename the away team columns . assign ( homeGame = 0 ), sort = True ) . sort_values ( by = 'gameId' ) # Sort the values . reset_index ( drop = True )) return multi_line_stats # Define a function which creates an EMA DataFrame from the stats DataFrame def create_stats_features_ema ( stats , span ): # Create a restructured DataFrames so that we can calculate EMA multi_line_stats = create_multiline_df_stats ( stats ) # Create a copy of the DataFrame ema_features = multi_line_stats [[ 'gameId' , 'team' , 'homeGame' ]] . copy () # Get the columns that we want to create EMA for feature_names = multi_line_stats . drop ( columns = [ 'gameId' , 'team' , 'homeGame' ]) . columns # Loop over the features for feature_name in feature_names : feature_ema = ( multi_line_stats . groupby ( 'team' )[ feature_name ] # Calculate the EMA . transform ( lambda row : row . ewm ( span = span , min_periods = 2 ) . mean () . shift ( 1 ))) # Shift the data down 1 so we don't leak data ema_features [ feature_name ] = feature_ema # Add the new feature to the DataFrame return ema_features # Apply the function stats_features = create_stats_features_ema ( stats , span = 5 ) stats_features . tail () gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9903 4952 Newcastle 1 4.301743 4.217300 11.789345 12.245066 0.797647 0.833658 0.644214 0.420832 2.323450e-10 3.333631e-01 11.335147 13.265955 3.211345 4.067990 1.848860 1.627140 9904 4953 Burnley 0 4.880132 5.165915 13.326703 8.800033 1.945502 0.667042 0.609440 0.529409 3.874405e-03 3.356120e-10 13.129631 10.642381 4.825874 3.970285 0.963527 0.847939 9905 4953 Fulham 1 4.550255 4.403060 10.188263 8.555589 2.531046 1.003553 0.860573 0.076949 1.002518e-04 8.670776e-03 17.463779 12.278877 8.334019 4.058213 0.980097 1.102974 9906 4954 Man United 1 3.832573 4.759683 11.640608 10.307946 1.397234 1.495032 1.034251 0.809280 6.683080e-05 1.320468e-05 8.963022 10.198642 3.216957 3.776900 1.040077 1.595650 9907 4954 Tottenham 0 3.042034 5.160211 8.991460 9.955635 1.332704 2.514789 0.573728 1.010491 4.522878e-08 1.354409e-05 12.543406 17.761004 3.757437 7.279845 1.478976 1.026601 As we can see, we now have averages for each team. Let's create a quick table to see the top 10 teams' goalsFor average EMAs since 2005. pd . DataFrame ( stats_features . groupby ( 'team' ) . goalsFor . mean () . sort_values ( ascending = False )[: 10 ]) goalsFor team Man United 1.895026 Chelsea 1.888892 Arsenal 1.876770 Man City 1.835863 Liverpool 1.771125 Tottenham 1.655063 Leicester 1.425309 Blackpool 1.390936 Everton 1.387110 Southampton 1.288349","title":"Feature Creation"},{"location":"modelling/EPLmlPython/#optimising-alpha","text":"It looks like Man United and Chelsea have been two of the best teams since 2005, based on goalsFor. Now that we have our stats features, we may be tempted to move on. However, we have arbitrarily chosen a span of 5. How do we know that this is the best value? We don't. Let's try and optimise this value. To do this, we will use a simple Logistic Regression model to create probabilistic predictions based on the stats features we created before. We will iterate a range of span values, from say, 3 to 15, and choose the value which produces a model with the lowest log loss, based on cross validation. To do this, we need to restructure our DataFrame back to how it was before. def restructure_stats_features ( stats_features ): non_features = [ 'homeGame' , 'team' , 'gameId' ] stats_features_restructured = ( stats_features . query ( 'homeGame == 1' ) . rename ( columns = { col : 'f_' + col + 'Home' for col in stats_features . columns if col not in non_features }) . rename ( columns = { 'team' : 'HomeTeam' }) . pipe ( pd . merge , ( stats_features . query ( 'homeGame == 0' ) . rename ( columns = { 'team' : 'AwayTeam' }) . rename ( columns = { col : 'f_' + col + 'Away' for col in stats_features . columns if col not in non_features })), on = [ 'gameId' ]) . pipe ( pd . merge , df [[ 'gameId' , 'result' ]], on = 'gameId' ) . dropna ()) return stats_features_restructured restructure_stats_features ( stats_features ) . head () gameId HomeTeam homeGame_x f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome AwayTeam homeGame_y f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway result 20 21 Birmingham 1 4.8 7.8 12.0 9.4 1.2 0.6 0.6 0.6 0.0 0.0 11.4 8.2 6.4 2.8 1.0 2.6 Middlesbrough 0 3.0 5.6 14.0 12.8 1.2 0.0 0.0 0.0 0.0 0.4 17.2 8.8 7.6 2.6 3.0 1.4 away 21 22 Portsmouth 1 2.6 4.6 21.8 16.6 2.0 0.6 1.0 0.0 0.0 0.0 8.0 10.4 3.6 4.0 3.2 1.8 Aston Villa 0 9.8 7.0 14.2 18.2 1.4 0.8 0.8 0.8 0.0 0.0 16.0 3.0 9.6 2.6 2.0 0.6 draw 22 23 Sunderland 1 5.0 5.0 11.6 18.0 1.8 0.4 1.0 0.4 0.4 0.6 14.6 6.0 5.2 3.2 1.2 2.6 Man City 0 7.8 3.6 8.6 12.4 0.6 1.2 0.6 0.6 0.0 0.0 10.6 11.4 2.4 6.8 3.0 1.4 away 23 24 Arsenal 1 3.0 7.4 17.0 18.6 0.6 0.8 0.0 0.0 0.4 0.0 6.2 11.4 4.0 6.6 1.6 1.8 Fulham 0 7.2 3.0 20.8 13.2 1.2 0.6 0.6 0.0 0.0 0.0 12.4 10.8 7.0 5.2 2.0 1.6 home 24 25 Blackburn 1 1.4 7.2 12.8 21.2 1.8 1.6 0.0 1.0 0.0 0.4 10.0 14.0 4.4 7.4 1.2 1.6 Tottenham 0 6.4 3.8 11.2 18.8 0.0 2.0 0.0 0.4 0.0 0.0 11.6 15.2 4.6 7.2 0.6 2.6 draw Now let's write a function that optimises our span based on log loss of the output of a Logistic Regression model. def optimise_alpha ( features ): le = LabelEncoder () y = le . fit_transform ( features . result ) # Encode the result from away, draw, home win to 0, 1, 2 X = features [[ col for col in features . columns if col . startswith ( 'f_' )]] # Only get the features - these all start with f_ lr = LogisticRegression () kfold = StratifiedKFold ( n_splits = 5 ) ave_cv_score = cross_val_score ( lr , X , y , scoring = 'neg_log_loss' , cv = kfold ) . mean () return ave_cv_score best_score = np . float ( 'inf' ) best_span = 0 cv_scores = [] # Iterate over a range of spans for span in range ( 1 , 120 , 3 ): stats_features = create_stats_features_ema ( stats , span = span ) restructured_stats_features = restructure_stats_features ( stats_features ) cv_score = optimise_alpha ( restructured_stats_features ) cv_scores . append ( cv_score ) if cv_score * - 1 < best_score : best_score = cv_score * - 1 best_span = span plt . style . use ( 'ggplot' ) plt . plot ( list ( range ( 1 , 120 , 3 )), ( pd . Series ( cv_scores ) *- 1 )) # Plot our results plt . title ( \"Optimising alpha\" ) plt . xlabel ( \"Span\" ) plt . ylabel ( \"Log Loss\" ) plt . show () print ( \"Our lowest log loss ( {:2f} ) occurred at a span of {} \" . format ( best_score , best_span )) Our lowest log loss (0.980835) occurred at a span of 55 The above method is just an example of how you can optimise hyparameters. Obviously this example has many limitations, such as attempting to optimise each statistic with the same alpha. However, for the rest of these tutorial series we will use this span value. Now let's create the rest of our features. For thorough explanations and the actual code behind some of the functions used, please refer to the data_preparation_functions.py script.","title":"Optimising Alpha"},{"location":"modelling/EPLmlPython/#creating-our-features-dataframe","text":"We will utilise pre-made functions to create all of our features in just a few lines of code. As part of this process we will create features which include margin weighted elo, an exponential average for asian handicap data, and odds as features. Our Elo function is essentially the same as the one we created in the AFL tutorial; if you would like to know more about Elo models please read this article. Note that the cell below may take a few minutes to run. # Create feature DataFrames features_all_games = create_all_games_features ( all_games ) C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive-models\\epl\\data_preparation_functions.py:419: RuntimeWarning: invalid value encountered in double_scalars .pipe(lambda df: (df.eloAgainst * df[goalsForOrAgainstCol]).sum() / df.eloAgainst.sum())) The features_all_games df includes elo for each team, as well as their win percentage at home and away over the past 5 and 38 games. For more information on how it was calculated, read through the data_preparation_functions script. features_all_games . head ( 3 ) Date awayWin awayWinPc38 awayWinPc5 eloAgainst eloFor gameId gameIdHistoric goalsAgainst goalsFor homeGame homeWin homeWinPc38 homeWinPc5 season team wtEloGoalsFor wtEloGoalsAgainst 0 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 2.0 1.0 1 0 NaN NaN 20012002 Charlton NaN NaN 1 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 1.0 2.0 0 0 NaN NaN 20012002 Everton NaN NaN 2 2001-08-18 0 NaN NaN 1500.0 1500.0 -1 2 1.0 2.0 1 1 NaN NaN 20012002 Derby NaN NaN The features_stats df includes all the expontential weighted averages for each stat in the stats df. # Create feature stats df features_stats = create_stats_features_ema ( stats , span = best_span ) features_stats . tail ( 3 ) gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9905 4953 Fulham 1 6.006967 5.045733 10.228997 9.965651 2.147069 1.093550 0.630485 0.364246 0.032937 0.043696 16.510067 11.718122 7.184386 4.645762 1.310424 1.389716 9906 4954 Man United 1 4.463018 5.461075 11.605712 10.870367 0.843222 1.586308 0.427065 0.730650 0.042588 0.027488 10.865754 13.003121 3.562675 4.626450 1.740735 1.712785 9907 4954 Tottenham 0 3.868619 6.362901 10.784145 10.140388 0.954928 2.100166 0.439129 0.799968 0.024351 0.026211 9.947515 16.460598 3.370010 6.136120 1.925005 1.364268 The features_odds df includes a moving average of some of the odds data. # Create feature_odds df features_odds = create_betting_features_ema ( betting , span = 10 ) features_odds . tail ( 3 ) gameId team avAsianHandicapOddsAgainst avAsianHandicapOddsFor avgreaterthan2.5 avlessthan2.5 sizeOfHandicap 9905 4953 Fulham 1.884552 1.985978 1.756776 2.128261 0.502253 9906 4954 Man United 1.871586 2.031787 1.900655 1.963478 -0.942445 9907 4954 Tottenham 1.947833 1.919607 1.629089 2.383593 -1.235630 The features market values has market values and the % of total market for each position. These values are in millions. # Create feature market values df features_market_values = create_market_values_features ( df ) # This creates a df with one game per row features_market_values . head ( 3 ) gameId Year HomeTeam AwayTeam defMktValH attMktValH gkMktValH totalMktValH midMktValH defMktValA attMktValA gkMktValA totalMktValA midMktValA attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% 0 1 2005 West Ham Blackburn 16.90 18.50 6.40 46.40 4.60 27.25 13.00 3.25 70.70 27.20 2.252911 1.583126 0.588168 3.477861 2.486940 4.010007 4.524247 2.297469 1.913986 2.916354 1 2 2005 Aston Villa Bolton 27.63 31.85 7.60 105.83 38.75 9.60 24.55 8.50 72.40 29.75 3.878659 2.989673 4.954673 3.803910 4.065926 1.412700 5.372543 6.008766 4.365456 2.986478 2 3 2005 Everton Man United 44.35 31.38 8.55 109.78 25.50 82.63 114.60 9.25 288.48 82.00 3.821423 13.955867 3.260494 10.484727 6.526378 12.159517 6.044111 6.538951 4.528392 11.899714 all_games_cols = [ 'Date' , 'gameId' , 'team' , 'season' , 'homeGame' , 'homeWinPc38' , 'homeWinPc5' , 'awayWinPc38' , 'awayWinPc5' , 'eloFor' , 'eloAgainst' , 'wtEloGoalsFor' , 'wtEloGoalsAgainst' ] # Join the features together features_multi_line = ( features_all_games [ all_games_cols ] . pipe ( pd . merge , features_stats . drop ( columns = 'homeGame' ), on = [ 'gameId' , 'team' ]) . pipe ( pd . merge , features_odds , on = [ 'gameId' , 'team' ])) # Put each instance on an individual row features_with_na = put_features_on_one_line ( features_multi_line ) market_val_feature_names = [ 'attMktH%' , 'attMktA%' , 'midMktH%' , 'midMktA%' , 'defMktH%' , 'defMktA%' , 'gkMktH%' , 'gkMktA%' , 'totalMktH%' , 'totalMktA%' ] # Merge our team values dataframe to features and result from df features_with_na = ( features_with_na . pipe ( pd . merge , ( features_market_values [ market_val_feature_names + [ 'gameId' ]]) . rename ({ col : 'f_' + col for col in market_val_feature_names }), on = 'gameId' ) . pipe ( pd . merge , df [[ 'HomeTeam' , 'AwayTeam' , 'gameId' , 'result' , 'B365A' , 'B365D' , 'B365H' ]], on = [ 'HomeTeam' , 'AwayTeam' , 'gameId' ])) # Drop NAs from calculating the rolling averages - don't drop Win Pc 38 and Win Pc 5 columns features = features_with_na . dropna ( subset = features_with_na . drop ( columns = [ col for col in features_with_na . columns if 'WinPc' in col ]) . columns ) # Fill NAs for the Win Pc columns features = features . fillna ( features . mean ()) features . head ( 3 ) Date gameId HomeTeam season homeGame f_homeWinPc38Home f_homeWinPc5Home f_awayWinPc38Home f_awayWinPc5Home f_eloForHome f_eloAgainstHome f_wtEloGoalsForHome f_wtEloGoalsAgainstHome f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome f_avAsianHandicapOddsAgainstHome f_avAsianHandicapOddsForHome f_avgreaterthan2.5Home f_avlessthan2.5Home f_sizeOfHandicapHome AwayTeam f_homeWinPc38Away f_homeWinPc5Away f_awayWinPc38Away f_awayWinPc5Away f_eloForAway f_eloAgainstAway f_wtEloGoalsForAway f_wtEloGoalsAgainstAway f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway f_avAsianHandicapOddsAgainstAway f_avAsianHandicapOddsForAway f_avgreaterthan2.5Away f_avlessthan2.5Away f_sizeOfHandicapAway attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% result B365A B365D B365H 20 2005-08-23 21 Birmingham 0506 1 0.394737 0.4 0.263158 0.2 1478.687038 1492.866048 1.061763 1.260223 4.981818 7.527273 12.000000 9.945455 1.018182 0.509091 0.509091 0.509091 0.000000 0.000000 11.945455 8.018182 6.490909 2.981818 1.000000 2.509091 1.9090 1.9455 2.0510 1.6735 -0.1375 Middlesbrough 0.394737 0.4 0.263158 0.2 1492.866048 1478.687038 1.12994 1.279873 2.545455 5.509091 13.545455 13.436364 1.018182 0.000000 0.000000 0.000000 0.0 0.490909 17.018182 8.072727 7.509091 2.509091 3.0 1.490909 1.9395 1.9095 2.0035 1.7155 0.3875 5.132983 5.260851 3.341048 4.289788 3.502318 4.168935 2.332815 3.216457 3.934396 4.522205 away 2.75 3.2 2.50 21 2005-08-23 22 Portsmouth 0506 1 0.447368 0.4 0.263158 0.4 1405.968416 1489.229314 1.147101 1.503051 2.509091 4.963636 21.981818 16.054545 2.000000 0.509091 1.000000 0.000000 0.000000 0.000000 8.454545 10.490909 3.963636 4.454545 3.018182 1.527273 1.8965 1.9690 2.0040 1.7005 0.2500 Aston Villa 0.447368 0.4 0.263158 0.4 1489.229314 1405.968416 1.17516 1.263229 9.527273 7.000000 14.472727 17.563636 1.490909 0.981818 0.981818 0.981818 0.0 0.000000 15.545455 3.000000 9.054545 2.509091 2.0 0.509091 1.8565 1.9770 1.8505 1.8485 0.7125 3.738614 3.878659 4.494368 4.954673 2.884262 4.065926 3.746642 5.372543 3.743410 4.365456 draw 2.75 3.2 2.50 22 2005-08-23 23 Sunderland 0506 1 0.236842 0.0 0.236842 0.4 1277.888970 1552.291880 0.650176 1.543716 5.000000 5.000000 12.418182 17.545455 1.981818 0.490909 1.000000 0.490909 0.490909 0.509091 14.509091 6.909091 5.018182 3.927273 1.018182 2.509091 1.8520 1.9915 1.8535 1.8500 0.7125 Man City 0.236842 0.0 0.236842 0.4 1552.291880 1277.888970 1.28875 1.287367 7.527273 3.509091 8.963636 12.490909 0.509091 1.018182 0.509091 0.509091 0.0 0.000000 10.963636 11.945455 2.490909 6.981818 3.0 1.490909 1.8150 2.0395 2.0060 1.7095 -0.2000 0.706318 3.750792 1.476812 1.070209 2.634096 4.455890 0.777605 4.913050 1.499427 3.151477 away 2.50 3.2 2.75 We now have a features DataFrame ready, with all the feature columns beginning with the \"f_\". In the next section, we will walk through the modelling process to try and find the best type of model to use.","title":"Creating our Features DataFrame"},{"location":"modelling/EPLmlPython/#03-model-building-hyperparameter-tuning","text":"Welcome to the third part of this Machine Learning Walkthrough. This tutorial will focus on the model building process, including how to tune hyperparameters. In the [next tutorial], we will create weekly predictions based on the model we have created here. Specifically, this tutorial will cover a few things: Choosing which Machine Learning algorithm to use from a variety of choices Hyperparameter Tuning Overfitting/Underfitting","title":"03. Model Building &amp; Hyperparameter Tuning"},{"location":"modelling/EPLmlPython/#choosing-an-algorithm","text":"The best way to decide on specific algorithm to use, is to try them all! To do this, we will define a function which we first used in our AFL Predictions tutorial. This will iterate over a number of algorithms and give us a good indication of which algorithms are suited for this dataset and exercise. Let's first use grab the features we created in the last tutorial. This may take a minute or two to run. ## Import libraries from data_preparation_functions import * import pandas as pd import numpy as np import matplotlib as plt import seaborn as sns import warnings from sklearn import linear_model , tree , discriminant_analysis , naive_bayes , ensemble , gaussian_process from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV from sklearn.metrics import log_loss , confusion_matrix warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.max_columns' , 100 ) features = create_feature_df () Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . To start our modelling process, we need to make a training set, a test set and a holdout set. As we are using cross validation, we will make our training set all of the seasons up until 2017/18, and we will use the 2017/18 season as the test set. feature_list = [ col for col in features . columns if col . startswith ( \"f_\" )] betting_features = [] le = LabelEncoder () # Initiate a label encoder to transform the labels 'away', 'draw', 'home' to 0, 1, 2 # Grab all seasons except for 17/18 to use CV with all_x = features . loc [ features . season != '1718' , [ 'gameId' ] + feature_list ] all_y = features . loc [ features . season != '1718' , 'result' ] all_y = le . fit_transform ( all_y ) # Create our training vector as the seasons except 16/17 and 17/18 train_x = features . loc [ ~ features . season . isin ([ '1617' , '1718' ]), [ 'gameId' ] + feature_list ] train_y = le . transform ( features . loc [ ~ features . season . isin ([ '1617' , '1718' ]), 'result' ]) # Create our holdout vectors as the 16/17 season holdout_x = features . loc [ features . season == '1617' , [ 'gameId' ] + feature_list ] holdout_y = le . transform ( features . loc [ features . season == '1617' , 'result' ]) # Create our test vectors as the 17/18 season test_x = features . loc [ features . season == '1718' , [ 'gameId' ] + feature_list ] test_y = le . transform ( features . loc [ features . season == '1718' , 'result' ]) # Create a list of standard classifiers classifiers = [ #GLM linear_model . LogisticRegressionCV (), #Navies Bayes naive_bayes . BernoulliNB (), naive_bayes . GaussianNB (), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis (), discriminant_analysis . QuadraticDiscriminantAnalysis (), #Ensemble Methods ensemble . AdaBoostClassifier (), ensemble . BaggingClassifier (), ensemble . ExtraTreesClassifier (), ensemble . GradientBoostingClassifier (), ensemble . RandomForestClassifier (), #Gaussian Processes gaussian_process . GaussianProcessClassifier (), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # xgb.XGBClassifier() ] def find_best_algorithms ( classifier_list , X , y ): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold ( n_splits = 5 ) # Grab the cross validation scores for each algorithm cv_results = [ cross_val_score ( classifier , X , y , scoring = \"neg_log_loss\" , cv = kfold ) for classifier in classifier_list ] cv_means = [ cv_result . mean () * - 1 for cv_result in cv_results ] cv_std = [ cv_result . std () for cv_result in cv_results ] algorithm_names = [ alg . __class__ . __name__ for alg in classifiers ] # Create a DataFrame of all the CV results cv_results = pd . DataFrame ({ \"Mean Log Loss\" : cv_means , \"Log Loss Std\" : cv_std , \"Algorithm\" : algorithm_names }) . sort_values ( by = 'Mean Log Loss' ) return cv_results algorithm_results = find_best_algorithms ( classifiers , all_x , all_y ) algorithm_results Mean Log Loss Log Loss Std Algorithm 0 0.966540 0.020347 LogisticRegressionCV 3 0.986679 0.015601 LinearDiscriminantAnalysis 1 1.015197 0.017466 BernoulliNB 10 1.098612 0.000000 GaussianProcessClassifier 5 1.101281 0.044383 AdaBoostClassifier 8 1.137778 0.153391 GradientBoostingClassifier 7 2.093981 0.284831 ExtraTreesClassifier 9 2.095088 0.130367 RandomForestClassifier 6 2.120571 0.503132 BaggingClassifier 4 4.065796 1.370119 QuadraticDiscriminantAnalysis 2 5.284171 0.826991 GaussianNB We can see that LogisticRegression seems to perform the best out of all the algorithms, and some algorithms have a very high log loss. This is most likely due to overfitting. It would definitely be useful to condense our features down to reduce the dimensionality of the dataset.","title":"Choosing an Algorithm"},{"location":"modelling/EPLmlPython/#hyperparameter-tuning","text":"For now, however, we will use logistic regression. Let's first try and tune a logistic regression model with cross validation. To do this, we will use grid search . Grid search essentially tries out each combination of values and finds the model with the lowest error metric, which in our case is log loss. 'C' in logistic regression determines the amount of regularization. Lower values increase regularization. # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.01 , 0.05 , 0.2 , 1 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } kfold = StratifiedKFold ( n_splits = 5 ) gs = GridSearchCV ( LogisticRegression (), param_grid = lr_grid , cv = kfold , scoring = 'neg_log_loss' ) gs . fit ( all_x , all_y ) print ( \"Best log loss: {} \" . format ( gs . best_score_ *- 1 )) best_lr_params = gs . best_params_ Best log loss : 0.9669551970849734","title":"Hyperparameter Tuning"},{"location":"modelling/EPLmlPython/#defining-a-baseline","text":"We should also define a baseline, as we don't really know if our log loss is good or bad. Randomly assigning a \u2153 chance to each selection yields a log loss of log3 = 1.09. However, what we are really interested in, is how our model performs relative to the odds. So let's find the log loss of the odds. # Finding the log loss of the odds log_loss ( all_y , 1 / all_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) 0.9590114943474463 This is good news: our algorithm almost beats the bookies in terms of log loss. It would be great if we could beat this result.","title":"Defining a Baseline"},{"location":"modelling/EPLmlPython/#analysing-the-errors-made","text":"Now that we have a logistic regression model tuned, let's see what type of errors it made. To do this we will look at the confusion matrix produced when we predict our holdout set. lr = LogisticRegression ( ** best_lr_params ) # Instantiate the model lr . fit ( train_x , train_y ) # Fit our model lr_predict = lr . predict ( holdout_x ) # Predict the holdout values # Create a confusion matrix c_matrix = ( pd . DataFrame ( confusion_matrix ( holdout_y , lr_predict ), columns = le . classes_ , index = le . classes_ ) . rename_axis ( 'Actual' ) . rename_axis ( 'Predicted' , axis = 'columns' )) c_matrix Predicted away draw home Actual away 77 0 32 draw 26 3 55 home 33 7 147 As we can see, when we predicted 'away' as the result, we correctly predicted 79 / 109 results, a hit rate of 70.6%. However, when we look at our draw hit rate, we only predicted 6 / 84 correctly, meaning we only had a hit rate of around 8.3%. For a more in depth analysis of our predictions, please skip to the Analysing Predictions & Staking Strategies section of the tutorial. Before we move on, however, let's use our model to predict the 17/18 season and compare how we went with the odds. # Get test predictions test_lr = LogisticRegression ( ** best_lr_params ) test_lr . fit ( all_x , all_y ) test_predictions_probs = lr . predict_proba ( test_x ) test_predictions = lr . predict ( test_x ) test_ll = log_loss ( test_y , test_predictions_probs ) test_accuracy = ( test_predictions == test_y ) . mean () print ( \"Our predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.2f} \" . format ( test_ll , test_accuracy )) Our predictions for the 2017/18 season have a log loss of: 0.95767 and an accuracy of: 0.56 # Get accuracy and log loss based on the odds odds_ll = log_loss ( test_y , 1 / test_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) odds_predictions = test_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]] . apply ( lambda row : row . idxmin ()[ 2 : 6 ], axis = 1 ) . values odds_accuracy = ( odds_predictions == le . inverse_transform ( test_y )) . mean () print ( \"Odds predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.3f} \" . format ( odds_ll , odds_accuracy )) Odds predictions for the 2017/18 season have a log loss of: 0.94635 and an accuracy of: 0.545","title":"Analysing the Errors Made"},{"location":"modelling/EPLmlPython/#results","text":"There we have it! The odds predicted 54.5% of EPL games correctly in the 2017/18 season, whilst our model predicted 54% correctly. This is a decent result for the first iteration of our model. In future iterations, we could wait a certain number of matches each season and calculate EMAs for on those first n games. This may help the issue of players switching clubs and teams becoming relatively stronger/weaker compared to previous seasons.","title":"Results"},{"location":"modelling/EPLmlPython/#04-weekly-predictions","text":"Welcome to the third part of this Machine Learning Walkthrough. This tutorial will be a walk through of creating weekly EPL predictions from the basic logistic regression model we built in the previous tutorial. We will then analyse our predictions and create staking strategies in the next tutorial. Specifically, this tutorial will cover a few things: Obtaining Weekly Odds / Game Info Using Betfair's API Data Wrangling This Week's Game Info Into Our Feature Set","title":"04. Weekly Predictions"},{"location":"modelling/EPLmlPython/#obtaining-weekly-odds-game-info-using-betfairs-api","text":"The first thing we need to do to create weekly predictions is get both the games being played this week, as well as match odds from Betfair to be used as features. To make this process easier, I have created a csv file with the fixture for the 2018/19 season. Let's load that now. ## Import libraries import pandas as pd from weekly_prediction_functions import * from data_preparation_functions import * from sklearn.metrics import log_loss , confusion_matrix import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.max_columns' , 100 ) fixture = ( pd . read_csv ( 'data/fixture.csv' ) . assign ( Date = lambda df : pd . to_datetime ( df . Date ))) fixture . head () Date Time (AEST) HomeTeam AwayTeam Venue TV Year round season 0 2018-08-11 5:00 AM Man United Leicester Old Trafford, Manchester Optus, Fox Sports (delay) 2018 1 1819 1 2018-08-11 9:30 PM Newcastle Tottenham St.James\u2019 Park, Newcastle Optus, SBS 2018 1 1819 2 2018-08-12 12:00 AM Bournemouth Cardiff Vitality Stadium, Bournemouth Optus 2018 1 1819 3 2018-08-12 12:00 AM Fulham Crystal Palace Craven Cottage, London Optus 2018 1 1819 4 2018-08-12 12:00 AM Huddersfield Chelsea John Smith\u2019s Stadium, Huddersfield Optus, Fox Sports (delay) 2018 1 1819 Now we are going to connect to the API and retrieve game level information for the next week. To do this, we will use an R script. If you are not familiar with R, don't worry, it is relatively simple to read through. For this, we will run the script weekly_game_info_puller.R. Go ahead and run that script now. Note that for this step, you will require a Betfair API App Key. If you don't have one, visit this page and follow the instructions . I will upload an updated weekly file, so you can follow along regardless of if you have an App Key or not. Let's load that file in now. game_info = create_game_info_df ( \"data/weekly_game_info.csv\" ) game_info . head ( 3 ) AwayTeam HomeTeam awaySelectionId drawSelectionId homeSelectionId draw marketId marketStartTime totalMatched eventId eventName homeOdds drawOdds awayOdds competitionId Date localMarketStartTime 0 Arsenal Cardiff 1096 58805 79343 The Draw 1.146897152 2018-09-02 12:30:00 30123.595116 28852020 Cardiff v Arsenal 7.00 4.3 1.62 10932509 2018-09-02 Sun September 2, 10:30PM 1 Bournemouth Chelsea 1141 58805 55190 The Draw 1.146875421 2018-09-01 14:00:00 30821.329656 28851426 Chelsea v Bournemouth 1.32 6.8 12.00 10932509 2018-09-01 Sun September 2, 12:00AM 2 Fulham Brighton 56764 58805 18567 The Draw 1.146875746 2018-09-01 14:00:00 16594.833096 28851429 Brighton v Fulham 2.36 3.5 3.50 10932509 2018-09-01 Sun September 2, 12:00AM Finally, we will use the API to grab the weekly odds. This R script is also provided, but I have also included the weekly odds csv for convenience. odds = ( pd . read_csv ( 'data/weekly_epl_odds.csv' ) . replace ({ 'Man Utd' : 'Man United' , 'C Palace' : 'Crystal Palace' })) odds . head ( 3 ) HomeTeam AwayTeam f_homeOdds f_drawOdds f_awayOdds 0 Leicester Liverpool 7.80 5.1 1.48 1 Brighton Fulham 2.36 3.5 3.50 2 Everton Huddersfield 1.54 4.4 8.20","title":"Obtaining Weekly Odds / Game Info Using Betfair's API"},{"location":"modelling/EPLmlPython/#data-wrangling-this-weeks-game-info-into-our-feature-set","text":"Now we have the arduous task of wrangling all of this info into a feature set that we can use to predict this week's games. Luckily our functions we created earlier should work if we just append the non-features to our main dataframe. df = create_df ( 'data/epl_data.csv' ) df . head () AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.50 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.90 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.00 2.38 8 A Wiley 2.75 3.25 2.40 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.30 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.40 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.00 2.10 8 M Riley 3.10 3.25 2.20 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.00 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.80 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.20 3.75 8 G Poll 1.80 3.30 4.50 2005 0506 3 0 1 away 3 6.0 13.0 0.0 7.0 4.0 2.0 Birmingham 2.87 3.25 2.37 2.80 3.20 2.30 56.0 21.0 0.00 1.69 2.04 2.87 2.05 1.81 3.16 2.31 1.77 2.24 3.05 2.11 1.85 3.30 2.60 36.0 2005-08-13 13 E0 0.0 0.0 D 6.0 12.0 0.0 15.0 7.0 0.0 0.0 D 1.0 Fulham 2.9 3.0 2.2 2.88 3.00 2.25 8 R Styles 2.80 3.25 2.35 2005 0506 4 0 0 draw 4 6.0 11.0 0.0 13.0 3.0 3.0 West Brom 5.00 3.40 1.72 4.80 3.45 1.65 55.0 23.0 -0.75 1.77 1.94 4.79 1.76 2.10 3.38 1.69 1.90 2.10 5.60 1.83 2.19 3.63 1.80 36.0 2005-08-13 13 E0 0.0 0.0 D 3.0 13.0 0.0 15.0 8.0 0.0 0.0 D 2.0 Man City 4.2 3.2 1.7 4.50 3.25 1.67 8 C Foy 5.00 3.25 1.75 2005 0506 5 0 0 draw Now we need to specify which game week we would like to predict. We will then filter the fixture for this game week and append this info to the main DataFrame round_to_predict = int ( input ( \"Which game week would you like to predict? Please input next week's Game Week \\n \" )) Which game week would you like to predict? Please input next week's Game Week 4 future_predictions = ( fixture . loc [ fixture [ 'round' ] == round_to_predict , [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]] . pipe ( pd . merge , odds , on = [ 'HomeTeam' , 'AwayTeam' ]) . rename ( columns = { 'f_homeOdds' : 'B365H' , 'f_awayOdds' : 'B365A' , 'f_drawOdds' : 'B365D' }) . assign ( season = lambda df : df . season . astype ( str ))) df_including_future_games = ( pd . read_csv ( 'data/epl_data.csv' , dtype = { 'season' : str }) . assign ( Date = lambda df : pd . to_datetime ( df . Date )) . pipe ( lambda df : df . dropna ( thresh = len ( df ) - 2 , axis = 1 )) # Drop cols with NAs . dropna ( axis = 0 ) # Drop rows with NAs . sort_values ( 'Date' ) . append ( future_predictions , sort = True ) . reset_index ( drop = True ) . assign ( gameId = lambda df : list ( df . index + 1 ), Year = lambda df : df . Date . apply ( lambda row : row . year ), homeWin = lambda df : df . apply ( lambda row : 1 if row . FTHG > row . FTAG else 0 , axis = 1 ), awayWin = lambda df : df . apply ( lambda row : 1 if row . FTAG > row . FTHG else 0 , axis = 1 ), result = lambda df : df . apply ( lambda row : 'home' if row . FTHG > row . FTAG else ( 'draw' if row . FTHG == row . FTAG else 'away' ), axis = 1 ))) df_including_future_games . tail ( 12 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 4952 4.0 8.0 0.0 12.0 2.0 1.0 Burnley 4.33 3.40 2.00 4.0 3.3 2.00 39.0 20.0 -0.25 1.65 2.22 4.14 2.22 1.69 3.36 1.98 1.72 2.31 4.5 2.32 1.74 3.57 2.04 36.0 2018-08-26 26.0 E0 2.0 4.0 H 6.0 11.0 0.0 25.0 12.0 2.0 3.0 H 2.0 Fulham 4.10 3.35 1.97 3.90 3.2 2.00 8.0 D Coote 4.33 3.4 2.0 2018 1819 4953 1 0 home 4953 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.9 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.0 1.76 2.25 3.40 2.67 40.0 2018-08-27 27.0 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.60 2.75 3.2 2.55 8.0 C Pawson 2.90 3.3 2.6 2018 1819 4954 0 1 away 4954 NaN NaN NaN NaN NaN NaN Liverpool 1.48 5.10 7.80 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-01 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Leicester NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4955 0 0 away 4955 NaN NaN NaN NaN NaN NaN Fulham 3.50 3.50 2.36 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Brighton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4956 0 0 away 4956 NaN NaN NaN NaN NaN NaN Man United 1.70 3.90 6.60 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Burnley NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4957 0 0 away 4957 NaN NaN NaN NaN NaN NaN Bournemouth 12.00 6.80 1.32 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Chelsea NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4958 0 0 away 4958 NaN NaN NaN NaN NaN NaN Southampton 4.50 3.55 2.04 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Crystal Palace NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4959 0 0 away 4959 NaN NaN NaN NaN NaN NaN Huddersfield 8.20 4.40 1.54 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Everton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4960 0 0 away 4960 NaN NaN NaN NaN NaN NaN Wolves 2.98 3.50 2.62 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN West Ham NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4961 0 0 away 4961 NaN NaN NaN NaN NaN NaN Newcastle 32.00 12.50 1.12 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Man City NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4962 0 0 away 4962 NaN NaN NaN NaN NaN NaN Arsenal 1.62 4.30 7.00 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Cardiff NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4963 0 0 away 4963 NaN NaN NaN NaN NaN NaN Tottenham 1.68 4.30 5.90 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-03 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Watford NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4964 0 0 away As we can see, what we have done is appended the Game information to our main DataFrame. The rest of the info is left as NAs, but this will be filled when we created our rolling average features. This is a 'hacky' type of way to complete this task, but works well as we can use the same functions that we created in the previous tutorials on this DataFrame. We now need to add the odds from our odds DataFrame, then we can just run our create features functions as usual.","title":"Data Wrangling This Week's Game Info Into Our Feature Set"},{"location":"modelling/EPLmlPython/#predicting-next-gameweeks-results","text":"Now that we have our feature DataFrame, all we need to do is split the feature DataFrame up into a training set and next week's games, then use the model we tuned in the last tutorial to create predictions! features = create_feature_df ( df = df_including_future_games ) Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . # Create a feature DataFrame for this week's games. production_df = pd . merge ( future_predictions , features , on = [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]) # Create a training DataFrame training_df = features [ ~ features . gameId . isin ( production_df . gameId )] feature_names = [ col for col in training_df if col . startswith ( 'f_' )] le = LabelEncoder () train_y = le . fit_transform ( training_df . result ) train_x = training_df [ feature_names ] lr = LogisticRegression ( C = 0.01 , solver = 'liblinear' ) lr . fit ( train_x , train_y ) predicted_probs = lr . predict_proba ( production_df [ feature_names ]) predicted_odds = 1 / predicted_probs # Assign the modelled odds to our predictions df predictions_df = ( production_df . loc [:, [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'B365H' , 'B365D' , 'B365A' ]] . assign ( homeModelledOdds = [ i [ 2 ] for i in predicted_odds ], drawModelledOdds = [ i [ 1 ] for i in predicted_odds ], awayModelledOdds = [ i [ 0 ] for i in predicted_odds ]) . rename ( columns = { 'B365H' : 'BetfairHomeOdds' , 'B365D' : 'BetfairDrawOdds' , 'B365A' : 'BetfairAwayOdds' })) predictions_df Date HomeTeam AwayTeam BetfairHomeOdds BetfairDrawOdds BetfairAwayOdds homeModelledOdds drawModelledOdds awayModelledOdds 0 2018-09-01 Leicester Liverpool 7.80 5.10 1.48 5.747661 5.249857 1.573478 1 2018-09-02 Brighton Fulham 2.36 3.50 3.50 2.183193 3.803120 3.584057 2 2018-09-02 Burnley Man United 6.60 3.90 1.70 5.282620 4.497194 1.699700 3 2018-09-02 Chelsea Bournemouth 1.32 6.80 12.00 1.308366 6.079068 14.047070 4 2018-09-02 Crystal Palace Southampton 2.04 3.55 4.50 2.202871 4.213695 3.239122 5 2018-09-02 Everton Huddersfield 1.54 4.40 8.20 1.641222 3.759249 8.020055 6 2018-09-02 West Ham Wolves 2.62 3.50 2.98 1.999816 4.000456 4.000279 7 2018-09-02 Man City Newcastle 1.12 12.50 32.00 1.043103 29.427939 136.231983 8 2018-09-02 Cardiff Arsenal 7.00 4.30 1.62 6.256929 4.893445 1.572767 9 2018-09-03 Watford Tottenham 5.90 4.30 1.68 5.643663 4.338926 1.688224 Above are the predictions for this Gameweek's matches. In the next tutorial we will explore the errors our model has made, and work on creating a profitable betting strategy.","title":"Predicting Next Gameweek's Results"},{"location":"modelling/EPLmlPython/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/brownlowModelTutorial/","text":"Modelling the Brownlow \u00b6 This walkthrough will provide a brief, yet effective tutorial on how to model the Brownlow medal. We will use data from 2010 to 2018, which includes Supercoach points and other useful stats. The output will be the number of votes predicted for each player in each match, and we will aggregate these to create aggregates for each team and for the whole competition. No doubt we'll have Mitchell right up the top, and if we don't, then we know we've done something wrong! # Import modules libraries import pandas as pd import h2o from h2o.automl import H2OAutoML import numpy as np from sklearn.preprocessing import StandardScaler import os import pickle # Change notebook settings pd . options . display . max_columns = None pd . options . display . max_rows = 300 from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" EDA - Read in the data \u00b6 I have collated this data using the fitzRoy R package and merging the afltables dataset with the footywire dataset, so that we can Supercoach and other advanced stats with Brownlow votes. Let's read in the data and have a sneak peak at what it looks like. brownlow_data = pd . read_csv ( 'data/afl_brownlow_data.csv' ) brownlow_data . tail ( 3 ) date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 76585 2018-08-26 2018 23 Etihad Stadium 12312 9708.0 M Wood 32 North Melbourne St Kilda Away 117 94 23 NaN 0 3 2 66.7 0 0 2 0 0 34 3 0 3 3 1 0 0 0 0 0 0 0 0 0 24 25 76586 2018-08-26 2018 23 Etihad Stadium 11755 9708.0 S Wright 19 North Melbourne St Kilda Away 117 94 23 NaN 10 17 22 75.9 0 0 0 0 1 83 16 13 29 8 0 0 2 0 3 0 4 4 1 0 107 96 76587 2018-08-26 2018 23 Etihad Stadium 11724 9708.0 J Ziebell 7 North Melbourne St Kilda Away 117 94 23 NaN 9 8 11 73.3 0 0 2 4 0 94 12 3 15 6 3 1 2 0 1 0 0 0 2 0 89 109 It looks like we've got about 76,000 rows of data and have stats like hitouts, clangers, effective disposals etc. Let's explore some certain scenarios. Using my domain knowledge of footy, I can hypothesise that if a player kicks 5 goals, he is pretty likely to poll votes. Similarly, if a player gets 30 possessions and 2+ goals, he is also probably likely to poll votes. Let's have a look at the mean votes for players for both of these situations. Exploring votes if a bag is kicked (5+ goals) \u00b6 brownlow_data . query ( 'G >= 5' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes is a player has kicked a bag:\" , brownlow_data . query ( 'G >= 5' ) . brownlow_votes . mean ()) season 2010 1.420455 2011 1.313433 2012 1.413333 2013 1.253731 2014 1.915254 2015 1.765625 2016 1.788732 2017 2.098361 2018 0.000000 Name : brownlow_votes , dtype : float64 Mean votes is a player has kicked a bag : 1.4708818635607321 Exploring votes if the player has 30+ possies & 2+ goals \u00b6 brownlow_data . query ( 'G >= 2 and D >= 30' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player has 30 possies and kicks 2+ goals:\" , brownlow_data . query ( 'G >= 2 and D >= 30' ) . brownlow_votes . mean ()) season 2010 1.826923 2011 1.756410 2012 2.118421 2013 2.000000 2014 2.253731 2015 2.047619 2016 2.103448 2017 2.050000 2018 0.000000 Name : brownlow_votes , dtype : float64 Mean votes if a player has 30 possies and kicks 2 + goals : 1.8741379310344828 As suspected, the average votes for these two situations is 1.87! That's huge. Let's get an idea of the average votes for each player. It should be around 6/44, as there are always 6 votes per match and around 44 players per match. brownlow_data . brownlow_votes . mean () 0.12347341475121326 So the average vote is 0.12. Let's see how this changes is the player is a captain. I have collected data on if players are captains from wikipedia and collated it into a csv. Let's load this in and create a \"Is the player captain\" feature, then check the average votes for captains. Create Is Player Captain Feature \u00b6 captains = pd . read_csv ( 'data/captains.csv' ) . set_index ( 'player' ) def is_captain_for_that_season ( captains_df , player , year ): if player in captains_df . index : # Get years they were captain seasons = captains_df . loc [ player ] . season . split ( '-' ) if len ( seasons ) == 1 : seasons_captain = list ( map ( int , seasons )) elif len ( seasons ) == 2 : if seasons [ 1 ] == '' : seasons_captain = list ( range ( int ( seasons [ 0 ]), 2019 )) else : seasons_captain = list ( range ( int ( seasons [ 0 ]), int ( seasons [ 1 ]) + 1 )) if year in seasons_captain : return 1 return 0 brownlow_data [ 'is_captain' ] = brownlow_data . apply ( lambda x : is_captain_for_that_season ( captains , x . player , x . season ), axis = 'columns' ) brownlow_data . query ( 'is_captain == 1' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player is captain:\" , brownlow_data . query ( 'is_captain == 1' ) . brownlow_votes . mean ()) season 2010 0.408497 2011 0.429936 2012 0.274194 2013 0.438725 2014 0.519663 2015 0.447222 2016 0.347826 2017 0.425806 2018 0.000000 Name : brownlow_votes , dtype : float64 Mean votes if a player is captain : 0.36661698956780925 This is significantly higher than if they aren't captain. What would be interesting is to look at the average difference in votes between when they were captain and when they weren't, to try and find if there is a 'captain bias' in brownlow votes. Go ahead and try. For now, we're going to move onto feature creation Feature Creation \u00b6 Let's make a range of features, including: Ratios of each statistic per game If the player is a captain If they kicked a bag (\u2158+) If they kicked 2 and had 30+ possies First we will make features of ratios. What is important is not how many of a certain stat a player has, but how much of that stat a player has relative to everyone else in the same match. It doesn't matter if Dusty Martin has 31 possessions if Tom Mitchell has had 50 - Mitchell is probably more likely to poll (assuming all else is equal). So rather than using the actual number of possessions for example, we can divide these possessions by the total amount of possessions in the game. To do this we'll use pandas groupby and transform methods. Create Ratios As Features \u00b6 %% time # Get a list of stats of which to create ratios for ratio_cols = [ 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'TOG' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] # Create a ratios df ratios = ( brownlow_data . copy () . loc [:, [ 'match_id' ] + ratio_cols ] . groupby ( 'match_id' ) . transform ( lambda x : x / x . sum ())) feature_cols = [ 'date' , 'season' , 'round' , 'venue' , 'ID' , 'match_id' , 'player' , 'jumper_no' , 'team' , 'opposition' , 'status' , 'team_score' , 'opposition_score' , 'margin' , 'brownlow_votes' ] # Create a features df - join the ratios to this df features = ( brownlow_data [ feature_cols ] . copy () . join ( ratios )) Wall time : 11.3 s features . head () date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 0 2010-03-28 2010 1 Domain Stadium 11807 5096.0 T Armstrong 38 Adelaide Fremantle Away 62 118 -56 0.0 0.013393 0.016304 0.015517 0.025529 0.0 0.0 0.00 0.013333 0.000000 0.020516 0.013441 0.014599 0.014049 0.022599 0.000000 0.000000 0.000000 0.000000 0.011494 0.000000 0.036145 0.018868 0.000000 0.037037 0.011568 0.005456 1 2010-03-28 2010 1 Domain Stadium 3968 5096.0 N Bock 44 Adelaide Fremantle Away 62 118 -56 0.0 0.008929 0.025362 0.022414 0.023875 0.0 0.0 0.00 0.080000 0.000000 0.027169 0.032258 0.012165 0.021711 0.016949 0.038462 0.000000 0.026549 0.029851 0.022989 0.000000 0.024096 0.018868 0.037037 0.000000 0.024422 0.021825 2 2010-03-28 2010 1 Domain Stadium 11712 5096.0 M Cook 8 Adelaide Fremantle Away 62 118 -56 0.0 0.035714 0.025362 0.031034 0.026746 0.0 0.0 0.00 0.013333 0.034483 0.021625 0.021505 0.031630 0.026820 0.022599 0.000000 0.045455 0.008850 0.014925 0.011494 0.028986 0.048193 0.056604 0.000000 0.037037 0.020887 0.019703 3 2010-03-28 2010 1 Domain Stadium 11700 5096.0 P Dangerfield 32 Adelaide Fremantle Away 62 118 -56 0.0 0.049107 0.016304 0.017241 0.015605 0.0 0.0 0.04 0.026667 0.172414 0.022734 0.021505 0.029197 0.025543 0.016949 0.038462 0.090909 0.026549 0.000000 0.022989 0.072464 0.024096 0.018868 0.074074 0.037037 0.024422 0.028190 4 2010-03-28 2010 1 Domain Stadium 85 5096.0 M Doughty 11 Adelaide Fremantle Away 62 118 -56 0.0 0.017857 0.021739 0.025862 0.027526 0.0 0.0 0.00 0.053333 0.000000 0.025229 0.010753 0.031630 0.021711 0.016949 0.000000 0.000000 0.026549 0.000000 0.011494 0.000000 0.036145 0.037736 0.000000 0.000000 0.018959 0.018490 Kicked A Bag Feature \u00b6 features [ 'kicked_a_bag' ] = brownlow_data . G . apply ( lambda x : 1 if x >= 5 else 0 ) Is Captain Feature \u00b6 features [ 'is_captain' ] = features . apply ( lambda x : is_captain_for_that_season ( captains , x . player , x . season ), axis = 'columns' ) Won the Game Feature \u00b6 features [ 'team_won' ] = np . where ( features . margin > 0 , 1 , 0 ) 30+ & 2+ Goals Feature \u00b6 features [ 'got_30_possies_2_goals' ] = np . where (( brownlow_data . G >= 2 ) & ( brownlow_data . D >= 30 ), 1 , 0 ) Previous Top 10 Finish Feature \u00b6 I have a strong feeling that past performance may be a predictor of future performance in the brownlow. For example, last year Dusty Martin won the Brownlow. The umpires may have a bias towards Dusty this year because he is known to be on their radar as being a good player. Let's create a feature which is categorical and is 1 if the player has previously finished in the top 10. Let's create a function for this and then apply it to the afltables dataset, which has data back to 1897. We will then create a lookup table for the top 10 for each season and merge this table with our current features df. afltables = pd . read_csv ( 'data/afltables_stats.csv' ) . query ( 'Season >= 2000' ) def replace_special_characters ( name ): name = name . replace ( \"'\" , \"\" ) . replace ( \"-\" , \" \" ) . lower () name_split = name . split () if len ( name_split ) > 2 : first_name = name_split [ 0 ] last_name = name_split [ - 1 ] name = first_name + ' ' + last_name name_split_2 = name . split () name = name_split_2 [ 0 ][ 0 ] + ' ' + name_split_2 [ 1 ] return name . title () afltables = ( afltables . assign ( player = lambda df : df [ 'First.name' ] + ' ' + df . Surname ) . assign ( player = lambda df : df . player . apply ( replace_special_characters )) . rename ( columns = { 'Brownlow.Votes' : 'brownlow_votes' , 'Season' : 'season' , 'Playing.for' : 'team' })) ### Create Top 10 rank look up table brownlow_votes_yearly = ( afltables . groupby ([ 'season' , 'player' , 'team' ], as_index = False ) . brownlow_votes . sum ()) brownlow_votes_yearly [ 'yearly_rank' ] = ( brownlow_votes_yearly . groupby ( 'season' ) . brownlow_votes . rank ( method = 'max' , ascending = False )) # Filter to only get a dataframe since 2000 and only the top 10 players from each season brownlow_votes_top_10 = brownlow_votes_yearly . query ( 'yearly_rank < 11 & season >= 2000' ) brownlow_votes_top_10 . head ( 3 ) def how_many_times_top_10 ( top_10_df , player , year ): times = len ( top_10_df [( top_10_df . player == player ) & ( top_10_df . season < year )]) return times features [ 'times_in_top_10' ] = features . apply ( lambda x : how_many_times_top_10 ( brownlow_votes_top_10 , x . player , x . season ), axis = 1 ) season player team brownlow_votes yearly_rank 27 2000.0 A Koutoufides Carlton 19.0 4.0 36 2000.0 A Mcleod Adelaide 20.0 3.0 105 2000.0 B Ratten Carlton 18.0 6.0 Average Brownlow Votes Per Game Last Season Feature \u00b6 # Create a brownlow votes lookup table brownlow_votes_lookup_table = ( brownlow_data . groupby ([ 'player' , 'team' , 'season' ], as_index = False ) . brownlow_votes . mean () . assign ( next_season = lambda df : df . season + 1 ) . rename ( columns = { 'brownlow_votes' : 'ave_votes_last_season' })) # Have a look at Cripps to check if it's working brownlow_votes_lookup_table [ brownlow_votes_lookup_table . player == 'P Cripps' ] # Merge it to our features df features_with_votes_last_season = ( pd . merge ( features , brownlow_votes_lookup_table . drop ( columns = 'season' ), left_on = [ 'player' , 'team' , 'season' ], right_on = [ 'player' , 'team' , 'next_season' ], how = 'left' ) . drop ( columns = [ 'next_season' ]) . fillna ( 0 )) player team season ave_votes_last_season next_season 4377 P Cripps Carlton 2014 0.000000 2015 4378 P Cripps Carlton 2015 0.300000 2016 4379 P Cripps Carlton 2016 0.857143 2017 4380 P Cripps Carlton 2017 0.333333 2018 4381 P Cripps Carlton 2018 0.000000 2019 Historic Performance Relative To Model Feature \u00b6 It is well known that some players are good Brownlow performers. For whatever reason, they always poll much better than their stats may suggest. Lance Franklin and Bontempelli are probably in this category. Perhaps these players have an X-factor that Machine Learning models struggle to pick up on. To get around this, let's create a feature which looks at the player's performance relative to the model's prediction. To do this, we'll need to train and predict 7 different models - from 2011 to 2017. To create a model for each season, we will use h2o's AutoML. If you're new to h2o, please read about it here. It can be used in both R and Python. The metric we will use for loss in Mean Absolute Error (MAE). As we are using regression, some values are negative. We will convert these negative values to 0 as it doesn't make sense to poll negative brownlow votes. Similarly, some matches won't predict exactly 6 votes, so we will scale these predictions so that we predict exactly 6 votes for each match. So that you don't have to train these models yourself, I have saved the models and we will load them in. If you are keen to train the models yourself, simply uncomment out the code below and run the cell. To bulk uncomment, highlight the rows and press ctrl + '/' h2o . init () # Uncomment the code below if you want to train the models yourself - otherwise, we will load them in the load cell from disk ## Join to our features df # aml_yearly_model_objects = {} # yearly_predictions_dfs = {} # feature_cols = ['margin', 'CP', 'UP', 'ED', 'DE', # 'CM', 'GA', 'MI5', 'one_perc', 'BO', 'TOG', 'K', 'HB', 'D', 'M', 'G', # 'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', 'FF', 'FA', 'AF', 'SC'] # for year in range(2011, 2018): # # Filter the data to only include past data # train_historic = brownlow_data[brownlow_data.season < year].copy() # # Convert to an h2o frame # train_h2o_historic = h2o.H2OFrame(train_historic) # # Create an AutoML object # aml = H2OAutoML(max_runtime_secs=30, # balance_classes=True, # seed=42) # # Train the model # aml.train(y='brownlow_votes', x=feature_cols, training_frame=train_h2o_historic) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/yearly_model_{year}') # # Append the best model to a list # aml_yearly_model_objects[year] = aml.leader # # Make predictions on test set for that year # test_historic = brownlow_data[brownlow_data.season == year].copy() # test_h2o_historic = h2o.H2OFrame(test_historic) # preds = aml.predict(test_h2o_historic).as_data_frame() # test_historic['predicted_votes'] = preds.values # # Convert negative predictions to 0 # test_historic['predicted_votes_neg_to_0'] = test_historic.predicted_votes.apply(lambda x: 0 if x < 0 else x) # # Create a total match votes column - which calculates the number of votes predicted in each game when the predictions # # are unscaled # test_historic['unscaled_match_votes'] = test_historic.groupby('match_id').predicted_votes_neg_to_0.transform('sum') # # Scale predictions # test_historic['predicted_votes_scaled'] = test_historic.predicted_votes_neg_to_0 / test_historic.unscaled_match_votes * 6 # # Aggregate the predictions # test_grouped = (test_historic.groupby(['player', 'team'], as_index=False) # .sum() # .sort_values(by='brownlow_votes', ascending=False) # .assign(mae=lambda df: abs(df.predicted_votes_scaled - df.brownlow_votes))) # test_grouped['error'] = test_grouped.predicted_votes_scaled - test_grouped.brownlow_votes # test_grouped['next_year'] = year + 1 # # Add this years predictions df to a dictionary to use later # yearly_predictions_dfs[year] = test_grouped # preds_errors = None # for key, value in yearly_predictions_dfs.items(): # if preds_errors is None: # preds_errors = value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']] # else: # preds_errors = preds_errors.append(value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']], sort=True) # with open('data/prediction_errors_df.pickle', 'wb') as handle: # pickle.dump(yearly_predicted_errors, handle) Checking whether there is an H2O instance running at http://localhost:54321 . connected. H2O cluster uptime: 1 hour 28 mins H2O cluster timezone: Australia/Hobart H2O data parsing timezone: UTC H2O cluster version: 3.20.0.4 H2O cluster version age: 1 month and 18 days H2O cluster name: H2O_from_python_WardJ_tt2ak5 H2O cluster total nodes: 1 H2O cluster free memory: 7.018 Gb H2O cluster total cores: 4 H2O cluster allowed cores: 4 H2O cluster status: locked, healthy H2O connection url: http://localhost:54321 H2O connection proxy: None H2O internal security: False H2O API Extensions: Algos, AutoML, Core V3, Core V4 Python version: 3.6.4 final # Load predictions error df with open ( 'data/prediction_errors_df.pickle' , 'rb' ) as handle : preds_errors = pickle . load ( handle ) # Look at last years predictions preds_errors . query ( 'next_year == 2018' ) . sort_values ( by = 'brownlow_votes' , ascending = False ) . head ( 20 ) brownlow_votes error next_year player predicted_votes_scaled season 139 36.0 -4.915157 2018 D Martin 31.084843 44374 486 33.0 -4.413780 2018 P Dangerfield 28.586220 42357 619 25.0 0.072223 2018 T Mitchell 25.072223 44374 279 23.0 -9.296209 2018 J Kennedy 13.703791 38323 376 22.0 -7.621336 2018 L Franklin 14.378664 44374 278 21.0 -3.016176 2018 J Kelly 17.983824 42357 519 20.0 -4.182915 2018 R Sloane 15.817085 44374 410 19.0 -7.740142 2018 M Bontempelli 11.259858 44374 483 18.0 -4.522835 2018 O Wines 13.477165 44374 121 17.0 -4.446749 2018 D Beams 12.553251 38323 390 16.0 -3.620094 2018 L Parker 12.379906 44374 561 15.0 -5.846506 2018 S Pendlebury 9.153494 32272 463 15.0 -4.402600 2018 N Fyfe 10.597400 42357 42 15.0 -4.787914 2018 B Ebert 10.212086 44374 651 15.0 0.824510 2018 Z Merrett 15.824510 42357 578 14.0 2.300607 2018 T Adams 16.300607 44374 34 14.0 -5.796604 2018 B Brown 8.203396 44374 172 14.0 -1.650752 2018 D Zorko 12.349248 42357 184 14.0 -1.233663 2018 G Ablett 12.766337 28238 389 14.0 -2.099613 2018 L Neale 11.900387 42357 Look at that! A simply Machine Learning ensemble model, using AutoML predicted last year's winner! That's impressive. As we can see it also predicted Bontempelli would only score 11.26, when he actually scored 19 - a huge discrepency. Let's use this as a feature. features_with_historic_perf_relative_to_model = \\ ( features_with_votes_last_season . pipe ( pd . merge , preds_errors [[ 'player' , 'next_year' , 'error' ]], left_on = [ 'player' , 'season' ], right_on = [ 'player' , 'next_year' ], how = 'left' ) . fillna ( 0 ) . rename ( columns = { 'error' : 'error_last_season' }) . drop_duplicates ( subset = [ 'player' , 'round' , 'SC' ])) Filtering the data to only include the top 20 SC for each match \u00b6 Logically, it is extremely unlikely that a player will poll votes if their Supercoach score is not in the top 20 players. By eliminating the other 20+ players, we can reduce the noise in the data, as we are almost certain the players won't poll from the bottom half. Let's explore how many players poll if they're not in the top 20, and then filter our df if this number is not significant. # Find number of players who vote when in top 15 SC brownlow_data [ 'SC_rank_match' ] = brownlow_data . groupby ( 'match_id' ) . SC . rank ( method = 'max' , ascending = False ) brownlow_data . query ( 'SC_rank_match > 20 and season > 2014' ) . brownlow_votes . value_counts () 0.0 18330 1.0 14 2.0 8 3.0 2 Name : brownlow_votes , dtype : int64 Since 2014, there have only been 24 players who have voted and not been in the top 20 SC. features_with_sc_rank = features_with_historic_perf_relative_to_model . copy () features_with_sc_rank [ 'SC_rank_match' ] = features_with_sc_rank . groupby ( 'match_id' ) . SC . rank ( method = 'max' , ascending = False ) # Filter out rows with a SC rank of below 20 features_with_sc_rank_filtered = features_with_sc_rank . query ( 'SC_rank_match <= 20' ) # Filter out 2010 and 2011 as we used these seasons to create historic model performance features features_last_before_train = features_with_sc_rank_filtered . query ( 'season != 2010 and season != 2011' ) . reset_index ( drop = True ) features_last_before_train . head ( 3 ) date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC kicked_a_bag is_captain team_won got_30_possies_2_goals times_in_top_10 ave_votes_last_season next_year error_last_season SC_rank_match 0 2012-03-31 2012 1 Metricon Stadium 11985 5347.0 I Callinan 37 Adelaide Gold Coast Away 137 68 69 0.0 0.031359 0.018947 0.016071 0.015745 0.0 0.000000 0.12 0.000000 0.000000 0.023333 0.031674 0.012270 0.023438 0.026042 0.068966 0.217391 0.041322 0.0 0.017241 0.013699 0.042553 0.000000 0.000000 0.032258 0.030593 0.026962 0 0 1 0 0 0.000000 2012.0 0.042048 14.0 1 2012-03-31 2012 1 Metricon Stadium 11700 5347.0 P Dangerfield 32 Adelaide Gold Coast Away 137 68 69 0.0 0.048780 0.023158 0.037500 0.026451 0.0 0.000000 0.04 0.009804 0.090909 0.025556 0.029412 0.036810 0.032552 0.015625 0.068966 0.000000 0.016529 0.0 0.051724 0.123288 0.042553 0.000000 0.000000 0.032258 0.027503 0.028173 0 0 1 0 0 0.333333 2012.0 -2.983636 12.0 2 2012-03-31 2012 1 Metricon Stadium 2381 5347.0 R Douglas 26 Adelaide Gold Coast Away 137 68 69 0.0 0.020906 0.033684 0.021429 0.019901 0.0 0.083333 0.04 0.049020 0.000000 0.023056 0.031674 0.015337 0.024740 0.031250 0.034483 0.130435 0.057851 0.0 0.043103 0.000000 0.031915 0.014493 0.064516 0.000000 0.033684 0.030597 0 0 1 0 0 0.000000 2012.0 1.724915 10.0 Modeling The 2017 Brownlow \u00b6 Now that we have all of our features, we can simply create a training set (2012-2016), and a test set (2017), and make our predictions for last year! We will use AutoML for this process again. Again, rather than waiting for the model to train, I will save the model so you can simply load it in. We will also scale our features. We can then see how our model went in predicting last year's brownlow, creating a baseline for this years' predictions. We will then predict this year's vote count. train_baseline = features_last_before_train . query ( \"season < 2017\" ) holdout = features_last_before_train . query ( \"season == 2017\" ) scale_cols = [ 'team_score' , 'opposition_score' , 'margin' , 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] other_feature_cols = [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' , 'times_in_top_10' , 'ave_votes_last_season' , 'error_last_season' , 'SC_rank_match' ] all_feature_cols = scale_cols + other_feature_cols # Scale features scaler = StandardScaler () train_baseline_scaled = train_baseline . copy () train_baseline_scaled [ scale_cols ] = scaler . fit_transform ( train_baseline [ scale_cols ]) holdout_scaled = holdout . copy () holdout_scaled [ scale_cols ] = scaler . transform ( holdout [ scale_cols ]) # Convert categorical columns to categoricals train_baseline_h2o = h2o . H2OFrame ( train_baseline_scaled ) holdout_h2o = h2o . H2OFrame ( holdout_scaled ) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_baseline_h2o [ col ] = train_baseline_h2o [ col ] . asfactor () holdout_h2o [ col ] = holdout_h2o [ col ] . asfactor () C:\\Users\\wardj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1] Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Below I have commented out training and saving the 2017 model. Rather than training it again, we will just load it in. Uncomment this part out if you want to train it yourself. # aml_2017_model = H2OAutoML(max_runtime_secs = 60*3, # balance_classes=True, # seed=42) # aml_2017_model.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_baseline_h2o) # save the model # model_path = h2o.save_model(model=aml_2017_model.leader, path=\"models\", force=True) # Get model id # model_name = aml_2017_model.leaderboard[0, 'model_id'] # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2017_model_v1') # Load model in aml_2017_model = h2o . load_model ( 'models/brownlow_2017_model_v1' ) # Predict the 2017 brownlow count preds_final_2017_model = aml_2017_model . predict ( holdout_h2o ) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total holdout = ( holdout . assign ( predicted_votes = preds_final_2017_model . as_data_frame () . values ) . assign ( predicted_votes_neg_to_0 = lambda df : df . predicted_votes . apply ( lambda x : 0 if x < 0 else x )) . assign ( unscaled_match_votes = lambda df : df . groupby ( 'match_id' ) . predicted_votes_neg_to_0 . transform ( 'sum' )) . assign ( predicted_votes_scaled = lambda df : df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2017 = ( holdout . groupby ([ 'player' , 'team' ], as_index = False ) . agg ({ 'brownlow_votes' : sum , 'predicted_votes_scaled' : sum , 'SC' : 'mean' , 'G' : 'mean' }) . sort_values ( by = 'brownlow_votes' , ascending = False ) . assign ( mae = lambda df : abs ( df . brownlow_votes - df . predicted_votes_scaled )) . reset_index ( drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% agg_predictions_2017 . head ( 15 ) player team brownlow_votes predicted_votes_scaled SC G mae 0 D Martin Richmond 36.0 37.271060 0.037862 0.064869 1.271060 1 P Dangerfield Geelong 33.0 39.288122 0.042441 0.070819 6.288122 2 T Mitchell Hawthorn 25.0 28.629859 0.036040 0.016928 3.629859 3 L Franklin Sydney 22.0 16.353733 0.034640 0.149203 5.646267 4 J Kelly GWS 21.0 19.565321 0.034652 0.033772 1.434679 5 R Sloane Adelaide 20.0 21.417347 0.037068 0.034821 1.417347 6 J Kennedy Sydney 20.0 13.671891 0.032014 0.030508 6.328109 7 M Bontempelli Western Bulldogs 19.0 17.461889 0.033233 0.040498 1.538111 8 D Beams Brisbane 17.0 15.414730 0.034848 0.044998 1.585270 9 O Wines Port Adelaide 16.0 12.973973 0.031601 0.021967 3.026027 10 N Fyfe Fremantle 15.0 11.926030 0.033761 0.031680 3.073970 11 S Pendlebury Collingwood 15.0 10.845214 0.033855 0.013660 4.154786 12 B Ebert Port Adelaide 15.0 7.633526 0.032795 0.008431 7.366474 13 L Parker Sydney 15.0 14.680079 0.031366 0.030311 0.319921 14 Z Merrett Essendon 15.0 21.063889 0.033737 0.015362 6.063889 So whilst our model predicted Dangerfield to win, it was pretty damn accurate! Let's find the MAE for the top 100, 50, 25, and 10, and then compare it to 2018's MAE in week, when the Brownlow has been counted. for top_x in [ 10 , 25 , 50 , 100 ]: temp_mae = round ( agg_predictions_2017 . iloc [: top_x ] . mae . mean (), 3 ) print ( f \"The Average Mean Absolute Error for the top { top_x } is { temp_mae } \" ) The Average Mean Absolute Error for the top 10 is 3.216 The Average Mean Absolute Error for the top 25 is 2.931 The Average Mean Absolute Error for the top 50 is 3.15 The Average Mean Absolute Error for the top 100 is 2.577 Modelling This Year's Brownlow \u00b6 Let's now predict this year's vote count. These predictions will be on the front page of the GitHub. train = features_last_before_train . query ( \"season < 2018\" ) test = features_last_before_train . query ( \"season == 2018\" ) # Scale features scaler = StandardScaler () train_scaled = train . copy () train_scaled [ scale_cols ] = scaler . fit_transform ( train [ scale_cols ]) test_scaled = test . copy () test_scaled [ scale_cols ] = scaler . transform ( test [ scale_cols ]) # Convert categorical columns to categoricals train_h2o = h2o . H2OFrame ( train_scaled ) test_h2o = h2o . H2OFrame ( test_scaled ) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_h2o [ col ] = train_h2o [ col ] . asfactor () test_h2o [ col ] = test_h2o [ col ] . asfactor () C:\\Users\\wardj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1] Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% # Train the model - this part is commented out as we will just load our model from disk # aml = H2OAutoML(max_runtime_secs = 60*3, # balance_classes=True, # seed=42) # aml.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_h2o) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2018_model_v1') # Load model in aml = h2o . load_model ( 'models/brownlow_2018_model_v1' ) # Predict the 2018 brownlow count preds_final_2018_model = aml . predict ( test_h2o ) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total test = ( test . assign ( predicted_votes = preds_final_2018_model . as_data_frame () . values ) . assign ( predicted_votes_neg_to_0 = lambda df : df . predicted_votes . apply ( lambda x : 0 if x < 0 else x )) . assign ( unscaled_match_votes = lambda df : df . groupby ( 'match_id' ) . predicted_votes_neg_to_0 . transform ( 'sum' )) . assign ( predicted_votes_scaled = lambda df : df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2018 = ( test . groupby ([ 'player' , 'team' ], as_index = False ) . agg ({ 'predicted_votes_scaled' : sum , 'match_id' : 'count' }) # shows how many games they played . sort_values ( by = 'predicted_votes_scaled' , ascending = False ) . reset_index ( drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% # Show the top 25 predictions agg_predictions_2018 . head ( 25 ) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 35.484614 20 1 M Gawn Melbourne 21.544278 22 2 D Martin Richmond 20.444488 19 3 B Grundy Collingwood 19.543511 22 4 C Oliver Melbourne 19.009628 20 5 J Macrae Western Bulldogs 18.931594 17 6 P Dangerfield Geelong 18.621242 21 7 D Beams Brisbane 17.621222 15 8 E Yeo West Coast 16.015638 20 9 L Neale Fremantle 15.495083 21 10 A Gaff West Coast 15.165629 18 11 D Heppell Essendon 15.083797 19 12 J Selwood Geelong 14.989096 18 13 S Sidebottom Collingwood 14.863136 18 14 N Fyfe Fremantle 14.692243 11 15 J Kennedy Sydney 14.404489 16 16 Z Merrett Essendon 13.632131 18 17 M Crouch Adelaide 13.503858 16 18 R Laird Adelaide 13.274869 19 19 P Cripps Carlton 13.240568 21 20 G Ablett Geelong 13.018950 15 21 L Franklin Sydney 12.792476 13 22 J Lloyd Sydney 12.174224 20 23 J Kelly GWS 11.982157 14 24 C Ward GWS 11.892443 19 print ( agg_predictions_2018 . head ( 15 )) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 35.484614 1 M Gawn Melbourne 21.544278 2 D Martin Richmond 20.444488 3 B Grundy Collingwood 19.543511 4 C Oliver Melbourne 19.009628 5 J Macrae Western Bulldogs 18.931594 6 P Dangerfield Geelong 18.621242 7 D Beams Brisbane 17.621222 8 E Yeo West Coast 16.015638 9 L Neale Fremantle 15.495083 10 A Gaff West Coast 15.165629 11 D Heppell Essendon 15.083797 12 J Selwood Geelong 14.989096 13 S Sidebottom Collingwood 14.863136 14 N Fyfe Fremantle 14.692243 Now that we have the top 25, let's also look at the top 3 from each team. agg_predictions_2018 . sort_values ( by = [ 'team' , 'predicted_votes_scaled' ], ascending = [ True , False ]) . groupby ( 'team' ) . head ( 3 ) player team predicted_votes_scaled match_id 17 M Crouch Adelaide 13.503858 16 18 R Laird Adelaide 13.274869 19 51 B Gibbs Adelaide 7.783425 18 7 D Beams Brisbane 17.621222 15 46 D Zorko Brisbane 8.123915 14 55 S Martin Brisbane 7.243428 19 19 P Cripps Carlton 13.240568 21 50 K Simpson Carlton 7.864995 18 88 E Curnow Carlton 3.725292 19 3 B Grundy Collingwood 19.543511 22 13 S Sidebottom Collingwood 14.863136 18 31 A Treloar Collingwood 10.487535 11 11 D Heppell Essendon 15.083797 19 16 Z Merrett Essendon 13.632131 18 57 D Smith Essendon 6.838857 20 9 L Neale Fremantle 15.495083 21 14 N Fyfe Fremantle 14.692243 11 74 M Walters Fremantle 5.120949 12 23 J Kelly GWS 11.982157 14 24 C Ward GWS 11.892443 19 25 S Coniglio GWS 11.785273 20 6 P Dangerfield Geelong 18.621242 21 12 J Selwood Geelong 14.989096 18 20 G Ablett Geelong 13.018950 15 80 J Witts Gold Coast 4.617273 13 85 J Lyons Gold Coast 4.043711 14 114 B Fiorini Gold Coast 2.798683 6 0 T Mitchell Hawthorn 35.484614 20 39 L Breust Hawthorn 9.286521 16 45 J Gunston Hawthorn 8.341824 19 1 M Gawn Melbourne 21.544278 22 4 C Oliver Melbourne 19.009628 20 32 J Hogan Melbourne 10.198873 13 26 S Higgins North Melbourne 11.327596 19 37 B Brown North Melbourne 9.550205 13 42 B Cunnington North Melbourne 8.857294 17 27 O Wines Port Adelaide 11.118795 16 36 R Gray Port Adelaide 9.606960 17 54 J Westhoff Port Adelaide 7.355449 21 2 D Martin Richmond 20.444488 19 30 J Riewoldt Richmond 10.557749 15 59 T Cotchin Richmond 6.741732 12 41 S Ross St Kilda 9.093395 17 48 J Steven St Kilda 8.099164 17 90 J Steele St Kilda 3.617769 16 15 J Kennedy Sydney 14.404489 16 21 L Franklin Sydney 12.792476 13 22 J Lloyd Sydney 12.174224 20 8 E Yeo West Coast 16.015638 20 10 A Gaff West Coast 15.165629 18 38 J Redden West Coast 9.368163 16 5 J Macrae Western Bulldogs 18.931594 17 40 M Bontempelli Western Bulldogs 9.272771 16 44 L Hunter Western Bulldogs 8.349606 17 If you're looking for a round by round breakdown, just have a look at the test dataframe. test [[ 'date' , 'round' , 'player' , 'team' , 'opposition' , 'margin' , 'SC' , 'predicted_votes_scaled' ]] . tail ( 25 ) date round player team opposition margin SC predicted_votes_scaled 27231 2018-08-26 23 R Lobb GWS Melbourne -45 0.027576 0.007501 27232 2018-08-26 23 H Perryman GWS Melbourne -45 0.025758 0.000000 27233 2018-08-26 23 D Shiel GWS Melbourne -45 0.026364 0.000000 27234 2018-08-26 23 A Tomlinson GWS Melbourne -45 0.026667 0.000000 27235 2018-08-26 23 C Ward GWS Melbourne -45 0.035455 0.332057 27236 2018-08-26 23 L Austin St Kilda North Melbourne -23 0.027853 0.003928 27237 2018-08-26 23 J Geary St Kilda North Melbourne -23 0.025129 0.000000 27238 2018-08-26 23 S Gilbert St Kilda North Melbourne -23 0.025734 0.000000 27239 2018-08-26 23 R Marshall St Kilda North Melbourne -23 0.028156 0.103389 27240 2018-08-26 23 B Paton St Kilda North Melbourne -23 0.023918 0.000000 27241 2018-08-26 23 S Ross St Kilda North Melbourne -23 0.039055 0.300392 27242 2018-08-26 23 J Steele St Kilda North Melbourne -23 0.042386 1.094723 27243 2018-08-26 23 J Steven St Kilda North Melbourne -23 0.046624 1.119684 27244 2018-08-26 23 R Clarke North Melbourne St Kilda 23 0.021193 0.050859 27245 2018-08-26 23 B Cunnington North Melbourne St Kilda 23 0.032698 0.212364 27246 2018-08-26 23 M Daw North Melbourne St Kilda 23 0.022404 0.021889 27247 2018-08-26 23 T Dumont North Melbourne St Kilda 23 0.049046 2.042550 27248 2018-08-26 23 T Goldstein North Melbourne St Kilda 23 0.037844 0.347199 27249 2018-08-26 23 S Higgins North Melbourne St Kilda 23 0.039358 0.508419 27250 2018-08-26 23 N Hrovat North Melbourne St Kilda 23 0.025129 0.012548 27251 2018-08-26 23 J Macmillan North Melbourne St Kilda 23 0.024220 0.000000 27252 2018-08-26 23 J Waite North Melbourne St Kilda 23 0.025734 0.057826 27253 2018-08-26 23 M Williams North Melbourne St Kilda 23 0.023312 0.000000 27254 2018-08-26 23 S Wright North Melbourne St Kilda 23 0.029064 0.093219 27255 2018-08-26 23 J Ziebell North Melbourne St Kilda 23 0.033000 0.031010 And there we have it! In a single notebook we have made a fairly good Brownlow predictive model. Enjoy. Disclaimer \u00b6 Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Modelling the Brownlow Medal"},{"location":"modelling/brownlowModelTutorial/#modelling-the-brownlow","text":"This walkthrough will provide a brief, yet effective tutorial on how to model the Brownlow medal. We will use data from 2010 to 2018, which includes Supercoach points and other useful stats. The output will be the number of votes predicted for each player in each match, and we will aggregate these to create aggregates for each team and for the whole competition. No doubt we'll have Mitchell right up the top, and if we don't, then we know we've done something wrong! # Import modules libraries import pandas as pd import h2o from h2o.automl import H2OAutoML import numpy as np from sklearn.preprocessing import StandardScaler import os import pickle # Change notebook settings pd . options . display . max_columns = None pd . options . display . max_rows = 300 from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\"","title":"Modelling the Brownlow"},{"location":"modelling/brownlowModelTutorial/#eda-read-in-the-data","text":"I have collated this data using the fitzRoy R package and merging the afltables dataset with the footywire dataset, so that we can Supercoach and other advanced stats with Brownlow votes. Let's read in the data and have a sneak peak at what it looks like. brownlow_data = pd . read_csv ( 'data/afl_brownlow_data.csv' ) brownlow_data . tail ( 3 ) date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 76585 2018-08-26 2018 23 Etihad Stadium 12312 9708.0 M Wood 32 North Melbourne St Kilda Away 117 94 23 NaN 0 3 2 66.7 0 0 2 0 0 34 3 0 3 3 1 0 0 0 0 0 0 0 0 0 24 25 76586 2018-08-26 2018 23 Etihad Stadium 11755 9708.0 S Wright 19 North Melbourne St Kilda Away 117 94 23 NaN 10 17 22 75.9 0 0 0 0 1 83 16 13 29 8 0 0 2 0 3 0 4 4 1 0 107 96 76587 2018-08-26 2018 23 Etihad Stadium 11724 9708.0 J Ziebell 7 North Melbourne St Kilda Away 117 94 23 NaN 9 8 11 73.3 0 0 2 4 0 94 12 3 15 6 3 1 2 0 1 0 0 0 2 0 89 109 It looks like we've got about 76,000 rows of data and have stats like hitouts, clangers, effective disposals etc. Let's explore some certain scenarios. Using my domain knowledge of footy, I can hypothesise that if a player kicks 5 goals, he is pretty likely to poll votes. Similarly, if a player gets 30 possessions and 2+ goals, he is also probably likely to poll votes. Let's have a look at the mean votes for players for both of these situations.","title":"EDA - Read in the data"},{"location":"modelling/brownlowModelTutorial/#exploring-votes-if-a-bag-is-kicked-5-goals","text":"brownlow_data . query ( 'G >= 5' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes is a player has kicked a bag:\" , brownlow_data . query ( 'G >= 5' ) . brownlow_votes . mean ()) season 2010 1.420455 2011 1.313433 2012 1.413333 2013 1.253731 2014 1.915254 2015 1.765625 2016 1.788732 2017 2.098361 2018 0.000000 Name : brownlow_votes , dtype : float64 Mean votes is a player has kicked a bag : 1.4708818635607321","title":"Exploring votes if a bag is kicked (5+ goals)"},{"location":"modelling/brownlowModelTutorial/#exploring-votes-if-the-player-has-30-possies-2-goals","text":"brownlow_data . query ( 'G >= 2 and D >= 30' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player has 30 possies and kicks 2+ goals:\" , brownlow_data . query ( 'G >= 2 and D >= 30' ) . brownlow_votes . mean ()) season 2010 1.826923 2011 1.756410 2012 2.118421 2013 2.000000 2014 2.253731 2015 2.047619 2016 2.103448 2017 2.050000 2018 0.000000 Name : brownlow_votes , dtype : float64 Mean votes if a player has 30 possies and kicks 2 + goals : 1.8741379310344828 As suspected, the average votes for these two situations is 1.87! That's huge. Let's get an idea of the average votes for each player. It should be around 6/44, as there are always 6 votes per match and around 44 players per match. brownlow_data . brownlow_votes . mean () 0.12347341475121326 So the average vote is 0.12. Let's see how this changes is the player is a captain. I have collected data on if players are captains from wikipedia and collated it into a csv. Let's load this in and create a \"Is the player captain\" feature, then check the average votes for captains.","title":"Exploring votes if the player has 30+ possies &amp; 2+ goals"},{"location":"modelling/brownlowModelTutorial/#create-is-player-captain-feature","text":"captains = pd . read_csv ( 'data/captains.csv' ) . set_index ( 'player' ) def is_captain_for_that_season ( captains_df , player , year ): if player in captains_df . index : # Get years they were captain seasons = captains_df . loc [ player ] . season . split ( '-' ) if len ( seasons ) == 1 : seasons_captain = list ( map ( int , seasons )) elif len ( seasons ) == 2 : if seasons [ 1 ] == '' : seasons_captain = list ( range ( int ( seasons [ 0 ]), 2019 )) else : seasons_captain = list ( range ( int ( seasons [ 0 ]), int ( seasons [ 1 ]) + 1 )) if year in seasons_captain : return 1 return 0 brownlow_data [ 'is_captain' ] = brownlow_data . apply ( lambda x : is_captain_for_that_season ( captains , x . player , x . season ), axis = 'columns' ) brownlow_data . query ( 'is_captain == 1' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player is captain:\" , brownlow_data . query ( 'is_captain == 1' ) . brownlow_votes . mean ()) season 2010 0.408497 2011 0.429936 2012 0.274194 2013 0.438725 2014 0.519663 2015 0.447222 2016 0.347826 2017 0.425806 2018 0.000000 Name : brownlow_votes , dtype : float64 Mean votes if a player is captain : 0.36661698956780925 This is significantly higher than if they aren't captain. What would be interesting is to look at the average difference in votes between when they were captain and when they weren't, to try and find if there is a 'captain bias' in brownlow votes. Go ahead and try. For now, we're going to move onto feature creation","title":"Create Is Player Captain Feature"},{"location":"modelling/brownlowModelTutorial/#feature-creation","text":"Let's make a range of features, including: Ratios of each statistic per game If the player is a captain If they kicked a bag (\u2158+) If they kicked 2 and had 30+ possies First we will make features of ratios. What is important is not how many of a certain stat a player has, but how much of that stat a player has relative to everyone else in the same match. It doesn't matter if Dusty Martin has 31 possessions if Tom Mitchell has had 50 - Mitchell is probably more likely to poll (assuming all else is equal). So rather than using the actual number of possessions for example, we can divide these possessions by the total amount of possessions in the game. To do this we'll use pandas groupby and transform methods.","title":"Feature Creation"},{"location":"modelling/brownlowModelTutorial/#create-ratios-as-features","text":"%% time # Get a list of stats of which to create ratios for ratio_cols = [ 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'TOG' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] # Create a ratios df ratios = ( brownlow_data . copy () . loc [:, [ 'match_id' ] + ratio_cols ] . groupby ( 'match_id' ) . transform ( lambda x : x / x . sum ())) feature_cols = [ 'date' , 'season' , 'round' , 'venue' , 'ID' , 'match_id' , 'player' , 'jumper_no' , 'team' , 'opposition' , 'status' , 'team_score' , 'opposition_score' , 'margin' , 'brownlow_votes' ] # Create a features df - join the ratios to this df features = ( brownlow_data [ feature_cols ] . copy () . join ( ratios )) Wall time : 11.3 s features . head () date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 0 2010-03-28 2010 1 Domain Stadium 11807 5096.0 T Armstrong 38 Adelaide Fremantle Away 62 118 -56 0.0 0.013393 0.016304 0.015517 0.025529 0.0 0.0 0.00 0.013333 0.000000 0.020516 0.013441 0.014599 0.014049 0.022599 0.000000 0.000000 0.000000 0.000000 0.011494 0.000000 0.036145 0.018868 0.000000 0.037037 0.011568 0.005456 1 2010-03-28 2010 1 Domain Stadium 3968 5096.0 N Bock 44 Adelaide Fremantle Away 62 118 -56 0.0 0.008929 0.025362 0.022414 0.023875 0.0 0.0 0.00 0.080000 0.000000 0.027169 0.032258 0.012165 0.021711 0.016949 0.038462 0.000000 0.026549 0.029851 0.022989 0.000000 0.024096 0.018868 0.037037 0.000000 0.024422 0.021825 2 2010-03-28 2010 1 Domain Stadium 11712 5096.0 M Cook 8 Adelaide Fremantle Away 62 118 -56 0.0 0.035714 0.025362 0.031034 0.026746 0.0 0.0 0.00 0.013333 0.034483 0.021625 0.021505 0.031630 0.026820 0.022599 0.000000 0.045455 0.008850 0.014925 0.011494 0.028986 0.048193 0.056604 0.000000 0.037037 0.020887 0.019703 3 2010-03-28 2010 1 Domain Stadium 11700 5096.0 P Dangerfield 32 Adelaide Fremantle Away 62 118 -56 0.0 0.049107 0.016304 0.017241 0.015605 0.0 0.0 0.04 0.026667 0.172414 0.022734 0.021505 0.029197 0.025543 0.016949 0.038462 0.090909 0.026549 0.000000 0.022989 0.072464 0.024096 0.018868 0.074074 0.037037 0.024422 0.028190 4 2010-03-28 2010 1 Domain Stadium 85 5096.0 M Doughty 11 Adelaide Fremantle Away 62 118 -56 0.0 0.017857 0.021739 0.025862 0.027526 0.0 0.0 0.00 0.053333 0.000000 0.025229 0.010753 0.031630 0.021711 0.016949 0.000000 0.000000 0.026549 0.000000 0.011494 0.000000 0.036145 0.037736 0.000000 0.000000 0.018959 0.018490","title":"Create Ratios As Features"},{"location":"modelling/brownlowModelTutorial/#kicked-a-bag-feature","text":"features [ 'kicked_a_bag' ] = brownlow_data . G . apply ( lambda x : 1 if x >= 5 else 0 )","title":"Kicked A Bag Feature"},{"location":"modelling/brownlowModelTutorial/#is-captain-feature","text":"features [ 'is_captain' ] = features . apply ( lambda x : is_captain_for_that_season ( captains , x . player , x . season ), axis = 'columns' )","title":"Is Captain Feature"},{"location":"modelling/brownlowModelTutorial/#won-the-game-feature","text":"features [ 'team_won' ] = np . where ( features . margin > 0 , 1 , 0 )","title":"Won the Game Feature"},{"location":"modelling/brownlowModelTutorial/#30-2-goals-feature","text":"features [ 'got_30_possies_2_goals' ] = np . where (( brownlow_data . G >= 2 ) & ( brownlow_data . D >= 30 ), 1 , 0 )","title":"30+ &amp; 2+ Goals Feature"},{"location":"modelling/brownlowModelTutorial/#previous-top-10-finish-feature","text":"I have a strong feeling that past performance may be a predictor of future performance in the brownlow. For example, last year Dusty Martin won the Brownlow. The umpires may have a bias towards Dusty this year because he is known to be on their radar as being a good player. Let's create a feature which is categorical and is 1 if the player has previously finished in the top 10. Let's create a function for this and then apply it to the afltables dataset, which has data back to 1897. We will then create a lookup table for the top 10 for each season and merge this table with our current features df. afltables = pd . read_csv ( 'data/afltables_stats.csv' ) . query ( 'Season >= 2000' ) def replace_special_characters ( name ): name = name . replace ( \"'\" , \"\" ) . replace ( \"-\" , \" \" ) . lower () name_split = name . split () if len ( name_split ) > 2 : first_name = name_split [ 0 ] last_name = name_split [ - 1 ] name = first_name + ' ' + last_name name_split_2 = name . split () name = name_split_2 [ 0 ][ 0 ] + ' ' + name_split_2 [ 1 ] return name . title () afltables = ( afltables . assign ( player = lambda df : df [ 'First.name' ] + ' ' + df . Surname ) . assign ( player = lambda df : df . player . apply ( replace_special_characters )) . rename ( columns = { 'Brownlow.Votes' : 'brownlow_votes' , 'Season' : 'season' , 'Playing.for' : 'team' })) ### Create Top 10 rank look up table brownlow_votes_yearly = ( afltables . groupby ([ 'season' , 'player' , 'team' ], as_index = False ) . brownlow_votes . sum ()) brownlow_votes_yearly [ 'yearly_rank' ] = ( brownlow_votes_yearly . groupby ( 'season' ) . brownlow_votes . rank ( method = 'max' , ascending = False )) # Filter to only get a dataframe since 2000 and only the top 10 players from each season brownlow_votes_top_10 = brownlow_votes_yearly . query ( 'yearly_rank < 11 & season >= 2000' ) brownlow_votes_top_10 . head ( 3 ) def how_many_times_top_10 ( top_10_df , player , year ): times = len ( top_10_df [( top_10_df . player == player ) & ( top_10_df . season < year )]) return times features [ 'times_in_top_10' ] = features . apply ( lambda x : how_many_times_top_10 ( brownlow_votes_top_10 , x . player , x . season ), axis = 1 ) season player team brownlow_votes yearly_rank 27 2000.0 A Koutoufides Carlton 19.0 4.0 36 2000.0 A Mcleod Adelaide 20.0 3.0 105 2000.0 B Ratten Carlton 18.0 6.0","title":"Previous Top 10 Finish Feature"},{"location":"modelling/brownlowModelTutorial/#average-brownlow-votes-per-game-last-season-feature","text":"# Create a brownlow votes lookup table brownlow_votes_lookup_table = ( brownlow_data . groupby ([ 'player' , 'team' , 'season' ], as_index = False ) . brownlow_votes . mean () . assign ( next_season = lambda df : df . season + 1 ) . rename ( columns = { 'brownlow_votes' : 'ave_votes_last_season' })) # Have a look at Cripps to check if it's working brownlow_votes_lookup_table [ brownlow_votes_lookup_table . player == 'P Cripps' ] # Merge it to our features df features_with_votes_last_season = ( pd . merge ( features , brownlow_votes_lookup_table . drop ( columns = 'season' ), left_on = [ 'player' , 'team' , 'season' ], right_on = [ 'player' , 'team' , 'next_season' ], how = 'left' ) . drop ( columns = [ 'next_season' ]) . fillna ( 0 )) player team season ave_votes_last_season next_season 4377 P Cripps Carlton 2014 0.000000 2015 4378 P Cripps Carlton 2015 0.300000 2016 4379 P Cripps Carlton 2016 0.857143 2017 4380 P Cripps Carlton 2017 0.333333 2018 4381 P Cripps Carlton 2018 0.000000 2019","title":"Average Brownlow Votes Per Game Last Season Feature"},{"location":"modelling/brownlowModelTutorial/#historic-performance-relative-to-model-feature","text":"It is well known that some players are good Brownlow performers. For whatever reason, they always poll much better than their stats may suggest. Lance Franklin and Bontempelli are probably in this category. Perhaps these players have an X-factor that Machine Learning models struggle to pick up on. To get around this, let's create a feature which looks at the player's performance relative to the model's prediction. To do this, we'll need to train and predict 7 different models - from 2011 to 2017. To create a model for each season, we will use h2o's AutoML. If you're new to h2o, please read about it here. It can be used in both R and Python. The metric we will use for loss in Mean Absolute Error (MAE). As we are using regression, some values are negative. We will convert these negative values to 0 as it doesn't make sense to poll negative brownlow votes. Similarly, some matches won't predict exactly 6 votes, so we will scale these predictions so that we predict exactly 6 votes for each match. So that you don't have to train these models yourself, I have saved the models and we will load them in. If you are keen to train the models yourself, simply uncomment out the code below and run the cell. To bulk uncomment, highlight the rows and press ctrl + '/' h2o . init () # Uncomment the code below if you want to train the models yourself - otherwise, we will load them in the load cell from disk ## Join to our features df # aml_yearly_model_objects = {} # yearly_predictions_dfs = {} # feature_cols = ['margin', 'CP', 'UP', 'ED', 'DE', # 'CM', 'GA', 'MI5', 'one_perc', 'BO', 'TOG', 'K', 'HB', 'D', 'M', 'G', # 'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', 'FF', 'FA', 'AF', 'SC'] # for year in range(2011, 2018): # # Filter the data to only include past data # train_historic = brownlow_data[brownlow_data.season < year].copy() # # Convert to an h2o frame # train_h2o_historic = h2o.H2OFrame(train_historic) # # Create an AutoML object # aml = H2OAutoML(max_runtime_secs=30, # balance_classes=True, # seed=42) # # Train the model # aml.train(y='brownlow_votes', x=feature_cols, training_frame=train_h2o_historic) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/yearly_model_{year}') # # Append the best model to a list # aml_yearly_model_objects[year] = aml.leader # # Make predictions on test set for that year # test_historic = brownlow_data[brownlow_data.season == year].copy() # test_h2o_historic = h2o.H2OFrame(test_historic) # preds = aml.predict(test_h2o_historic).as_data_frame() # test_historic['predicted_votes'] = preds.values # # Convert negative predictions to 0 # test_historic['predicted_votes_neg_to_0'] = test_historic.predicted_votes.apply(lambda x: 0 if x < 0 else x) # # Create a total match votes column - which calculates the number of votes predicted in each game when the predictions # # are unscaled # test_historic['unscaled_match_votes'] = test_historic.groupby('match_id').predicted_votes_neg_to_0.transform('sum') # # Scale predictions # test_historic['predicted_votes_scaled'] = test_historic.predicted_votes_neg_to_0 / test_historic.unscaled_match_votes * 6 # # Aggregate the predictions # test_grouped = (test_historic.groupby(['player', 'team'], as_index=False) # .sum() # .sort_values(by='brownlow_votes', ascending=False) # .assign(mae=lambda df: abs(df.predicted_votes_scaled - df.brownlow_votes))) # test_grouped['error'] = test_grouped.predicted_votes_scaled - test_grouped.brownlow_votes # test_grouped['next_year'] = year + 1 # # Add this years predictions df to a dictionary to use later # yearly_predictions_dfs[year] = test_grouped # preds_errors = None # for key, value in yearly_predictions_dfs.items(): # if preds_errors is None: # preds_errors = value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']] # else: # preds_errors = preds_errors.append(value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']], sort=True) # with open('data/prediction_errors_df.pickle', 'wb') as handle: # pickle.dump(yearly_predicted_errors, handle) Checking whether there is an H2O instance running at http://localhost:54321 . connected. H2O cluster uptime: 1 hour 28 mins H2O cluster timezone: Australia/Hobart H2O data parsing timezone: UTC H2O cluster version: 3.20.0.4 H2O cluster version age: 1 month and 18 days H2O cluster name: H2O_from_python_WardJ_tt2ak5 H2O cluster total nodes: 1 H2O cluster free memory: 7.018 Gb H2O cluster total cores: 4 H2O cluster allowed cores: 4 H2O cluster status: locked, healthy H2O connection url: http://localhost:54321 H2O connection proxy: None H2O internal security: False H2O API Extensions: Algos, AutoML, Core V3, Core V4 Python version: 3.6.4 final # Load predictions error df with open ( 'data/prediction_errors_df.pickle' , 'rb' ) as handle : preds_errors = pickle . load ( handle ) # Look at last years predictions preds_errors . query ( 'next_year == 2018' ) . sort_values ( by = 'brownlow_votes' , ascending = False ) . head ( 20 ) brownlow_votes error next_year player predicted_votes_scaled season 139 36.0 -4.915157 2018 D Martin 31.084843 44374 486 33.0 -4.413780 2018 P Dangerfield 28.586220 42357 619 25.0 0.072223 2018 T Mitchell 25.072223 44374 279 23.0 -9.296209 2018 J Kennedy 13.703791 38323 376 22.0 -7.621336 2018 L Franklin 14.378664 44374 278 21.0 -3.016176 2018 J Kelly 17.983824 42357 519 20.0 -4.182915 2018 R Sloane 15.817085 44374 410 19.0 -7.740142 2018 M Bontempelli 11.259858 44374 483 18.0 -4.522835 2018 O Wines 13.477165 44374 121 17.0 -4.446749 2018 D Beams 12.553251 38323 390 16.0 -3.620094 2018 L Parker 12.379906 44374 561 15.0 -5.846506 2018 S Pendlebury 9.153494 32272 463 15.0 -4.402600 2018 N Fyfe 10.597400 42357 42 15.0 -4.787914 2018 B Ebert 10.212086 44374 651 15.0 0.824510 2018 Z Merrett 15.824510 42357 578 14.0 2.300607 2018 T Adams 16.300607 44374 34 14.0 -5.796604 2018 B Brown 8.203396 44374 172 14.0 -1.650752 2018 D Zorko 12.349248 42357 184 14.0 -1.233663 2018 G Ablett 12.766337 28238 389 14.0 -2.099613 2018 L Neale 11.900387 42357 Look at that! A simply Machine Learning ensemble model, using AutoML predicted last year's winner! That's impressive. As we can see it also predicted Bontempelli would only score 11.26, when he actually scored 19 - a huge discrepency. Let's use this as a feature. features_with_historic_perf_relative_to_model = \\ ( features_with_votes_last_season . pipe ( pd . merge , preds_errors [[ 'player' , 'next_year' , 'error' ]], left_on = [ 'player' , 'season' ], right_on = [ 'player' , 'next_year' ], how = 'left' ) . fillna ( 0 ) . rename ( columns = { 'error' : 'error_last_season' }) . drop_duplicates ( subset = [ 'player' , 'round' , 'SC' ]))","title":"Historic Performance Relative To Model Feature"},{"location":"modelling/brownlowModelTutorial/#filtering-the-data-to-only-include-the-top-20-sc-for-each-match","text":"Logically, it is extremely unlikely that a player will poll votes if their Supercoach score is not in the top 20 players. By eliminating the other 20+ players, we can reduce the noise in the data, as we are almost certain the players won't poll from the bottom half. Let's explore how many players poll if they're not in the top 20, and then filter our df if this number is not significant. # Find number of players who vote when in top 15 SC brownlow_data [ 'SC_rank_match' ] = brownlow_data . groupby ( 'match_id' ) . SC . rank ( method = 'max' , ascending = False ) brownlow_data . query ( 'SC_rank_match > 20 and season > 2014' ) . brownlow_votes . value_counts () 0.0 18330 1.0 14 2.0 8 3.0 2 Name : brownlow_votes , dtype : int64 Since 2014, there have only been 24 players who have voted and not been in the top 20 SC. features_with_sc_rank = features_with_historic_perf_relative_to_model . copy () features_with_sc_rank [ 'SC_rank_match' ] = features_with_sc_rank . groupby ( 'match_id' ) . SC . rank ( method = 'max' , ascending = False ) # Filter out rows with a SC rank of below 20 features_with_sc_rank_filtered = features_with_sc_rank . query ( 'SC_rank_match <= 20' ) # Filter out 2010 and 2011 as we used these seasons to create historic model performance features features_last_before_train = features_with_sc_rank_filtered . query ( 'season != 2010 and season != 2011' ) . reset_index ( drop = True ) features_last_before_train . head ( 3 ) date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC kicked_a_bag is_captain team_won got_30_possies_2_goals times_in_top_10 ave_votes_last_season next_year error_last_season SC_rank_match 0 2012-03-31 2012 1 Metricon Stadium 11985 5347.0 I Callinan 37 Adelaide Gold Coast Away 137 68 69 0.0 0.031359 0.018947 0.016071 0.015745 0.0 0.000000 0.12 0.000000 0.000000 0.023333 0.031674 0.012270 0.023438 0.026042 0.068966 0.217391 0.041322 0.0 0.017241 0.013699 0.042553 0.000000 0.000000 0.032258 0.030593 0.026962 0 0 1 0 0 0.000000 2012.0 0.042048 14.0 1 2012-03-31 2012 1 Metricon Stadium 11700 5347.0 P Dangerfield 32 Adelaide Gold Coast Away 137 68 69 0.0 0.048780 0.023158 0.037500 0.026451 0.0 0.000000 0.04 0.009804 0.090909 0.025556 0.029412 0.036810 0.032552 0.015625 0.068966 0.000000 0.016529 0.0 0.051724 0.123288 0.042553 0.000000 0.000000 0.032258 0.027503 0.028173 0 0 1 0 0 0.333333 2012.0 -2.983636 12.0 2 2012-03-31 2012 1 Metricon Stadium 2381 5347.0 R Douglas 26 Adelaide Gold Coast Away 137 68 69 0.0 0.020906 0.033684 0.021429 0.019901 0.0 0.083333 0.04 0.049020 0.000000 0.023056 0.031674 0.015337 0.024740 0.031250 0.034483 0.130435 0.057851 0.0 0.043103 0.000000 0.031915 0.014493 0.064516 0.000000 0.033684 0.030597 0 0 1 0 0 0.000000 2012.0 1.724915 10.0","title":"Filtering the data to only include the top 20 SC for each match"},{"location":"modelling/brownlowModelTutorial/#modeling-the-2017-brownlow","text":"Now that we have all of our features, we can simply create a training set (2012-2016), and a test set (2017), and make our predictions for last year! We will use AutoML for this process again. Again, rather than waiting for the model to train, I will save the model so you can simply load it in. We will also scale our features. We can then see how our model went in predicting last year's brownlow, creating a baseline for this years' predictions. We will then predict this year's vote count. train_baseline = features_last_before_train . query ( \"season < 2017\" ) holdout = features_last_before_train . query ( \"season == 2017\" ) scale_cols = [ 'team_score' , 'opposition_score' , 'margin' , 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] other_feature_cols = [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' , 'times_in_top_10' , 'ave_votes_last_season' , 'error_last_season' , 'SC_rank_match' ] all_feature_cols = scale_cols + other_feature_cols # Scale features scaler = StandardScaler () train_baseline_scaled = train_baseline . copy () train_baseline_scaled [ scale_cols ] = scaler . fit_transform ( train_baseline [ scale_cols ]) holdout_scaled = holdout . copy () holdout_scaled [ scale_cols ] = scaler . transform ( holdout [ scale_cols ]) # Convert categorical columns to categoricals train_baseline_h2o = h2o . H2OFrame ( train_baseline_scaled ) holdout_h2o = h2o . H2OFrame ( holdout_scaled ) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_baseline_h2o [ col ] = train_baseline_h2o [ col ] . asfactor () holdout_h2o [ col ] = holdout_h2o [ col ] . asfactor () C:\\Users\\wardj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1] Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Below I have commented out training and saving the 2017 model. Rather than training it again, we will just load it in. Uncomment this part out if you want to train it yourself. # aml_2017_model = H2OAutoML(max_runtime_secs = 60*3, # balance_classes=True, # seed=42) # aml_2017_model.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_baseline_h2o) # save the model # model_path = h2o.save_model(model=aml_2017_model.leader, path=\"models\", force=True) # Get model id # model_name = aml_2017_model.leaderboard[0, 'model_id'] # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2017_model_v1') # Load model in aml_2017_model = h2o . load_model ( 'models/brownlow_2017_model_v1' ) # Predict the 2017 brownlow count preds_final_2017_model = aml_2017_model . predict ( holdout_h2o ) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total holdout = ( holdout . assign ( predicted_votes = preds_final_2017_model . as_data_frame () . values ) . assign ( predicted_votes_neg_to_0 = lambda df : df . predicted_votes . apply ( lambda x : 0 if x < 0 else x )) . assign ( unscaled_match_votes = lambda df : df . groupby ( 'match_id' ) . predicted_votes_neg_to_0 . transform ( 'sum' )) . assign ( predicted_votes_scaled = lambda df : df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2017 = ( holdout . groupby ([ 'player' , 'team' ], as_index = False ) . agg ({ 'brownlow_votes' : sum , 'predicted_votes_scaled' : sum , 'SC' : 'mean' , 'G' : 'mean' }) . sort_values ( by = 'brownlow_votes' , ascending = False ) . assign ( mae = lambda df : abs ( df . brownlow_votes - df . predicted_votes_scaled )) . reset_index ( drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% agg_predictions_2017 . head ( 15 ) player team brownlow_votes predicted_votes_scaled SC G mae 0 D Martin Richmond 36.0 37.271060 0.037862 0.064869 1.271060 1 P Dangerfield Geelong 33.0 39.288122 0.042441 0.070819 6.288122 2 T Mitchell Hawthorn 25.0 28.629859 0.036040 0.016928 3.629859 3 L Franklin Sydney 22.0 16.353733 0.034640 0.149203 5.646267 4 J Kelly GWS 21.0 19.565321 0.034652 0.033772 1.434679 5 R Sloane Adelaide 20.0 21.417347 0.037068 0.034821 1.417347 6 J Kennedy Sydney 20.0 13.671891 0.032014 0.030508 6.328109 7 M Bontempelli Western Bulldogs 19.0 17.461889 0.033233 0.040498 1.538111 8 D Beams Brisbane 17.0 15.414730 0.034848 0.044998 1.585270 9 O Wines Port Adelaide 16.0 12.973973 0.031601 0.021967 3.026027 10 N Fyfe Fremantle 15.0 11.926030 0.033761 0.031680 3.073970 11 S Pendlebury Collingwood 15.0 10.845214 0.033855 0.013660 4.154786 12 B Ebert Port Adelaide 15.0 7.633526 0.032795 0.008431 7.366474 13 L Parker Sydney 15.0 14.680079 0.031366 0.030311 0.319921 14 Z Merrett Essendon 15.0 21.063889 0.033737 0.015362 6.063889 So whilst our model predicted Dangerfield to win, it was pretty damn accurate! Let's find the MAE for the top 100, 50, 25, and 10, and then compare it to 2018's MAE in week, when the Brownlow has been counted. for top_x in [ 10 , 25 , 50 , 100 ]: temp_mae = round ( agg_predictions_2017 . iloc [: top_x ] . mae . mean (), 3 ) print ( f \"The Average Mean Absolute Error for the top { top_x } is { temp_mae } \" ) The Average Mean Absolute Error for the top 10 is 3.216 The Average Mean Absolute Error for the top 25 is 2.931 The Average Mean Absolute Error for the top 50 is 3.15 The Average Mean Absolute Error for the top 100 is 2.577","title":"Modeling The 2017 Brownlow"},{"location":"modelling/brownlowModelTutorial/#modelling-this-years-brownlow","text":"Let's now predict this year's vote count. These predictions will be on the front page of the GitHub. train = features_last_before_train . query ( \"season < 2018\" ) test = features_last_before_train . query ( \"season == 2018\" ) # Scale features scaler = StandardScaler () train_scaled = train . copy () train_scaled [ scale_cols ] = scaler . fit_transform ( train [ scale_cols ]) test_scaled = test . copy () test_scaled [ scale_cols ] = scaler . transform ( test [ scale_cols ]) # Convert categorical columns to categoricals train_h2o = h2o . H2OFrame ( train_scaled ) test_h2o = h2o . H2OFrame ( test_scaled ) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_h2o [ col ] = train_h2o [ col ] . asfactor () test_h2o [ col ] = test_h2o [ col ] . asfactor () C:\\Users\\wardj\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1] Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% # Train the model - this part is commented out as we will just load our model from disk # aml = H2OAutoML(max_runtime_secs = 60*3, # balance_classes=True, # seed=42) # aml.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_h2o) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2018_model_v1') # Load model in aml = h2o . load_model ( 'models/brownlow_2018_model_v1' ) # Predict the 2018 brownlow count preds_final_2018_model = aml . predict ( test_h2o ) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total test = ( test . assign ( predicted_votes = preds_final_2018_model . as_data_frame () . values ) . assign ( predicted_votes_neg_to_0 = lambda df : df . predicted_votes . apply ( lambda x : 0 if x < 0 else x )) . assign ( unscaled_match_votes = lambda df : df . groupby ( 'match_id' ) . predicted_votes_neg_to_0 . transform ( 'sum' )) . assign ( predicted_votes_scaled = lambda df : df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2018 = ( test . groupby ([ 'player' , 'team' ], as_index = False ) . agg ({ 'predicted_votes_scaled' : sum , 'match_id' : 'count' }) # shows how many games they played . sort_values ( by = 'predicted_votes_scaled' , ascending = False ) . reset_index ( drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100% # Show the top 25 predictions agg_predictions_2018 . head ( 25 ) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 35.484614 20 1 M Gawn Melbourne 21.544278 22 2 D Martin Richmond 20.444488 19 3 B Grundy Collingwood 19.543511 22 4 C Oliver Melbourne 19.009628 20 5 J Macrae Western Bulldogs 18.931594 17 6 P Dangerfield Geelong 18.621242 21 7 D Beams Brisbane 17.621222 15 8 E Yeo West Coast 16.015638 20 9 L Neale Fremantle 15.495083 21 10 A Gaff West Coast 15.165629 18 11 D Heppell Essendon 15.083797 19 12 J Selwood Geelong 14.989096 18 13 S Sidebottom Collingwood 14.863136 18 14 N Fyfe Fremantle 14.692243 11 15 J Kennedy Sydney 14.404489 16 16 Z Merrett Essendon 13.632131 18 17 M Crouch Adelaide 13.503858 16 18 R Laird Adelaide 13.274869 19 19 P Cripps Carlton 13.240568 21 20 G Ablett Geelong 13.018950 15 21 L Franklin Sydney 12.792476 13 22 J Lloyd Sydney 12.174224 20 23 J Kelly GWS 11.982157 14 24 C Ward GWS 11.892443 19 print ( agg_predictions_2018 . head ( 15 )) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 35.484614 1 M Gawn Melbourne 21.544278 2 D Martin Richmond 20.444488 3 B Grundy Collingwood 19.543511 4 C Oliver Melbourne 19.009628 5 J Macrae Western Bulldogs 18.931594 6 P Dangerfield Geelong 18.621242 7 D Beams Brisbane 17.621222 8 E Yeo West Coast 16.015638 9 L Neale Fremantle 15.495083 10 A Gaff West Coast 15.165629 11 D Heppell Essendon 15.083797 12 J Selwood Geelong 14.989096 13 S Sidebottom Collingwood 14.863136 14 N Fyfe Fremantle 14.692243 Now that we have the top 25, let's also look at the top 3 from each team. agg_predictions_2018 . sort_values ( by = [ 'team' , 'predicted_votes_scaled' ], ascending = [ True , False ]) . groupby ( 'team' ) . head ( 3 ) player team predicted_votes_scaled match_id 17 M Crouch Adelaide 13.503858 16 18 R Laird Adelaide 13.274869 19 51 B Gibbs Adelaide 7.783425 18 7 D Beams Brisbane 17.621222 15 46 D Zorko Brisbane 8.123915 14 55 S Martin Brisbane 7.243428 19 19 P Cripps Carlton 13.240568 21 50 K Simpson Carlton 7.864995 18 88 E Curnow Carlton 3.725292 19 3 B Grundy Collingwood 19.543511 22 13 S Sidebottom Collingwood 14.863136 18 31 A Treloar Collingwood 10.487535 11 11 D Heppell Essendon 15.083797 19 16 Z Merrett Essendon 13.632131 18 57 D Smith Essendon 6.838857 20 9 L Neale Fremantle 15.495083 21 14 N Fyfe Fremantle 14.692243 11 74 M Walters Fremantle 5.120949 12 23 J Kelly GWS 11.982157 14 24 C Ward GWS 11.892443 19 25 S Coniglio GWS 11.785273 20 6 P Dangerfield Geelong 18.621242 21 12 J Selwood Geelong 14.989096 18 20 G Ablett Geelong 13.018950 15 80 J Witts Gold Coast 4.617273 13 85 J Lyons Gold Coast 4.043711 14 114 B Fiorini Gold Coast 2.798683 6 0 T Mitchell Hawthorn 35.484614 20 39 L Breust Hawthorn 9.286521 16 45 J Gunston Hawthorn 8.341824 19 1 M Gawn Melbourne 21.544278 22 4 C Oliver Melbourne 19.009628 20 32 J Hogan Melbourne 10.198873 13 26 S Higgins North Melbourne 11.327596 19 37 B Brown North Melbourne 9.550205 13 42 B Cunnington North Melbourne 8.857294 17 27 O Wines Port Adelaide 11.118795 16 36 R Gray Port Adelaide 9.606960 17 54 J Westhoff Port Adelaide 7.355449 21 2 D Martin Richmond 20.444488 19 30 J Riewoldt Richmond 10.557749 15 59 T Cotchin Richmond 6.741732 12 41 S Ross St Kilda 9.093395 17 48 J Steven St Kilda 8.099164 17 90 J Steele St Kilda 3.617769 16 15 J Kennedy Sydney 14.404489 16 21 L Franklin Sydney 12.792476 13 22 J Lloyd Sydney 12.174224 20 8 E Yeo West Coast 16.015638 20 10 A Gaff West Coast 15.165629 18 38 J Redden West Coast 9.368163 16 5 J Macrae Western Bulldogs 18.931594 17 40 M Bontempelli Western Bulldogs 9.272771 16 44 L Hunter Western Bulldogs 8.349606 17 If you're looking for a round by round breakdown, just have a look at the test dataframe. test [[ 'date' , 'round' , 'player' , 'team' , 'opposition' , 'margin' , 'SC' , 'predicted_votes_scaled' ]] . tail ( 25 ) date round player team opposition margin SC predicted_votes_scaled 27231 2018-08-26 23 R Lobb GWS Melbourne -45 0.027576 0.007501 27232 2018-08-26 23 H Perryman GWS Melbourne -45 0.025758 0.000000 27233 2018-08-26 23 D Shiel GWS Melbourne -45 0.026364 0.000000 27234 2018-08-26 23 A Tomlinson GWS Melbourne -45 0.026667 0.000000 27235 2018-08-26 23 C Ward GWS Melbourne -45 0.035455 0.332057 27236 2018-08-26 23 L Austin St Kilda North Melbourne -23 0.027853 0.003928 27237 2018-08-26 23 J Geary St Kilda North Melbourne -23 0.025129 0.000000 27238 2018-08-26 23 S Gilbert St Kilda North Melbourne -23 0.025734 0.000000 27239 2018-08-26 23 R Marshall St Kilda North Melbourne -23 0.028156 0.103389 27240 2018-08-26 23 B Paton St Kilda North Melbourne -23 0.023918 0.000000 27241 2018-08-26 23 S Ross St Kilda North Melbourne -23 0.039055 0.300392 27242 2018-08-26 23 J Steele St Kilda North Melbourne -23 0.042386 1.094723 27243 2018-08-26 23 J Steven St Kilda North Melbourne -23 0.046624 1.119684 27244 2018-08-26 23 R Clarke North Melbourne St Kilda 23 0.021193 0.050859 27245 2018-08-26 23 B Cunnington North Melbourne St Kilda 23 0.032698 0.212364 27246 2018-08-26 23 M Daw North Melbourne St Kilda 23 0.022404 0.021889 27247 2018-08-26 23 T Dumont North Melbourne St Kilda 23 0.049046 2.042550 27248 2018-08-26 23 T Goldstein North Melbourne St Kilda 23 0.037844 0.347199 27249 2018-08-26 23 S Higgins North Melbourne St Kilda 23 0.039358 0.508419 27250 2018-08-26 23 N Hrovat North Melbourne St Kilda 23 0.025129 0.012548 27251 2018-08-26 23 J Macmillan North Melbourne St Kilda 23 0.024220 0.000000 27252 2018-08-26 23 J Waite North Melbourne St Kilda 23 0.025734 0.057826 27253 2018-08-26 23 M Williams North Melbourne St Kilda 23 0.023312 0.000000 27254 2018-08-26 23 S Wright North Melbourne St Kilda 23 0.029064 0.093219 27255 2018-08-26 23 J Ziebell North Melbourne St Kilda 23 0.033000 0.031010 And there we have it! In a single notebook we have made a fairly good Brownlow predictive model. Enjoy.","title":"Modelling This Year's Brownlow"},{"location":"modelling/brownlowModelTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/howToModel/","text":"Intro to modelling \u00b6 Want to learn how to create your own predictive model using sports or racing data, but you don\u2019t know where to start? We\u2019re here to help. The Data Scientists at Betfair have put together the first few steps we suggest you take to get you started on your data modelling journey. We also run occasional data modelling workshops to help you get the basics down \u2013 reach out and let us know if you\u2019re interested in being notified about upcoming data events. Choose your language \u00b6 There are lots of programming languages to choose from. For our data modelling workshops we work in R and Python, as they\u2019re both relatively easy to learn and designed for working with data. If you\u2019re new to these languages, here are some resources that will help get you set up. Language 1: R \u00b6 What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Take a look at the some of the existing R libraries you can use if you want to connect to our API, including abettor and our Data Scientists\u2019 R repo . Language 2: Python \u00b6 What is Python? Download and install Anaconda Distribution \u2013 this will install Python and a heap of data science packages along with it Find a data source \u00b6 Finding quality data is crucial to being able to create a successful model. We have lots of historical Exchange data that we\u2019re happy to share, and there are lots of other sources of sports or racing specific data available online, depending on what you\u2019re looking for. For our workshops we use historical NBA odds data from the Exchange ( which you can download directly from here , along with NBA game data from a variety of sources including: ESPN.com NBA.com basketball-reference.com Stattleship\u2019s API Learn to Program \u00b6 Okay, so easier said than done, but you don't actually need a high level of programming knowledge to be able to build a decent model, and there are so many excellent resources available online that the barrier to entry is much lower than it's been in the past. These are some of our favourites if you want to learn to use R or Python for data modelling: Dataquest \u2013 free coding resource for learning both Python and R for data science Datacamp \u2013 another popular free resource to learn both R and Python for data science Codeacademy \u2013 free online programming courses with community engagement We've also shared a R repo for connecting with our API , which might make that part of the learning process easier for you, if you go down that path. Learn how to model data \u00b6 We\u2019ve put together some articles to give you an introduction to some of the different approaches you can take to modelling data, but again there are also lots of resources available online. Here are some good places to start: Work through the modelling tutorials we've put together using AFL and soccer data This Introduction to Tennis Modelling gives a good overview of ranking-based models, regression-based models and point-based models How we used ELO and machine learning as different approaches to modelling the recent World Cup Get your hands dirty \u00b6 The best way to learn is by doing. Make sure you have a solid foundation knowledge to work from, then get excited, get your hands dirty and see what you can create! Here are a final few thoughts to help you decide where to from here: Make sure you\u2019ve got your betting basics and wagering fundamentals knowledge solid Learn about the importance of ratings and prices and get inspired by the models created by our Data Scientists Consider how you could use our API in building and automating your model Read about how successful some of our customers have been in their modelling journeys","title":"Intro to modelling"},{"location":"modelling/howToModel/#intro-to-modelling","text":"Want to learn how to create your own predictive model using sports or racing data, but you don\u2019t know where to start? We\u2019re here to help. The Data Scientists at Betfair have put together the first few steps we suggest you take to get you started on your data modelling journey. We also run occasional data modelling workshops to help you get the basics down \u2013 reach out and let us know if you\u2019re interested in being notified about upcoming data events.","title":"Intro to modelling"},{"location":"modelling/howToModel/#choose-your-language","text":"There are lots of programming languages to choose from. For our data modelling workshops we work in R and Python, as they\u2019re both relatively easy to learn and designed for working with data. If you\u2019re new to these languages, here are some resources that will help get you set up.","title":"Choose your language"},{"location":"modelling/howToModel/#language-1-r","text":"What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Take a look at the some of the existing R libraries you can use if you want to connect to our API, including abettor and our Data Scientists\u2019 R repo .","title":"Language 1: R"},{"location":"modelling/howToModel/#language-2-python","text":"What is Python? Download and install Anaconda Distribution \u2013 this will install Python and a heap of data science packages along with it","title":"Language 2: Python"},{"location":"modelling/howToModel/#find-a-data-source","text":"Finding quality data is crucial to being able to create a successful model. We have lots of historical Exchange data that we\u2019re happy to share, and there are lots of other sources of sports or racing specific data available online, depending on what you\u2019re looking for. For our workshops we use historical NBA odds data from the Exchange ( which you can download directly from here , along with NBA game data from a variety of sources including: ESPN.com NBA.com basketball-reference.com Stattleship\u2019s API","title":"Find a data source"},{"location":"modelling/howToModel/#learn-to-program","text":"Okay, so easier said than done, but you don't actually need a high level of programming knowledge to be able to build a decent model, and there are so many excellent resources available online that the barrier to entry is much lower than it's been in the past. These are some of our favourites if you want to learn to use R or Python for data modelling: Dataquest \u2013 free coding resource for learning both Python and R for data science Datacamp \u2013 another popular free resource to learn both R and Python for data science Codeacademy \u2013 free online programming courses with community engagement We've also shared a R repo for connecting with our API , which might make that part of the learning process easier for you, if you go down that path.","title":"Learn to Program"},{"location":"modelling/howToModel/#learn-how-to-model-data","text":"We\u2019ve put together some articles to give you an introduction to some of the different approaches you can take to modelling data, but again there are also lots of resources available online. Here are some good places to start: Work through the modelling tutorials we've put together using AFL and soccer data This Introduction to Tennis Modelling gives a good overview of ranking-based models, regression-based models and point-based models How we used ELO and machine learning as different approaches to modelling the recent World Cup","title":"Learn how to model data"},{"location":"modelling/howToModel/#get-your-hands-dirty","text":"The best way to learn is by doing. Make sure you have a solid foundation knowledge to work from, then get excited, get your hands dirty and see what you can create! Here are a final few thoughts to help you decide where to from here: Make sure you\u2019ve got your betting basics and wagering fundamentals knowledge solid Learn about the importance of ratings and prices and get inspired by the models created by our Data Scientists Consider how you could use our API in building and automating your model Read about how successful some of our customers have been in their modelling journeys","title":"Get your hands dirty"},{"location":"modelling/howToModelTheAusOpen/","text":"How to model the Australian Open \u00b6 Betfair\u2019s Data Scientists Team are putting together a collection of articles on How to Build a Model and submit an entry to the Betfair Aus Open Datathon . This article will outline their thought process and share their approach. Subsequent articles will be posted with code examples that outline how this approach can be put into practice. Tools for creating our model \u00b6 We will be providing a step by step tutorial in two languages \u2013 Python and R. These are the two most popular languages used in data science nowadays. Both code examples will follow identical approaches. Tournament structure \u00b6 The Datathon structure requires contestants to predict every possible tournament match-up only using data that is available at the start of the tournament. This means we can\u2019t use information from previous rounds (data from Round 1 matches for potential Round 2 matches and so on). For example, if we were to just train our model on all the tennis matches in the data set, our model would have been trained assuming that it had the result from previous rounds in the Australian Open. But this isn\u2019t the case, so we need to account for this nuance of the competition. We need to ensure that we don\u2019t include previous round data from the same tournament in the way we structure our features for predicting results. How to set up data and features \u00b6 In predictive modelling language \u2013 features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options Training the model We can train the model on every tennis match in the data set or We can only train the model on Australian Open matches Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. In the end, we decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface \u2013 hard court. Next decision is to decide the features (or the metrics we feed into the model, which makes the decision on who the winner is going to be). We don\u2019t have a definitive list of features that we will use, but we will most likely keep the number of features quite low (between 4-5). Features set Likely features may include: ELO First serve % Winners-unforced error ratio We will also use the difference between opponents' statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. This will reduce the dimensionality of the model. A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, the difference in ELO rating etc. Target variable \u00b6 Our target variable (what we are trying to predict) is whether player A wins or not against player B. In machine learning terms this is a classification problem. The output will be a probability number between 0 and 1. A number closer to 0 means Player B is likely to win, and a number closer to 1 will mean Player A is likely to win. Another positive of a probabilistic outcome is that they can easily be converted to odds, and can also be compared with the historical Betfair odds that have been provided, and test if our model would have been profitable for previous seasons. Sports modelling nuances \u00b6 Sports data is inherently complex to model. Generally when predicting something, like \u201cwill it rain today\u201d, you have information for that day, such as the temperature, which you can use in formulating your prediction. However with sports data, you cannot use the majority of information that is provided in the raw dataset, such as aces, winners, etc, as this will create what is called feature leakage \u2013 using data from after the event, which you won\u2019t have access to before the event, to predict the result. You will also need to use historic results in such a way that will have predictive power for the sports event that you are trying to predict. This means that you run into little nuances like needing to use rolling averages, as well as whether to model each match on a single row or multiple rows. In the next article in this series, we will show you how we tackle this problem using code examples that anyone can replicate. Disclaimer \u00b6 Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"How to model the Australian Open"},{"location":"modelling/howToModelTheAusOpen/#how-to-model-the-australian-open","text":"Betfair\u2019s Data Scientists Team are putting together a collection of articles on How to Build a Model and submit an entry to the Betfair Aus Open Datathon . This article will outline their thought process and share their approach. Subsequent articles will be posted with code examples that outline how this approach can be put into practice.","title":"How to model the Australian Open"},{"location":"modelling/howToModelTheAusOpen/#tools-for-creating-our-model","text":"We will be providing a step by step tutorial in two languages \u2013 Python and R. These are the two most popular languages used in data science nowadays. Both code examples will follow identical approaches.","title":"Tools for creating our model"},{"location":"modelling/howToModelTheAusOpen/#tournament-structure","text":"The Datathon structure requires contestants to predict every possible tournament match-up only using data that is available at the start of the tournament. This means we can\u2019t use information from previous rounds (data from Round 1 matches for potential Round 2 matches and so on). For example, if we were to just train our model on all the tennis matches in the data set, our model would have been trained assuming that it had the result from previous rounds in the Australian Open. But this isn\u2019t the case, so we need to account for this nuance of the competition. We need to ensure that we don\u2019t include previous round data from the same tournament in the way we structure our features for predicting results.","title":"Tournament structure"},{"location":"modelling/howToModelTheAusOpen/#how-to-set-up-data-and-features","text":"In predictive modelling language \u2013 features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options Training the model We can train the model on every tennis match in the data set or We can only train the model on Australian Open matches Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. In the end, we decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface \u2013 hard court. Next decision is to decide the features (or the metrics we feed into the model, which makes the decision on who the winner is going to be). We don\u2019t have a definitive list of features that we will use, but we will most likely keep the number of features quite low (between 4-5). Features set Likely features may include: ELO First serve % Winners-unforced error ratio We will also use the difference between opponents' statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. This will reduce the dimensionality of the model. A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, the difference in ELO rating etc.","title":"How to set up data and features"},{"location":"modelling/howToModelTheAusOpen/#target-variable","text":"Our target variable (what we are trying to predict) is whether player A wins or not against player B. In machine learning terms this is a classification problem. The output will be a probability number between 0 and 1. A number closer to 0 means Player B is likely to win, and a number closer to 1 will mean Player A is likely to win. Another positive of a probabilistic outcome is that they can easily be converted to odds, and can also be compared with the historical Betfair odds that have been provided, and test if our model would have been profitable for previous seasons.","title":"Target variable"},{"location":"modelling/howToModelTheAusOpen/#sports-modelling-nuances","text":"Sports data is inherently complex to model. Generally when predicting something, like \u201cwill it rain today\u201d, you have information for that day, such as the temperature, which you can use in formulating your prediction. However with sports data, you cannot use the majority of information that is provided in the raw dataset, such as aces, winners, etc, as this will create what is called feature leakage \u2013 using data from after the event, which you won\u2019t have access to before the event, to predict the result. You will also need to use historic results in such a way that will have predictive power for the sports event that you are trying to predict. This means that you run into little nuances like needing to use rolling averages, as well as whether to model each match on a single row or multiple rows. In the next article in this series, we will show you how we tackle this problem using code examples that anyone can replicate.","title":"Sports modelling nuances"},{"location":"modelling/howToModelTheAusOpen/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/","text":"Betfair is one of the only betting platforms in the world that demands winning clients. Unlike bookies, we don\u2019t ban you when you succeed. We need you, and we want you to be able to keep improving your strategies so you win more. We're here to help you in your automation journey, and this site is dedicated to sharing the tools and resources you need to succeed in this journey. Accessing our API \u00b6 As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help Historic Data \u00b6 We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data Using third party tools for automation \u00b6 Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Overview Ratings automation Market favourite automation Tipping automation Automating multiple simultaneous markets Gruss Ratings automation Market favourite automation Automating multiple simultaneous markets Cymatic Trader Ratings automation BF Bot Manager Double or Bust Data modelling \u00b6 An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL modelling series AFL modelling series Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies Inspiration & information \u00b6 The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Some extra info There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. Need extra help? \u00b6 If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au . We have a dedicated in-house resource that is here to automate your betting strategies.","title":"Index"},{"location":"thirdPartyTools/#accessing-our-api","text":"As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help","title":"Accessing our API"},{"location":"thirdPartyTools/#historic-data","text":"We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data","title":"Historic Data"},{"location":"thirdPartyTools/#using-third-party-tools-for-automation","text":"Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Overview Ratings automation Market favourite automation Tipping automation Automating multiple simultaneous markets Gruss Ratings automation Market favourite automation Automating multiple simultaneous markets Cymatic Trader Ratings automation BF Bot Manager Double or Bust","title":"Using third party tools for automation"},{"location":"thirdPartyTools/#data-modelling","text":"An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL modelling series AFL modelling series Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies","title":"Data modelling"},{"location":"thirdPartyTools/#inspiration-information","text":"The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Some extra info There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy.","title":"Inspiration &amp; information"},{"location":"thirdPartyTools/#need-extra-help","text":"If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au . We have a dedicated in-house resource that is here to automate your betting strategies.","title":"Need extra help?"},{"location":"thirdPartyTools/EloModelIntro/","text":"Bet Angel Pro: Ratings automation \u00b6 Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro \u00b6 Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the auotmation. We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Assigning multiple markets to your Excel worksheets in Bet Angel so you dont miss a race: Betfair automating simultaneous markets tutorial - Set up \u00b6 Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Downloading & formatting ratings \u00b6 Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet RunnerName refers to the entire column H in the 'RATINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds2<UserOverround, TimeTillJump2<UserTimeTillJump, ISBLANK(InPlay2)), BACKLAY, \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds3<UserOverround, TimeTillJump3<UserTimeTillJump, ISBLANK(InPlay3)), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",G9) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake)) - Connecting to Bet Angel \u00b6 Video walk through \u00b6 We've put together a litte video walk through to help make this process easier. - Selecting markets \u00b6 We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. - Linking the spreadsheet \u00b6 Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features \u00b6 Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells \u00b6 The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation \u00b6 Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time. - Editing the spreadsheet \u00b6 The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Bet Angel Pro: Ratings automation"},{"location":"thirdPartyTools/EloModelIntro/#bet-angel-pro-ratings-automation","text":"","title":"Bet Angel Pro: Ratings automation"},{"location":"thirdPartyTools/EloModelIntro/#automating-a-thoroughbred-ratings-strategy-across-multiple-markets-using-bet-angel-pro","text":"Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro"},{"location":"thirdPartyTools/EloModelIntro/#-the-plan","text":"Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the auotmation. We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Assigning multiple markets to your Excel worksheets in Bet Angel so you dont miss a race: Betfair automating simultaneous markets tutorial","title":"- The plan"},{"location":"thirdPartyTools/EloModelIntro/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"thirdPartyTools/EloModelIntro/#-downloading-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.","title":"- Downloading &amp; formatting ratings"},{"location":"thirdPartyTools/EloModelIntro/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/EloModelIntro/#-trigger-to-place-bet","text":"In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/EloModelIntro/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet RunnerName refers to the entire column H in the 'RATINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds2<UserOverround, TimeTillJump2<UserTimeTillJump, ISBLANK(InPlay2)), BACKLAY, \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds3<UserOverround, TimeTillJump3<UserTimeTillJump, ISBLANK(InPlay3)), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/EloModelIntro/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",G9) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake))","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/EloModelIntro/#-connecting-to-bet-angel","text":"","title":"- Connecting to Bet Angel"},{"location":"thirdPartyTools/EloModelIntro/#video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"Video walk through"},{"location":"thirdPartyTools/EloModelIntro/#-selecting-markets","text":"We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature.","title":"- Selecting markets"},{"location":"thirdPartyTools/EloModelIntro/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/EloModelIntro/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/EloModelIntro/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"thirdPartyTools/EloModelIntro/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"thirdPartyTools/EloModelIntro/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time.","title":"- Turning off bet confirmation"},{"location":"thirdPartyTools/EloModelIntro/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"thirdPartyTools/EloModelIntro/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts.","title":"Areas for improvement"},{"location":"thirdPartyTools/EloModelIntro/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/EloModelIntro/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/Kelly/","text":"Kelly Criterion staking \u00b6 Automating with Kelly staking method \u00b6 In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Bet Angel Pro and Gruss have a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and Gruss - we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 We'll be building on the Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Bet Angel Ratings tutorial Rules: here's the spreadsheet for Bet Angel we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Rules: here's the spreadsheet for Gruss Understanding how the Kelly Criterion staking strategy works Tools: Bet Angel Pro and Gruss Betting Assistant - Recapping the strategy covered in the Ratings automation tutorial \u00b6 We'll be using the same trigger strategy that's outlined in the Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N) for Bet Angel users or 'MARKET' worksheet (column S) for Gruss users. We've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here . - Set up \u00b6 Make sure you've downloaded and installed Bet Angel Pro or Gruss, and signed in. For Bet Angel users, Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. Bet Angel Gruss - Writing your rules \u00b6 We're using a customised version of the Ratings tutorial template for Bet Angel Professional and Gruss Betting Assistant to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell C6 of the 'BET ANGEL' worksheet or cell I2 of the 'MARKET' worksheet for Gruss users UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake StakeType refers to cell X1 of the \"SETTINGS' worksheet Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet. Stepping through each step: \u00b6 Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column =IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType)) And you're set! \u00b6 Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Kelly Criterion staking"},{"location":"thirdPartyTools/Kelly/#kelly-criterion-staking","text":"","title":"Kelly Criterion staking"},{"location":"thirdPartyTools/Kelly/#automating-with-kelly-staking-method","text":"In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Bet Angel Pro and Gruss have a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and Gruss - we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating with Kelly staking method"},{"location":"thirdPartyTools/Kelly/#-the-plan","text":"We'll be building on the Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Bet Angel Ratings tutorial Rules: here's the spreadsheet for Bet Angel we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Rules: here's the spreadsheet for Gruss Understanding how the Kelly Criterion staking strategy works Tools: Bet Angel Pro and Gruss Betting Assistant","title":"- The plan"},{"location":"thirdPartyTools/Kelly/#-recapping-the-strategy-covered-in-the-ratings-automation-tutorial","text":"We'll be using the same trigger strategy that's outlined in the Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N) for Bet Angel users or 'MARKET' worksheet (column S) for Gruss users. We've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here .","title":"- Recapping the strategy covered in the Ratings automation tutorial"},{"location":"thirdPartyTools/Kelly/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro or Gruss, and signed in. For Bet Angel users, Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. Bet Angel Gruss","title":"- Set up"},{"location":"thirdPartyTools/Kelly/#-writing-your-rules","text":"We're using a customised version of the Ratings tutorial template for Bet Angel Professional and Gruss Betting Assistant to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/Kelly/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell C6 of the 'BET ANGEL' worksheet or cell I2 of the 'MARKET' worksheet for Gruss users UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake StakeType refers to cell X1 of the \"SETTINGS' worksheet Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/Kelly/#stepping-through-each-step","text":"Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column =IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"Stepping through each step:"},{"location":"thirdPartyTools/Kelly/#-preparing-the-spreadsheet","text":"You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/Kelly/#and-youre-set","text":"Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off.","title":"And you're set!"},{"location":"thirdPartyTools/Kelly/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this.","title":"Areas for improvement"},{"location":"thirdPartyTools/Kelly/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/Kelly/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelKellyStake/","text":"Bet Angel Pro: Kelly Criterion staking \u00b6 Automating with Kelly staking method and Bet Angel Pro \u00b6 In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 We'll be building on the Bet Angel Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Bet Angel Ratings tutorial Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Understanding how the Kelly Criterion staking strategy works Tool: Bet Angel Pro - Recapping the strategy covered in the Bet Angel ratings automation tutorial \u00b6 We'll be using the same trigger strategy that's outlined in the Bet Angel Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . The trigger has been simplified in this tutorial and we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N). We've also added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Bet Angel ratings tutorial, we highly recommend that you do so as to understand how the concept of the bet placement trigger works. The tutorial can be found here . - Set up \u00b6 Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Writing your rules \u00b6 We're using a customised version of the Bet Angel Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell C6 of the 'BET ANGEL' worksheet StakeType refers to cell I5 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet. Stepping through each step: \u00b6 Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column =IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType)) And you're set! \u00b6 Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Kelly Criterion staking"},{"location":"thirdPartyTools/betAngelKellyStake/#bet-angel-pro-kelly-criterion-staking","text":"","title":"Bet Angel Pro: Kelly Criterion staking"},{"location":"thirdPartyTools/betAngelKellyStake/#automating-with-kelly-staking-method-and-bet-angel-pro","text":"In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating with Kelly staking method and Bet Angel Pro"},{"location":"thirdPartyTools/betAngelKellyStake/#-the-plan","text":"We'll be building on the Bet Angel Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Bet Angel Ratings tutorial Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Understanding how the Kelly Criterion staking strategy works Tool: Bet Angel Pro","title":"- The plan"},{"location":"thirdPartyTools/betAngelKellyStake/#-recapping-the-strategy-covered-in-the-bet-angel-ratings-automation-tutorial","text":"We'll be using the same trigger strategy that's outlined in the Bet Angel Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . The trigger has been simplified in this tutorial and we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N). We've also added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Bet Angel ratings tutorial, we highly recommend that you do so as to understand how the concept of the bet placement trigger works. The tutorial can be found here .","title":"- Recapping the strategy covered in the Bet Angel ratings automation tutorial"},{"location":"thirdPartyTools/betAngelKellyStake/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"thirdPartyTools/betAngelKellyStake/#-writing-your-rules","text":"We're using a customised version of the Bet Angel Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/betAngelKellyStake/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell C6 of the 'BET ANGEL' worksheet StakeType refers to cell I5 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/betAngelKellyStake/#stepping-through-each-step","text":"Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column =IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"Stepping through each step:"},{"location":"thirdPartyTools/betAngelKellyStake/#-preparing-the-spreadsheet","text":"You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/betAngelKellyStake/#and-youre-set","text":"Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelKellyStake/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this.","title":"Areas for improvement"},{"location":"thirdPartyTools/betAngelKellyStake/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/betAngelKellyStake/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/","text":"Bet Angel Pro: Market favourite automation \u00b6 Automating a market favourite strategy using Bet Angel Pro \u00b6 Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them. Building on our previous articles , we're using the spreadsheet functionality available in Bet Angel Pro to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Bet Angel Pro - Set up \u00b6 Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back runners when: the selection's available to back price (Column G) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this the scheduled event start time is less and greater than what we specify Back market percentage is less than a certain value that we choose the event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell C4 in the 'OPTIONS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets UserOverround refers to cell C3 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market MinTime refers to cell C2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell E2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market TakeSP refers to cell C5 in the 'OPTIONS' worksheet which allows you to change a single value more easily. This is our trigger on Excel formula: ``` excel tab=\"Multi line\" =IF( AND( (COUNT( G G 9, G G 11, G G 13, G G 15, G G 17, G G 19, G G 21, G G 23, G G 25, G G 27, G G 29, G G 31, G G 33, G G 35, G G 37, G G 39, G G 41, G G 43, G G 45, G G 47, G G 49, G G 51, G G 53, G G 55, G G 57, G G 59, G G 61, G G 63, G G 65, G G 67)-RANK(G9,( G G 9, G G 11, G G 13, G G 15, G G 17, G G 19, G G 21, G G 23, G G 25, G G 27, G G 29, G G 31, G G 33, G G 35, G G 37, G G 39, G G 41, G G 43, G G 45, G G 47, G G 49, G G 51, G G 53, G G 55, G G 57, G G 59, G G 61, G G 63, G G 65, G G 67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column G) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") TimeTillJump1 < MaxTime and > MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C2 and greater than cell E2 in the 'OPTIONS' worksheet (named 'MinTime' and 'MaxTime' respectively), as we need to both place the bet and then convert it to a BSP bet before the off (more on this later). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 (named 'TimeTillJump1')in the 'SETTINGS' worksheet, where we've already done the calculations for you. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") Overrounds1 < UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C3 of the 'OPTIONS' worksheet =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to G1 in the 'BET ANGEL' worksheet, if this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") Result: if the statement above is true, the formula returns 'BACK', at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, ISBLANK(InPlay2)), \"BACK\", \"\") Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, ISBLANK(InPlay2)), \"BACK\", \"\") Convert bets to Betfair Starting Price: Bet Angel Pro doesn't offer the option to place straight BSP bets, so we've got around that here by placing the bets initially at odds of 1000 (which won't get matched for short favourites), and then at certain amount of seconds from the scheduled start using what Bet Angel calls a 'Global Command' to convert all unmatched bets to BSP. The exact number of seconds can be easily changed by updating cell C5 from the 'OPTIONS' worksheet. This formula goes in cell L6 of the 'BET ANGEL' worksheet, and once it's triggered the bets will automatically convert. Remember, just like the main trigger, each worksheet's global command will need to be updated to reference its respective TimeTillJump calculation. -- 'BET ANGEL' worksheet global command formula =IF(TimeTillJump1 < TakeSP, \"TAKE_SP_ALL\", \"\") -- 'BET ANGEL 2' worksheet global command formula =IF(TimeTillJump2 < TakeSP, \"TAKE_SP_ALL\", \"\") -- 'BET ANGEL 3' worksheet global command formula =IF(TimeTillJump3 < TakeSP, \"TAKE_SP_ALL\", \"\") Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( (COUNT( G G 9, G G 11, G G 13, G G 15, G G 17, G G 19, G G 21, G G 23, G G 25, G G 27, G G 29, G G 31, G G 33, G G 35, G G 37, G G 39, G G 41, G G 43, G G 45, G G 47, G G 49, G G 51, G G 53, G G 55, G G 57, G G 59, G G 61, G G 63, G G 65, G G 67)-RANK(G9,( G G 9, G G 11, G G 13, G G 15, G G 17, G G 19, G G 21, G G 23, G G 25, G G 27, G G 29, G G 31, G G 33, G G 35, G G 37, G G 39, G G 41, G G 43, G G 45, G G 47, G G 49, G G 51, G G 53, G G 55, G G 57, G G 59, G G 61, G G 63, G G 65, G G 67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",\"1000\") Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat staking here, so will just place $10 on each runner. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",\"10\") Global Command: this is what triggers the open bets to convert to BSP, and only goes in one cell, L6. As soon as the countdown timer reaches less than 60 seconds this will fire. As mentioned above, remember to update the formula for each worksheet. =IF(TimeTillJump1 < TakeSP, \"TAKE_SP_ALL\", \"\") - You know the drill \u00b6 The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Bet Angel Pro. - Video walk through \u00b6 We've put together a litte video walk through to help make this process easier. - Selecting markets \u00b6 We used the markets menu in the Guardian tool to navigate to the tracks we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). - Linking the spreadsheet \u00b6 Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features \u00b6 Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells \u00b6 The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as my check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status and global status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation \u00b6 Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. - Editing the spreadsheet \u00b6 The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. If the market changes significantly in those last few minutes and a third selection shortens in past the two we've placed bets on you could end up with bets on more than the intended two runner. This is something you could check for in your bet rule if you wanted to ensure you were only backing a set number of runners. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Market favourite automation"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#bet-angel-pro-market-favourite-automation","text":"","title":"Bet Angel Pro: Market favourite automation"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#automating-a-market-favourite-strategy-using-bet-angel-pro","text":"Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them. Building on our previous articles , we're using the spreadsheet functionality available in Bet Angel Pro to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions.","title":"Automating a market favourite strategy using Bet Angel Pro"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-the-plan","text":"Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Bet Angel Pro","title":"- The plan"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: the selection's available to back price (Column G) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this the scheduled event start time is less and greater than what we specify Back market percentage is less than a certain value that we choose the event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell C4 in the 'OPTIONS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets UserOverround refers to cell C3 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market MinTime refers to cell C2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell E2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market TakeSP refers to cell C5 in the 'OPTIONS' worksheet which allows you to change a single value more easily. This is our trigger on Excel formula: ``` excel tab=\"Multi line\" =IF( AND( (COUNT( G G 9, G G 11, G G 13, G G 15, G G 17, G G 19, G G 21, G G 23, G G 25, G G 27, G G 29, G G 31, G G 33, G G 35, G G 37, G G 39, G G 41, G G 43, G G 45, G G 47, G G 49, G G 51, G G 53, G G 55, G G 57, G G 59, G G 61, G G 63, G G 65, G G 67)-RANK(G9,( G G 9, G G 11, G G 13, G G 15, G G 17, G G 19, G G 21, G G 23, G G 25, G G 27, G G 29, G G 31, G G 33, G G 35, G G 37, G G 39, G G 41, G G 43, G G 45, G G 47, G G 49, G G 51, G G 53, G G 55, G G 57, G G 59, G G 61, G G 63, G G 65, G G 67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column G) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") TimeTillJump1 < MaxTime and > MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C2 and greater than cell E2 in the 'OPTIONS' worksheet (named 'MinTime' and 'MaxTime' respectively), as we need to both place the bet and then convert it to a BSP bet before the off (more on this later). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 (named 'TimeTillJump1')in the 'SETTINGS' worksheet, where we've already done the calculations for you. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") Overrounds1 < UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C3 of the 'OPTIONS' worksheet =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to G1 in the 'BET ANGEL' worksheet, if this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") Result: if the statement above is true, the formula returns 'BACK', at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, ISBLANK(InPlay2)), \"BACK\", \"\") Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, ISBLANK(InPlay2)), \"BACK\", \"\") Convert bets to Betfair Starting Price: Bet Angel Pro doesn't offer the option to place straight BSP bets, so we've got around that here by placing the bets initially at odds of 1000 (which won't get matched for short favourites), and then at certain amount of seconds from the scheduled start using what Bet Angel calls a 'Global Command' to convert all unmatched bets to BSP. The exact number of seconds can be easily changed by updating cell C5 from the 'OPTIONS' worksheet. This formula goes in cell L6 of the 'BET ANGEL' worksheet, and once it's triggered the bets will automatically convert. Remember, just like the main trigger, each worksheet's global command will need to be updated to reference its respective TimeTillJump calculation. -- 'BET ANGEL' worksheet global command formula =IF(TimeTillJump1 < TakeSP, \"TAKE_SP_ALL\", \"\") -- 'BET ANGEL 2' worksheet global command formula =IF(TimeTillJump2 < TakeSP, \"TAKE_SP_ALL\", \"\") -- 'BET ANGEL 3' worksheet global command formula =IF(TimeTillJump3 < TakeSP, \"TAKE_SP_ALL\", \"\") Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( (COUNT( G G 9, G G 11, G G 13, G G 15, G G 17, G G 19, G G 21, G G 23, G G 25, G G 27, G G 29, G G 31, G G 33, G G 35, G G 37, G G 39, G G 41, G G 43, G G 45, G G 47, G G 49, G G 51, G G 53, G G 55, G G 57, G G 59, G G 61, G G 63, G G 65, G G 67)-RANK(G9,( G G 9, G G 11, G G 13, G G 15, G G 17, G G 19, G G 21, G G 23, G G 25, G G 27, G G 29, G G 31, G G 33, G G 35, G G 37, G G 39, G G 41, G G 43, G G 45, G G 47, G G 49, G G 51, G G 53, G G 55, G G 57, G G 59, G G 61, G G 63, G G 65, G G 67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",\"1000\") Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat staking here, so will just place $10 on each runner. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",\"10\") Global Command: this is what triggers the open bets to convert to BSP, and only goes in one cell, L6. As soon as the countdown timer reaches less than 60 seconds this will fire. As mentioned above, remember to update the formula for each worksheet. =IF(TimeTillJump1 < TakeSP, \"TAKE_SP_ALL\", \"\")","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-you-know-the-drill","text":"The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Bet Angel Pro.","title":"- You know the drill"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"- Video walk through"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-selecting-markets","text":"We used the markets menu in the Guardian tool to navigate to the tracks we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up).","title":"- Selecting markets"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as my check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status and global status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings.","title":"- Turning off bet confirmation"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. If the market changes significantly in those last few minutes and a third selection shortens in past the two we've placed bets on you could end up with bets on more than the intended two runner. This is something you could check for in your bet rule if you wanted to ensure you were only backing a set number of runners.","title":"Areas for improvement"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/betAngelMarketFavouriteAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelRatingsAutomation/","text":"Bet Angel Pro: Ratings automation \u00b6 Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro \u00b6 Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the automation. We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Assigning multiple markets to your Excel worksheets in Bet Angel so you don't miss a race: Betfair automating simultaneous markets tutorial - Set up \u00b6 Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Downloading & formatting ratings \u00b6 Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet SelectionID refers to the entire column G in the 'RATINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. Instead of matching the runner name from the market to that which is defined in our ratings, we're using their selection ID instead. As the Selection ID is unique to each runner, it will help us eliminate any potential errors or issues. Because the ratings from the Betfair data scientists includes Selection ID, there's no additional work or mucking around to get this implemented. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",G9) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake)) - Connecting to Bet Angel \u00b6 Video walk through \u00b6 We've put together a litte video walk through to help make this process easier. - Selecting markets \u00b6 We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. - Linking the spreadsheet \u00b6 Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features \u00b6 Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells \u00b6 The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation \u00b6 Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time. - Editing the spreadsheet \u00b6 The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Ratings automation"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#bet-angel-pro-ratings-automation","text":"","title":"Bet Angel Pro: Ratings automation"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#automating-a-thoroughbred-ratings-strategy-across-multiple-markets-using-bet-angel-pro","text":"Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-the-plan","text":"Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the automation. We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Assigning multiple markets to your Excel worksheets in Bet Angel so you don't miss a race: Betfair automating simultaneous markets tutorial","title":"- The plan"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-downloading-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.","title":"- Downloading &amp; formatting ratings"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-trigger-to-place-bet","text":"In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet SelectionID refers to the entire column G in the 'RATINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. Instead of matching the runner name from the market to that which is defined in our ratings, we're using their selection ID instead. As the Selection ID is unique to each runner, it will help us eliminate any potential errors or issues. Because the ratings from the Betfair data scientists includes Selection ID, there's no additional work or mucking around to get this implemented. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",G9) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake))","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-connecting-to-bet-angel","text":"","title":"- Connecting to Bet Angel"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"Video walk through"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-selecting-markets","text":"We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature.","title":"- Selecting markets"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time.","title":"- Turning off bet confirmation"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts.","title":"Areas for improvement"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/betAngelRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/","text":"Bet Angel Pro: Automating simultaneous markets \u00b6 Dont miss out on a market with simultaneous automation \u00b6 If you have a concern of missing markets due to delays or unforeseen circumstances, Bet Angel is able to work off multiple worksheets for different meetings, all from the same workbook. For example, if we are wanting to automate markets at 1.45pm at Swan Hill, 1.55pm at Townsville and 2.02pm at Menangle, but the Swan Hill market is delayed, we don't want to miss out on the Townsville or Menangle markets. Click Here to download the spreadsheet that we have edited which will allow you to bet on multiple markets To set this up, we need to make sure that there are enough instances of the \u2018Bet Angel\u2019 worksheet to cover markets in the event of a delay or on track issue. In this context, three worksheets should be enough to cover the days markets if some of them are delayed for whatever reason. We've created a special Excel file with macros that will allow up to three markets to be linked at the same time. These macros automatically clear the contents of each sheet when they detect a market change and clear any errors that may occur. If you want to have more worksheets to be linked to different markets, you will need to update the macros accordingly so that they all work independently from one another. Simultaneous markets spreadsheet Please note that the above edited excel workbook does not include any automated strategies. You will need to add this in yourself or take a look at our Multiple market ratings tutorial Once this is done, save the file and close out of it completely (Bet Angel will open it back up for us automatically when we\u2019re ready to start our automation). In Bet Angel, follow the usual process of clicking the \u2018Guardian\u2019 Icon, select your markets and the usual \u2018Browse for file\u2019 button to locate your Excel file. The only thing you will need to do differently for Bet Angel is to simply allocate a specific worksheet to a particular market that you are betting on. As each market concludes, the assigned worksheet will then reset and then re-assign itself to the next market in the guaradian list. When you\u2019re ready for automation to take over, click the \u2018Connect\u2019 check box and then it will do its thing. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Simultaneous markets"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#bet-angel-pro-automating-simultaneous-markets","text":"","title":"Bet Angel Pro: Automating simultaneous markets"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#dont-miss-out-on-a-market-with-simultaneous-automation","text":"If you have a concern of missing markets due to delays or unforeseen circumstances, Bet Angel is able to work off multiple worksheets for different meetings, all from the same workbook. For example, if we are wanting to automate markets at 1.45pm at Swan Hill, 1.55pm at Townsville and 2.02pm at Menangle, but the Swan Hill market is delayed, we don't want to miss out on the Townsville or Menangle markets. Click Here to download the spreadsheet that we have edited which will allow you to bet on multiple markets To set this up, we need to make sure that there are enough instances of the \u2018Bet Angel\u2019 worksheet to cover markets in the event of a delay or on track issue. In this context, three worksheets should be enough to cover the days markets if some of them are delayed for whatever reason. We've created a special Excel file with macros that will allow up to three markets to be linked at the same time. These macros automatically clear the contents of each sheet when they detect a market change and clear any errors that may occur. If you want to have more worksheets to be linked to different markets, you will need to update the macros accordingly so that they all work independently from one another. Simultaneous markets spreadsheet Please note that the above edited excel workbook does not include any automated strategies. You will need to add this in yourself or take a look at our Multiple market ratings tutorial Once this is done, save the file and close out of it completely (Bet Angel will open it back up for us automatically when we\u2019re ready to start our automation). In Bet Angel, follow the usual process of clicking the \u2018Guardian\u2019 Icon, select your markets and the usual \u2018Browse for file\u2019 button to locate your Excel file. The only thing you will need to do differently for Bet Angel is to simply allocate a specific worksheet to a particular market that you are betting on. As each market concludes, the assigned worksheet will then reset and then re-assign itself to the next market in the guaradian list. When you\u2019re ready for automation to take over, click the \u2018Connect\u2019 check box and then it will do its thing.","title":"Dont miss out on a market with simultaneous automation"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/betAngelSimultaneousMarkets/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelTippingAutomation/","text":"Bet Angel Pro: Tipping automation \u00b6 Automating a (non-ratings based) tipping strategy using Bet Angel Pro \u00b6 We all love getting some good racing tips, but who has time to sit and place bets all day? Wouldn't it be easier if you could take those tips and get a program to automatically place the bets on your behalf? This is what we're going to explore here - we'll be using Bet Angel Pro to place bets automatically based on a set of tips. This is our first-time using Bet Angel for this approach and we are very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 We have a set of tips that we've taken from our DataScientists' Racing Prediction Model, but this approach should work for any set of tips you may have. Our goal is to create an automated process which will let us choose our tips for the day, then walk away and the program do the leg work. Here we'll step through how we went about getting Bet Angel Pro to place bets on the favourite runner identified by Betfair's Data Scientists . There are no ratings associated with these tips, so we're happy to take Betfair's Starting Price instead of a price for these bets. If you want to follow along and try this approach yourself you'll need to download Bet Angel Pro and sign up for either a subscription or at least a test period. They have a 14 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Tips: Betfair Data Scientists' Racing Prediction Model Rules: here's the spreadsheet , we used to automate our tips but you may need to tweak it a bit to suit your own tips. Tool: Bet Angel Pro - Set up \u00b6 First up we need to make sure we've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our tips and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back or lay runners when: The runners name has been specified in our tipping list Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial RunnerName refers to the entire column A in the 'TIP' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H2 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BackStake refers to cell H3 in the 'SETTINGS' worksheet and like the name suggests, will be the stake for any back bets that are triggered LayStake refers to cell H4 in the 'SETTINGS' worksheet and will be the stake for any lay bets that are triggered BetType is the entire B column in the 'TIP' worksheet. Depending on your tip for each runner, you can choose whether you want a back or lay bet to be triggered for that runner This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) ``` excel tab=\"Single line\" =IF(AND(COUNTIF(RunnerName,B9)>0,TimeTillJump1<UserTimeTillJump,Overrounds1<UserOverround,ISBLANK(InPlay1)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\") Stepping through each step: Checking whether the runner is to have a bet placed: Here the trigger is checking the list of runners in the 'TIPS' worksheet so it can decide whether we want a bet to be placed or not. If the name does appear in our list, then it returns a TRUE flag and continues with the next trigger condition. If the name is not in the list, then no bet will be placed. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell H2 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E9 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Result: if the statement above is true, check whether BACK or LAY has been selected in column B of the 'TIPS' worksheet for that runner. Whatever has been specified, trigger that bet type. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump2<UserTimeTillJump, Overrounds2<UserOverround, ISBLANK(InPlay2)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump3<UserTimeTillJump, Overrounds3<UserOverround, ISBLANK(InPlay3)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false [COUNTIF function:] Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump3<UserTimeTillJump, Overrounds3<UserOverround, ISBLANK(InPlay3)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) ``` excel tab=\"Single line\" =IF(AND(COUNTIF(RunnerName,B9)>0,TimeTillJump3<UserTimeTillJump,Overrounds3<UserOverround,ISBLANK(InPlay3)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\") Odds: Because we will be taking the BSP, we want to ensure that the initial bets that we place are not matched so that the \"TAKE_SP_ALL\" command can trigger for the global command. To do this, it checks the bet type for that particular runner. If it is Backing, then place odds at 1000 and if its going to be a lay bet, then set odds at 1.01 Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR which we're also using, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"LAY\",1.01,1000)),\"\") Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are using seperate variables for a back bet and lay bet. These variables can be easily changed from the 'SETTINGS' tab. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"BACK\", BackStake,LayStake)),\"\") - Connecting to Bet Angel \u00b6 Video walk through \u00b6 We've put together a litte video walk through to help make this process easier. - Selecting markets \u00b6 We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. - Linking the spreadsheet \u00b6 Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! \u00b6 Once you've set your rules up and got comfortable using Bet Angel Pro it should only take number of seconds to load the markets up and choose your selections for the day. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to us at automation@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Tipping automation"},{"location":"thirdPartyTools/betAngelTippingAutomation/#bet-angel-pro-tipping-automation","text":"","title":"Bet Angel Pro: Tipping automation"},{"location":"thirdPartyTools/betAngelTippingAutomation/#automating-a-non-ratings-based-tipping-strategy-using-bet-angel-pro","text":"We all love getting some good racing tips, but who has time to sit and place bets all day? Wouldn't it be easier if you could take those tips and get a program to automatically place the bets on your behalf? This is what we're going to explore here - we'll be using Bet Angel Pro to place bets automatically based on a set of tips. This is our first-time using Bet Angel for this approach and we are very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions.","title":"Automating a (non-ratings based) tipping strategy using Bet Angel Pro"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-the-plan","text":"We have a set of tips that we've taken from our DataScientists' Racing Prediction Model, but this approach should work for any set of tips you may have. Our goal is to create an automated process which will let us choose our tips for the day, then walk away and the program do the leg work. Here we'll step through how we went about getting Bet Angel Pro to place bets on the favourite runner identified by Betfair's Data Scientists . There are no ratings associated with these tips, so we're happy to take Betfair's Starting Price instead of a price for these bets. If you want to follow along and try this approach yourself you'll need to download Bet Angel Pro and sign up for either a subscription or at least a test period. They have a 14 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Tips: Betfair Data Scientists' Racing Prediction Model Rules: here's the spreadsheet , we used to automate our tips but you may need to tweak it a bit to suit your own tips. Tool: Bet Angel Pro","title":"- The plan"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-set-up","text":"First up we need to make sure we've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our tips and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"-  Writing your rules"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-trigger-to-place-bet","text":"In short, we want to back or lay runners when: The runners name has been specified in our tipping list Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial RunnerName refers to the entire column A in the 'TIP' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H2 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BackStake refers to cell H3 in the 'SETTINGS' worksheet and like the name suggests, will be the stake for any back bets that are triggered LayStake refers to cell H4 in the 'SETTINGS' worksheet and will be the stake for any lay bets that are triggered BetType is the entire B column in the 'TIP' worksheet. Depending on your tip for each runner, you can choose whether you want a back or lay bet to be triggered for that runner This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) ``` excel tab=\"Single line\" =IF(AND(COUNTIF(RunnerName,B9)>0,TimeTillJump1<UserTimeTillJump,Overrounds1<UserOverround,ISBLANK(InPlay1)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\") Stepping through each step: Checking whether the runner is to have a bet placed: Here the trigger is checking the list of runners in the 'TIPS' worksheet so it can decide whether we want a bet to be placed or not. If the name does appear in our list, then it returns a TRUE flag and continues with the next trigger condition. If the name is not in the list, then no bet will be placed. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell H2 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E9 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Result: if the statement above is true, check whether BACK or LAY has been selected in column B of the 'TIPS' worksheet for that runner. Whatever has been specified, trigger that bet type. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump2<UserTimeTillJump, Overrounds2<UserOverround, ISBLANK(InPlay2)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump3<UserTimeTillJump, Overrounds3<UserOverround, ISBLANK(InPlay3)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false [COUNTIF function:] Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump3<UserTimeTillJump, Overrounds3<UserOverround, ISBLANK(InPlay3)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) ``` excel tab=\"Single line\" =IF(AND(COUNTIF(RunnerName,B9)>0,TimeTillJump3<UserTimeTillJump,Overrounds3<UserOverround,ISBLANK(InPlay3)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\") Odds: Because we will be taking the BSP, we want to ensure that the initial bets that we place are not matched so that the \"TAKE_SP_ALL\" command can trigger for the global command. To do this, it checks the bet type for that particular runner. If it is Backing, then place odds at 1000 and if its going to be a lay bet, then set odds at 1.01 Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR which we're also using, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"LAY\",1.01,1000)),\"\") Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are using seperate variables for a back bet and lay bet. These variables can be easily changed from the 'SETTINGS' tab. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"BACK\", BackStake,LayStake)),\"\")","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-connecting-to-bet-angel","text":"","title":"- Connecting to Bet Angel"},{"location":"thirdPartyTools/betAngelTippingAutomation/#video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"Video walk through"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-selecting-markets","text":"We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature.","title":"- Selecting markets"},{"location":"thirdPartyTools/betAngelTippingAutomation/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/betAngelTippingAutomation/#and-youre-set","text":"Once you've set your rules up and got comfortable using Bet Angel Pro it should only take number of seconds to load the markets up and choose your selections for the day. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/betAngelTippingAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to us at automation@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/betAngelTippingAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngeladvanced/","text":"Bet Angel: An Advanced guide \u00b6 Automation \u00b6 Bet Angel offers two forms of automation through their Guardian interface: Triggered betting which operates by linking pre-defined commands together to form an automated strategy Use Excel to monitor multiple markets and execute bets or trades based upon triggers set within the Excel spreadsheet within one or more markets. The triggered betting feature is a good way to implement simple automations quickly while using Excel gives you a greater level of control and sophistication as you have the ability to use formulas and even create your own macros using Visual Basic for Applications (VBA). Triggered Betting \u00b6 To begin forming your automation in the triggered betting feature within Bet Angel, you will need to open the Guardian feature by clicking on the icon on the main Bet Angel toolbar. The Guardian window will open where you have a number of tabs to select from. First, we want to have a list of markets that we want to automate. From the \u2018Market Selection\u2019 tab select each market (use Ctrl to select multiple) you want Click the \u2018Add Bet Angel markets\u2019 button. Keep in mind that the order that the markets are in the list, will be the order that your automation will go through. Tip: Click on the \u2018Start time\u2019 column to order your list based on race start time. Guardian will work through your list in order. Within the \u2018Markets\u2019 tab you will see the \u2018Refresh interval\u2019 dropdown box. This instructs Bet Angel how often to refresh the markets in the list. We recommend setting this value to be the lowest possible by: Navigate back to the Main Bet Angel home screen Click the \u2018Settings\u2019 tab Select \u2018Edit Settings\u2019 Click the \u2018Communications\u2019 tab Check \u2018Use Exchange Streaming\u2019 Click \u2018Save\u2019 then \u2018Close\u2019 Navigate to the Guardian screen and select \u201820ms\u2019 from the \u2018Refresh interval\u2019 dropdown box. For triggered betting automations: Click on the \u2018Automation\u2019 tab You can create an automation file from scratch and apply it to your selected markets or import other people's Bet Angel automation files. To get started: Highlight one of the markets in your list and select \u2018Create a new rules file for the selected market\u2019. The Automation rules editor window will appear where you can start defining all the settings needed for your automation. Rule settings are divided into 5 different tabs: \u2018General\u2019 \u2013 where the type of action is defined and when that action will be triggered in relation to the market \u2018Parameters\u2019 \u2013 Here you\u2019ll specify the finer details of your bets such as the odds, stake and global settings \u2018Conditions\u2019 \u2013 create the conditions that have to be met before the rule triggers such as the markets weight of money or volume \u2018Signals\u2019 \u2013 for more complex automations, you may choose to set your rule to give off a certain signal when it triggers which in turn will cause other rules to trigger when they\u2019re set to listen for that signal \u2018Stored Values\u2019 \u2013 which allows you to record things like a Back or Lay bet, last traded price or volume which then can be used as conditions for other rules. There\u2019s a lot of options here, so we recommend taking a look at online resources such as the Bet Angel user guide and Youtube tutorials to get a more in depth understanding of what each option does and how it may apply to your specific strategy. Once you have finished creating your automation, remember to click the save icon and give it a name. Now you can close the Automation Rules editor window and select your automation from the drop down box called \u2018Rules file name\u2019. Once your automation rule file is selected, in the \u2018Rules File Usage\u2019 you can choose from: \u2018Apply rules to all markets\u2019, \u2018Apply rules to selected markets \u2018 \u2018Remove rules from selected markets\u2019. When your automation file is linked to a market it will appear in the \u2018Automation Rules\u2019 column and will begin automating your strategy for you. Custom columns - Advanced \u00b6 Please see our Intermediate guide on custom columns here before starting this advanced tutorial. Custom columns can be very useful to tailor your betting experience when placing manual bets. They can also be used to trigger betting automations. To link an automation to a custom column, click the star icon on the \u2018One-click\u2019 betting screen which will bring up your custom column settings window. Select \u2018Start an Automation Servant\u2019 from the \u2018Action\u2019 drop down box (highlighted below) Then from the \u2018Rules File\u2019 drop down box select the name of your automation file. Add steps here (same as intermediate tute) to add the column After you\u2019ve saved the settings for your custom column, whenever the custom column is clicked on, it will trigger your automation. Applications for this can be varied such as backing a specific runner while laying the rest of the field and much more. Excel Automation \u00b6 Using an excel spreadsheet is a great tool to implement complex strategies through Bet Angel. The main way you will go about setting up an automation is to control when a back, lay or take SP command is printed into specific cells which Bet Angel continuously checks. If Bet Angel detects that a certain cell contains \u201cBACK\u201d, it will place a back bet. The same applies with lay bets as well. Formulas and macros can be used to control when these messages are sent to Bet Angel which can be tied into various market factors or conditions. To begin setting up an Excel automation: click on the icon to open Guardian and then click the Excel tab. From here, you can choose a specific Excel template that you have set up on your computer or import an automation that has been downloaded online using: \u2018Browse for file\u2019 button and then clicking \u2018Open workbook\u2019. By default, Bet Angel won't automatically start using the Excel sheet to start placing bets until you check the boxes next to: \u2018Connect\u2019, \u2018Auto-bind Bet Angel sheets\u2019 and \u2018Auto-clear bindings\u2019 options. Then select your file name from the \u2018Excel Sheet\u2019 column next to the markets you want to connect Once this is done, Bet Angel will start populating your spreadsheet with market data and start listening for the betting commands. If you try to edit the Excel sheet while it\u2019s connected with Bet Angel it will most likely cause an error. If you want to make changes to your Excel workbook, simply untick the \u2018Connect\u2019 option in Guardian, make your changes and re select connect. As previously mentioned, Bet Angel will populate data into specific cells and will listen for betting commands from other specific cells. You will need to be mindful of this If you wish to start adding functionality to your excel workbook to make sure that you\u2019re not placing it in a cell/s that Bet Angel will use. Bet Angel will simply override whatever you have entered with its own data. For more tutorials on using the Bet Angel Excel function, take a look through our Automation Hub where we have created a number of tutorials for different strategies. Resources \u00b6 Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel User Guide Triggered Betting Custom Columns Excel Automation Youtube Triggered Betting Custom Column (Advanced) Automation Hub Ratings Automation tutorial Market favourite automation Tipping automation tutorial Simultaneous markets tutorial Kelly criterion staking tutorial Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Bet Angel - An advanced guide"},{"location":"thirdPartyTools/betAngeladvanced/#bet-angel-an-advanced-guide","text":"","title":"Bet Angel: An Advanced guide"},{"location":"thirdPartyTools/betAngeladvanced/#automation","text":"Bet Angel offers two forms of automation through their Guardian interface: Triggered betting which operates by linking pre-defined commands together to form an automated strategy Use Excel to monitor multiple markets and execute bets or trades based upon triggers set within the Excel spreadsheet within one or more markets. The triggered betting feature is a good way to implement simple automations quickly while using Excel gives you a greater level of control and sophistication as you have the ability to use formulas and even create your own macros using Visual Basic for Applications (VBA).","title":"Automation"},{"location":"thirdPartyTools/betAngeladvanced/#triggered-betting","text":"To begin forming your automation in the triggered betting feature within Bet Angel, you will need to open the Guardian feature by clicking on the icon on the main Bet Angel toolbar. The Guardian window will open where you have a number of tabs to select from. First, we want to have a list of markets that we want to automate. From the \u2018Market Selection\u2019 tab select each market (use Ctrl to select multiple) you want Click the \u2018Add Bet Angel markets\u2019 button. Keep in mind that the order that the markets are in the list, will be the order that your automation will go through. Tip: Click on the \u2018Start time\u2019 column to order your list based on race start time. Guardian will work through your list in order. Within the \u2018Markets\u2019 tab you will see the \u2018Refresh interval\u2019 dropdown box. This instructs Bet Angel how often to refresh the markets in the list. We recommend setting this value to be the lowest possible by: Navigate back to the Main Bet Angel home screen Click the \u2018Settings\u2019 tab Select \u2018Edit Settings\u2019 Click the \u2018Communications\u2019 tab Check \u2018Use Exchange Streaming\u2019 Click \u2018Save\u2019 then \u2018Close\u2019 Navigate to the Guardian screen and select \u201820ms\u2019 from the \u2018Refresh interval\u2019 dropdown box. For triggered betting automations: Click on the \u2018Automation\u2019 tab You can create an automation file from scratch and apply it to your selected markets or import other people's Bet Angel automation files. To get started: Highlight one of the markets in your list and select \u2018Create a new rules file for the selected market\u2019. The Automation rules editor window will appear where you can start defining all the settings needed for your automation. Rule settings are divided into 5 different tabs: \u2018General\u2019 \u2013 where the type of action is defined and when that action will be triggered in relation to the market \u2018Parameters\u2019 \u2013 Here you\u2019ll specify the finer details of your bets such as the odds, stake and global settings \u2018Conditions\u2019 \u2013 create the conditions that have to be met before the rule triggers such as the markets weight of money or volume \u2018Signals\u2019 \u2013 for more complex automations, you may choose to set your rule to give off a certain signal when it triggers which in turn will cause other rules to trigger when they\u2019re set to listen for that signal \u2018Stored Values\u2019 \u2013 which allows you to record things like a Back or Lay bet, last traded price or volume which then can be used as conditions for other rules. There\u2019s a lot of options here, so we recommend taking a look at online resources such as the Bet Angel user guide and Youtube tutorials to get a more in depth understanding of what each option does and how it may apply to your specific strategy. Once you have finished creating your automation, remember to click the save icon and give it a name. Now you can close the Automation Rules editor window and select your automation from the drop down box called \u2018Rules file name\u2019. Once your automation rule file is selected, in the \u2018Rules File Usage\u2019 you can choose from: \u2018Apply rules to all markets\u2019, \u2018Apply rules to selected markets \u2018 \u2018Remove rules from selected markets\u2019. When your automation file is linked to a market it will appear in the \u2018Automation Rules\u2019 column and will begin automating your strategy for you.","title":"Triggered Betting"},{"location":"thirdPartyTools/betAngeladvanced/#custom-columns-advanced","text":"Please see our Intermediate guide on custom columns here before starting this advanced tutorial. Custom columns can be very useful to tailor your betting experience when placing manual bets. They can also be used to trigger betting automations. To link an automation to a custom column, click the star icon on the \u2018One-click\u2019 betting screen which will bring up your custom column settings window. Select \u2018Start an Automation Servant\u2019 from the \u2018Action\u2019 drop down box (highlighted below) Then from the \u2018Rules File\u2019 drop down box select the name of your automation file. Add steps here (same as intermediate tute) to add the column After you\u2019ve saved the settings for your custom column, whenever the custom column is clicked on, it will trigger your automation. Applications for this can be varied such as backing a specific runner while laying the rest of the field and much more.","title":"Custom columns - Advanced"},{"location":"thirdPartyTools/betAngeladvanced/#excel-automation","text":"Using an excel spreadsheet is a great tool to implement complex strategies through Bet Angel. The main way you will go about setting up an automation is to control when a back, lay or take SP command is printed into specific cells which Bet Angel continuously checks. If Bet Angel detects that a certain cell contains \u201cBACK\u201d, it will place a back bet. The same applies with lay bets as well. Formulas and macros can be used to control when these messages are sent to Bet Angel which can be tied into various market factors or conditions. To begin setting up an Excel automation: click on the icon to open Guardian and then click the Excel tab. From here, you can choose a specific Excel template that you have set up on your computer or import an automation that has been downloaded online using: \u2018Browse for file\u2019 button and then clicking \u2018Open workbook\u2019. By default, Bet Angel won't automatically start using the Excel sheet to start placing bets until you check the boxes next to: \u2018Connect\u2019, \u2018Auto-bind Bet Angel sheets\u2019 and \u2018Auto-clear bindings\u2019 options. Then select your file name from the \u2018Excel Sheet\u2019 column next to the markets you want to connect Once this is done, Bet Angel will start populating your spreadsheet with market data and start listening for the betting commands. If you try to edit the Excel sheet while it\u2019s connected with Bet Angel it will most likely cause an error. If you want to make changes to your Excel workbook, simply untick the \u2018Connect\u2019 option in Guardian, make your changes and re select connect. As previously mentioned, Bet Angel will populate data into specific cells and will listen for betting commands from other specific cells. You will need to be mindful of this If you wish to start adding functionality to your excel workbook to make sure that you\u2019re not placing it in a cell/s that Bet Angel will use. Bet Angel will simply override whatever you have entered with its own data. For more tutorials on using the Bet Angel Excel function, take a look through our Automation Hub where we have created a number of tutorials for different strategies.","title":"Excel Automation"},{"location":"thirdPartyTools/betAngeladvanced/#resources","text":"Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel User Guide Triggered Betting Custom Columns Excel Automation Youtube Triggered Betting Custom Column (Advanced) Automation Hub Ratings Automation tutorial Market favourite automation Tipping automation tutorial Simultaneous markets tutorial Kelly criterion staking tutorial","title":"Resources"},{"location":"thirdPartyTools/betAngeladvanced/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelbeginners/","text":"Bet Angel: A beginner\u2019s guide \u00b6 Installing Bet Angel \u00b6 Bet Angel offers three products for use with Betfair. We\u2019ll be using Bet Angel Professional in these tutorials as it is the Ultimate Trading Toolkit containing all the advanced tools and features every trader needs. To start using and learning Bet Angel Professional, first you need to download and install the program from the Bet Angel website. By downloading it from the Bet Angel website, you can ensure that you\u2019re installing the most up to date version of the software which can be obtained from here. Make sure that you save the downloaded .exe file somewhere that you can easily access. Once it has finished downloading, double click the file (which will be called BAP_1_54_1.exe or something similar) to begin the setup process. For more details on downloading and installing Bet Angel see their handy user guide here. Follow the prompts throughout the installation and once completed, open Bet Angel Professional. Registering Bet Angel \u00b6 Step 1 After opening Bet Angel Pro you will see a login dialogue box. Click on the \u2018Register using a serial number\u2026\u2019 button (highlighted below). Step 2 \u2013 Free Trial. (Skip to Step 4 if you already have a Bet Angel license). Bet Angel offer a free 14-day trial for first time users. Select the option \u201cI wish to register for a FREE trial\u2026\u201d as highlighted below. Then enter your \u201cBetfair User Name\u201d and \u201cPassword\u201d details, and click \u2018Register Account\u2019. Step 3 A new window will appear where you will be prompted to provide your first and last name along with your email address. From here you\u2019ll need to verify your email by clicking a link that will be sent to you in an automated email by Bet Angel. If you don\u2019t see an automatic email from Bet Angel, make sure you check your junk / spam folder. Step 4 \u2013 Purchased serial number (Skip to Step 5 if you have registered for a free trial) Select \u201cI have a serial number......\u201d. You will now see a registration box in which you must enter the serial number sent to you along with the Betfair account username and password (for account verification). Your account password will NOT be stored or sent to us. Once you have entered your details click on the \u201cRegister Account\u201d button. If you have not already done so a window will appear asking you to first associate a\u202fverified e-mail\u202faddress with your Betfair Client ID. This is a one-time process and will make activating future subscriptions much quicker and easier. Otherwise immediate access will be granted, and a confirmation message will appear on your screen. Step 5 Once you have submitted your register account request, log into Bet Angel by entering your \u201cBetfair User Name\u201d and \u201cPassword\u201d in the boxes highlighted below. You can select either \u201cLive mode\u201d or \u201cPractice mode\u201d at the log in screen. We recommend starting in Practice mode as this will allow you to experiment with Bet Angel without placing any bets to the Betfair exchange which is great while learning the ropes. Note that it can take up to 15 minutes before you have full access to Bet Angel, so be patient while the system sets you up in the background. Ensure that you tick the \u201cI have read and fully understand the risk notice\u201d and the \u201cI accept the Betfair API Terms and conditions\u201d boxes and then click the \u201cLog in\u201d button. Basic Setup \u00b6 Once we\u2019re logged into Bet Angel, the first thing that we\u2019re going to want to do is to configure Bet Angel so that it behaves in a way that is best suited for what we\u2019re going to use it for. Each user will have their own configuration preferences and we strongly recommend getting acquainted with the various options available to take full advantage of your Bet Angel experience. To understand all of the options available in Bet Angel, we recommend taking a look at the Bet Angel user guide. To get started, simply click the \u2018Settings\u2019 tab from the top toolbar and then \u2018Edit settings\u2019 from the drop-down box. The Settings window will then appear and is organized into 10 sections; Display: to alter the colour preferences for market views, font size and weight of money Staking: where you can set default stakes and liabilities which will be applied to every market Behavior: Enable / disable confirm bet warnings as well as other program specific warnings Green Up: Specific options that affect unmatched bets when closing / greening a trade Communications: Control the data that Bet Angel is pulling from the Betfair exchange API Ladder: Change the colour scheme, columns, charts and more for the ladder functionality Automation: Modify some basic automation controls for bookmaking, ditching, Back and Lay Charts: Customise how charts are displayed in Bet Angel such as colour Sound Alerts: change what sounds will be heard from Bet Angel such as market jump warnings Excel: Control additional information which is populated in a connected excel sheet such as saddle cloth numbers, stall number projected / actual SP and more. The most common settings to edit straight away are: - Turning off \u201cConfirm Bets\u201d - As it states, unticking this option will mean you don\u2019t need to confirm bets. This is one of the advantages of using a 3 rd party tool \u2013 please be aware that this will make betting faster, so gamble responsibly . To use the ladder functionality within Bet Angel, go to the Ladder tab and then click \u2018Show the Ladder Settings editor\u2019 which will bring up another window with a lot more options to play with. A lot of users like to enable the \u201cShow last traded volume\u201d option by Clicking on the \u201cGeneral\u201d tab and then check \u201cShow last traded volume\u201d then Clicking on the \u201cColumns\u201d tab and under \u2018Last traded price chart column\u2019 (near top of window) check the \u2018Show chart column\u2019 box. Whatever settings you decide to use, you can save them as a settings profile and use different profiles for different types of betting. Just click \u2018save\u2019 from the Settings window and give your settings profile a name. You can easily switch between different settings profiles that you\u2019ve created by clicking the \u2018Settings\u2019 tab and then choosing the profile name from the dropdown box next to \u2018Load settings\u2019 To help you through getting setup in Bet Angel, take a look at the Bet Angel: Recommended Settings From Betfair on the Betfair Hub created specifically to help you get up and running. Market Selection \u00b6 To open a market so we can start placing bets, first we need to select the market that we want to look at. Click the \u2018File\u2019 tab from the main navigation bar and click \u2018Select Market\u2019. This will open the \u201cMarket Selection\u201d window where you can browse all the different markets available on the Betfair exchange. Click the arrows to the left of the menu items to expand and see the specific markets available. Once you\u2019ve found a market that you\u2019re interested in, click on it once then click the \u2018switch to market\u2019 button. Another handy feature is to create a quick pick list for specific markets that you\u2019re interested in. For example, if you\u2019re only interested in win horse racing markets within Australia, you can click the \u201cSettings\u201d tab in the \u201cMarket Selection\u201d window and modify the options to be applicable to what you\u2019re interested in. For example here we have selected Australian Horse Race Win markets by: Checking the \u201cAUS\u201d box under \u201cHorse Races\u201d Checking the \u201cWin Markets\u201d under \u201cHorse Races\u201d Checking the box next to \u201cDisplay event start time first\u201d Then only those markets will appear in the \u2018Quick Picks\u2019 tab. Once you have clicked the \u2018Switch to market\u2019 button for your chosen market, you can easily cycle through to the next scheduled market by pressing Ctrl+N (or Ctrl+P for the previous market) without the need to return back to the market selection window. Default stakes and lay to a maximum liability \u00b6 If you\u2019re a person who wants to place bets quickly in order to secure the best price, then you\u2019ll also want to be able to set a default stake for your bets. In a market, you\u2019ll see two columns named \u2018Back stake\u2019 and \u2018Lay stake\u2019. You can easily change the stake value for each runner to whatever you like by clicking on the number and entering your own value manually. You can also set a default stake that will apply to all runners within a market. Simply change the value at the top of the column (highlighted in yellow) and ensure the \u2018All\u2019 check box is selected. This can also be done for the lay column in the exact same way. But what if we wanted to set a maximum liability for our lay bets as opposed to a straight stake? This is where the staking method option comes into play. Simply change the \u201cStaking Method\u201d drop down box (top of screen) to \u2018By Liability (Lay only)\u2019 and any values which are entered in the lay stake column becomes your maximum liability. Your lay stake is calculated behind the scenes to ensure that you don\u2019t accidentally empty your Betfair account should your lay bet lose. One click betting \u00b6 Previously, we went over the available settings in Bet Angel to switch off an option called \u2018Confirm bets?\u2019 under the \u2018Behavior\u2019 tab of the settings window. This option is usually enabled by default and will cause a pop-up window to appear asking for you to confirm whether you want a bet to be placed on the exchange. By switching this setting off, you can place bets onto the exchange using a single click meaning that you can react much faster to changes in a market. This will also affect other views within Bet Angel such as the ladder feature within Bet Angel. Refresh Settings \u00b6 Bet Angel gives you the ability to control how often data is refreshed from the Betfair exchange through a number of options in settings. We recommend making sure that the refresh rate for Bet Angel is set to as low as possible to ensure that you are seeing the most up to date data at any one time. Click the \u2018Settings\u2019 tab and then Select \u2018Edit Settings\u2019 and then Click the \u2018Communications\u2019 tab. Check \u2018Use Exchange Streaming\u2019 Click \u2018Save\u2019 then \u2018Close\u2019. On the Bet Angel menu bar, change the \u2018Refresh every\u2019 option to 20ms (note 20ms will only be available if you have checked \u2018Use Exchange Streaming\u2019 in the previous step). Fill or Kill \u00b6 Fill or kill is a useful tool to instruct Bet Angel to cancel (kill) a bet if it\u2019s not matched within a period of time you have specified. To set up fill or kill, simply follow the following steps: Ensure that the \u2018Use Global Settings when placing a bet\u2019 is enabled (see highlighted button below) Tick \u2018Place fill or Kill bets\u2019 in the header menu In the \u2018seconds delay\u2019 box specify the number of seconds you would like Bet Angel to wait until it cancels the bet should it not be matched. Bet Angel recommends to ensure that a minimum value of 0.5 (half a second) is always applied to the fill or kill. Resources \u00b6 Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel User Guide Installation Settings Market Selection One Click betting Refresh settings Fill or Kill Youtube cccc Using the settings feature One click trading screen Getting Bet Angel to update ten times faster - Refresh settings Betfair Hub Bet Angel overview Recommended settings from Betfair What next? \u00b6 Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Bet Angel - A beginner's guide"},{"location":"thirdPartyTools/betAngelbeginners/#bet-angel-a-beginners-guide","text":"","title":"Bet Angel: A beginner\u2019s guide"},{"location":"thirdPartyTools/betAngelbeginners/#installing-bet-angel","text":"Bet Angel offers three products for use with Betfair. We\u2019ll be using Bet Angel Professional in these tutorials as it is the Ultimate Trading Toolkit containing all the advanced tools and features every trader needs. To start using and learning Bet Angel Professional, first you need to download and install the program from the Bet Angel website. By downloading it from the Bet Angel website, you can ensure that you\u2019re installing the most up to date version of the software which can be obtained from here. Make sure that you save the downloaded .exe file somewhere that you can easily access. Once it has finished downloading, double click the file (which will be called BAP_1_54_1.exe or something similar) to begin the setup process. For more details on downloading and installing Bet Angel see their handy user guide here. Follow the prompts throughout the installation and once completed, open Bet Angel Professional.","title":"Installing Bet Angel"},{"location":"thirdPartyTools/betAngelbeginners/#registering-bet-angel","text":"Step 1 After opening Bet Angel Pro you will see a login dialogue box. Click on the \u2018Register using a serial number\u2026\u2019 button (highlighted below). Step 2 \u2013 Free Trial. (Skip to Step 4 if you already have a Bet Angel license). Bet Angel offer a free 14-day trial for first time users. Select the option \u201cI wish to register for a FREE trial\u2026\u201d as highlighted below. Then enter your \u201cBetfair User Name\u201d and \u201cPassword\u201d details, and click \u2018Register Account\u2019. Step 3 A new window will appear where you will be prompted to provide your first and last name along with your email address. From here you\u2019ll need to verify your email by clicking a link that will be sent to you in an automated email by Bet Angel. If you don\u2019t see an automatic email from Bet Angel, make sure you check your junk / spam folder. Step 4 \u2013 Purchased serial number (Skip to Step 5 if you have registered for a free trial) Select \u201cI have a serial number......\u201d. You will now see a registration box in which you must enter the serial number sent to you along with the Betfair account username and password (for account verification). Your account password will NOT be stored or sent to us. Once you have entered your details click on the \u201cRegister Account\u201d button. If you have not already done so a window will appear asking you to first associate a\u202fverified e-mail\u202faddress with your Betfair Client ID. This is a one-time process and will make activating future subscriptions much quicker and easier. Otherwise immediate access will be granted, and a confirmation message will appear on your screen. Step 5 Once you have submitted your register account request, log into Bet Angel by entering your \u201cBetfair User Name\u201d and \u201cPassword\u201d in the boxes highlighted below. You can select either \u201cLive mode\u201d or \u201cPractice mode\u201d at the log in screen. We recommend starting in Practice mode as this will allow you to experiment with Bet Angel without placing any bets to the Betfair exchange which is great while learning the ropes. Note that it can take up to 15 minutes before you have full access to Bet Angel, so be patient while the system sets you up in the background. Ensure that you tick the \u201cI have read and fully understand the risk notice\u201d and the \u201cI accept the Betfair API Terms and conditions\u201d boxes and then click the \u201cLog in\u201d button.","title":"Registering Bet Angel"},{"location":"thirdPartyTools/betAngelbeginners/#basic-setup","text":"Once we\u2019re logged into Bet Angel, the first thing that we\u2019re going to want to do is to configure Bet Angel so that it behaves in a way that is best suited for what we\u2019re going to use it for. Each user will have their own configuration preferences and we strongly recommend getting acquainted with the various options available to take full advantage of your Bet Angel experience. To understand all of the options available in Bet Angel, we recommend taking a look at the Bet Angel user guide. To get started, simply click the \u2018Settings\u2019 tab from the top toolbar and then \u2018Edit settings\u2019 from the drop-down box. The Settings window will then appear and is organized into 10 sections; Display: to alter the colour preferences for market views, font size and weight of money Staking: where you can set default stakes and liabilities which will be applied to every market Behavior: Enable / disable confirm bet warnings as well as other program specific warnings Green Up: Specific options that affect unmatched bets when closing / greening a trade Communications: Control the data that Bet Angel is pulling from the Betfair exchange API Ladder: Change the colour scheme, columns, charts and more for the ladder functionality Automation: Modify some basic automation controls for bookmaking, ditching, Back and Lay Charts: Customise how charts are displayed in Bet Angel such as colour Sound Alerts: change what sounds will be heard from Bet Angel such as market jump warnings Excel: Control additional information which is populated in a connected excel sheet such as saddle cloth numbers, stall number projected / actual SP and more. The most common settings to edit straight away are: - Turning off \u201cConfirm Bets\u201d - As it states, unticking this option will mean you don\u2019t need to confirm bets. This is one of the advantages of using a 3 rd party tool \u2013 please be aware that this will make betting faster, so gamble responsibly . To use the ladder functionality within Bet Angel, go to the Ladder tab and then click \u2018Show the Ladder Settings editor\u2019 which will bring up another window with a lot more options to play with. A lot of users like to enable the \u201cShow last traded volume\u201d option by Clicking on the \u201cGeneral\u201d tab and then check \u201cShow last traded volume\u201d then Clicking on the \u201cColumns\u201d tab and under \u2018Last traded price chart column\u2019 (near top of window) check the \u2018Show chart column\u2019 box. Whatever settings you decide to use, you can save them as a settings profile and use different profiles for different types of betting. Just click \u2018save\u2019 from the Settings window and give your settings profile a name. You can easily switch between different settings profiles that you\u2019ve created by clicking the \u2018Settings\u2019 tab and then choosing the profile name from the dropdown box next to \u2018Load settings\u2019 To help you through getting setup in Bet Angel, take a look at the Bet Angel: Recommended Settings From Betfair on the Betfair Hub created specifically to help you get up and running.","title":"Basic Setup"},{"location":"thirdPartyTools/betAngelbeginners/#market-selection","text":"To open a market so we can start placing bets, first we need to select the market that we want to look at. Click the \u2018File\u2019 tab from the main navigation bar and click \u2018Select Market\u2019. This will open the \u201cMarket Selection\u201d window where you can browse all the different markets available on the Betfair exchange. Click the arrows to the left of the menu items to expand and see the specific markets available. Once you\u2019ve found a market that you\u2019re interested in, click on it once then click the \u2018switch to market\u2019 button. Another handy feature is to create a quick pick list for specific markets that you\u2019re interested in. For example, if you\u2019re only interested in win horse racing markets within Australia, you can click the \u201cSettings\u201d tab in the \u201cMarket Selection\u201d window and modify the options to be applicable to what you\u2019re interested in. For example here we have selected Australian Horse Race Win markets by: Checking the \u201cAUS\u201d box under \u201cHorse Races\u201d Checking the \u201cWin Markets\u201d under \u201cHorse Races\u201d Checking the box next to \u201cDisplay event start time first\u201d Then only those markets will appear in the \u2018Quick Picks\u2019 tab. Once you have clicked the \u2018Switch to market\u2019 button for your chosen market, you can easily cycle through to the next scheduled market by pressing Ctrl+N (or Ctrl+P for the previous market) without the need to return back to the market selection window.","title":"Market Selection"},{"location":"thirdPartyTools/betAngelbeginners/#default-stakes-and-lay-to-a-maximum-liability","text":"If you\u2019re a person who wants to place bets quickly in order to secure the best price, then you\u2019ll also want to be able to set a default stake for your bets. In a market, you\u2019ll see two columns named \u2018Back stake\u2019 and \u2018Lay stake\u2019. You can easily change the stake value for each runner to whatever you like by clicking on the number and entering your own value manually. You can also set a default stake that will apply to all runners within a market. Simply change the value at the top of the column (highlighted in yellow) and ensure the \u2018All\u2019 check box is selected. This can also be done for the lay column in the exact same way. But what if we wanted to set a maximum liability for our lay bets as opposed to a straight stake? This is where the staking method option comes into play. Simply change the \u201cStaking Method\u201d drop down box (top of screen) to \u2018By Liability (Lay only)\u2019 and any values which are entered in the lay stake column becomes your maximum liability. Your lay stake is calculated behind the scenes to ensure that you don\u2019t accidentally empty your Betfair account should your lay bet lose.","title":"Default stakes and lay to a maximum liability"},{"location":"thirdPartyTools/betAngelbeginners/#one-click-betting","text":"Previously, we went over the available settings in Bet Angel to switch off an option called \u2018Confirm bets?\u2019 under the \u2018Behavior\u2019 tab of the settings window. This option is usually enabled by default and will cause a pop-up window to appear asking for you to confirm whether you want a bet to be placed on the exchange. By switching this setting off, you can place bets onto the exchange using a single click meaning that you can react much faster to changes in a market. This will also affect other views within Bet Angel such as the ladder feature within Bet Angel.","title":"One click betting"},{"location":"thirdPartyTools/betAngelbeginners/#refresh-settings","text":"Bet Angel gives you the ability to control how often data is refreshed from the Betfair exchange through a number of options in settings. We recommend making sure that the refresh rate for Bet Angel is set to as low as possible to ensure that you are seeing the most up to date data at any one time. Click the \u2018Settings\u2019 tab and then Select \u2018Edit Settings\u2019 and then Click the \u2018Communications\u2019 tab. Check \u2018Use Exchange Streaming\u2019 Click \u2018Save\u2019 then \u2018Close\u2019. On the Bet Angel menu bar, change the \u2018Refresh every\u2019 option to 20ms (note 20ms will only be available if you have checked \u2018Use Exchange Streaming\u2019 in the previous step).","title":"Refresh Settings"},{"location":"thirdPartyTools/betAngelbeginners/#fill-or-kill","text":"Fill or kill is a useful tool to instruct Bet Angel to cancel (kill) a bet if it\u2019s not matched within a period of time you have specified. To set up fill or kill, simply follow the following steps: Ensure that the \u2018Use Global Settings when placing a bet\u2019 is enabled (see highlighted button below) Tick \u2018Place fill or Kill bets\u2019 in the header menu In the \u2018seconds delay\u2019 box specify the number of seconds you would like Bet Angel to wait until it cancels the bet should it not be matched. Bet Angel recommends to ensure that a minimum value of 0.5 (half a second) is always applied to the fill or kill.","title":"Fill or Kill"},{"location":"thirdPartyTools/betAngelbeginners/#resources","text":"Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel User Guide Installation Settings Market Selection One Click betting Refresh settings Fill or Kill Youtube cccc Using the settings feature One click trading screen Getting Bet Angel to update ten times faster - Refresh settings Betfair Hub Bet Angel overview Recommended settings from Betfair","title":"Resources"},{"location":"thirdPartyTools/betAngelbeginners/#what-next","text":"","title":"What next?"},{"location":"thirdPartyTools/betAngelbeginners/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/betAngelintermediate/","text":"Bet Angel: An Intermediate guide \u00b6 The Ladder interface \u00b6 The Bet Angel Ladder interface allows you to gain a deeper view of how bets are being placed on a small number of runners as opposed to the high-level overview of a market that we get from the \u2018One-Click\u2019 view. This also allows you to get an idea of any potential trends and help you make a more informed decision about your own bets. By default, the ladder interface will show you the first three runners in a market. A handy button to keep in mind is the \u2018123\u2019 button which will automatically adjust which runners you see and order them in ascending price order with the favourite placed in \u2018Ladder 1\u2019 position. You can also easily specify the runners you would like to look at by simply selecting them from the drop-down box above each ladder. Just like the \u2018One-click screen\u2019, you can click within the blue (back) or pink (lay) columns to place a bet at those specific odds, allowing you to not only get a more detailed view of what the market is doing but quickly place bets of your own. In-Play trader view \u00b6 The in-play trader view is an alternative way to look and place bets on markets which are in-play. It gives an overview of the highest and lowest price points traded for runners and allows you to easily place bets onto the market. To open the in-play trader view: click the in-play trader icon near the top right of the main Bet Angel screen. Once you\u2019ve clicked the button, a new window will open up showing every runner in the race. As you can see, there are a number of dots and lines which give us specific information relating to where bets are being placed for each runner. The blue dot indicates the last back price, the pink shows the last lay price and the yellow indicates the last traded price. The grey bar gives us the highest and lowest traded range of that particular runner. You\u2019ll also notice that there are a number of options that you can use to customize your experience with the tool depending on what information is important to you. At the top left of the window from left to right, you can hide the blue, pink and yellow dots to simplify your view change the window to reflect the win percentage scale or tick scale sort your runner list depending of a range of factors change the height of your runner rows adjust the labels for the dots to reflect different information You can also use the in-play trader view to place bets just like in the one-click screen or ladder view. You can customise what betting action to take when the left mouse button is clicked (place a back bet, a lay bet or even set that the left mouse button will back while the right button lays). By default, the odds that bets are placed at and the runner you bet on in this view will be determined by the location of your mouse cursor. It pays to be mindful of this when using this view, especially if you have confirm bets turned off as an accidental click on the in-play trader view may cause you to place a bet that you didn\u2019t want. This being said, the price that bets are placed at can be easily changed to a different factor such as the best available back price instead of the mouse cursor location. Custom Columns \u00b6 Custom columns are an extremely helpful way to speed up how you bet in Bet Angel markets. They allow you customize not only the information that you see in the one-click betting screen, but also lets you place specific bets faster into the market. This is particularly useful when placing bets in-play and speed is a considerable factor for your strategy. To get started with your own custom columns click the yellow star icon which will bring up the custom column's editor. Once the editor window pops up: Click \u2018New\u2019 Then enter a Profile Name for your custom column and click \u2018OK\u2019 Choose from the configure options (see below) Then click \u2018Save Column\u2019 You will have a range of options to configure your columns: Custom Column Profiles \u2013 Create multiple profiles for different betting strategies that you may implement. Each profile can have different columns that do different things and these profiles allow you to easily switch between different modes depending on what you\u2019re wanting to do in Bet Angel. General: Title \u2013 Give your column a name to make it easily identifiable (this will be the name of the column in the column chooser list and title in the column header on the Bet Angel screen) ToolTip \u2013 a custom info box that appears when the mouse is hovered over your custom column Action \u2013 Specify what happens when you click on the column, whether it places a back or lay bet, cancels a bet or triggers an automation to run Button colour \u2013 customise the colour of your column to make it easily distinguishable Display \u2013 Choose whether each box within your custom column is populated with the runners odds or text Odds \u2013 If you decide to have odds populated in your custom column, you can choose from: fixed odds, ticks offset from the best back price ticks offset from the best lay price a percentage offset from best lay price Stake \u2013 This allows you to program a pre-determined stake that can be triggered when you click in the column, essentially giving you your own bet placement shortcut buttons. Once you have set up your custom column the way that you would like it to operate, the next step is to make it visible on your screen so you can use it. Select your custom column name from the drop down box next to the yellow star icon Click on icon Scroll through the list and choose your custom column Your custom column should now be added to the right of the existing columns (see image below) If you haven\u2019t created a profile for your custom columns, click the settings button (see below), then choose columns where pop up window will appear. Find your custom column from the list and tick the box next to it. Offset Bets \u00b6 Offset bets are extremely easy to achieve in Bet Angel thanks to a simple to use tool at the top of the One-click screen. Here you can choose to use a number of different offset bet actions and specify the number of ticks or percentage that you want your offset bet to be placed at. In the above example, if I placed a back bet on the runner named \u201c1. Andrew Swagger\u201d, the back bet will be placed at $55 and then Bet Angel will automatically place a lay bet on the same runner 2 ticks lower. Dutching \u00b6 The dutching screen within Bet Angel is often overlooked by Bet Angel users but is extremely handy, especially if your preference is lower risk (and lower profit/loss). The dutching screen will only place back bets into a market (while the bookmaking screen will only place lay bets-see next section). Bet Angel makes dutching a lot easier by allowing you to select a number of runners within a market that you think that the runner may be amongst and place bets with a specific stake, achieve a target profit or set a minimum stake for Bet Angel to use. In the above example, we\u2019ve selected 5 runners, out of a market of 10, who we think could possibly win the race. Our aim in this scenario is to come away with a profit of $20 if one of our selected runners win. Bet Angel will do all the calculations for you to work out the odds and stake for each runner, but you do have the ability to override a specific runners target profit \u2013 especially handy if you're not completely confident that a particular runner will win or not but you hold more or less confidence than the rest of your selections. For example, for 8. Nobodys Puppet, which the best available back price is 10.5, we may think that there is still a possibility for it to win, so we can override the target profit for that runner to be $0 which will mean that if it wins, we won't make any profit but we also won't lose our stake. You\u2019ll notice that there are 3 check box columns: Back, Lay and Manual. These refer to the odds that you\u2019ll be using. If the back-check box is selected, then you\u2019ll be using the best available back odds and the same concept applies if you have the lay check box selected. Manual will allow you to set your own odds for bets to be placed onto the market. Bet Angel will also display the total stake for all of the bets to be placed based on your settings as well as the potential profit (indicated in green) should one of your selections win and the potential loss should they lose (indicated in red). For a more in depth run-through of the Dutching functionality, check out the Bet Angel YouTube video here . Bookmaking \u00b6 The bookmaking screen operates almost identically to the Dutching screen except that it will only place lay bets into the market. The aim of the game in this screen to create a book that ends up being over 100% so that we can secure a profit. To see an example of the bookmaking screen in action, take a look at the Bet Angel video on Youtube here . Charting \u00b6 One of the main benefits to using programs such as Bet Angel is the functionality that allows you to gain a more in depth insight into how a market is going, see trends and help you make smarter calls when it comes to placing your own bets. Charts within Bet Angel takes the information from the Betfair exchange and illustrates it in a way that allows you to interpret complex data in a more concise visual format. Advanced charting is a feature that is easily accessed from the One-click, Ladder and manual bet screens and can be accessed by clicking the graph icon next to a runners name. From here, a chart window will appear with various graphs and options to change the type of graph and the data being displayed. For more detail and information regarding Bet Angels advanced charts, check out their video tutorial here . Resources \u00b6 Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel user guide Ladder Mode In-Play trader view Custom columns Offset bets Dutching Bookmaking Youtube Ladder Mode In-Play trader view Custom Columns Dutching Bookmaking What next? \u00b6 Now that you've got the intermediate level sorted, take the next step and have a look at our Advanced guide to Bet Angel . Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Bet Angel - An intermediate guide"},{"location":"thirdPartyTools/betAngelintermediate/#bet-angel-an-intermediate-guide","text":"","title":"Bet Angel: An Intermediate guide"},{"location":"thirdPartyTools/betAngelintermediate/#the-ladder-interface","text":"The Bet Angel Ladder interface allows you to gain a deeper view of how bets are being placed on a small number of runners as opposed to the high-level overview of a market that we get from the \u2018One-Click\u2019 view. This also allows you to get an idea of any potential trends and help you make a more informed decision about your own bets. By default, the ladder interface will show you the first three runners in a market. A handy button to keep in mind is the \u2018123\u2019 button which will automatically adjust which runners you see and order them in ascending price order with the favourite placed in \u2018Ladder 1\u2019 position. You can also easily specify the runners you would like to look at by simply selecting them from the drop-down box above each ladder. Just like the \u2018One-click screen\u2019, you can click within the blue (back) or pink (lay) columns to place a bet at those specific odds, allowing you to not only get a more detailed view of what the market is doing but quickly place bets of your own.","title":"The Ladder interface"},{"location":"thirdPartyTools/betAngelintermediate/#in-play-trader-view","text":"The in-play trader view is an alternative way to look and place bets on markets which are in-play. It gives an overview of the highest and lowest price points traded for runners and allows you to easily place bets onto the market. To open the in-play trader view: click the in-play trader icon near the top right of the main Bet Angel screen. Once you\u2019ve clicked the button, a new window will open up showing every runner in the race. As you can see, there are a number of dots and lines which give us specific information relating to where bets are being placed for each runner. The blue dot indicates the last back price, the pink shows the last lay price and the yellow indicates the last traded price. The grey bar gives us the highest and lowest traded range of that particular runner. You\u2019ll also notice that there are a number of options that you can use to customize your experience with the tool depending on what information is important to you. At the top left of the window from left to right, you can hide the blue, pink and yellow dots to simplify your view change the window to reflect the win percentage scale or tick scale sort your runner list depending of a range of factors change the height of your runner rows adjust the labels for the dots to reflect different information You can also use the in-play trader view to place bets just like in the one-click screen or ladder view. You can customise what betting action to take when the left mouse button is clicked (place a back bet, a lay bet or even set that the left mouse button will back while the right button lays). By default, the odds that bets are placed at and the runner you bet on in this view will be determined by the location of your mouse cursor. It pays to be mindful of this when using this view, especially if you have confirm bets turned off as an accidental click on the in-play trader view may cause you to place a bet that you didn\u2019t want. This being said, the price that bets are placed at can be easily changed to a different factor such as the best available back price instead of the mouse cursor location.","title":"In-Play trader view"},{"location":"thirdPartyTools/betAngelintermediate/#custom-columns","text":"Custom columns are an extremely helpful way to speed up how you bet in Bet Angel markets. They allow you customize not only the information that you see in the one-click betting screen, but also lets you place specific bets faster into the market. This is particularly useful when placing bets in-play and speed is a considerable factor for your strategy. To get started with your own custom columns click the yellow star icon which will bring up the custom column's editor. Once the editor window pops up: Click \u2018New\u2019 Then enter a Profile Name for your custom column and click \u2018OK\u2019 Choose from the configure options (see below) Then click \u2018Save Column\u2019 You will have a range of options to configure your columns: Custom Column Profiles \u2013 Create multiple profiles for different betting strategies that you may implement. Each profile can have different columns that do different things and these profiles allow you to easily switch between different modes depending on what you\u2019re wanting to do in Bet Angel. General: Title \u2013 Give your column a name to make it easily identifiable (this will be the name of the column in the column chooser list and title in the column header on the Bet Angel screen) ToolTip \u2013 a custom info box that appears when the mouse is hovered over your custom column Action \u2013 Specify what happens when you click on the column, whether it places a back or lay bet, cancels a bet or triggers an automation to run Button colour \u2013 customise the colour of your column to make it easily distinguishable Display \u2013 Choose whether each box within your custom column is populated with the runners odds or text Odds \u2013 If you decide to have odds populated in your custom column, you can choose from: fixed odds, ticks offset from the best back price ticks offset from the best lay price a percentage offset from best lay price Stake \u2013 This allows you to program a pre-determined stake that can be triggered when you click in the column, essentially giving you your own bet placement shortcut buttons. Once you have set up your custom column the way that you would like it to operate, the next step is to make it visible on your screen so you can use it. Select your custom column name from the drop down box next to the yellow star icon Click on icon Scroll through the list and choose your custom column Your custom column should now be added to the right of the existing columns (see image below) If you haven\u2019t created a profile for your custom columns, click the settings button (see below), then choose columns where pop up window will appear. Find your custom column from the list and tick the box next to it.","title":"Custom Columns"},{"location":"thirdPartyTools/betAngelintermediate/#offset-bets","text":"Offset bets are extremely easy to achieve in Bet Angel thanks to a simple to use tool at the top of the One-click screen. Here you can choose to use a number of different offset bet actions and specify the number of ticks or percentage that you want your offset bet to be placed at. In the above example, if I placed a back bet on the runner named \u201c1. Andrew Swagger\u201d, the back bet will be placed at $55 and then Bet Angel will automatically place a lay bet on the same runner 2 ticks lower.","title":"Offset Bets"},{"location":"thirdPartyTools/betAngelintermediate/#dutching","text":"The dutching screen within Bet Angel is often overlooked by Bet Angel users but is extremely handy, especially if your preference is lower risk (and lower profit/loss). The dutching screen will only place back bets into a market (while the bookmaking screen will only place lay bets-see next section). Bet Angel makes dutching a lot easier by allowing you to select a number of runners within a market that you think that the runner may be amongst and place bets with a specific stake, achieve a target profit or set a minimum stake for Bet Angel to use. In the above example, we\u2019ve selected 5 runners, out of a market of 10, who we think could possibly win the race. Our aim in this scenario is to come away with a profit of $20 if one of our selected runners win. Bet Angel will do all the calculations for you to work out the odds and stake for each runner, but you do have the ability to override a specific runners target profit \u2013 especially handy if you're not completely confident that a particular runner will win or not but you hold more or less confidence than the rest of your selections. For example, for 8. Nobodys Puppet, which the best available back price is 10.5, we may think that there is still a possibility for it to win, so we can override the target profit for that runner to be $0 which will mean that if it wins, we won't make any profit but we also won't lose our stake. You\u2019ll notice that there are 3 check box columns: Back, Lay and Manual. These refer to the odds that you\u2019ll be using. If the back-check box is selected, then you\u2019ll be using the best available back odds and the same concept applies if you have the lay check box selected. Manual will allow you to set your own odds for bets to be placed onto the market. Bet Angel will also display the total stake for all of the bets to be placed based on your settings as well as the potential profit (indicated in green) should one of your selections win and the potential loss should they lose (indicated in red). For a more in depth run-through of the Dutching functionality, check out the Bet Angel YouTube video here .","title":"Dutching"},{"location":"thirdPartyTools/betAngelintermediate/#bookmaking","text":"The bookmaking screen operates almost identically to the Dutching screen except that it will only place lay bets into the market. The aim of the game in this screen to create a book that ends up being over 100% so that we can secure a profit. To see an example of the bookmaking screen in action, take a look at the Bet Angel video on Youtube here .","title":"Bookmaking"},{"location":"thirdPartyTools/betAngelintermediate/#charting","text":"One of the main benefits to using programs such as Bet Angel is the functionality that allows you to gain a more in depth insight into how a market is going, see trends and help you make smarter calls when it comes to placing your own bets. Charts within Bet Angel takes the information from the Betfair exchange and illustrates it in a way that allows you to interpret complex data in a more concise visual format. Advanced charting is a feature that is easily accessed from the One-click, Ladder and manual bet screens and can be accessed by clicking the graph icon next to a runners name. From here, a chart window will appear with various graphs and options to change the type of graph and the data being displayed. For more detail and information regarding Bet Angels advanced charts, check out their video tutorial here .","title":"Charting"},{"location":"thirdPartyTools/betAngelintermediate/#resources","text":"Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel user guide Ladder Mode In-Play trader view Custom columns Offset bets Dutching Bookmaking Youtube Ladder Mode In-Play trader view Custom Columns Dutching Bookmaking","title":"Resources"},{"location":"thirdPartyTools/betAngelintermediate/#what-next","text":"Now that you've got the intermediate level sorted, take the next step and have a look at our Advanced guide to Bet Angel .","title":"What next?"},{"location":"thirdPartyTools/betAngelintermediate/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/","text":"Cymatic Trader: Ratings automation \u00b6 Automating a thoroughbred ratings strategy using Cymatic Trader \u00b6 Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings tutorials for Bet Angel and Gruss, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Cymatic Trader. Cymatic Trader has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Cymatic Trader and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 We'll step through how we went about getting Cymatic Trader to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Cymatic Trader - Set up \u00b6 Make sure you've downloaded and installed Cymatic Trader, and signed in. Once you open the program, you will see an Excel icon which is where we will link our spreadsheet to Cymatic Trader - Downloading & formatting ratings \u00b6 Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Cymatic Trader template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using a customised Cymatic Trader template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet RunnerName refers to the entire column H in the 'RATINGS' worksheet Overrounds refers to cell BJ7 in the 'CYMATIC'worksheet, where the overrounds for the current market are calculated. UserOverround refers to cell G4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell D4 in the 'SETTINGS' worksheet UserTimeTillJump refers to cell G3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell E4 in the 'CYMATIC' worksheet. Cymatic Trader will populate a 'FALSE' flag leading up to the jump BACKLAY refers to cell G5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our Excel formula trigger: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell G5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Back market percentage (Overrounds) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell G3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell D4 of the 'SETTINGS' worksheet (named 'TimeTillJump'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is populated with the 'FALSE' flag, it's safe to place bets. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste the trigger formula into the relevant cells on each row in the 'Command' (BA) column. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column BA (BA8 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell I8 for the first runner). This goes in column BB (BB8 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are cells that are not populated with runners. A similar effect to IFERROR, if Cymatic Trader hasn't populated cell A8 with a runner name, then dont populate this cell at all. =IF(A8=0,\"\",I8) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A8=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(I8-1), stake*(J8/(J8-1))-stake)) - Selecting markets \u00b6 We used the Navigator menu in Cymatic Trader to navigate to the tracks we had ratings for. If you wanted to include all horse or greyhound races for a day you could use the 'autopilot' tool to do this more efficiently. Once you've chosen the races you're interested in tick the 'autopilot' button and Gruss will automatically cycle through each market for you. - Linking the spreadsheet \u00b6 Click the Excel icon in the main tool bar and then 'connect Excel' from the drop down menu. From here, you will be able to point Cymatic Trader in the direction of where your Excel sheet is located on your computer. Make sure 'Enable Trigger Commands' is selected and 'Clear status cells when selecting different market\" if you are automating a series of markets. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Cymatic Trader it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Ratings automation"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#cymatic-trader-ratings-automation","text":"","title":"Cymatic Trader: Ratings automation"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#automating-a-thoroughbred-ratings-strategy-using-cymatic-trader","text":"Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings tutorials for Bet Angel and Gruss, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Cymatic Trader. Cymatic Trader has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Cymatic Trader and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a thoroughbred ratings strategy using Cymatic Trader"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-the-plan","text":"We'll step through how we went about getting Cymatic Trader to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Cymatic Trader","title":"- The plan"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Cymatic Trader, and signed in. Once you open the program, you will see an Excel icon which is where we will link our spreadsheet to Cymatic Trader","title":"- Set up"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-downloading-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Cymatic Trader template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.","title":"- Downloading &amp; formatting ratings"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using a customised Cymatic Trader template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-trigger-to-place-bet","text":"In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet RunnerName refers to the entire column H in the 'RATINGS' worksheet Overrounds refers to cell BJ7 in the 'CYMATIC'worksheet, where the overrounds for the current market are calculated. UserOverround refers to cell G4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell D4 in the 'SETTINGS' worksheet UserTimeTillJump refers to cell G3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell E4 in the 'CYMATIC' worksheet. Cymatic Trader will populate a 'FALSE' flag leading up to the jump BACKLAY refers to cell G5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our Excel formula trigger: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell G5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Back market percentage (Overrounds) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell G3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell D4 of the 'SETTINGS' worksheet (named 'TimeTillJump'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is populated with the 'FALSE' flag, it's safe to place bets. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste the trigger formula into the relevant cells on each row in the 'Command' (BA) column. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column BA (BA8 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell I8 for the first runner). This goes in column BB (BB8 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are cells that are not populated with runners. A similar effect to IFERROR, if Cymatic Trader hasn't populated cell A8 with a runner name, then dont populate this cell at all. =IF(A8=0,\"\",I8) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A8=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(I8-1), stake*(J8/(J8-1))-stake))","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-selecting-markets","text":"We used the Navigator menu in Cymatic Trader to navigate to the tracks we had ratings for. If you wanted to include all horse or greyhound races for a day you could use the 'autopilot' tool to do this more efficiently. Once you've chosen the races you're interested in tick the 'autopilot' button and Gruss will automatically cycle through each market for you.","title":"- Selecting markets"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#-linking-the-spreadsheet","text":"Click the Excel icon in the main tool bar and then 'connect Excel' from the drop down menu. From here, you will be able to point Cymatic Trader in the direction of where your Excel sheet is located on your computer. Make sure 'Enable Trigger Commands' is selected and 'Clear status cells when selecting different market\" if you are automating a series of markets.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Cymatic Trader it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts.","title":"Areas for improvement"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/cymaticTraderRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/grussKellyStake/","text":"Gruss Betting Assistant: Kelly Criterion staking \u00b6 Automating with Kelly staking method and Gruss Betting Assistant \u00b6 In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 We'll be building on the Gruss Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Gruss Ratings tutorial Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Understanding how the Kelly Criterion staking strategy works Tool: Gruss Betting Assistant - Recapping the strategy covered in the Gruss ratings automation tutorial \u00b6 We'll be using the same trigger strategy that's outlined in the Gruss Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'MARKET' worksheet (column S) and we've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Gruss ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here . - Set up \u00b6 Make sure you've downloaded and installed Gruss, and signed in. - Writing your rules \u00b6 We're using a customised version of the Gruss Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell I2 of the 'MARKET' worksheet UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake StakeType refers to cell X1 of the \"SETTINGS' worksheet Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet. Stepping through each step: \u00b6 Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('MARKET' worksheet) and return the best available back odds from the G column =IFERROR(INDEX(Market!$B$5:$M$50,MATCH(H2,Market!$A$5:$A$50,0),5),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Gruss populates in cell I2 of the 'MARKET' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType)) - You know the drill \u00b6 The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss. - Selecting markets \u00b6 Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps. - Linking the spreadsheet \u00b6 This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Show balance which is required for this staking strategy to work Then click OK and the sheet with be linked with the program. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Kelly Criterion staking"},{"location":"thirdPartyTools/grussKellyStake/#gruss-betting-assistant-kelly-criterion-staking","text":"","title":"Gruss Betting Assistant: Kelly Criterion staking"},{"location":"thirdPartyTools/grussKellyStake/#automating-with-kelly-staking-method-and-gruss-betting-assistant","text":"In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating with Kelly staking method and Gruss Betting Assistant"},{"location":"thirdPartyTools/grussKellyStake/#-the-plan","text":"We'll be building on the Gruss Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Gruss Ratings tutorial Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Understanding how the Kelly Criterion staking strategy works Tool: Gruss Betting Assistant","title":"- The plan"},{"location":"thirdPartyTools/grussKellyStake/#-recapping-the-strategy-covered-in-the-gruss-ratings-automation-tutorial","text":"We'll be using the same trigger strategy that's outlined in the Gruss Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'MARKET' worksheet (column S) and we've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Gruss ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here .","title":"- Recapping the strategy covered in the Gruss ratings automation tutorial"},{"location":"thirdPartyTools/grussKellyStake/#-set-up","text":"Make sure you've downloaded and installed Gruss, and signed in.","title":"- Set up"},{"location":"thirdPartyTools/grussKellyStake/#-writing-your-rules","text":"We're using a customised version of the Gruss Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/grussKellyStake/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell I2 of the 'MARKET' worksheet UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake StakeType refers to cell X1 of the \"SETTINGS' worksheet Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/grussKellyStake/#stepping-through-each-step","text":"Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('MARKET' worksheet) and return the best available back odds from the G column =IFERROR(INDEX(Market!$B$5:$M$50,MATCH(H2,Market!$A$5:$A$50,0),5),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Gruss populates in cell I2 of the 'MARKET' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"Stepping through each step:"},{"location":"thirdPartyTools/grussKellyStake/#-preparing-the-spreadsheet","text":"You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/grussKellyStake/#-you-know-the-drill","text":"The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss.","title":"- You know the drill"},{"location":"thirdPartyTools/grussKellyStake/#-selecting-markets","text":"Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.","title":"- Selecting markets"},{"location":"thirdPartyTools/grussKellyStake/#-linking-the-spreadsheet","text":"This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Show balance which is required for this staking strategy to work Then click OK and the sheet with be linked with the program.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/grussKellyStake/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/grussKellyStake/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this.","title":"Areas for improvement"},{"location":"thirdPartyTools/grussKellyStake/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/grussKellyStake/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/","text":"Gruss Betting Assistant: Market favourite automation \u00b6 Automating a market favourite strategy using Gruss Betting Assistant \u00b6 Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them. Building on our previous articles , we're using the spreadsheet functionality available in Gruss Betting Assistant to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Gruss Betting Assistant - Set up \u00b6 Make sure you've downloaded and installed Gruss, and signed in. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back runners when: the selection's available to back price (Column F) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this the scheduled event start time is less and greater than what we specify Back market percentage is less than a certain value that we choose the event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell C5 in the 'SETTINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell Y4 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets UserOverround refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell C9, C10 and C11 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market MinTime refers to cell C3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell E3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell E2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Not in Play\" for each market MarketStatus1, MarketStatus2, MarketStatus3 refers to cell F2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Suspended\" for each market This is our trigger on Excel formula: ``` excel tab=\"Multi line\" =IF( AND( (COUNT( F F 5, F F 6, F F 7, F F 8, F F 9, F F 10, F F 11, F F 12, F F 13, F F 14, F F 15, F F 16, F F 17, F F 18, F F 19, F F 20, F F 21, F F 22, F F 23, F F 24, F F 25, F F 26, F F 27, F F 28, F F 29, F F 30, F F 31, F F 32, F F 33, F F 34, F F 35, F F 36, F F 37, F F 38, F F 39, F F 40, F F 41, F F 42, F F 43, F F 44, F F 45, F F 46, F F 47, F F 48, F F 49, F F 50) -RANK(F5,( F F 5, F F 6, F F 7, F F 8, F F 9, F F 10, F F 11, F F 12, F F 13, F F 14, F F 15, F F 16, F F 17, F F 18, F F 19, F F 20, F F 21, F F 22, F F 23, F F 24, F F 25, F F 26, F F 27, F F 28, F F 29, F F 30, F F 31, F F 32, F F 33, F F 34, F F 35, F F 36, F F 37, F F 38, F F 39, F F 40, F F 41, F F 42, F F 43, F F 44, F F 45, F F 46, F F 47, F F 48, F F 49, F F 50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") ``` excel tab=\"Single line\" =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1<>\"Suspended\"),\"BACK-SP\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column F) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") TimeTillJump1 < MaxTime and > MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C3 and greater than cell E3 in the 'SETTINGS' worksheet (named 'MinTime' and 'MaxTime' respectively). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. We'll keep it simple by referencing the value in cell C9 (named 'TimeTillJump1') in the 'SETTINGS' worksheet, where we've already done the calculations for you. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") Overrounds1 < UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C4 of the 'SETTINGS' worksheet =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to E2 in the 'MARKET' worksheet, if this cell displays \"Not In Play\" as a value, it's safe to place bets. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") MarketStatus1: checks whether the event is suspended, by checking if Gruss has populated cell F2 in the \"MARKET\" worksheet with \"Suspended\". If the cell has any other value, it's safe to place bets. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") Result: if the statement above is true, the formula returns 'BACK-SP', at which point the bet will trigger, taking BSP. If any of the previous conditions are not met, then no bet will be placed and the cell will remin blank. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") updating the trigger for 'MARKET 2' and 'MARKET 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'MARKET 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, InPlay2=\"Not In Play\", MarketStatus2<>\"Suspended\"), \"BACK-SP\", \"\") Trigger for 'MARKET 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump3 < MaxTime, TimeTillJump3 > MinTime, Overrounds3<UserOverrounds, InPlay3=\"Not In Play\", MarketStatus3<>\"Suspended\"), \"BACK-SP\", \"\") Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste this formula into the relevant cells for each of the runners. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( (COUNT( F F 5, F F 6, F F 7, F F 8, F F 9, F F 10, F F 11, F F 12, F F 13, F F 14, F F 15, F F 16, F F 17, F F 18, F F 19, F F 20, F F 21, F F 22, F F 23, F F 24, F F 25, F F 26, F F 27, F F 28, F F 29, F F 30, F F 31, F F 32, F F 33, F F 34, F F 35, F F 36, F F 37, F F 38, F F 39, F F 40, F F 41, F F 42, F F 43, F F 44, F F 45, F F 46, F F 47, F F 48, F F 49, F F 50) -RANK(F5,( F F 5, F F 6, F F 7, F F 8, F F 9, F F 10, F F 11, F F 12, F F 13, F F 14, F F 15, F F 16, F F 17, F F 18, F F 19, F F 20, F F 21, F F 22, F F 23, F F 24, F F 25, F F 26, F F 27, F F 28, F F 29, F F 30, F F 31, F F 32, F F 33, F F 34, F F 35, F F 36, F F 37, F F 38, F F 39, F F 40, F F 41, F F 42, F F 43, F F 44, F F 45, F F 46, F F 47, F F 48, F F 49, F F 50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1<>\"Suspended\"),\"BACK-SP\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column A. A similar effect to IFERROR, if Gruss hasn't populated cell A5 with a runner name, then dont populate this cell at all. =IF(A5=\"\",\"\",\"1000\") Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat stake here, so will just place $10 on each runner. This goes in column S (S5 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A5=\"\",\"\",stake) - You know the drill \u00b6 The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss. - Selecting markets \u00b6 Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps. - Linking the spreadsheet \u00b6 This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Market favourite automation"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#gruss-betting-assistant-market-favourite-automation","text":"","title":"Gruss Betting Assistant: Market favourite automation"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#automating-a-market-favourite-strategy-using-gruss-betting-assistant","text":"Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them. Building on our previous articles , we're using the spreadsheet functionality available in Gruss Betting Assistant to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions.","title":"Automating a market favourite strategy using Gruss Betting Assistant"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-the-plan","text":"Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Gruss Betting Assistant","title":"- The plan"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-set-up","text":"Make sure you've downloaded and installed Gruss, and signed in.","title":"- Set up"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: the selection's available to back price (Column F) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this the scheduled event start time is less and greater than what we specify Back market percentage is less than a certain value that we choose the event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell C5 in the 'SETTINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell Y4 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets UserOverround refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell C9, C10 and C11 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market MinTime refers to cell C3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell E3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell E2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Not in Play\" for each market MarketStatus1, MarketStatus2, MarketStatus3 refers to cell F2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Suspended\" for each market This is our trigger on Excel formula: ``` excel tab=\"Multi line\" =IF( AND( (COUNT( F F 5, F F 6, F F 7, F F 8, F F 9, F F 10, F F 11, F F 12, F F 13, F F 14, F F 15, F F 16, F F 17, F F 18, F F 19, F F 20, F F 21, F F 22, F F 23, F F 24, F F 25, F F 26, F F 27, F F 28, F F 29, F F 30, F F 31, F F 32, F F 33, F F 34, F F 35, F F 36, F F 37, F F 38, F F 39, F F 40, F F 41, F F 42, F F 43, F F 44, F F 45, F F 46, F F 47, F F 48, F F 49, F F 50) -RANK(F5,( F F 5, F F 6, F F 7, F F 8, F F 9, F F 10, F F 11, F F 12, F F 13, F F 14, F F 15, F F 16, F F 17, F F 18, F F 19, F F 20, F F 21, F F 22, F F 23, F F 24, F F 25, F F 26, F F 27, F F 28, F F 29, F F 30, F F 31, F F 32, F F 33, F F 34, F F 35, F F 36, F F 37, F F 38, F F 39, F F 40, F F 41, F F 42, F F 43, F F 44, F F 45, F F 46, F F 47, F F 48, F F 49, F F 50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") ``` excel tab=\"Single line\" =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1<>\"Suspended\"),\"BACK-SP\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column F) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") TimeTillJump1 < MaxTime and > MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C3 and greater than cell E3 in the 'SETTINGS' worksheet (named 'MinTime' and 'MaxTime' respectively). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. We'll keep it simple by referencing the value in cell C9 (named 'TimeTillJump1') in the 'SETTINGS' worksheet, where we've already done the calculations for you. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") Overrounds1 < UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C4 of the 'SETTINGS' worksheet =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to E2 in the 'MARKET' worksheet, if this cell displays \"Not In Play\" as a value, it's safe to place bets. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") MarketStatus1: checks whether the event is suspended, by checking if Gruss has populated cell F2 in the \"MARKET\" worksheet with \"Suspended\". If the cell has any other value, it's safe to place bets. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") Result: if the statement above is true, the formula returns 'BACK-SP', at which point the bet will trigger, taking BSP. If any of the previous conditions are not met, then no bet will be placed and the cell will remin blank. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") updating the trigger for 'MARKET 2' and 'MARKET 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'MARKET 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, InPlay2=\"Not In Play\", MarketStatus2<>\"Suspended\"), \"BACK-SP\", \"\") Trigger for 'MARKET 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump3 < MaxTime, TimeTillJump3 > MinTime, Overrounds3<UserOverrounds, InPlay3=\"Not In Play\", MarketStatus3<>\"Suspended\"), \"BACK-SP\", \"\") Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cells for each of the runners. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( (COUNT( F F 5, F F 6, F F 7, F F 8, F F 9, F F 10, F F 11, F F 12, F F 13, F F 14, F F 15, F F 16, F F 17, F F 18, F F 19, F F 20, F F 21, F F 22, F F 23, F F 24, F F 25, F F 26, F F 27, F F 28, F F 29, F F 30, F F 31, F F 32, F F 33, F F 34, F F 35, F F 36, F F 37, F F 38, F F 39, F F 40, F F 41, F F 42, F F 43, F F 44, F F 45, F F 46, F F 47, F F 48, F F 49, F F 50) -RANK(F5,( F F 5, F F 6, F F 7, F F 8, F F 9, F F 10, F F 11, F F 12, F F 13, F F 14, F F 15, F F 16, F F 17, F F 18, F F 19, F F 20, F F 21, F F 22, F F 23, F F 24, F F 25, F F 26, F F 27, F F 28, F F 29, F F 30, F F 31, F F 32, F F 33, F F 34, F F 35, F F 36, F F 37, F F 38, F F 39, F F 40, F F 41, F F 42, F F 43, F F 44, F F 45, F F 46, F F 47, F F 48, F F 49, F F 50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1<>\"Suspended\"),\"BACK-SP\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column A. A similar effect to IFERROR, if Gruss hasn't populated cell A5 with a runner name, then dont populate this cell at all. =IF(A5=\"\",\"\",\"1000\") Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat stake here, so will just place $10 on each runner. This goes in column S (S5 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A5=\"\",\"\",stake)","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-you-know-the-drill","text":"The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss.","title":"- You know the drill"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-selecting-markets","text":"Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.","title":"- Selecting markets"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#-linking-the-spreadsheet","text":"This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this.","title":"Areas for improvement"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/grussMarketFavouriteAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/grussRatingsAutomation/","text":"Gruss Betting Assistant: Ratings automation \u00b6 Automating a greyhound ratings strategy using Gruss \u00b6 Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings automation tutorial for Bet Angel, but here we'll be using the ratings for greyhounds, created by the data science team at Betfair and incorporate them into our automation in Gruss. Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan \u00b6 We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian and Queensland greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Gruss Betting Assistant's Excel triggered betting feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. Here we'll step through how we went about getting Gruss to place bets using these ratings . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Obviously, you can use your own ratings and change the rules according to what your strategy is. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet We set up with our rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Gruss Betting Assistant - Set up \u00b6 Make sure you've downloaded and installed Gruss Betting Assistant , and signed in. - Downloading & formatting ratings \u00b6 Here we're using the Betfair's Data Scientists' greyhound ratings model for greyhound racing but alternatively you can follow the same process using the Betfair's Data Scientists' thoroughbred Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Gruss template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. - Writing your rules \u00b6 As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. There are lots of posts on the Gruss Forum on the topic if you want to explore it more yourself. This is how we used Excel to implement our set of rules. - Trigger to place bet \u00b6 In short, we want to back runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas \u00b6 Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column J in the 'RATINGS' worksheet SelectionID refers to the entire column G in the 'RATINGS' worksheet Overround refers to cell Y in the 'MARKET'worksheet, where the overrounds for the current market are calculated. UserOverround refers to cell C5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell C9 in the 'SETTINGS' worksheet UserTimeTillJump refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell E2 in the 'MARKET' worksheet. Gruss will populate a 'Not In Play' status leading up to the jump BACKLAY refers to cell C6 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners MARKETSTATUS refers to F2 in the 'MARKET' worksheet This is our trigger for the 'MARKET' worksheet ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus<>\"Suspended\"),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell D6). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Back market percentage (Overround) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell C4 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If cell E2 in the 'MARKET' worksheet has a 'Not In Play' flag, it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Market Status checking whether the event has been suspended - if there is any flag other than 'Suspended' in cell F2 of the 'MARKET' worksheet, it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet \u00b6 You need to copy/paste these three formulas into the relevant cell on each runner - we did a few extra rows than the number of runners in the markets we were looking at, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner) in the 'MARKET' worksheet. ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus<>\"Suspended\"),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell F5 for the first runner). This goes in column R (R5 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. =IF(F5=0,\"\",F5) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A5=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(F5-1),stake*(H5/(H5-1))-stake) - Selecting markets \u00b6 Gruss makes it really easy to select markets in bulk. You could go through an add each market you have in your ratings individually, but it's much easier to just use the quick Pick functionality to add all Australian racing win markets. This is safe, because bets will only fire when they link up with a runner in your 'RATINGS' sheet. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps. - Linking the spreadsheet \u00b6 This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement \u00b6 There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Ratings automation"},{"location":"thirdPartyTools/grussRatingsAutomation/#gruss-betting-assistant-ratings-automation","text":"","title":"Gruss Betting Assistant: Ratings automation"},{"location":"thirdPartyTools/grussRatingsAutomation/#automating-a-greyhound-ratings-strategy-using-gruss","text":"Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings automation tutorial for Bet Angel, but here we'll be using the ratings for greyhounds, created by the data science team at Betfair and incorporate them into our automation in Gruss. Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a greyhound ratings strategy using Gruss"},{"location":"thirdPartyTools/grussRatingsAutomation/#-the-plan","text":"We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian and Queensland greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Gruss Betting Assistant's Excel triggered betting feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. Here we'll step through how we went about getting Gruss to place bets using these ratings . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Obviously, you can use your own ratings and change the rules according to what your strategy is. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet We set up with our rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Gruss Betting Assistant","title":"- The plan"},{"location":"thirdPartyTools/grussRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Gruss Betting Assistant , and signed in.","title":"- Set up"},{"location":"thirdPartyTools/grussRatingsAutomation/#-downloading-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' greyhound ratings model for greyhound racing but alternatively you can follow the same process using the Betfair's Data Scientists' thoroughbred Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Gruss template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.","title":"- Downloading &amp; formatting ratings"},{"location":"thirdPartyTools/grussRatingsAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. There are lots of posts on the Gruss Forum on the topic if you want to explore it more yourself. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"thirdPartyTools/grussRatingsAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"thirdPartyTools/grussRatingsAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column J in the 'RATINGS' worksheet SelectionID refers to the entire column G in the 'RATINGS' worksheet Overround refers to cell Y in the 'MARKET'worksheet, where the overrounds for the current market are calculated. UserOverround refers to cell C5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell C9 in the 'SETTINGS' worksheet UserTimeTillJump refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell E2 in the 'MARKET' worksheet. Gruss will populate a 'Not In Play' status leading up to the jump BACKLAY refers to cell C6 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners MARKETSTATUS refers to F2 in the 'MARKET' worksheet This is our trigger for the 'MARKET' worksheet ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus<>\"Suspended\"),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell D6). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Back market percentage (Overround) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell C4 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If cell E2 in the 'MARKET' worksheet has a 'Not In Play' flag, it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Market Status checking whether the event has been suspended - if there is any flag other than 'Suspended' in cell F2 of the 'MARKET' worksheet, it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"thirdPartyTools/grussRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste these three formulas into the relevant cell on each runner - we did a few extra rows than the number of runners in the markets we were looking at, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner) in the 'MARKET' worksheet. ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus<>\"Suspended\"),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell F5 for the first runner). This goes in column R (R5 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. =IF(F5=0,\"\",F5) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A5=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(F5-1),stake*(H5/(H5-1))-stake)","title":"- Preparing the spreadsheet"},{"location":"thirdPartyTools/grussRatingsAutomation/#-selecting-markets","text":"Gruss makes it really easy to select markets in bulk. You could go through an add each market you have in your ratings individually, but it's much easier to just use the quick Pick functionality to add all Australian racing win markets. This is safe, because bets will only fire when they link up with a runner in your 'RATINGS' sheet. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.","title":"- Selecting markets"},{"location":"thirdPartyTools/grussRatingsAutomation/#-linking-the-spreadsheet","text":"This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program.","title":"- Linking the spreadsheet"},{"location":"thirdPartyTools/grussRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/grussRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this.","title":"Areas for improvement"},{"location":"thirdPartyTools/grussRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au","title":"What next?"},{"location":"thirdPartyTools/grussRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/","text":"Gruss: Automating simultaneous markets \u00b6 Don't miss out on a market with simultaneous automation \u00b6 If you have a concern of missing markets due to delays or unforeseen circumstances at a market, Gruss is able to work off multiple worksheets for different meetings, all from the same workbook. For example, we can have worksheet one from our spreadsheet work through the markets taking place in Flemington, while worksheet 2 will work through markets at Sandown, worksheet 3 works through markets at Bendigo etc. To Set this up, we need to do need to duplicate the main \u2018Market\u2019 worksheet and have enough instances of the sheet for each meeting. There are four meetings that I want to cover, so I have duplicated the worksheet four times and renamed them to help me keep track. After these changes have been made, restart Gruss so that the changes go into effect. Once Gruss is back open, as before, click \u2018Market\u2019, \u2018Add to Quick Pick List\u2019, \u2018Horse Racing\u2019 (Or any event you are automating for), the country that you would like to bet and then \u2018All Win\u2019. This should generate a bunch of tabs in Gruss, each tab containing all the markets for a specific meeting. Simply close the ones you don\u2019t want. Once you\u2019re ready, click the \u2018Excel\u2019 menu in Gruss, and \u2018Log Multiple sheets Quick link\u2019. Select your Excel Workbook, assign the worksheets, select \u2018Enable Triggered Betting\u2019, \u201cCLEAR trigger clears matched odds\u2019, \u2018Clear bet refs on auto select markets\u2019, \u2018Clear bet refs on manual select market\u2019 and finally \u2018Auto Select First Market\u2019. Your screen should look something like this: When you\u2019re ready for automation to take over, click \u2018Start logging\u2019 at the bottom of the window. And you're set! \u00b6 Once you've set your spreadsheet set up and you're comfortable using Gruss it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? \u00b6 We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Simultaneous markets"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#gruss-automating-simultaneous-markets","text":"","title":"Gruss: Automating simultaneous markets"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#dont-miss-out-on-a-market-with-simultaneous-automation","text":"If you have a concern of missing markets due to delays or unforeseen circumstances at a market, Gruss is able to work off multiple worksheets for different meetings, all from the same workbook. For example, we can have worksheet one from our spreadsheet work through the markets taking place in Flemington, while worksheet 2 will work through markets at Sandown, worksheet 3 works through markets at Bendigo etc. To Set this up, we need to do need to duplicate the main \u2018Market\u2019 worksheet and have enough instances of the sheet for each meeting. There are four meetings that I want to cover, so I have duplicated the worksheet four times and renamed them to help me keep track. After these changes have been made, restart Gruss so that the changes go into effect. Once Gruss is back open, as before, click \u2018Market\u2019, \u2018Add to Quick Pick List\u2019, \u2018Horse Racing\u2019 (Or any event you are automating for), the country that you would like to bet and then \u2018All Win\u2019. This should generate a bunch of tabs in Gruss, each tab containing all the markets for a specific meeting. Simply close the ones you don\u2019t want. Once you\u2019re ready, click the \u2018Excel\u2019 menu in Gruss, and \u2018Log Multiple sheets Quick link\u2019. Select your Excel Workbook, assign the worksheets, select \u2018Enable Triggered Betting\u2019, \u201cCLEAR trigger clears matched odds\u2019, \u2018Clear bet refs on auto select markets\u2019, \u2018Clear bet refs on manual select market\u2019 and finally \u2018Auto Select First Market\u2019. Your screen should look something like this: When you\u2019re ready for automation to take over, click \u2018Start logging\u2019 at the bottom of the window.","title":"Don't miss out on a market with simultaneous automation"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"thirdPartyTools/grusslSimultaneousMarkets/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/overview/","text":"Third party tools overview \u00b6 There are many third party applications that can be used in conjunction with Betfair which can help wagering from more one-click bets to automating your strategies. If you haven\u2019t before used a third-party betting tool, below is a overview of the most popular tools available. If you have any suggestions for new tutorials / improvements please reach out to automation@betfair.com.au - We'd love to hear your thoughts and feedback. Links to third party Tools \u00b6 Bet Angel Pro // Gruss // Market Feeder // Cymatic Trader Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Tools Overview"},{"location":"thirdPartyTools/overview/#third-party-tools-overview","text":"There are many third party applications that can be used in conjunction with Betfair which can help wagering from more one-click bets to automating your strategies. If you haven\u2019t before used a third-party betting tool, below is a overview of the most popular tools available. If you have any suggestions for new tutorials / improvements please reach out to automation@betfair.com.au - We'd love to hear your thoughts and feedback.","title":"Third party tools overview"},{"location":"thirdPartyTools/overview/#links-to-third-party-tools","text":"Bet Angel Pro // Gruss // Market Feeder // Cymatic Trader","title":"Links to third party Tools"},{"location":"thirdPartyTools/overview/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/BfBotManager/BfBotManager/","text":"Bf Bot Manager is software designed to help you automate your betting and trading at Betfair betting exchange. Bf Bot Manager allows you to run unlimited number of strategies/bots at the same time. You can have one strategy betting on favourites, second one trading on horse races, third one betting on tennis matches and several strategies betting on football events or tipster tips. You can set software to automatically download or import your or tipster betting tips and bet on them just few seconds before event start time. Your settings can be exported to file allowing you to easily share strategy with your friends or to create backup. Software also supports manual bet placement and has ladder and grid controls for one click betting. Hedge (green/red up) functionality can be set to execute automatically or on single click. Bf Bot Manager supports simulation mode, many staking plans, loss recovery and much more. It even has football statistics for major leagues allowing you to automatically bet only on teams with specific previous results. You can also order custom functionality that we can develop for you and adjust settings to suit your own requirements. BF BOT MANAGER KEY FEATURES Custom Bots Dutching Automation Greening Tools Simulation Mode One-click Betting To find out more and to sign up for a FREE 5 Day trial, head to the BF Bot Manager Website","title":"BfBotManager"},{"location":"thirdPartyTools/CymaticTrader/CymaticTrader/","text":"Taking trading to another level, this fully featured, super fast trading software has many unique features developed over a number of years. Intuitive throughout, it was designed for professional traders but is equally welcomed by novices. It was the first program to actually reveal your estimated position in the exchange order book queue (PIQ), helping you gain an advantage when trading manually or with the built in robot. PIQ is now an essential feature for many traders. You can open multiple markets simultaneously and perform one-click trading using customizable ladders or grids. Greening, tick-offset, stop-loss and a range of other features are included, as you\u2019d expect in a great low-latency trading app. The Excel integration capability enables you to view real-time prices in Excel, trigger orders from Excel and even create your own fully automated trading robot! The integrated advanced charting is capable of displaying candle-stick, bar or line charts, in various time frames, plus a huge range of technical analysis indicators. Zooming, panning and drawing magnetic trend lines are all made easy. Another unique feature is the \u2018API Monitor\u2019, it can instantly reveal errors or bottlenecks caused by the internet or the exchange \u2013 thus helping you avoid trading during bad periods that other traders may often be oblivious to. It also calculates real time statistics such as the latency of various types of calls to the Betfair API for your current trading session. Warning messages and statistical reports can even be automatically emailed to you by the software. The software is well suited to single or multiple monitor scenarios. Downloading/filtering of betting history and account statements is also included. GRUSS KEY FEATURES Rapid keyboard betting short cuts Advanced charting Football Odds predictor Ladder interface Excel integration Practice mode To find out more and to sign up for a FREE 14 Day trial, head to the Cymatic Trader Website .","title":"Overview - Cymatic Trader"},{"location":"thirdPartyTools/Elo/Elo/","text":"- What is Elo? \u00b6 The Elo rating system was orginally created by Hungarian born physics professor Arpad Elo who was a chess master and competed within the United States Chess Federation (USCF) and developed the system as an alternative to other rating systems that were considered to be inaccurate. Rather than rating a players performance by their overall wins and losses like previous systems, the Elo rating system works by assigning each player or team an Elo rating. When a team or player beats another, the winning side gains a portion of the losing sides points. This difference between Elo ratings between competitors is then used to create a probability for a particular outcome. The Elo rating system has become so popular since its inception, it is widely used today behind the scenes for a range of applications such as dating websites to determince compatibility between matches and even video game tournaments to ensure players of a similar skill level are matched together. - How does Elo work? \u00b6 For the purpose of this explanation, we'll be using Elo in the context of rating AFL teams. Each team begins with an Elo rating of 1500 and will gain or lose a portion of their Elo from / to the opposite team depending if they win or lose a game. For example, if team A has 1500 Elo and they beat team B - who also started with a 1500 Elo, then a certain portion of Elo (usually 30) is deducted from the loser and awarded to the winner. In this scenario, Team A will have an Elo rating of 1530 and Team B will be left with 1470. What makes Elo great for betting in sports is that it can be converted to a probability which in turn can be converted into odds used to place bets. - The Algorithm \u00b6 Each team will have their own Elo rating which will be determined based on their historical performance. If a team has never played a game, then they will begin with an Elo rating of 1500. The following formula is used to calculate the probability of a win for each team: A crucial consideration of any Elo rating system is the implementation of what is known as the K-factor. This determines the \"sensitivity\" how much of an impact a win or a loss has on a teams Elo rating. A good K-factor is generally around 32 but this can be adjusted to suit. Once the match has been concluded, each teams Elo is then updated to reflect any changes to their Elo rating, taking into account the full sequence of wins and loses up to the most recent match played. For this, the below formula is used: Where elo_i (t+1) is the updated Elo, elo_i (t) was their Elo before the match, K is the K-factor mentioned before (which we determined to be 32), outcome is an indicator of the match outcome (1 if it was won by team A, 0 if a loss), and Pwin was the pre-match probability of winning for team A, as given by the previous formula. If two teams play their first match, both with Elo of 1500, Pwin is equal to 0.5 so, for a K-factor of 32, the winning team would gain 16 points and the losing team would lose 16 points. In their next match, the teams start with Elo ratings of 1,516 and 1,484 respectively, and by updating their rating for all matches played, their current Elo rating can be calculated. - Taking Elo to the next level \u00b6 If you're interested in learning more about Elo and implementing a dynamic K-factor instead of a static one for greater accuracy, check out the article we've created in the Betfair Hub which applies Elo in the context of modelling tennis . Doing so can bring greater accuracy, especially when applied to the AFL season as the K-factor can be set to change depending on the importance of a particular match such as finals and early / late season games. Resources Learn more about the origins of Elo by visiting the Elo Wikipedia page Take a look at the Elo Tennis model article on the Betfair Hub Here's another good resource to help gain a better understanding of how Elo works and is applied: The Math behind Elo Disclaimer \u00b6 Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Elo"},{"location":"thirdPartyTools/Elo/Elo/#-what-is-elo","text":"The Elo rating system was orginally created by Hungarian born physics professor Arpad Elo who was a chess master and competed within the United States Chess Federation (USCF) and developed the system as an alternative to other rating systems that were considered to be inaccurate. Rather than rating a players performance by their overall wins and losses like previous systems, the Elo rating system works by assigning each player or team an Elo rating. When a team or player beats another, the winning side gains a portion of the losing sides points. This difference between Elo ratings between competitors is then used to create a probability for a particular outcome. The Elo rating system has become so popular since its inception, it is widely used today behind the scenes for a range of applications such as dating websites to determince compatibility between matches and even video game tournaments to ensure players of a similar skill level are matched together.","title":"- What is Elo?"},{"location":"thirdPartyTools/Elo/Elo/#-how-does-elo-work","text":"For the purpose of this explanation, we'll be using Elo in the context of rating AFL teams. Each team begins with an Elo rating of 1500 and will gain or lose a portion of their Elo from / to the opposite team depending if they win or lose a game. For example, if team A has 1500 Elo and they beat team B - who also started with a 1500 Elo, then a certain portion of Elo (usually 30) is deducted from the loser and awarded to the winner. In this scenario, Team A will have an Elo rating of 1530 and Team B will be left with 1470. What makes Elo great for betting in sports is that it can be converted to a probability which in turn can be converted into odds used to place bets.","title":"- How does Elo work?"},{"location":"thirdPartyTools/Elo/Elo/#-the-algorithm","text":"Each team will have their own Elo rating which will be determined based on their historical performance. If a team has never played a game, then they will begin with an Elo rating of 1500. The following formula is used to calculate the probability of a win for each team: A crucial consideration of any Elo rating system is the implementation of what is known as the K-factor. This determines the \"sensitivity\" how much of an impact a win or a loss has on a teams Elo rating. A good K-factor is generally around 32 but this can be adjusted to suit. Once the match has been concluded, each teams Elo is then updated to reflect any changes to their Elo rating, taking into account the full sequence of wins and loses up to the most recent match played. For this, the below formula is used: Where elo_i (t+1) is the updated Elo, elo_i (t) was their Elo before the match, K is the K-factor mentioned before (which we determined to be 32), outcome is an indicator of the match outcome (1 if it was won by team A, 0 if a loss), and Pwin was the pre-match probability of winning for team A, as given by the previous formula. If two teams play their first match, both with Elo of 1500, Pwin is equal to 0.5 so, for a K-factor of 32, the winning team would gain 16 points and the losing team would lose 16 points. In their next match, the teams start with Elo ratings of 1,516 and 1,484 respectively, and by updating their rating for all matches played, their current Elo rating can be calculated.","title":"- The Algorithm"},{"location":"thirdPartyTools/Elo/Elo/#-taking-elo-to-the-next-level","text":"If you're interested in learning more about Elo and implementing a dynamic K-factor instead of a static one for greater accuracy, check out the article we've created in the Betfair Hub which applies Elo in the context of modelling tennis . Doing so can bring greater accuracy, especially when applied to the AFL season as the K-factor can be set to change depending on the importance of a particular match such as finals and early / late season games. Resources Learn more about the origins of Elo by visiting the Elo Wikipedia page Take a look at the Elo Tennis model article on the Betfair Hub Here's another good resource to help gain a better understanding of how Elo works and is applied: The Math behind Elo","title":"- Taking Elo to the next level"},{"location":"thirdPartyTools/Elo/Elo/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"thirdPartyTools/Gruss/Gruss/","text":"Gruss Software offers one-click betting in the standard or ladder type interface and also the ability to link into Excel using pre-defined triggers to place bets. Betting Assistant deploys a wealth of functionality such as green-up, fill or kill, stop loss, dutch betting, coupon market view and more. Gruss Software is a constantly evolving solution where the developers listen to the users and their needs. Users can communicate directly with the developers, make suggestions and exchange ideas on the Betting Assistant Forum. GRUSS KEY FEATURES One click betting Dutching tool Ladder interface One click betting Advanced market navigation Excel integration To find out more and to sign up for a FREE 30 Day trial, head to the Gruss Software Website .","title":"Overview - Gruss"},{"location":"thirdPartyTools/MarketFeeder/MarketFeeder/","text":"Auto-Green-Up, Auto-Dutch and implement triggered betting with MarketFeeder Pro. As well as the standard manual betting tools you know and love, the auto-trading functions allow for stress-free betting with the triggered betting feature. A trigger is an effective way to maximise your time and safe-guard you from any errors. A short set of instructions can be set to perform one of 50 actions, including backing, laying, cancelling bets, slowing or speeding up market refresh, greening up and spreading the loss, alerting, emailing and indeed being your second hand on Betfair. MARKETFEEDER PRO KEY FEATURES Monitoring Simultanious Markets 0.3 Second Refresh Rate Automated Market Search tool Triggered Betting Excel Spreadsheet Interaction Automatic Greenup Automatic Dutching Ladder Interface for Scalping To find out more and to sign up for a FREE 1 month trial, head to the MarketFeeder Pro Website","title":"MarketFeeder"},{"location":"thirdPartyTools/betAngel/betAngel/","text":"Bet Angel is one of the longest established API Products available. Bet Angel has three separate products available for Betfair customers. Bet Angel Basic is free and useful for placing occasional bets but has limited trading and professional tools. Bet Angel Trader contains the essential trading tools and Bet Angel Professional is the ultimate trading tool kit providing all the advanced tools and features required. Bet Angel Betting Applications offer a complete suite of tools all of which can be used with live or via a fully featured, risk free, practice mode. In a fully customisable interface, you can Back & Lay across multiple screens on a range of sports. The Guardian Advanced Automation feature allows you to determine your automation rules with ease. Enabling complex automated betting based on timing, price triggers, comparative odds conditions and more. Automatically Cash-Out when your desired profit level is achieved and assure yourself a level profit (greening) no matter the result. For those who wish to write their own automated betting or trading strategies, Bet Angel can link to Microsoft Excel offering automation opportunities that are limitless. BET ANGEL KEY FEATURES Practice Mode One Click Betting Ladder Interface Advanced Charting Dutching and Bookmaking tools Trigger based automation Excel Spreadhseet integration Cash-Out To find out more and to sign up for a FREE 14 Day trial, head to the Bet Angel Website","title":"Overview - Bet Angel Pro"}]}
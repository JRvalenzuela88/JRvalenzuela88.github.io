{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1 { display: none; } _ Betfair is one of the only betting platforms in the world that demands winning clients. Unlike bookies, we don\u2019t ban you when you succeed. We need you, and we want you to be able to keep improving your strategies so you win more. We're here to help you in your automation journey, and this site is dedicated to sharing the tools and resources you need to succeed in this journey. Accessing Betfair's APIs Betfair has a set of customer-facing transactional APIs to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Golden rules of automation Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange and an AwesomeBetfair list of external repos we think are worth visiting The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support. Historic pricing data We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Tutorials JSON to CSV in Python Backtesting ratings using historic data in Python Automated betting angles: no modelling required Do #theyknow ? Analysing Betfair market formation and market movements Wisdom of the crowd? Analysing & understanding BSP Other resources Betfair data sources Data processor to generate CSVs from the historic JSON files Historic Data FAQs & sample data Historic Data Specifications The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data. If you are interested in the data available please reach out . Data modelling We have a series of modelling tutorials created by community members ranging from racing to sports, including greyhound modelling using form data in Python , an AFL tutorial in Python and Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies Automation tools Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are probably the most popular automation tools used by Australian customers. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Professional Gruss Betting Assistant Cymatic Trader Geeks Toy Inspiration & information There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. Analytical Meet Up recordings The Banker: A Quant's AFL Betting Strategy Our Twitter community is really active Betfair Quants Slack Group betfair quants is really active community-owned Slack group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation. Need extra help? If you\u2019re looking for bespoke advice or have extra questions, please contact us at automation@betfair.com.au . Our automation team are here to support you in automating your betting strategies.","title":"Home"},{"location":"#_","text":"Betfair is one of the only betting platforms in the world that demands winning clients. Unlike bookies, we don\u2019t ban you when you succeed. We need you, and we want you to be able to keep improving your strategies so you win more. We're here to help you in your automation journey, and this site is dedicated to sharing the tools and resources you need to succeed in this journey.","title":"_"},{"location":"#accessing-betfairs-apis","text":"Betfair has a set of customer-facing transactional APIs to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Golden rules of automation Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange and an AwesomeBetfair list of external repos we think are worth visiting The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support.","title":"Accessing Betfair's APIs"},{"location":"#historic-pricing-data","text":"We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section .","title":"Historic pricing data"},{"location":"#tutorials","text":"JSON to CSV in Python Backtesting ratings using historic data in Python Automated betting angles: no modelling required Do #theyknow ? Analysing Betfair market formation and market movements Wisdom of the crowd? Analysing & understanding BSP","title":"Tutorials"},{"location":"#other-resources","text":"Betfair data sources Data processor to generate CSVs from the historic JSON files Historic Data FAQs & sample data Historic Data Specifications The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data. If you are interested in the data available please reach out .","title":"Other resources"},{"location":"#data-modelling","text":"We have a series of modelling tutorials created by community members ranging from racing to sports, including greyhound modelling using form data in Python , an AFL tutorial in Python and Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies","title":"Data modelling"},{"location":"#automation-tools","text":"Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are probably the most popular automation tools used by Australian customers. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Professional Gruss Betting Assistant Cymatic Trader Geeks Toy","title":"Automation tools"},{"location":"#inspiration-information","text":"There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. Analytical Meet Up recordings The Banker: A Quant's AFL Betting Strategy Our Twitter community is really active Betfair Quants Slack Group betfair quants is really active community-owned Slack group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation.","title":"Inspiration &amp; information"},{"location":"#need-extra-help","text":"If you\u2019re looking for bespoke advice or have extra questions, please contact us at automation@betfair.com.au . Our automation team are here to support you in automating your betting strategies.","title":"Need extra help?"},{"location":"aboutUs/test/","text":"Meet the Team Ben Douglas Slack Handle: BenD My Role: Automation Manager - Working with customers to help them get their strategies automated and supplying customers with resources to increase their edge Date Started at Betfair: Feb 2020 My Edge: Ability to focus for hours on end, so I can record every bet in a spreadsheet and track whether I'm winning or losing Ivan Zhou Slack Handle: Ivan Zhou My Role: Client Development Executive - Helping clients automate their strategies and models Date Started at Betfair: Jul 2021 My Edge: Mastery of the imfamous 'for' loop in python. Horribly inefficient but works like a charm. Best Winning Moment: Modelled the Share Price of Cardinal Resources (CDV) Perfectly 1 year in advance ($0.34 -> $0.70)","title":"Meet the Team"},{"location":"aboutUs/test/#meet-the-team","text":"","title":"Meet the Team"},{"location":"aboutUs/test/#ben-douglas","text":"Slack Handle: BenD My Role: Automation Manager - Working with customers to help them get their strategies automated and supplying customers with resources to increase their edge Date Started at Betfair: Feb 2020 My Edge: Ability to focus for hours on end, so I can record every bet in a spreadsheet and track whether I'm winning or losing","title":"Ben Douglas"},{"location":"aboutUs/test/#ivan-zhou","text":"Slack Handle: Ivan Zhou My Role: Client Development Executive - Helping clients automate their strategies and models Date Started at Betfair: Jul 2021 My Edge: Mastery of the imfamous 'for' loop in python. Horribly inefficient but works like a charm. Best Winning Moment: Modelled the Share Price of Cardinal Resources (CDV) Perfectly 1 year in advance ($0.34 -> $0.70)","title":"Ivan Zhou"},{"location":"api/GoldenRulesofAutomation/","text":"Golden Rules of Automation | Some rules of thumb to help on your modelling & automation journey So, you're interested in modelling and automation? That's great to hear, but between having that interest and actually developing a fully working automated betting strategy is often a lot of effort, mistakes and iterative learning. Here we share some of our internal 'golden rules' of automation - those lessons we've learnt ourselves or heard from others in the community - in the hope that this might help to expedite your automation journey and help you avoid making some of the same mistakes we have along the way! As a high level philosophy, we suggest taking a risk averse approach to modelling and automation, avoiding bias in your model and back testing, being conservative in your staking and setting your automation up with fail safes and, if in doubt, to simply not bet. Hopefully this philosophy will stand you in good stead on your automation journey! Modelling Obviously before you can have an automation you need a model, of one form or another. Although this isn't the main focus of this article we do have several tutorials written by our in-house data scientists and modellers from the wider community that might be valuable: Greyhound modelling using form data in Python Modelling the Aus Open EPL ML walk through in Python AFL modelling in Python Back testing Once you've developed the first cut of your model you're going to want to make sure you test your strategy using historical data before you start betting with real money. Although you can never know exactly how a strategy is going to play out in the real world, particularly in terms of the 'butterfly effect' that your activity might have on the wider market, there is a lot you can do to test your theories before putting them into practice. We've put together several tutorials on how to use the JSON historical Betfair pricing data which is available back to 2016, including one specifically focused on back testing: Back testing ratings in Python To prevent data leakage it's important to make sure to only use data that is available before the outcome of the event you are modelling begins e.g. the BSP is only known after a Horse Racing market goes in play, therefore make sure you don't include the BSP of the race you're modelling, as you won't have it before the jump when you might be looking to bet. It sounds simple but it's one that catches the best of us more often than we'd like to admit! When back testing make sure to partition your dataset: It is common in data science to split your dataset into training, testing and validation sets, this way you can create and train your strategy on the training dataset and test and validate your strategy on separate datasets. Strategies can often become overfitted to the dataset they were trained on, leading to strategies that may not be generalizable or don't hold in real life. Making sure you separate your testing and validation datasets will help mitigate this risk by back testing your strategy on an out of sample data set. If you don't have the data you need to back test your strategy reach out to Data@betfair.com.au and we'll see what we can do! Staking Hand in hand with your back testing you need to develop a staking strategy. Different people can bet the same model and get very different results, and those differences come down to the betting and staking strategies you use to implement your model. Here are some things to consider when developing your staking and bank management strategy: The staking approach that you use should mirror the staking approach used when back testing your strategy If you want to change your staking you should consider re-running your back testing before going live with any changes. Is there a maximum stake size you want to have on any given bet/selection? If you are using a pure Kelly staking approach you may end up putting a large portion of your account onto a single selection - capped Kelly is a potential option here. Minimum bet sizes If you are using Kelly or some variable staking measure it's important to note that Betfair has minimum bet sizes (AUD5 unless you have minimum bets removed from your app key for testing, when it's then 1p), and lay bets have minimum bet liabilities . If your strategy is creating ratings, it might be worth checking the difference between your rated price and the market price; if your ratings are significantly different to the market odds close to the jump then a general rule of thumb is that your model is probably missing something as the market is (generally speaking), the best source of truth. This is particularly important to consider when your staking strategies increase your stake size based on the size of your edge e.g. Kelly. There are a couple of ways of handling this, one way of doing so is to set a maximum stake size, effectively capping the size of your bets, and another to stay out of the market if the difference is too significant. What impact might your strategy have on the market? For example, if you're trying to place large stakes on longer prices you may crunch the price in or not get fully matched, either of which would not be in keeping with your back testing and lead to unexpected variance. Automation Choose a tool that suits the job Depending on the complexity of your strategy and your skillset. If you can achieve what you need using one of the many tools that exist for automation on the Exchange , great! If not, and you have some coding skills, probably the next best option is to see whether there's a code library around in a language that you're familiar with. We've put together a collection of some of the most popular libraries and repos for interacting with the Exchange APIs that might help you decide where to start. Code modularization It's easy to dive straight into it when you're building a bot, but being intentional in your design can save you grief in the long term. Writing modular code will help create a more robust pipeline that makes it easier to diagnose errors when they occur and deploy fixes and is much easier to maintain. It's also worth commenting your code as you go (we know, do as we say, not as we do!), but it really makes it easier to come back to your code down the track and understand what your logic is doing. The same principle also applies to using existing tools. Logging Your bot will need debugging, guaranteed, and having a decent logging functionality in place from the beginning will save you grief during the process. Consider implementing a function to log important information as you need to check it, such as the ratings/predictions generated by your model, bet placement attempts from your bot and other relevant fields. This will be really helpful for: Diagnosing where and how things are going wrong often you can see there's an issue, but you don't know where or what the issue is. Optimizing your automation program in the future Logging your model's predictions can also be useful to further evaluate your strategy once you go live so it's worth saving them for later use (more on this under the Monitoring section). Automatic Stop & Manual Override The best and worst thing about automated bots is that they work autonomously, but that also means that at times they can do things you don't want them to do when you're not actively monitoring them. To this end it's worth considering implementing a function that stops your automation from placing bets if a certain situation arises, for example if the same bet is placed multiple times in a short amount of time. This can help to minimise the potential risks and damage of automation bugs. Similarly, creating a manual override function that you can use at any time in case of emergencies to instantly stop all bet placement and/or to cancel all unmatched bets may also be worth considering. Worst case this can be a kill switch, but you need to be prepared in advance for these situations, and they will happen, so, you want to make sure you can resolve them with as little negative impact as possible. Going Live Start Small If you have an edge that's worth having it's unlikely that it will disappear overnight, and you really should consider starting with small stakes when you're implementing a new model or strategy to ensure that reality matches theory before you stake up. There are a few reasons to start small before considering scaling up: This allows you to evaluate your strategies live without too much skin in the game. It's really easy to have a live bot that doesn't track against the back testing performance, often because there was a flaw in the logic used to back test or there was too much biasing in the model. This also ensures your bet placement system is working correctly with minimal risk; there are too many stories of large multimillion dollar, financial trading companies who have fallen to trading errors which have wiped them out e.g., Knight Capital - you don't want to make the same mistake! Starting small also allows you to gain confidence that your program and strategies are behaving as expected before you start putting more substantial stakes through the system. Testing When you first turn on your automation on it can be a good idea to keep an eye on your bot to make sure it's doing what you expect it to do: This will give you a better understanding of how your automation works in practice, and you can quickly pick up where things work differently to expected. This is especially important for strategies which place multiple bets and cancel bets allowing you to optimise code to trade more or less aggressively. You will also be able to catch if there are any errors in your logic or bet placement before it has a chance to cause you too much grief. Keep a Small Account Balance At the end of the day, in the worst-case scenario you can't lose more than the balance that's in your betting account, so it can be worth considering limiting the available balance while you're testing a new strategy or bot just to be on the safe side. There are a few things to consider: Make sure to do a quick estimate of how much working capital is required to cover your expected liabilities - you equally don't want your bot to run out of available funds when you do want bets to go on. An alternative approach to this is to use the customerStrategyRef field in the API to cap the amount you can have on any given strategy. Once you have tested your system and are happy with how it's working you can increase your bank size. Monitoring Congratulations, you made it and you've got your model up and running all on its own, fantastic! So, what's next? Despite the temptation, neither model or automated strategies should really be left to their own devices long term without regular monitoring and sense checks, to make sure that your model is still solid and nothing has changed in your data or the wider market to undermine your potential edge. Variance Any long-term strategy is likely to see variance across time, and it is challenging to know when a period of poor performance is expected variance or whether it's a result of something changing in your data, model efficacy or the wider market. A couple of things to consider: - Regularly compare your back tests against reality, i.e., for the last 30 days what did your simulation say your outcomes should be, and how does that compare with what really happened? If these are significantly different you may want to do some digging. - Consider what time period you want to review your automation, once it's established. Some people check daily, others weekly, and others again look at it monthly. Whatever your preference, this helps to give you a framework to work against and regular accountability. Resources If you want to learn more, here are some resources that might be valuable: Meet ups and workshop recordings API resources Historic pricing data Modelling tutorials Over to you! Building and automating a model on the Exchange can be great fun, and hopefully some of this resonates with you or you can apply within your automation journey. If we can help along the way please reach out! Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Golden rules of automation"},{"location":"api/GoldenRulesofAutomation/#golden-rules-of-automation","text":"| Some rules of thumb to help on your modelling & automation journey So, you're interested in modelling and automation? That's great to hear, but between having that interest and actually developing a fully working automated betting strategy is often a lot of effort, mistakes and iterative learning. Here we share some of our internal 'golden rules' of automation - those lessons we've learnt ourselves or heard from others in the community - in the hope that this might help to expedite your automation journey and help you avoid making some of the same mistakes we have along the way! As a high level philosophy, we suggest taking a risk averse approach to modelling and automation, avoiding bias in your model and back testing, being conservative in your staking and setting your automation up with fail safes and, if in doubt, to simply not bet. Hopefully this philosophy will stand you in good stead on your automation journey!","title":"Golden Rules of Automation"},{"location":"api/GoldenRulesofAutomation/#modelling","text":"Obviously before you can have an automation you need a model, of one form or another. Although this isn't the main focus of this article we do have several tutorials written by our in-house data scientists and modellers from the wider community that might be valuable: Greyhound modelling using form data in Python Modelling the Aus Open EPL ML walk through in Python AFL modelling in Python","title":"Modelling"},{"location":"api/GoldenRulesofAutomation/#back-testing","text":"Once you've developed the first cut of your model you're going to want to make sure you test your strategy using historical data before you start betting with real money. Although you can never know exactly how a strategy is going to play out in the real world, particularly in terms of the 'butterfly effect' that your activity might have on the wider market, there is a lot you can do to test your theories before putting them into practice. We've put together several tutorials on how to use the JSON historical Betfair pricing data which is available back to 2016, including one specifically focused on back testing: Back testing ratings in Python To prevent data leakage it's important to make sure to only use data that is available before the outcome of the event you are modelling begins e.g. the BSP is only known after a Horse Racing market goes in play, therefore make sure you don't include the BSP of the race you're modelling, as you won't have it before the jump when you might be looking to bet. It sounds simple but it's one that catches the best of us more often than we'd like to admit! When back testing make sure to partition your dataset: It is common in data science to split your dataset into training, testing and validation sets, this way you can create and train your strategy on the training dataset and test and validate your strategy on separate datasets. Strategies can often become overfitted to the dataset they were trained on, leading to strategies that may not be generalizable or don't hold in real life. Making sure you separate your testing and validation datasets will help mitigate this risk by back testing your strategy on an out of sample data set. If you don't have the data you need to back test your strategy reach out to Data@betfair.com.au and we'll see what we can do!","title":"Back testing"},{"location":"api/GoldenRulesofAutomation/#staking","text":"Hand in hand with your back testing you need to develop a staking strategy. Different people can bet the same model and get very different results, and those differences come down to the betting and staking strategies you use to implement your model. Here are some things to consider when developing your staking and bank management strategy: The staking approach that you use should mirror the staking approach used when back testing your strategy If you want to change your staking you should consider re-running your back testing before going live with any changes. Is there a maximum stake size you want to have on any given bet/selection? If you are using a pure Kelly staking approach you may end up putting a large portion of your account onto a single selection - capped Kelly is a potential option here. Minimum bet sizes If you are using Kelly or some variable staking measure it's important to note that Betfair has minimum bet sizes (AUD5 unless you have minimum bets removed from your app key for testing, when it's then 1p), and lay bets have minimum bet liabilities . If your strategy is creating ratings, it might be worth checking the difference between your rated price and the market price; if your ratings are significantly different to the market odds close to the jump then a general rule of thumb is that your model is probably missing something as the market is (generally speaking), the best source of truth. This is particularly important to consider when your staking strategies increase your stake size based on the size of your edge e.g. Kelly. There are a couple of ways of handling this, one way of doing so is to set a maximum stake size, effectively capping the size of your bets, and another to stay out of the market if the difference is too significant. What impact might your strategy have on the market? For example, if you're trying to place large stakes on longer prices you may crunch the price in or not get fully matched, either of which would not be in keeping with your back testing and lead to unexpected variance.","title":"Staking"},{"location":"api/GoldenRulesofAutomation/#automation","text":"","title":"Automation"},{"location":"api/GoldenRulesofAutomation/#choose-a-tool-that-suits-the-job","text":"Depending on the complexity of your strategy and your skillset. If you can achieve what you need using one of the many tools that exist for automation on the Exchange , great! If not, and you have some coding skills, probably the next best option is to see whether there's a code library around in a language that you're familiar with. We've put together a collection of some of the most popular libraries and repos for interacting with the Exchange APIs that might help you decide where to start.","title":"Choose a tool that suits the job"},{"location":"api/GoldenRulesofAutomation/#code-modularization","text":"It's easy to dive straight into it when you're building a bot, but being intentional in your design can save you grief in the long term. Writing modular code will help create a more robust pipeline that makes it easier to diagnose errors when they occur and deploy fixes and is much easier to maintain. It's also worth commenting your code as you go (we know, do as we say, not as we do!), but it really makes it easier to come back to your code down the track and understand what your logic is doing. The same principle also applies to using existing tools.","title":"Code modularization"},{"location":"api/GoldenRulesofAutomation/#logging","text":"Your bot will need debugging, guaranteed, and having a decent logging functionality in place from the beginning will save you grief during the process. Consider implementing a function to log important information as you need to check it, such as the ratings/predictions generated by your model, bet placement attempts from your bot and other relevant fields. This will be really helpful for: Diagnosing where and how things are going wrong often you can see there's an issue, but you don't know where or what the issue is. Optimizing your automation program in the future Logging your model's predictions can also be useful to further evaluate your strategy once you go live so it's worth saving them for later use (more on this under the Monitoring section).","title":"Logging"},{"location":"api/GoldenRulesofAutomation/#automatic-stop-manual-override","text":"The best and worst thing about automated bots is that they work autonomously, but that also means that at times they can do things you don't want them to do when you're not actively monitoring them. To this end it's worth considering implementing a function that stops your automation from placing bets if a certain situation arises, for example if the same bet is placed multiple times in a short amount of time. This can help to minimise the potential risks and damage of automation bugs. Similarly, creating a manual override function that you can use at any time in case of emergencies to instantly stop all bet placement and/or to cancel all unmatched bets may also be worth considering. Worst case this can be a kill switch, but you need to be prepared in advance for these situations, and they will happen, so, you want to make sure you can resolve them with as little negative impact as possible.","title":"Automatic Stop &amp; Manual Override"},{"location":"api/GoldenRulesofAutomation/#going-live","text":"","title":"Going Live"},{"location":"api/GoldenRulesofAutomation/#start-small","text":"If you have an edge that's worth having it's unlikely that it will disappear overnight, and you really should consider starting with small stakes when you're implementing a new model or strategy to ensure that reality matches theory before you stake up. There are a few reasons to start small before considering scaling up: This allows you to evaluate your strategies live without too much skin in the game. It's really easy to have a live bot that doesn't track against the back testing performance, often because there was a flaw in the logic used to back test or there was too much biasing in the model. This also ensures your bet placement system is working correctly with minimal risk; there are too many stories of large multimillion dollar, financial trading companies who have fallen to trading errors which have wiped them out e.g., Knight Capital - you don't want to make the same mistake! Starting small also allows you to gain confidence that your program and strategies are behaving as expected before you start putting more substantial stakes through the system.","title":"Start Small"},{"location":"api/GoldenRulesofAutomation/#testing","text":"When you first turn on your automation on it can be a good idea to keep an eye on your bot to make sure it's doing what you expect it to do: This will give you a better understanding of how your automation works in practice, and you can quickly pick up where things work differently to expected. This is especially important for strategies which place multiple bets and cancel bets allowing you to optimise code to trade more or less aggressively. You will also be able to catch if there are any errors in your logic or bet placement before it has a chance to cause you too much grief.","title":"Testing"},{"location":"api/GoldenRulesofAutomation/#keep-a-small-account-balance","text":"At the end of the day, in the worst-case scenario you can't lose more than the balance that's in your betting account, so it can be worth considering limiting the available balance while you're testing a new strategy or bot just to be on the safe side. There are a few things to consider: Make sure to do a quick estimate of how much working capital is required to cover your expected liabilities - you equally don't want your bot to run out of available funds when you do want bets to go on. An alternative approach to this is to use the customerStrategyRef field in the API to cap the amount you can have on any given strategy. Once you have tested your system and are happy with how it's working you can increase your bank size.","title":"Keep a Small Account Balance"},{"location":"api/GoldenRulesofAutomation/#monitoring","text":"Congratulations, you made it and you've got your model up and running all on its own, fantastic! So, what's next? Despite the temptation, neither model or automated strategies should really be left to their own devices long term without regular monitoring and sense checks, to make sure that your model is still solid and nothing has changed in your data or the wider market to undermine your potential edge.","title":"Monitoring"},{"location":"api/GoldenRulesofAutomation/#variance","text":"Any long-term strategy is likely to see variance across time, and it is challenging to know when a period of poor performance is expected variance or whether it's a result of something changing in your data, model efficacy or the wider market. A couple of things to consider: - Regularly compare your back tests against reality, i.e., for the last 30 days what did your simulation say your outcomes should be, and how does that compare with what really happened? If these are significantly different you may want to do some digging. - Consider what time period you want to review your automation, once it's established. Some people check daily, others weekly, and others again look at it monthly. Whatever your preference, this helps to give you a framework to work against and regular accountability.","title":"Variance"},{"location":"api/GoldenRulesofAutomation/#resources","text":"If you want to learn more, here are some resources that might be valuable: Meet ups and workshop recordings API resources Historic pricing data Modelling tutorials","title":"Resources"},{"location":"api/GoldenRulesofAutomation/#over-to-you","text":"Building and automating a model on the Exchange can be great fun, and hopefully some of this resonates with you or you can apply within your automation journey. If we can help along the way please reach out!","title":"Over to you!"},{"location":"api/GoldenRulesofAutomation/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"api/How_to_Automate_1/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); How to Automate I: Understanding Flumine I have had a go at automating models and trading bots on Betfair for the better part of a year now, although I am not successful enough to quit my full time job, I've come across a tonne of different hurdles and issues. Resources online can sometimes be scarce or hard to understand if you are not either a software engineer or have a dev background. So this is the first part of a multipart series of that go through all the things you need to know to reach the end goal of learning how to automate the model Bruno taught us how to build in his greyhound modelling tutorial . At the very end we will also create a script to simulate the exchange to backtest the strategies we have created so that we can optimize our strategies. We will be using the Flumine package and all the code we create will be available on github so you can in your own models and strategies. This series will be split into five parts which goes into: Part I - Understanding how Flumine works Part II - Automating backing or laying the 1st/2nd/.../nth favourite Part III - How to Automate one of Betfair's Data Science Models Part IV - How to Automate the model Bruno taught us how to build Part V - How to simulate the Exchange to backtest and optimise our strategies Understanding the Betfair API Before we dive into the Flumine package, or any code lets gain some understanding about the Betfair API. Betfair offers their API in two forms a Rest API and a Push API often call the polling and streaming API. We don't really need to know the technical differences between a Rest API and a Push API, but there are a few key differences on the Betfair API that is important to note: Rest API (Polling) Only returns a snap shot of data, everytime you want more data you must make a new request and wait for a response Market catalogue and all information available in market catalogue such as Runner names or Market event names are available Push API (Streaming) Only need to connect once, any updates will be sent to you as an update Only contains some information such as prices, Market catalogue is not available Because streaming gives real time pricing information it is much better than polling, but it doesn't include some key information such as the names of horses/sport teams. So, the solution is to use both streaming and polling together. If you are crazy and like to build everything from scratch, feel free to work out how to use them together and to build out your betting infrastructure. But if you would like to remain sane the great thing about Flumine is that it automatically combines the two together, so you get the benefit of both real time pricing and also all the information provided in the polling API. There is both documentation for the Betfair API and for Flumine however a quick note is that the documentation for the Betfair API is generally in camelCase e.g. selectionId or marketId whereas Flumine being a python package follows the PEP 8 style guide e.g. selection_id and market_id. Basically this just means whenever we see something on the Betfair documentation page the equivalent for Flumine will be in lower_case_with_underscores. General code structure and context Flumine is designed as a general framework that allows you to create custom strategies and operationalise them on specific markets, handling all the bet placement and market subscription. Using Flumine your general code structure will look like this: Login Create your strategy as a new Python Class Choose the markets/sports and controls for your strategy Adding workers Run your strategy The bulk of the coding required is a simple copy paste job, there are only a few things you need to change such as what you specify in your strategy Login This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here . You will also need to have streaming enabled on your API app key, to get it enabled email automation@betfair.com.au You can use the Flumine package with or without certificates. There have been quite a lot of discussions of how useful the security certificates are on the Betcode (formerly Betfairlightweight) slack group , but the general consensus is that its not too useful. Considering it is an extreme hassle to create the certificates and there is no really added benefit I prefer to log in without the certificates. However, if I haven't dissuaded you there are detailed instructions on how to generate certificates. For a windows machine, follow the instructions outlined here . For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here . You should then create a folder for your certs, perhaps named 'certs' and grab the path location. Besides that, the code for logging in will basically always be the same, so you can always copy and paste this! Be sure to fill in your username, password, appkeys (and the directory where your security certificates are stored if you created them). # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) Create your strategy as a new Python class This is probably the only slightly difficult part and after reading through this tutorial, it should be super easy! Before we delve straight into how this part works, we first need to understand some basics of how Classes work in Python. If you know this already, feel free to skip to the Creating our strategy section. Background info - Python Object Oriented Programming: Classes and Methods Let's create a new class in Python. This will allow us to define an object of this type later on, just like how we can create an integer, string or boolean. class Fruit (): # Initialize # Sets the attributes of the fruit object def __init__ ( self , colour , taste , size , price_per_size ) -> None : self . colour = colour self . taste = taste self . size = size self . price_per_size = price_per_size pass Now that we have created a new class called Fruit , we can now create Fruit objects, the same way we can define an object as a string or an integer. For example if we wanted to define a variable x as an integer with the value 6: # define a new variable 'x' as an integer with the value 6 x = int ( 6 ) print ( f 'the value of x is { x } , we can see x is of the type { type ( x ) } ' ) the value of x is 6, we can see x is of the type <class 'int'> Now we can do the same thing with Fruit : # define a new variable 'z' as fruit with the colour green, taste sweet, size 6 and price 2.99 z = Fruit ( colour = 'green' , taste = 'sweet' , size = 6 , price_per_size = 2.99 ) print ( f 'the value of z is { z } , z is of the type { type ( z ) } ' ) the value of z is <__main__.Fruit object at 0x7f6fb6794df0>, z is of the type <class '__main__.Fruit'> # Attributes of the fruit object print ( f 'The value of z.colour is: { z . colour } ' ) print ( f 'The value of z.taste is: { z . taste } ' ) print ( f 'The value of z.size is: { z . size } ' ) print ( f 'The value of z.price_per_size is: { z . price_per_size } ' ) The value of z.colour is: green The value of z.taste is: sweet The value of z.size is: 6 The value of z.price_per_size is: 2.99 In Python methods are functions that you can define inside a class which run when called. For example let's take a look at the upper() method for strings: x = 'hello world' x 'hello world' # Example of a method: x . upper () 'HELLO WORLD' Let's add a few methods to the fruit class we created earlier to calculate the total price for us: class Fruit (): # Initialize def __init__ ( self , colour , taste , size , price_per_size ) -> None : self . colour = colour self . taste = taste self . size = size self . price_per_size = price_per_size pass ### New part (rest is the same): # Creating first method, print the total price def print_total_price ( self ): print ( f 'The total_price is { self . size * self . price_per_size } ' ) # Creating second method, return the second price def return_total_price ( self ): return ( self . size * self . price_per_size ) Now that we have defined the methods, we can call them like this: # We need to define z again z = Fruit ( colour = 'green' , taste = 'sweet' , size = 6 , price_per_size = 2.99 ) # Use the methods we just created z . print_total_price () z . return_total_price () The total_price is 17.94 17.94 Class inheritance Now that we know a bit about Classes and Methods in Python, all that's left to learn is Class inheritance. In Python can create a class of another class e.g. class Class_2(Class_1): xxx Class_2 is known as the child class and Class_1 is known as the parent class The child class (Class_2) inherit all the attributes and methods of the parent class (Class_1) - But the key thing for us is that the child class (Class_2) can override the methods and attributes it inherits from the parent class (Class_1) So, going back to our fruit example lets create a child class that inherit from the fruit class as an example: class Avocado ( Fruit ): ### Override the first method but not the second method # Override first method def print_total_price ( self ): print ( f 'The total_price is { self . price_per_size } ' ) We have now created another new class, this one called avocado, it is the same as the original parent class (Fruit), but we have overridden the print_total_price method # create an object that is of the class avocado a = Avocado ( colour = 'green' , taste = 'good idk??' , size = 2 , price_per_size = 1 ) a <__main__.Avocado at 0x7f6fb6f60f10> We can see all the attributes behave in the way we expect: # Attributes of the avocado object print ( f 'The value of a.colour is: { a . colour } ' ) print ( f 'The value of a.taste is: { a . taste } ' ) print ( f 'The value of a.size is: { a . size } ' ) print ( f 'The value of a.price_per_size is: { a . price_per_size } ' ) The value of a.colour is: green The value of a.taste is: good idk?? The value of a.size is: 2 The value of a.price_per_size is: 1 But now when we call our two methods, return_total_price works in the same way as a fruit class would, but now print_total_price is different because we have overridden it: a . print_total_price () a . return_total_price () The total_price is 1 2 Now that we know how class in heritance works, we have armed ourselves with everything we need to know how to work with Flumine so let's tie everything together. Creating our strategy as a child class from BaseStrategy Bringing what we have learned about methods and class inheritance. Flumine already has a class called BaseStrategy that is designed to be used as a parent class. Each of the methods defined in BaseStrategy are called automatically at specific times such as when someone places a bet. The idea is to you take BaseStrategy as your Parent Class and then write over the methods that get automatically called with what we want our bot to do. Flumine essentially loops through and automatically calls the methods that have been defined, so all you need to do is override the methods, to suit your strategy. If we adopt the way Flumine does things to our fruit example it will look a little like this: # Example with the fruit strategy fruit_market = [ a , z ] for each_fruit in fruit_market : print ( f 'For the fruit: { each_fruit } ' ) each_fruit . print_total_price () For the fruit: <__main__.Avocado object at 0x7f6fb6f60f10> The total_price is 1 For the fruit: <__main__.Fruit object at 0x7f6fb6f60280> The total_price is 17.94 Let's go through an example with a simple strategy that I tested many, many times to understand the intricacies of Flumine before we move onto anything more complex. Let's attempt to lay all selections at a price of 1.01, the good thing is as long as this isn't inplay we basically never get matched using this strategy so we can use it to test a tonne of things. The other good thing about Flumine is that by default you can only have 1 trade live (waiting to be matched) per selection at any one time. So running the below code will only place one bet per selection and another will not be placed untill the first get matched. If we take a closer look at the documentation and source code we can get an idea of the methods available and the ones that are run automatically. Basically the way Flumine works is any code you have under start runs when you first hit play, then whenever there is an update to the market check_market_book runs and if that returns true then process_market_book and process_orders will run. An update is whenever anyone places/cancels/modifies a bet for that specific market e.g. R7 Flemmington Win. By default Flumine will run continuously without stopping (we will learn how to make it stop later on). For something simple like placing a single bet per selection in a race check_market_book and process_market_book are pretty much the only method we really needed to edit. So our code structure will look something like this: def start(): # This is called when you first start up your strategy, generally don't have anything important here def check_market_book(): # You need this to return True, otherwise process_market_book won't run, by default it will return False # generally used to check if the market is open and if not, we skip that market def process_market_book(): # This is where you want the bulk of the logic for your strategy # Any code here will initially run once when check_market_book() returns True and also run each time anyone on on the exchange places or cancels a order this market # This is where I prefer to have my bet placement logic If you have something in mind that is more complicated such as needing the constantly change the price of your bets, then you can test out process_orders . But for now, let's have a crack at implementing our simple strategy. # Import necessary libraries from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook # Create a new strategy as a new class called LayStrategy, this in turn will allow us to create a new Python object later # LayStrategy is a child class inheriting from a class in Flumine we imported above called BaseStrategy class LayStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'LayStrategy'\" ) # Prevent looking at markets that are closed def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first initial time, process_market_book runs every single time someone places, updates or cancels a bet def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : for runner in market_book . runners : # Loops through each of the runners in the race if runner . status == \"ACTIVE\" : # If the runner is active (hasen't been scratched) # Place a lay bet at a price of 1.01 with $5 volume trade = Trade ( market_id = market_book . market_id , # The market_id for the specific market selection_id = runner . selection_id , # The selection_id of the horse/dog/team handicap = runner . handicap , # The handicap of the horse/dog/team strategy = self , # Strategy this bet is part of: itself (LayStrategy) ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = 1.01 , size = 5.00 ) # Lay bet, Limit order price of 1.01 size = $5 ) market . place_order ( order ) # Place the order You may have noticed that when I did for runner in market_book.runners: I knew market_book had runners as an attribute, this is because I've spent ages in Flumine, but you can find all this information in Betfair Documentation page , and I would recommend the Betting Type Definitions page . If you are reading through the docs and want to use something from market_book you already have that readily available, but if you want to use something from the polling API such as market_catalogue then Flumine has this available as an attribute under market . So you will need to do something like: market.market_catalogue . We will go through an example of this in How to Automate IV . Quick word of warning, the first few seconds after Flumine starts market.market_catalogue will return None as it hasn't requested data from the polling api yet, but give it a few seconds and it will run fine. Back to the above strategy, you may be thinking that in LayStrategy we will be placing millions of lay bets of $5 at odds of 1.01 because we place a bet whenever process_market_book get called, which happens anytime someone in the same market places, updates or cancels a bet. But it won't because by default there are controls in place that limits the number of bets Flumine will place. Later on we will learn how to adjust them. This means that while the Flumine is incredibly powerful it could be devastating with incorrect code. Choose the markets/sports and controls for your strategy Now that we have created our strategy all we need to do is to choose what sports to run it on and any trading controls we may have. To actually turn on our strategy we need to define a new variable as a LayStrategy object. Going back to our Avocado example it would look like this: fresh_avocado = Avocado ( colour = 'green/yellow' , taste = 'creamy' , size = 10 , price_per_size = 2.99 ) fresh_avocado <__main__.Avocado at 0x7f6fb73294f0> When we define fresh_avocado as an Avocado class, we need to include multiple attributes such as colour, taste, size and price_per_size. As avocado is a child class of fruit, we can take a look at fruit for what we need to include. We need to do the same thing for our LayStrategy. The attributes that we need to include can be found in the documentation for BaseStrategy : We can see that some of the attributes have default values such as max_order_exposure , but others have None . You can play around with them, for now lets just set market_filter to only bet on greyhound win markets. If you ever get confused (it happens) you can take a look at the Betting Type Definitions . This is actually one of the really cool things about Flumine, all you need to do is point it at a particular criteria e.g. all greyhound win markets in Australia and it will run your strategy on all those markets. strategy = LayStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds country_codes = [ \"AU\" ], # Australia market_types = [ \"WIN\" ], # Win Markets market_ids = [ '1.196189930' ] # Murray Bridge R5 ) ) And now we can add the strategy to framework and run it! framework . add_strategy ( strategy ) # Running this will place real bets! framework . run () Troubleshooting (the most important part that is one line of code) Sooner or later, you will run into some sort of error. It's bound to happen. But instead of spending hours scratching your head and contemplating throwing out your laptop there is a simple one-line solution: import logging # technically two lines since you need to import the library logging . basicConfig ( filename = 'how_to_automate_1.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) This is called logging and there is a two part YouTube tutorial that explains it far better than I ever could. Basically, the line of code above will enable logging and create a new file called 'how_to_automate_1.log' in the same folder as this Python script. By default Flumine has a tonne of logging which will now be enabled with the line above and write logs to that file. So anytime there is an error we can easily pinpoint what the error is. Put that line of code somewhere near the top of your script and it will save you hours wondering why your code doesn't work. There is one slight drawback, and that basically everything that indicates your code is running smoothly will now go into the log file instead of printing out in your terminal. I would recommend the first time you run your strategy to open the log file so you can see what Flumine is doing, it will look a little like this: Let's do something stupid, so we can see how easy it is to pinpoint errors with logging. Let's do the same strategy as above, but with a price of 0.9 instead of 1.01, clearly something that isn't allowed # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import logging # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) logging . basicConfig ( filename = 'how_to_automate_1.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) class LayStrategy ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'LayStrategy'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : for runner in market_book . runners : if runner . status == \"ACTIVE\" : # Place a lay bet at a price of 0.9 with $5 size trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = 0.9 , size = 5 ) ) market . place_order ( order ) strategy = LayStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], country_codes = [ \"AU\" ], market_types = [ \"WIN\" ], ) ) framework . add_strategy ( strategy ) framework . run () If we open the log file a few seconds after running the code (you can open it using your code editor or a text editor like notepad) we can see that it shows us what the error is: (I have spent a lot of time looking through log files...) Conclusion and next steps Now that we understand the basics of how Flumine works, this is where the fun begins! There are three more parts of this series which goes more in-depth into automating different angles, the Betfair datascience models and eventually our final goal: our own model. Part II - Automating backing or laying the 1st/2nd/.../nth favourite Part III - Automating a Betfair model Part IV - Automating your own model Part V - How to simulate the Exchange to backtest and optimise our strategies Complete Code Run the code from your ide by using py <filename> .py, making sure you amend the path to point to your input data. Download from Github # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import logging # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) logging . basicConfig ( filename = 'how_to_automate_1.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # Create a new strategy as a new class called LayStrategy, this in turn will allow us to create a new Python object later # LayStrategy is a child class inheriting from a class in Flumine we imported above called BaseStrategy class LayStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'LayStrategy'\" ) # Prevent looking at markets that are closed def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first initial time, process_market_book runs every single time someone places, updates or cancels a bet def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : for runner in market_book . runners : # Loops through each of the runners in the race if runner . status == \"ACTIVE\" : # If the runner is active (hasen't been scratched) # Place a lay bet at a price of 1.01 with $5 volume trade = Trade ( market_id = market_book . market_id , # The market_id for the specific market selection_id = runner . selection_id , # The selection_id of the horse/dog/team handicap = runner . handicap , # The handicap of the horse/dog/team strategy = self , # Strategy this bet is part of: itself (LayStrategy) ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = 1.01 , size = 5.00 ) # Lay bet, Limit order price of 1.01 size = $5 ) market . place_order ( order ) # Place the order strategy = LayStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds country_codes = [ \"AU\" ], # Australia market_types = [ \"WIN\" ], # Win Markets ) ) framework . add_strategy ( strategy ) framework . run () Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"How to Automate 1"},{"location":"api/How_to_Automate_1/#how-to-automate-i-understanding-flumine","text":"I have had a go at automating models and trading bots on Betfair for the better part of a year now, although I am not successful enough to quit my full time job, I've come across a tonne of different hurdles and issues. Resources online can sometimes be scarce or hard to understand if you are not either a software engineer or have a dev background. So this is the first part of a multipart series of that go through all the things you need to know to reach the end goal of learning how to automate the model Bruno taught us how to build in his greyhound modelling tutorial . At the very end we will also create a script to simulate the exchange to backtest the strategies we have created so that we can optimize our strategies. We will be using the Flumine package and all the code we create will be available on github so you can in your own models and strategies. This series will be split into five parts which goes into: Part I - Understanding how Flumine works Part II - Automating backing or laying the 1st/2nd/.../nth favourite Part III - How to Automate one of Betfair's Data Science Models Part IV - How to Automate the model Bruno taught us how to build Part V - How to simulate the Exchange to backtest and optimise our strategies","title":"How to Automate I: Understanding Flumine"},{"location":"api/How_to_Automate_1/#understanding-the-betfair-api","text":"Before we dive into the Flumine package, or any code lets gain some understanding about the Betfair API. Betfair offers their API in two forms a Rest API and a Push API often call the polling and streaming API. We don't really need to know the technical differences between a Rest API and a Push API, but there are a few key differences on the Betfair API that is important to note: Rest API (Polling) Only returns a snap shot of data, everytime you want more data you must make a new request and wait for a response Market catalogue and all information available in market catalogue such as Runner names or Market event names are available Push API (Streaming) Only need to connect once, any updates will be sent to you as an update Only contains some information such as prices, Market catalogue is not available Because streaming gives real time pricing information it is much better than polling, but it doesn't include some key information such as the names of horses/sport teams. So, the solution is to use both streaming and polling together. If you are crazy and like to build everything from scratch, feel free to work out how to use them together and to build out your betting infrastructure. But if you would like to remain sane the great thing about Flumine is that it automatically combines the two together, so you get the benefit of both real time pricing and also all the information provided in the polling API. There is both documentation for the Betfair API and for Flumine however a quick note is that the documentation for the Betfair API is generally in camelCase e.g. selectionId or marketId whereas Flumine being a python package follows the PEP 8 style guide e.g. selection_id and market_id. Basically this just means whenever we see something on the Betfair documentation page the equivalent for Flumine will be in lower_case_with_underscores.","title":"Understanding the Betfair API"},{"location":"api/How_to_Automate_1/#general-code-structure-and-context","text":"Flumine is designed as a general framework that allows you to create custom strategies and operationalise them on specific markets, handling all the bet placement and market subscription. Using Flumine your general code structure will look like this: Login Create your strategy as a new Python Class Choose the markets/sports and controls for your strategy Adding workers Run your strategy The bulk of the coding required is a simple copy paste job, there are only a few things you need to change such as what you specify in your strategy","title":"General code structure and context"},{"location":"api/How_to_Automate_1/#login","text":"This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here . You will also need to have streaming enabled on your API app key, to get it enabled email automation@betfair.com.au You can use the Flumine package with or without certificates. There have been quite a lot of discussions of how useful the security certificates are on the Betcode (formerly Betfairlightweight) slack group , but the general consensus is that its not too useful. Considering it is an extreme hassle to create the certificates and there is no really added benefit I prefer to log in without the certificates. However, if I haven't dissuaded you there are detailed instructions on how to generate certificates. For a windows machine, follow the instructions outlined here . For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here . You should then create a folder for your certs, perhaps named 'certs' and grab the path location. Besides that, the code for logging in will basically always be the same, so you can always copy and paste this! Be sure to fill in your username, password, appkeys (and the directory where your security certificates are stored if you created them). # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client)","title":"Login"},{"location":"api/How_to_Automate_1/#create-your-strategy-as-a-new-python-class","text":"This is probably the only slightly difficult part and after reading through this tutorial, it should be super easy! Before we delve straight into how this part works, we first need to understand some basics of how Classes work in Python. If you know this already, feel free to skip to the Creating our strategy section.","title":"Create your strategy as a new Python class"},{"location":"api/How_to_Automate_1/#background-info-python-object-oriented-programming","text":"","title":"Background info - Python Object Oriented Programming:"},{"location":"api/How_to_Automate_1/#classes-and-methods","text":"Let's create a new class in Python. This will allow us to define an object of this type later on, just like how we can create an integer, string or boolean. class Fruit (): # Initialize # Sets the attributes of the fruit object def __init__ ( self , colour , taste , size , price_per_size ) -> None : self . colour = colour self . taste = taste self . size = size self . price_per_size = price_per_size pass Now that we have created a new class called Fruit , we can now create Fruit objects, the same way we can define an object as a string or an integer. For example if we wanted to define a variable x as an integer with the value 6: # define a new variable 'x' as an integer with the value 6 x = int ( 6 ) print ( f 'the value of x is { x } , we can see x is of the type { type ( x ) } ' ) the value of x is 6, we can see x is of the type <class 'int'> Now we can do the same thing with Fruit : # define a new variable 'z' as fruit with the colour green, taste sweet, size 6 and price 2.99 z = Fruit ( colour = 'green' , taste = 'sweet' , size = 6 , price_per_size = 2.99 ) print ( f 'the value of z is { z } , z is of the type { type ( z ) } ' ) the value of z is <__main__.Fruit object at 0x7f6fb6794df0>, z is of the type <class '__main__.Fruit'> # Attributes of the fruit object print ( f 'The value of z.colour is: { z . colour } ' ) print ( f 'The value of z.taste is: { z . taste } ' ) print ( f 'The value of z.size is: { z . size } ' ) print ( f 'The value of z.price_per_size is: { z . price_per_size } ' ) The value of z.colour is: green The value of z.taste is: sweet The value of z.size is: 6 The value of z.price_per_size is: 2.99 In Python methods are functions that you can define inside a class which run when called. For example let's take a look at the upper() method for strings: x = 'hello world' x 'hello world' # Example of a method: x . upper () 'HELLO WORLD' Let's add a few methods to the fruit class we created earlier to calculate the total price for us: class Fruit (): # Initialize def __init__ ( self , colour , taste , size , price_per_size ) -> None : self . colour = colour self . taste = taste self . size = size self . price_per_size = price_per_size pass ### New part (rest is the same): # Creating first method, print the total price def print_total_price ( self ): print ( f 'The total_price is { self . size * self . price_per_size } ' ) # Creating second method, return the second price def return_total_price ( self ): return ( self . size * self . price_per_size ) Now that we have defined the methods, we can call them like this: # We need to define z again z = Fruit ( colour = 'green' , taste = 'sweet' , size = 6 , price_per_size = 2.99 ) # Use the methods we just created z . print_total_price () z . return_total_price () The total_price is 17.94 17.94","title":"Classes and Methods"},{"location":"api/How_to_Automate_1/#class-inheritance","text":"Now that we know a bit about Classes and Methods in Python, all that's left to learn is Class inheritance. In Python can create a class of another class e.g. class Class_2(Class_1): xxx Class_2 is known as the child class and Class_1 is known as the parent class The child class (Class_2) inherit all the attributes and methods of the parent class (Class_1) - But the key thing for us is that the child class (Class_2) can override the methods and attributes it inherits from the parent class (Class_1) So, going back to our fruit example lets create a child class that inherit from the fruit class as an example: class Avocado ( Fruit ): ### Override the first method but not the second method # Override first method def print_total_price ( self ): print ( f 'The total_price is { self . price_per_size } ' ) We have now created another new class, this one called avocado, it is the same as the original parent class (Fruit), but we have overridden the print_total_price method # create an object that is of the class avocado a = Avocado ( colour = 'green' , taste = 'good idk??' , size = 2 , price_per_size = 1 ) a <__main__.Avocado at 0x7f6fb6f60f10> We can see all the attributes behave in the way we expect: # Attributes of the avocado object print ( f 'The value of a.colour is: { a . colour } ' ) print ( f 'The value of a.taste is: { a . taste } ' ) print ( f 'The value of a.size is: { a . size } ' ) print ( f 'The value of a.price_per_size is: { a . price_per_size } ' ) The value of a.colour is: green The value of a.taste is: good idk?? The value of a.size is: 2 The value of a.price_per_size is: 1 But now when we call our two methods, return_total_price works in the same way as a fruit class would, but now print_total_price is different because we have overridden it: a . print_total_price () a . return_total_price () The total_price is 1 2 Now that we know how class in heritance works, we have armed ourselves with everything we need to know how to work with Flumine so let's tie everything together.","title":"Class inheritance"},{"location":"api/How_to_Automate_1/#creating-our-strategy-as-a-child-class-from-basestrategy","text":"Bringing what we have learned about methods and class inheritance. Flumine already has a class called BaseStrategy that is designed to be used as a parent class. Each of the methods defined in BaseStrategy are called automatically at specific times such as when someone places a bet. The idea is to you take BaseStrategy as your Parent Class and then write over the methods that get automatically called with what we want our bot to do. Flumine essentially loops through and automatically calls the methods that have been defined, so all you need to do is override the methods, to suit your strategy. If we adopt the way Flumine does things to our fruit example it will look a little like this: # Example with the fruit strategy fruit_market = [ a , z ] for each_fruit in fruit_market : print ( f 'For the fruit: { each_fruit } ' ) each_fruit . print_total_price () For the fruit: <__main__.Avocado object at 0x7f6fb6f60f10> The total_price is 1 For the fruit: <__main__.Fruit object at 0x7f6fb6f60280> The total_price is 17.94 Let's go through an example with a simple strategy that I tested many, many times to understand the intricacies of Flumine before we move onto anything more complex. Let's attempt to lay all selections at a price of 1.01, the good thing is as long as this isn't inplay we basically never get matched using this strategy so we can use it to test a tonne of things. The other good thing about Flumine is that by default you can only have 1 trade live (waiting to be matched) per selection at any one time. So running the below code will only place one bet per selection and another will not be placed untill the first get matched. If we take a closer look at the documentation and source code we can get an idea of the methods available and the ones that are run automatically. Basically the way Flumine works is any code you have under start runs when you first hit play, then whenever there is an update to the market check_market_book runs and if that returns true then process_market_book and process_orders will run. An update is whenever anyone places/cancels/modifies a bet for that specific market e.g. R7 Flemmington Win. By default Flumine will run continuously without stopping (we will learn how to make it stop later on). For something simple like placing a single bet per selection in a race check_market_book and process_market_book are pretty much the only method we really needed to edit. So our code structure will look something like this: def start(): # This is called when you first start up your strategy, generally don't have anything important here def check_market_book(): # You need this to return True, otherwise process_market_book won't run, by default it will return False # generally used to check if the market is open and if not, we skip that market def process_market_book(): # This is where you want the bulk of the logic for your strategy # Any code here will initially run once when check_market_book() returns True and also run each time anyone on on the exchange places or cancels a order this market # This is where I prefer to have my bet placement logic If you have something in mind that is more complicated such as needing the constantly change the price of your bets, then you can test out process_orders . But for now, let's have a crack at implementing our simple strategy. # Import necessary libraries from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook # Create a new strategy as a new class called LayStrategy, this in turn will allow us to create a new Python object later # LayStrategy is a child class inheriting from a class in Flumine we imported above called BaseStrategy class LayStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'LayStrategy'\" ) # Prevent looking at markets that are closed def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first initial time, process_market_book runs every single time someone places, updates or cancels a bet def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : for runner in market_book . runners : # Loops through each of the runners in the race if runner . status == \"ACTIVE\" : # If the runner is active (hasen't been scratched) # Place a lay bet at a price of 1.01 with $5 volume trade = Trade ( market_id = market_book . market_id , # The market_id for the specific market selection_id = runner . selection_id , # The selection_id of the horse/dog/team handicap = runner . handicap , # The handicap of the horse/dog/team strategy = self , # Strategy this bet is part of: itself (LayStrategy) ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = 1.01 , size = 5.00 ) # Lay bet, Limit order price of 1.01 size = $5 ) market . place_order ( order ) # Place the order You may have noticed that when I did for runner in market_book.runners: I knew market_book had runners as an attribute, this is because I've spent ages in Flumine, but you can find all this information in Betfair Documentation page , and I would recommend the Betting Type Definitions page . If you are reading through the docs and want to use something from market_book you already have that readily available, but if you want to use something from the polling API such as market_catalogue then Flumine has this available as an attribute under market . So you will need to do something like: market.market_catalogue . We will go through an example of this in How to Automate IV . Quick word of warning, the first few seconds after Flumine starts market.market_catalogue will return None as it hasn't requested data from the polling api yet, but give it a few seconds and it will run fine. Back to the above strategy, you may be thinking that in LayStrategy we will be placing millions of lay bets of $5 at odds of 1.01 because we place a bet whenever process_market_book get called, which happens anytime someone in the same market places, updates or cancels a bet. But it won't because by default there are controls in place that limits the number of bets Flumine will place. Later on we will learn how to adjust them. This means that while the Flumine is incredibly powerful it could be devastating with incorrect code.","title":"Creating our strategy as a child class from BaseStrategy"},{"location":"api/How_to_Automate_1/#choose-the-marketssports-and-controls-for-your-strategy","text":"Now that we have created our strategy all we need to do is to choose what sports to run it on and any trading controls we may have. To actually turn on our strategy we need to define a new variable as a LayStrategy object. Going back to our Avocado example it would look like this: fresh_avocado = Avocado ( colour = 'green/yellow' , taste = 'creamy' , size = 10 , price_per_size = 2.99 ) fresh_avocado <__main__.Avocado at 0x7f6fb73294f0> When we define fresh_avocado as an Avocado class, we need to include multiple attributes such as colour, taste, size and price_per_size. As avocado is a child class of fruit, we can take a look at fruit for what we need to include. We need to do the same thing for our LayStrategy. The attributes that we need to include can be found in the documentation for BaseStrategy : We can see that some of the attributes have default values such as max_order_exposure , but others have None . You can play around with them, for now lets just set market_filter to only bet on greyhound win markets. If you ever get confused (it happens) you can take a look at the Betting Type Definitions . This is actually one of the really cool things about Flumine, all you need to do is point it at a particular criteria e.g. all greyhound win markets in Australia and it will run your strategy on all those markets. strategy = LayStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds country_codes = [ \"AU\" ], # Australia market_types = [ \"WIN\" ], # Win Markets market_ids = [ '1.196189930' ] # Murray Bridge R5 ) ) And now we can add the strategy to framework and run it! framework . add_strategy ( strategy ) # Running this will place real bets! framework . run ()","title":"Choose the markets/sports and controls for your strategy"},{"location":"api/How_to_Automate_1/#troubleshooting-the-most-important-part-that-is-one-line-of-code","text":"Sooner or later, you will run into some sort of error. It's bound to happen. But instead of spending hours scratching your head and contemplating throwing out your laptop there is a simple one-line solution: import logging # technically two lines since you need to import the library logging . basicConfig ( filename = 'how_to_automate_1.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) This is called logging and there is a two part YouTube tutorial that explains it far better than I ever could. Basically, the line of code above will enable logging and create a new file called 'how_to_automate_1.log' in the same folder as this Python script. By default Flumine has a tonne of logging which will now be enabled with the line above and write logs to that file. So anytime there is an error we can easily pinpoint what the error is. Put that line of code somewhere near the top of your script and it will save you hours wondering why your code doesn't work. There is one slight drawback, and that basically everything that indicates your code is running smoothly will now go into the log file instead of printing out in your terminal. I would recommend the first time you run your strategy to open the log file so you can see what Flumine is doing, it will look a little like this: Let's do something stupid, so we can see how easy it is to pinpoint errors with logging. Let's do the same strategy as above, but with a price of 0.9 instead of 1.01, clearly something that isn't allowed # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import logging # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) logging . basicConfig ( filename = 'how_to_automate_1.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) class LayStrategy ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'LayStrategy'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : for runner in market_book . runners : if runner . status == \"ACTIVE\" : # Place a lay bet at a price of 0.9 with $5 size trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = 0.9 , size = 5 ) ) market . place_order ( order ) strategy = LayStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], country_codes = [ \"AU\" ], market_types = [ \"WIN\" ], ) ) framework . add_strategy ( strategy ) framework . run () If we open the log file a few seconds after running the code (you can open it using your code editor or a text editor like notepad) we can see that it shows us what the error is: (I have spent a lot of time looking through log files...)","title":"Troubleshooting (the most important part that is one line of code)"},{"location":"api/How_to_Automate_1/#conclusion-and-next-steps","text":"Now that we understand the basics of how Flumine works, this is where the fun begins! There are three more parts of this series which goes more in-depth into automating different angles, the Betfair datascience models and eventually our final goal: our own model. Part II - Automating backing or laying the 1st/2nd/.../nth favourite Part III - Automating a Betfair model Part IV - Automating your own model Part V - How to simulate the Exchange to backtest and optimise our strategies","title":"Conclusion and next steps"},{"location":"api/How_to_Automate_1/#complete-code","text":"Run the code from your ide by using py <filename> .py, making sure you amend the path to point to your input data. Download from Github # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import logging # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) logging . basicConfig ( filename = 'how_to_automate_1.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # Create a new strategy as a new class called LayStrategy, this in turn will allow us to create a new Python object later # LayStrategy is a child class inheriting from a class in Flumine we imported above called BaseStrategy class LayStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'LayStrategy'\" ) # Prevent looking at markets that are closed def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first initial time, process_market_book runs every single time someone places, updates or cancels a bet def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : for runner in market_book . runners : # Loops through each of the runners in the race if runner . status == \"ACTIVE\" : # If the runner is active (hasen't been scratched) # Place a lay bet at a price of 1.01 with $5 volume trade = Trade ( market_id = market_book . market_id , # The market_id for the specific market selection_id = runner . selection_id , # The selection_id of the horse/dog/team handicap = runner . handicap , # The handicap of the horse/dog/team strategy = self , # Strategy this bet is part of: itself (LayStrategy) ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = 1.01 , size = 5.00 ) # Lay bet, Limit order price of 1.01 size = $5 ) market . place_order ( order ) # Place the order strategy = LayStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds country_codes = [ \"AU\" ], # Australia market_types = [ \"WIN\" ], # Win Markets ) ) framework . add_strategy ( strategy ) framework . run ()","title":"Complete Code"},{"location":"api/How_to_Automate_1/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"api/How_to_Automate_2/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); How to Automate II: Backing or laying the 1st/2nd/.../nth favourite shot This tutorial is Part two of the How to Automate series and follows on logically from the How to Automate I: Understanding Flumine tutorial we shared previously. Make sure you look at part one, before diving into this one, as it will help you understand the general Flumine code structure and how it works. This tutorial will delve a bit deeper and give us more confidence working with Flumine. Not only will we get a cool new strategy, but we will be moving closer to learning how to automate our own model. As always please reach out with feedback, suggestions or queries. Please submit a pull request if you catch any bugs or have other improvement suggestions! Context Say for example you have done some research and found there is significant published academic literature on the existence of a favourite-longshot bias. As a result, you want to automate a strategy backing all favourites or second favourites in thoroughbred racing markets. If this is the case, then this is the perfect tutorial for you! Taking a quick look on Google Scholar we can see there is indeed plenty of published papers on this topic (almost as if I planned it). Many of these Journals are high quality too! According to the ABDC Journal Rankings : Scottish Journal of Political Economy has rating of A The Economic Journal is A* Applied Economics is A* However at this point I must stress to the reader that these papers are quite old and were mainly published in the early 2000s. Schwert (2003) suggests that many of the market anomalies, discovered in financial markets as a contradiction to market efficiency, disappear once they have been published in academic literature. So, although these researchers may have found the existence of a favourite-longshot bias in betting markets in the early 2000s, they likely no longer exist as the publication of their findings leads to an increase in market efficiency. Schwert, G.W., 2003. Anomalies and market efficiency. Handbook of the Economics of Finance, 1, pp.939-974. Login This is basically always the same # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) Creating our strategy Formulating our strategy This is where the fun begins! If you haven't already, read through How to Automate I: Understanding Flumine to get a grasp on what the general code structure of Flumine looks like. The basic code structure will always be the same, but we will tailor it to whatever strategy we are trying to run. We have a few basic requirements for this strategy: We want to find what horse is the favourite / second favourite / third favourite and so on We want to place a bet on that horse We want to only place a single bet on that horse and on the market in general Hold up, how do we decide which horse is the favourite? There are two things we must consider here: What price should we be using to determine if a horse is the favourite? Back price Lay price Some sort of mid point Last traded price Some combination?? What point in time should we reference the price to determine the favourite? 10 mins before the jump 5 mins before the jump 30 secs before the jump?? These decisions can make huge differences, and it's up to you to do your own analysis and decided which is best. For this example, we will keep it simple and place a Back bet on the favourite, and use the last traded price, 60 seconds before the scheduled jump time Implementing our strategy Now that we have decided on using the last traded price for each horse, we actually need a way to find this in real time. Luckily, the Betfair documentation provides everything we need. In Betfairlightweight and Flumine wrapper for the Betfair API, runner has the attribute last_price_traded , so to access it we can simply call runner.last_price_traded . Looping through all the runners in a race we can collect the last_price_traded for all runners and then convert that into a dataframe: snapshot_last_price_traded = [] for runner in market_book.runners: snapshot_last_price_traded.append([runner.selection_id,runner.last_price_traded]) snapshot_last_price_traded = pd.DataFrame(snapshot_last_price_traded, columns=['selection_id','last_traded_price']) So, we end up with a DataFrame like this: Let's sort the rows by the last_traded_price and define a new variable called the fav_selection_id which is the selection_id of the horse we want to bet on (first favourite horse). To get the corresponding selection_id we need to select the value from the first row of the selection_id column. As Python starts indexing rows from 0, instead of 1, we will be selecting the 0th index to get the value of the first row. (If your strategy was backing/laying the second favourite you will need to be selecting the 1st index) snapshot_last_price_traded = snapshot_last_price_traded.sort_values(by = ['last_traded_price']) fav_selection_id = snapshot_last_price_traded['selection_id'].iloc[0] We also need a way to validate bets so that we don't bet on multiple selections. This is because process_market_book will run every time anyone places or cancels a bet on that market. So we need to prevent multiple bet placements. This is usually easily done by setting max_trade_count = 1 however, looking at the documentation we can see that this is only on a per selection basis. As the favourite horse can change over time then we may end up betting multiple times if the favourite changes. In most situations this probably isn't an issue, as you can just limit betting to a timeframe e.g., from 60 seconds before the jump to 50 seconds before the jump. However, if the market is very illiquid there may not be any bets placed or cancelled in the time frame. So, let's come up with a way that our strategy can only bet once without using the timeframe as a work around. According to the documentation for Flumine we can get the runner_context, which will allow us to collect info on the trades, matched and waiting to be matched. If we loop through all the runners in the market, we can turn that into a DataFrame: runner_context = [] for runner in market_book.runners: runner_context = self.get_runner_context( market.market_id, runner.selection_id, runner.handicap ) snapshot_runner_context.append([runner_context.selection_id, runner_context.executable_orders, runner_context.live_trade_count, runner_context.trade_count]) snapshot_runner_context = pd.DataFrame(snapshot_runner_context, columns=['selection_id','executable_orders','live_trade_count','trade_count']) This will return a DataFrame like this: Now we can simply just check if the sum last 3 columns equal zero, to validate that no bets have been placed: snapshot_runner_context.iloc[:,1:].sum().sum() Let's put this all together and pepper in some logging so we know what's happening: # Import necessary libraries from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import pandas as pd import numpy as np # Logging import logging logging . basicConfig ( filename = 'how_to_automate_2.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(lineno)d : %(message)s ' ) class BackFavStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'BackFavStrategy'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Collect data on last price traded and the number of bets we have placed snapshot_last_price_traded = [] snapshot_runner_context = [] for runner in market_book . runners : snapshot_last_price_traded . append ([ runner . selection_id , runner . last_price_traded ]) # Get runner context for each runner runner_context = self . get_runner_context ( market . market_id , runner . selection_id , runner . handicap ) snapshot_runner_context . append ([ runner_context . selection_id , runner_context . executable_orders , runner_context . live_trade_count , runner_context . trade_count ]) # Convert last price traded data to dataframe snapshot_last_price_traded = pd . DataFrame ( snapshot_last_price_traded , columns = [ 'selection_id' , 'last_traded_price' ]) # Find the selection_id of the favourite snapshot_last_price_traded = snapshot_last_price_traded . sort_values ( by = [ 'last_traded_price' ]) fav_selection_id = snapshot_last_price_traded [ 'selection_id' ] . iloc [ 0 ] logging . info ( snapshot_last_price_traded ) # logging # Convert data on number of bets we have placed to a dataframe snapshot_runner_context = pd . DataFrame ( snapshot_runner_context , columns = [ 'selection_id' , 'executable_orders' , 'live_trade_count' , 'trade_count' ]) logging . info ( snapshot_runner_context ) # logging for runner in market_book . runners : if runner . status == \"ACTIVE\" and market . seconds_to_start < 60 and market_book . inplay == False and runner . selection_id == fav_selection_id and snapshot_runner_context . iloc [:, 1 :] . sum () . sum () == 0 : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . last_price_traded , size = 5 ) ) market . place_order ( order ) Running our strategy Now that we have our strategy ready, we can point it to a sport, and let it run. This time we will add some trading controls because we are likely to get matched. Let's specify that we are only comfortable with 1 bet at any time with a maximum exposure of $20 strategy = BackFavStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_trade_count = 1 , # max total number of trades per runner max_live_trade_count = 1 , # max live (with executable orders) trades per runner max_selection_exposure = 20 , # max exposure of 20 per horse max_order_exposure = 20 # Max bet sizes of $20 ) Before we start running our strategy lets learn how the Flumine framework actually works. I basically glossed over this in Part I as its not entirely necessary to get something up and running, but in a more realistic sense its very useful and we now have a good strategy to test it on. When we log into using Flumine we define a framework which is a Flumine object: trading = betfairlightweight.APIClient('username','password',app_key='appkey') client = clients.BetfairClient(trading, interactive_login=True) framework = Flumine(client=client) When we run our strategies, we are actually running framework . You can think of framework as a video game character with nothing equiped, when we add different strategies its like equipping a weapon like a sword or bow. We can also add other things to help us out such as LiveLoggingControl (which we will learn to create a bit futher down), and an autoterminate function so we don't need to manually turn it off each day, this is like adding buffs or armour that can help our character to progress. We need to define the thing that we are adding onto our framework and then Flumine will have specific functions that allow us to equip our strategies and helper functions. For example to add a strategy we just need to do add_strategy() , the list of Flumine functions that you can use to add things to framework are available here . Once we have our character ready with strategies and help code equiped we can do framework.run() and that will run your framework with all the strategies and all the supporting supporting code attached. framework . add_strategy ( strategy ) Automatic Terminate This code is a direct copy and paste from the examples and works like a charm out of the box. It runs every 60 seconds and checks if all markets starting today have been closed for at least 20 minutes. If it has then it will stop our automation. import logging import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent logger = logging . getLogger ( __name__ ) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" # Creates a list of all markets markets = list ( flumine . markets . markets . values ()) # from the above list, create a list of markets that are expected to start today markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] # counts the markets that are expected to start today if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) # if the number of markets that are expected to start today then stop flumine if market_count == 0 : logger . info ( \"No more markets available, terminating framework\" ) flumine . handler_queue . put ( TerminationEvent ( flumine )) Now that we have created our terminate function we have to add it to our framework # Add the auto terminate to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) Let's also create something that records our bets in a nice csv/excel file so we can review how we went later on. Although this is also called logging it will create a clean csv file called \"orders_hta_2.csv\" that looks like this: This becomes super useful when we have more than one strategy so we can track how each strategy is performing (it also reads nicely into a pandas DataFrame!). This will also allow us to do some analysis such as check how often our bets get matched. This code is copied from the examples with some very slight changes. import os import csv import logging from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # checks if the file \"orders_hta_2.csv\" already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_2.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_2.csv\" , \"w\" ) as m : # Create orders_hta_2.csv with the first row as the FIELDNAMES we specified above csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event # gives us a list of our orders for a market that has already settled with open ( \"orders_hta_2.csv\" , \"a\" ) as m : # open orders_hta_2.csv and append a new row of data (orders) for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : # Create a dictionary of data we want to append to our csv file order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) # maps our dictionary to output rows csv_writer . writerow ( order_data ) # append data to csv files except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) Now let's add the bet logging to our framework and run everything at once: framework . add_logging_control ( LiveLoggingControl () ) framework . run () starting strategy 'BackFavStrategy' There are some other things you can add such as workers that run automatically in the background at set times (e.g. every 10 seconds) and are independent of market updates. There is some documentation available but to be honest I've never had to use them. If you are doing something that is a bit more intense or needs to be run at set time intervals then they are probably useful so take a look. Conclusion and next steps We have so far done zero backtesting on this strategy, and blindly following strategies from published papers that are over 20 years old is a sure fire way to lose money. But hopefully this gives you an idea of the things you can accomplish with Flumine. If you are keen on backtesting your own betting angles, I would suggest taking a look at our tutorial which goes into depth on how to backtest Automated Betting Angles in Python using historical data. Now that we have a better understanding of Flumine we are getting very close to automating our own model. We still have three parts remaining in this series which will take you step by step through: Part III - Automating a Betfair model Part IV - Automating your own model Part V - How to simulate the Exchange to backtest and optimise our strategies Complete code Run the code from your ide by using py <filename> .py, making sure you amend the path to point to your input data. Download from Github # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import pandas as pd import numpy as np import logging import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent import os import csv import logging from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logging . basicConfig ( filename = 'how_to_automate_2.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(lineno)d : %(message)s ' ) # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) class BackFavStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'BackFavStrategy'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Collect data on last price traded and the number of bets we have placed snapshot_last_price_traded = [] snapshot_runner_context = [] for runner in market_book . runners : snapshot_last_price_traded . append ([ runner . selection_id , runner . last_price_traded ]) # Get runner context for each runner runner_context = self . get_runner_context ( market . market_id , runner . selection_id , runner . handicap ) snapshot_runner_context . append ([ runner_context . selection_id , runner_context . executable_orders , runner_context . live_trade_count , runner_context . trade_count ]) # Convert last price traded data to dataframe snapshot_last_price_traded = pd . DataFrame ( snapshot_last_price_traded , columns = [ 'selection_id' , 'last_traded_price' ]) # Find the selection_id of the favourite snapshot_last_price_traded = snapshot_last_price_traded . sort_values ( by = [ 'last_traded_price' ]) fav_selection_id = snapshot_last_price_traded [ 'selection_id' ] . iloc [ 0 ] logging . info ( snapshot_last_price_traded ) # logging # Convert data on number of bets we have placed to a dataframe snapshot_runner_context = pd . DataFrame ( snapshot_runner_context , columns = [ 'selection_id' , 'executable_orders' , 'live_trade_count' , 'trade_count' ]) logging . info ( snapshot_runner_context ) # logging for runner in market_book . runners : if runner . status == \"ACTIVE\" and market . seconds_to_start < 60 and market_book . inplay == False and runner . selection_id == fav_selection_id and snapshot_runner_context . iloc [:, 1 :] . sum () . sum () == 0 : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . last_price_traded , size = 5 ) ) market . place_order ( order ) logger = logging . getLogger ( __name__ ) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : logger . info ( \"No more markets available, terminating framework\" ) flumine . handler_queue . put ( TerminationEvent ( flumine )) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_2.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_2.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_2.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_2.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) strategy = BackFavStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_trade_count = 1 , # max total number of trades per runner max_live_trade_count = 1 , # max live (with executable orders) trades per runner max_selection_exposure = 20 , # max exposure of 20 per horse max_order_exposure = 20 # Max bet sizes of $20 ) framework . add_strategy ( strategy ) # Add the auto terminate to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) framework . add_logging_control ( LiveLoggingControl () ) framework . run () # run our framework Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"How to Automate 2"},{"location":"api/How_to_Automate_2/#how-to-automate-ii-backing-or-laying-the-1st2ndnth-favourite-shot","text":"This tutorial is Part two of the How to Automate series and follows on logically from the How to Automate I: Understanding Flumine tutorial we shared previously. Make sure you look at part one, before diving into this one, as it will help you understand the general Flumine code structure and how it works. This tutorial will delve a bit deeper and give us more confidence working with Flumine. Not only will we get a cool new strategy, but we will be moving closer to learning how to automate our own model. As always please reach out with feedback, suggestions or queries. Please submit a pull request if you catch any bugs or have other improvement suggestions!","title":"How to Automate II: Backing or laying the 1st/2nd/.../nth favourite shot"},{"location":"api/How_to_Automate_2/#context","text":"Say for example you have done some research and found there is significant published academic literature on the existence of a favourite-longshot bias. As a result, you want to automate a strategy backing all favourites or second favourites in thoroughbred racing markets. If this is the case, then this is the perfect tutorial for you! Taking a quick look on Google Scholar we can see there is indeed plenty of published papers on this topic (almost as if I planned it). Many of these Journals are high quality too! According to the ABDC Journal Rankings : Scottish Journal of Political Economy has rating of A The Economic Journal is A* Applied Economics is A* However at this point I must stress to the reader that these papers are quite old and were mainly published in the early 2000s. Schwert (2003) suggests that many of the market anomalies, discovered in financial markets as a contradiction to market efficiency, disappear once they have been published in academic literature. So, although these researchers may have found the existence of a favourite-longshot bias in betting markets in the early 2000s, they likely no longer exist as the publication of their findings leads to an increase in market efficiency. Schwert, G.W., 2003. Anomalies and market efficiency. Handbook of the Economics of Finance, 1, pp.939-974.","title":"Context"},{"location":"api/How_to_Automate_2/#login","text":"This is basically always the same # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client)","title":"Login"},{"location":"api/How_to_Automate_2/#creating-our-strategy","text":"","title":"Creating our strategy"},{"location":"api/How_to_Automate_2/#formulating-our-strategy","text":"This is where the fun begins! If you haven't already, read through How to Automate I: Understanding Flumine to get a grasp on what the general code structure of Flumine looks like. The basic code structure will always be the same, but we will tailor it to whatever strategy we are trying to run. We have a few basic requirements for this strategy: We want to find what horse is the favourite / second favourite / third favourite and so on We want to place a bet on that horse We want to only place a single bet on that horse and on the market in general Hold up, how do we decide which horse is the favourite? There are two things we must consider here: What price should we be using to determine if a horse is the favourite? Back price Lay price Some sort of mid point Last traded price Some combination?? What point in time should we reference the price to determine the favourite? 10 mins before the jump 5 mins before the jump 30 secs before the jump?? These decisions can make huge differences, and it's up to you to do your own analysis and decided which is best. For this example, we will keep it simple and place a Back bet on the favourite, and use the last traded price, 60 seconds before the scheduled jump time","title":"Formulating our strategy"},{"location":"api/How_to_Automate_2/#implementing-our-strategy","text":"Now that we have decided on using the last traded price for each horse, we actually need a way to find this in real time. Luckily, the Betfair documentation provides everything we need. In Betfairlightweight and Flumine wrapper for the Betfair API, runner has the attribute last_price_traded , so to access it we can simply call runner.last_price_traded . Looping through all the runners in a race we can collect the last_price_traded for all runners and then convert that into a dataframe: snapshot_last_price_traded = [] for runner in market_book.runners: snapshot_last_price_traded.append([runner.selection_id,runner.last_price_traded]) snapshot_last_price_traded = pd.DataFrame(snapshot_last_price_traded, columns=['selection_id','last_traded_price']) So, we end up with a DataFrame like this: Let's sort the rows by the last_traded_price and define a new variable called the fav_selection_id which is the selection_id of the horse we want to bet on (first favourite horse). To get the corresponding selection_id we need to select the value from the first row of the selection_id column. As Python starts indexing rows from 0, instead of 1, we will be selecting the 0th index to get the value of the first row. (If your strategy was backing/laying the second favourite you will need to be selecting the 1st index) snapshot_last_price_traded = snapshot_last_price_traded.sort_values(by = ['last_traded_price']) fav_selection_id = snapshot_last_price_traded['selection_id'].iloc[0] We also need a way to validate bets so that we don't bet on multiple selections. This is because process_market_book will run every time anyone places or cancels a bet on that market. So we need to prevent multiple bet placements. This is usually easily done by setting max_trade_count = 1 however, looking at the documentation we can see that this is only on a per selection basis. As the favourite horse can change over time then we may end up betting multiple times if the favourite changes. In most situations this probably isn't an issue, as you can just limit betting to a timeframe e.g., from 60 seconds before the jump to 50 seconds before the jump. However, if the market is very illiquid there may not be any bets placed or cancelled in the time frame. So, let's come up with a way that our strategy can only bet once without using the timeframe as a work around. According to the documentation for Flumine we can get the runner_context, which will allow us to collect info on the trades, matched and waiting to be matched. If we loop through all the runners in the market, we can turn that into a DataFrame: runner_context = [] for runner in market_book.runners: runner_context = self.get_runner_context( market.market_id, runner.selection_id, runner.handicap ) snapshot_runner_context.append([runner_context.selection_id, runner_context.executable_orders, runner_context.live_trade_count, runner_context.trade_count]) snapshot_runner_context = pd.DataFrame(snapshot_runner_context, columns=['selection_id','executable_orders','live_trade_count','trade_count']) This will return a DataFrame like this: Now we can simply just check if the sum last 3 columns equal zero, to validate that no bets have been placed: snapshot_runner_context.iloc[:,1:].sum().sum() Let's put this all together and pepper in some logging so we know what's happening: # Import necessary libraries from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import pandas as pd import numpy as np # Logging import logging logging . basicConfig ( filename = 'how_to_automate_2.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(lineno)d : %(message)s ' ) class BackFavStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'BackFavStrategy'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Collect data on last price traded and the number of bets we have placed snapshot_last_price_traded = [] snapshot_runner_context = [] for runner in market_book . runners : snapshot_last_price_traded . append ([ runner . selection_id , runner . last_price_traded ]) # Get runner context for each runner runner_context = self . get_runner_context ( market . market_id , runner . selection_id , runner . handicap ) snapshot_runner_context . append ([ runner_context . selection_id , runner_context . executable_orders , runner_context . live_trade_count , runner_context . trade_count ]) # Convert last price traded data to dataframe snapshot_last_price_traded = pd . DataFrame ( snapshot_last_price_traded , columns = [ 'selection_id' , 'last_traded_price' ]) # Find the selection_id of the favourite snapshot_last_price_traded = snapshot_last_price_traded . sort_values ( by = [ 'last_traded_price' ]) fav_selection_id = snapshot_last_price_traded [ 'selection_id' ] . iloc [ 0 ] logging . info ( snapshot_last_price_traded ) # logging # Convert data on number of bets we have placed to a dataframe snapshot_runner_context = pd . DataFrame ( snapshot_runner_context , columns = [ 'selection_id' , 'executable_orders' , 'live_trade_count' , 'trade_count' ]) logging . info ( snapshot_runner_context ) # logging for runner in market_book . runners : if runner . status == \"ACTIVE\" and market . seconds_to_start < 60 and market_book . inplay == False and runner . selection_id == fav_selection_id and snapshot_runner_context . iloc [:, 1 :] . sum () . sum () == 0 : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . last_price_traded , size = 5 ) ) market . place_order ( order )","title":"Implementing our strategy"},{"location":"api/How_to_Automate_2/#running-our-strategy","text":"Now that we have our strategy ready, we can point it to a sport, and let it run. This time we will add some trading controls because we are likely to get matched. Let's specify that we are only comfortable with 1 bet at any time with a maximum exposure of $20 strategy = BackFavStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_trade_count = 1 , # max total number of trades per runner max_live_trade_count = 1 , # max live (with executable orders) trades per runner max_selection_exposure = 20 , # max exposure of 20 per horse max_order_exposure = 20 # Max bet sizes of $20 ) Before we start running our strategy lets learn how the Flumine framework actually works. I basically glossed over this in Part I as its not entirely necessary to get something up and running, but in a more realistic sense its very useful and we now have a good strategy to test it on. When we log into using Flumine we define a framework which is a Flumine object: trading = betfairlightweight.APIClient('username','password',app_key='appkey') client = clients.BetfairClient(trading, interactive_login=True) framework = Flumine(client=client) When we run our strategies, we are actually running framework . You can think of framework as a video game character with nothing equiped, when we add different strategies its like equipping a weapon like a sword or bow. We can also add other things to help us out such as LiveLoggingControl (which we will learn to create a bit futher down), and an autoterminate function so we don't need to manually turn it off each day, this is like adding buffs or armour that can help our character to progress. We need to define the thing that we are adding onto our framework and then Flumine will have specific functions that allow us to equip our strategies and helper functions. For example to add a strategy we just need to do add_strategy() , the list of Flumine functions that you can use to add things to framework are available here . Once we have our character ready with strategies and help code equiped we can do framework.run() and that will run your framework with all the strategies and all the supporting supporting code attached. framework . add_strategy ( strategy )","title":"Running our strategy"},{"location":"api/How_to_Automate_2/#automatic-terminate","text":"This code is a direct copy and paste from the examples and works like a charm out of the box. It runs every 60 seconds and checks if all markets starting today have been closed for at least 20 minutes. If it has then it will stop our automation. import logging import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent logger = logging . getLogger ( __name__ ) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" # Creates a list of all markets markets = list ( flumine . markets . markets . values ()) # from the above list, create a list of markets that are expected to start today markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] # counts the markets that are expected to start today if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) # if the number of markets that are expected to start today then stop flumine if market_count == 0 : logger . info ( \"No more markets available, terminating framework\" ) flumine . handler_queue . put ( TerminationEvent ( flumine )) Now that we have created our terminate function we have to add it to our framework # Add the auto terminate to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) Let's also create something that records our bets in a nice csv/excel file so we can review how we went later on. Although this is also called logging it will create a clean csv file called \"orders_hta_2.csv\" that looks like this: This becomes super useful when we have more than one strategy so we can track how each strategy is performing (it also reads nicely into a pandas DataFrame!). This will also allow us to do some analysis such as check how often our bets get matched. This code is copied from the examples with some very slight changes. import os import csv import logging from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # checks if the file \"orders_hta_2.csv\" already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_2.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_2.csv\" , \"w\" ) as m : # Create orders_hta_2.csv with the first row as the FIELDNAMES we specified above csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event # gives us a list of our orders for a market that has already settled with open ( \"orders_hta_2.csv\" , \"a\" ) as m : # open orders_hta_2.csv and append a new row of data (orders) for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : # Create a dictionary of data we want to append to our csv file order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) # maps our dictionary to output rows csv_writer . writerow ( order_data ) # append data to csv files except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) Now let's add the bet logging to our framework and run everything at once: framework . add_logging_control ( LiveLoggingControl () ) framework . run () starting strategy 'BackFavStrategy' There are some other things you can add such as workers that run automatically in the background at set times (e.g. every 10 seconds) and are independent of market updates. There is some documentation available but to be honest I've never had to use them. If you are doing something that is a bit more intense or needs to be run at set time intervals then they are probably useful so take a look.","title":"Automatic Terminate"},{"location":"api/How_to_Automate_2/#conclusion-and-next-steps","text":"We have so far done zero backtesting on this strategy, and blindly following strategies from published papers that are over 20 years old is a sure fire way to lose money. But hopefully this gives you an idea of the things you can accomplish with Flumine. If you are keen on backtesting your own betting angles, I would suggest taking a look at our tutorial which goes into depth on how to backtest Automated Betting Angles in Python using historical data. Now that we have a better understanding of Flumine we are getting very close to automating our own model. We still have three parts remaining in this series which will take you step by step through: Part III - Automating a Betfair model Part IV - Automating your own model Part V - How to simulate the Exchange to backtest and optimise our strategies","title":"Conclusion and next steps"},{"location":"api/How_to_Automate_2/#complete-code","text":"Run the code from your ide by using py <filename> .py, making sure you amend the path to point to your input data. Download from Github # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import pandas as pd import numpy as np import logging import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent import os import csv import logging from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logging . basicConfig ( filename = 'how_to_automate_2.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(lineno)d : %(message)s ' ) # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) class BackFavStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'BackFavStrategy'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Collect data on last price traded and the number of bets we have placed snapshot_last_price_traded = [] snapshot_runner_context = [] for runner in market_book . runners : snapshot_last_price_traded . append ([ runner . selection_id , runner . last_price_traded ]) # Get runner context for each runner runner_context = self . get_runner_context ( market . market_id , runner . selection_id , runner . handicap ) snapshot_runner_context . append ([ runner_context . selection_id , runner_context . executable_orders , runner_context . live_trade_count , runner_context . trade_count ]) # Convert last price traded data to dataframe snapshot_last_price_traded = pd . DataFrame ( snapshot_last_price_traded , columns = [ 'selection_id' , 'last_traded_price' ]) # Find the selection_id of the favourite snapshot_last_price_traded = snapshot_last_price_traded . sort_values ( by = [ 'last_traded_price' ]) fav_selection_id = snapshot_last_price_traded [ 'selection_id' ] . iloc [ 0 ] logging . info ( snapshot_last_price_traded ) # logging # Convert data on number of bets we have placed to a dataframe snapshot_runner_context = pd . DataFrame ( snapshot_runner_context , columns = [ 'selection_id' , 'executable_orders' , 'live_trade_count' , 'trade_count' ]) logging . info ( snapshot_runner_context ) # logging for runner in market_book . runners : if runner . status == \"ACTIVE\" and market . seconds_to_start < 60 and market_book . inplay == False and runner . selection_id == fav_selection_id and snapshot_runner_context . iloc [:, 1 :] . sum () . sum () == 0 : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . last_price_traded , size = 5 ) ) market . place_order ( order ) logger = logging . getLogger ( __name__ ) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : logger . info ( \"No more markets available, terminating framework\" ) flumine . handler_queue . put ( TerminationEvent ( flumine )) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_2.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_2.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_2.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_2.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) strategy = BackFavStrategy ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_trade_count = 1 , # max total number of trades per runner max_live_trade_count = 1 , # max live (with executable orders) trades per runner max_selection_exposure = 20 , # max exposure of 20 per horse max_order_exposure = 20 # Max bet sizes of $20 ) framework . add_strategy ( strategy ) # Add the auto terminate to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) framework . add_logging_control ( LiveLoggingControl () ) framework . run () # run our framework","title":"Complete code"},{"location":"api/How_to_Automate_2/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"api/How_to_Automate_3/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); How to Automate III: Betfair Data Scientists Models This tutorial is Part three of the How to Automate series and follows on logically from the How to Automate II tutorial we shared previously. If you're still new to Flumine we suggest you take a look at part one and part two before diving into this one. The overall goal of the How to Automate series was to learn how to use Flumine to Automate our own Betting Model. So far we have covered how Flumine works, lets now put it into action with one of Betfair's own data science models which will give us a solid foundation for part four where we automate our own model. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Context Betfair\u2019s own Data Scientists have created a few inhouse models which produce ratings that you can use as racing and sporting tips. In our previous monthly meet ups, we learnt how to Backtest these models ourselves , now we will learn how actually automate those models! We will be automating the thoroughbred ratings model and the greyhound's model, but you can easily apply this to any model and if you have a specific angle you want to automate, you can just copy & paste our code and modify it to your liking! Scrape Today's Model Ratings Let's quickly scrape the ratings that we want to automate, all of the Betfair Data Science models are found on the Betfair Hub Models page. If we follow the links to the Horse Ratings Model and the Greyhounds Ratings Model we find that URL links behind the ratings download buttons have a consistent URL pattern that is easy to scrape. You end up with a link like this for the horse ratings model: https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=2022-03-09&presenter=RatingsPresenter&csv=true and one like this for the greyhounds ratings model: https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=2022-03-09&presenter=RatingsPresenter&csv=true We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe by simply changing the date in the url. import pandas as pd # Thoroughbred model (named the kash-ratings-model) kash_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=' kash_url_2 = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) # todays date formatted as YYYY-mm-dd kash_url_3 = '&presenter=RatingsPresenter&csv=true' kash_url = kash_url_1 + kash_url_2 + kash_url_3 kash_url 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=2022-02-25&presenter=RatingsPresenter&csv=true' # Greyhounds model (named the iggy-joey-model) iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 iggy_url 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=2022-02-25&presenter=RatingsPresenter&csv=true' Now that we have the URL, that dynamically changes to the current date, let's scrape the data using pandas and do some quick cleaning. We only really need three variables, the market_id so we know what market to bet on, the selection_id to know which horse/dog the rating is referring to and the most important one the rating . # Download todays thoroughbred ratings kash_df = pd . read_csv ( kash_url ) ## Data clearning # Rename Columns kash_df = kash_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) # Only keep columns we need kash_df = kash_df [[ 'market_id' , 'selection_id' , 'rating' ]] # Convert market_id to string kash_df [ 'market_id' ] = kash_df [ 'market_id' ] . astype ( str ) kash_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id rating 0 1.195173067 4218988 210.17 1 1.195173067 28551850 8.32 2 1.195173067 37261788 6.24 3 1.195173067 20357386 5.60 4 1.195173067 20466004 8.14 ... ... ... ... 442 1.195174127 43035476 4.68 443 1.195174127 27077942 188.93 444 1.195174127 26735524 16.85 445 1.195174127 21661954 192.68 446 1.195174127 24265033 13.85 447 rows \u00d7 3 columns Let's also set up the Dataframe so that we can easily get our Rating when given market_id and selection_id : # Set market_id and selection_id as index for easy referencing kash_df = kash_df . set_index ([ 'market_id' , 'selection_id' ]) kash_df # e.g. can reference like this: # df.loc['1.195173067'].loc['4218988'] # to return 210.17 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rating market_id selection_id 1.195173067 4218988 210.17 28551850 8.32 37261788 6.24 20357386 5.60 20466004 8.14 ... ... ... 1.195174127 43035476 4.68 27077942 188.93 26735524 16.85 21661954 192.68 24265033 13.85 447 rows \u00d7 1 columns Let's do the same thing for the greyhound model # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) ## Data clearning # Rename Columns iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) # Only keep columns we need iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] # Convert market_id to string iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) iggy_df # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) iggy_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rating market_id selection_id 1.195241412 43066395 12.80 43066396 12.17 43066397 11.10 43066398 5.85 42256274 2.85 ... ... ... 1.195242071 42424961 7.73 42588368 18.47 35695588 7.33 42334315 7.14 38830169 41.51 960 rows \u00d7 1 columns Automate Todays Model Ratings Now that we have all of our ratings in a nice clean DataFrame we can easily automate them in Flumine. Log into Flumine # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) Create Our Strategy in Flumine Strategy for Thoroughbreds This is where you can come up with exciting and innovative strategies! But because we are lame, let's start off with a simple strategy that checks prices 60 seconds before the jump and Backs anything greater than our ratings and Lays anything less than our ratings all with a flat stake size of $5 If you get confused about the code structure for how Flumine and BaseStrategy works, be sure to take a check out How to Automate I as it dives right into how to use Flumine. Our main logic is: Check how far out from the jump we are If we are within 60 seconds from the jump check each horse's market prices If best Back price > model price then place a $5 Back bet If best Lay price < model price then place a $5 Lay bet # Import libraries and logging from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import pandas as pd import logging # Will create a file called how_to_automate_3.log in our current working directory logging . basicConfig ( filename = 'how_to_automate_3.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # New strategy called FlatKashModel for the Thoroughbreds model class FlatKashModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatKashModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : # Check runner hasn't scratched and that first layer of back or lay price exists if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ] and runner . ex . available_to_lay [ 0 ]: # If best available to back price is > rated price then flat $5 bet if runner . ex . available_to_back [ 0 ][ 'price' ] > kash_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . ex . available_to_lay [ 0 ][ 'price' ] < kash_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) Strategy for Greyhounds Let's create the same strategy for the Greyhounds model. The only difference here is that when we reference the model price, we are referencing the DataFrame iggy_df instead of kash_df. (An alternate way we could do this if we wanted would be to combine both strategies together and just append the dataframes together, but lets just keep it nice and simple for now) # New strategy called FlatIggyModel for the Greyhound model class FlatIggyModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatIggyModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : # Check runner hasn't scratched and that first layer of back or lay price exists if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ] and runner . ex . available_to_lay [ 0 ]: # If best available to back price is > rated price then flat $5 bet if runner . ex . available_to_back [ 0 ][ 'price' ] > iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . ex . available_to_lay [ 0 ][ 'price' ] < iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) Running our strategy Now that we have created two strategies we need to add both to the frame work along with the auto-terminate and bet logging we made in How to Automate II thoroughbreds_strategy = FlatKashModel ( market_filter = streaming_market_filter ( event_type_ids = [ \"7\" ], # Horse Racing country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_order_exposure = 50 , # Max bet sizes of $50 max_trade_count = 1 , # Max of trade/bet attempt per selection max_live_trade_count = 1 , # Max of 1 unmatched Bet per selection ) greyhounds_strategy = FlatIggyModel ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhound Racing country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_order_exposure = 50 , # Max bet sizes of $50 max_trade_count = 1 , # Max of trade/bet attempt per selection max_live_trade_count = 1 , # Max of 1 unmatched Bet per selection ) framework . add_strategy ( thoroughbreds_strategy ) # Add horse racing strategy to our framework framework . add_strategy ( greyhounds_strategy ) # Add greyhound racing strategy to our framework # import logging import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent # logger = logging.getLogger(__name__) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : # logger.info(\"No more markets available, terminating framework\") flumine . handler_queue . put ( TerminationEvent ( flumine )) # Add the stopped to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) import os import csv import logging from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_3.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_3.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_3.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_3.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) framework . add_logging_control ( LiveLoggingControl () ) framework . run () # run all our strategies starting strategy 'FlatKashModel' starting strategy 'FlatIggyModel' Conclusion and next steps There we have it. We now have a bot that you can turn on at the start of a day by hitting the run all button. It will automatically scrape all the data online, place bets throughout the day and at the end of the day stop itself. Then on the next day you can hit run all again, without needing to update anything. While Betfair's own Data Science models are easy to automate you're also not likely to become rich. They are also available to everyone freely online, so any edge is likely already priced in. That's why for the next part of this series we will be learning How to Automate your own model and straight after learning How to simulate the Exchange to backtest and optimise our strategies Complete code Run the code from your ide by using py <filename> .py, making sure you amend the path to point to your input data. Download from Github import pandas as pd from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import logging import betfairlightweight from flumine import Flumine , clients import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent import os import csv from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes # Will create a file called how_to_automate_3.log in our current working directory logging . basicConfig ( filename = 'how_to_automate_3.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # Thoroughbred model (named the kash-ratings-model) kash_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=' kash_url_2 = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) # todays date formatted as YYYY-mm-dd kash_url_3 = '&presenter=RatingsPresenter&csv=true' kash_url = kash_url_1 + kash_url_2 + kash_url_3 kash_url # Greyhounds model (named the iggy-joey-model) iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 iggy_url # Download todays thoroughbred ratings kash_df = pd . read_csv ( kash_url ) ## Data clearning # Rename Columns kash_df = kash_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) # Only keep columns we need kash_df = kash_df [[ 'market_id' , 'selection_id' , 'rating' ]] # Convert market_id to string kash_df [ 'market_id' ] = kash_df [ 'market_id' ] . astype ( str ) kash_df # Set market_id and selection_id as index for easy referencing kash_df = kash_df . set_index ([ 'market_id' , 'selection_id' ]) kash_df # e.g. can reference like this: # df.loc['1.195173067'].loc['4218988'] # to return 210.17 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) ## Data clearning # Rename Columns iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) # Only keep columns we need iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] # Convert market_id to string iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) iggy_df # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) iggy_df # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) # Will create a file called how_to_automate_3.log in our current working directory logging . basicConfig ( filename = 'how_to_automate_3.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # New strategy called FlatKashModel for the Thoroughbreds model class FlatKashModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatKashModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : # Check runner hasn't scratched and that first layer of back or lay price exists if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ] and runner . ex . available_to_lay [ 0 ]: # If best available to back price is > rated price then flat $5 bet if runner . ex . available_to_back [ 0 ][ 'price' ] > kash_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . ex . available_to_lay [ 0 ][ 'price' ] < kash_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # New strategy called FlatIggyModel for the Greyhound model class FlatIggyModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatIggyModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : # Check runner hasn't scratched and that first layer of back or lay price exists if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ] and runner . ex . available_to_lay [ 0 ]: # If best available to back price is > rated price then flat $5 bet if runner . ex . available_to_back [ 0 ][ 'price' ] > iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . ex . available_to_lay [ 0 ][ 'price' ] < iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) logger = logging . getLogger ( __name__ ) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : # logger.info(\"No more markets available, terminating framework\") flumine . handler_queue . put ( TerminationEvent ( flumine )) logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_2.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_3.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_3.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_3.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) thoroughbreds_strategy = FlatKashModel ( market_filter = streaming_market_filter ( event_type_ids = [ \"7\" ], # Horse Racing country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_order_exposure = 50 , # Max bet sizes of $50 max_trade_count = 1 , # Max of trade/bet attempt per selection max_live_trade_count = 1 , # Max of 1 unmatched Bet per selection ) greyhounds_strategy = FlatIggyModel ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhound Racing country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_order_exposure = 50 , # Max bet sizes of $50 max_trade_count = 1 , # Max of trade/bet attempt per selection max_live_trade_count = 1 , # Max of 1 unmatched Bet per selection ) framework . add_strategy ( thoroughbreds_strategy ) # Add horse racing strategy to our framework framework . add_strategy ( greyhounds_strategy ) # Add greyhound racing strategy to our framework # Add the stopped to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) framework . add_logging_control ( LiveLoggingControl () ) framework . run () # run all our strategies Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"How to Automate 3"},{"location":"api/How_to_Automate_3/#how-to-automate-iii-betfair-data-scientists-models","text":"This tutorial is Part three of the How to Automate series and follows on logically from the How to Automate II tutorial we shared previously. If you're still new to Flumine we suggest you take a look at part one and part two before diving into this one. The overall goal of the How to Automate series was to learn how to use Flumine to Automate our own Betting Model. So far we have covered how Flumine works, lets now put it into action with one of Betfair's own data science models which will give us a solid foundation for part four where we automate our own model. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!","title":"How to Automate III: Betfair Data Scientists Models"},{"location":"api/How_to_Automate_3/#context","text":"Betfair\u2019s own Data Scientists have created a few inhouse models which produce ratings that you can use as racing and sporting tips. In our previous monthly meet ups, we learnt how to Backtest these models ourselves , now we will learn how actually automate those models! We will be automating the thoroughbred ratings model and the greyhound's model, but you can easily apply this to any model and if you have a specific angle you want to automate, you can just copy & paste our code and modify it to your liking!","title":"Context"},{"location":"api/How_to_Automate_3/#scrape-todays-model-ratings","text":"Let's quickly scrape the ratings that we want to automate, all of the Betfair Data Science models are found on the Betfair Hub Models page. If we follow the links to the Horse Ratings Model and the Greyhounds Ratings Model we find that URL links behind the ratings download buttons have a consistent URL pattern that is easy to scrape. You end up with a link like this for the horse ratings model: https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=2022-03-09&presenter=RatingsPresenter&csv=true and one like this for the greyhounds ratings model: https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=2022-03-09&presenter=RatingsPresenter&csv=true We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe by simply changing the date in the url. import pandas as pd # Thoroughbred model (named the kash-ratings-model) kash_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=' kash_url_2 = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) # todays date formatted as YYYY-mm-dd kash_url_3 = '&presenter=RatingsPresenter&csv=true' kash_url = kash_url_1 + kash_url_2 + kash_url_3 kash_url 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=2022-02-25&presenter=RatingsPresenter&csv=true' # Greyhounds model (named the iggy-joey-model) iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 iggy_url 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=2022-02-25&presenter=RatingsPresenter&csv=true' Now that we have the URL, that dynamically changes to the current date, let's scrape the data using pandas and do some quick cleaning. We only really need three variables, the market_id so we know what market to bet on, the selection_id to know which horse/dog the rating is referring to and the most important one the rating . # Download todays thoroughbred ratings kash_df = pd . read_csv ( kash_url ) ## Data clearning # Rename Columns kash_df = kash_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) # Only keep columns we need kash_df = kash_df [[ 'market_id' , 'selection_id' , 'rating' ]] # Convert market_id to string kash_df [ 'market_id' ] = kash_df [ 'market_id' ] . astype ( str ) kash_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id rating 0 1.195173067 4218988 210.17 1 1.195173067 28551850 8.32 2 1.195173067 37261788 6.24 3 1.195173067 20357386 5.60 4 1.195173067 20466004 8.14 ... ... ... ... 442 1.195174127 43035476 4.68 443 1.195174127 27077942 188.93 444 1.195174127 26735524 16.85 445 1.195174127 21661954 192.68 446 1.195174127 24265033 13.85 447 rows \u00d7 3 columns Let's also set up the Dataframe so that we can easily get our Rating when given market_id and selection_id : # Set market_id and selection_id as index for easy referencing kash_df = kash_df . set_index ([ 'market_id' , 'selection_id' ]) kash_df # e.g. can reference like this: # df.loc['1.195173067'].loc['4218988'] # to return 210.17 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rating market_id selection_id 1.195173067 4218988 210.17 28551850 8.32 37261788 6.24 20357386 5.60 20466004 8.14 ... ... ... 1.195174127 43035476 4.68 27077942 188.93 26735524 16.85 21661954 192.68 24265033 13.85 447 rows \u00d7 1 columns Let's do the same thing for the greyhound model # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) ## Data clearning # Rename Columns iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) # Only keep columns we need iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] # Convert market_id to string iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) iggy_df # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) iggy_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rating market_id selection_id 1.195241412 43066395 12.80 43066396 12.17 43066397 11.10 43066398 5.85 42256274 2.85 ... ... ... 1.195242071 42424961 7.73 42588368 18.47 35695588 7.33 42334315 7.14 38830169 41.51 960 rows \u00d7 1 columns","title":"Scrape Today's Model Ratings"},{"location":"api/How_to_Automate_3/#automate-todays-model-ratings","text":"Now that we have all of our ratings in a nice clean DataFrame we can easily automate them in Flumine.","title":"Automate Todays Model Ratings"},{"location":"api/How_to_Automate_3/#log-into-flumine","text":"# Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client)","title":"Log into Flumine"},{"location":"api/How_to_Automate_3/#create-our-strategy-in-flumine","text":"","title":"Create Our Strategy in Flumine"},{"location":"api/How_to_Automate_3/#strategy-for-thoroughbreds","text":"This is where you can come up with exciting and innovative strategies! But because we are lame, let's start off with a simple strategy that checks prices 60 seconds before the jump and Backs anything greater than our ratings and Lays anything less than our ratings all with a flat stake size of $5 If you get confused about the code structure for how Flumine and BaseStrategy works, be sure to take a check out How to Automate I as it dives right into how to use Flumine. Our main logic is: Check how far out from the jump we are If we are within 60 seconds from the jump check each horse's market prices If best Back price > model price then place a $5 Back bet If best Lay price < model price then place a $5 Lay bet # Import libraries and logging from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import pandas as pd import logging # Will create a file called how_to_automate_3.log in our current working directory logging . basicConfig ( filename = 'how_to_automate_3.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # New strategy called FlatKashModel for the Thoroughbreds model class FlatKashModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatKashModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : # Check runner hasn't scratched and that first layer of back or lay price exists if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ] and runner . ex . available_to_lay [ 0 ]: # If best available to back price is > rated price then flat $5 bet if runner . ex . available_to_back [ 0 ][ 'price' ] > kash_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . ex . available_to_lay [ 0 ][ 'price' ] < kash_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order )","title":"Strategy for Thoroughbreds"},{"location":"api/How_to_Automate_3/#strategy-for-greyhounds","text":"Let's create the same strategy for the Greyhounds model. The only difference here is that when we reference the model price, we are referencing the DataFrame iggy_df instead of kash_df. (An alternate way we could do this if we wanted would be to combine both strategies together and just append the dataframes together, but lets just keep it nice and simple for now) # New strategy called FlatIggyModel for the Greyhound model class FlatIggyModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatIggyModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : # Check runner hasn't scratched and that first layer of back or lay price exists if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ] and runner . ex . available_to_lay [ 0 ]: # If best available to back price is > rated price then flat $5 bet if runner . ex . available_to_back [ 0 ][ 'price' ] > iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . ex . available_to_lay [ 0 ][ 'price' ] < iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order )","title":"Strategy for Greyhounds"},{"location":"api/How_to_Automate_3/#running-our-strategy","text":"Now that we have created two strategies we need to add both to the frame work along with the auto-terminate and bet logging we made in How to Automate II thoroughbreds_strategy = FlatKashModel ( market_filter = streaming_market_filter ( event_type_ids = [ \"7\" ], # Horse Racing country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_order_exposure = 50 , # Max bet sizes of $50 max_trade_count = 1 , # Max of trade/bet attempt per selection max_live_trade_count = 1 , # Max of 1 unmatched Bet per selection ) greyhounds_strategy = FlatIggyModel ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhound Racing country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_order_exposure = 50 , # Max bet sizes of $50 max_trade_count = 1 , # Max of trade/bet attempt per selection max_live_trade_count = 1 , # Max of 1 unmatched Bet per selection ) framework . add_strategy ( thoroughbreds_strategy ) # Add horse racing strategy to our framework framework . add_strategy ( greyhounds_strategy ) # Add greyhound racing strategy to our framework # import logging import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent # logger = logging.getLogger(__name__) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : # logger.info(\"No more markets available, terminating framework\") flumine . handler_queue . put ( TerminationEvent ( flumine )) # Add the stopped to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) import os import csv import logging from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_3.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_3.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_3.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_3.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) framework . add_logging_control ( LiveLoggingControl () ) framework . run () # run all our strategies starting strategy 'FlatKashModel' starting strategy 'FlatIggyModel'","title":"Running our strategy"},{"location":"api/How_to_Automate_3/#conclusion-and-next-steps","text":"There we have it. We now have a bot that you can turn on at the start of a day by hitting the run all button. It will automatically scrape all the data online, place bets throughout the day and at the end of the day stop itself. Then on the next day you can hit run all again, without needing to update anything. While Betfair's own Data Science models are easy to automate you're also not likely to become rich. They are also available to everyone freely online, so any edge is likely already priced in. That's why for the next part of this series we will be learning How to Automate your own model and straight after learning How to simulate the Exchange to backtest and optimise our strategies","title":"Conclusion and next steps"},{"location":"api/How_to_Automate_3/#complete-code","text":"Run the code from your ide by using py <filename> .py, making sure you amend the path to point to your input data. Download from Github import pandas as pd from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import logging import betfairlightweight from flumine import Flumine , clients import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent import os import csv from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes # Will create a file called how_to_automate_3.log in our current working directory logging . basicConfig ( filename = 'how_to_automate_3.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # Thoroughbred model (named the kash-ratings-model) kash_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date=' kash_url_2 = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) # todays date formatted as YYYY-mm-dd kash_url_3 = '&presenter=RatingsPresenter&csv=true' kash_url = kash_url_1 + kash_url_2 + kash_url_3 kash_url # Greyhounds model (named the iggy-joey-model) iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 iggy_url # Download todays thoroughbred ratings kash_df = pd . read_csv ( kash_url ) ## Data clearning # Rename Columns kash_df = kash_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) # Only keep columns we need kash_df = kash_df [[ 'market_id' , 'selection_id' , 'rating' ]] # Convert market_id to string kash_df [ 'market_id' ] = kash_df [ 'market_id' ] . astype ( str ) kash_df # Set market_id and selection_id as index for easy referencing kash_df = kash_df . set_index ([ 'market_id' , 'selection_id' ]) kash_df # e.g. can reference like this: # df.loc['1.195173067'].loc['4218988'] # to return 210.17 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) ## Data clearning # Rename Columns iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) # Only keep columns we need iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] # Convert market_id to string iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) iggy_df # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) iggy_df # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) # Will create a file called how_to_automate_3.log in our current working directory logging . basicConfig ( filename = 'how_to_automate_3.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # New strategy called FlatKashModel for the Thoroughbreds model class FlatKashModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatKashModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : # Check runner hasn't scratched and that first layer of back or lay price exists if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ] and runner . ex . available_to_lay [ 0 ]: # If best available to back price is > rated price then flat $5 bet if runner . ex . available_to_back [ 0 ][ 'price' ] > kash_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . ex . available_to_lay [ 0 ][ 'price' ] < kash_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # New strategy called FlatIggyModel for the Greyhound model class FlatIggyModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatIggyModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # If time is less than 1min and we haven't placed a bet yet then look at our ratings and place bet if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : # Check runner hasn't scratched and that first layer of back or lay price exists if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ] and runner . ex . available_to_lay [ 0 ]: # If best available to back price is > rated price then flat $5 bet if runner . ex . available_to_back [ 0 ][ 'price' ] > iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . ex . available_to_lay [ 0 ][ 'price' ] < iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) logger = logging . getLogger ( __name__ ) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : # logger.info(\"No more markets available, terminating framework\") flumine . handler_queue . put ( TerminationEvent ( flumine )) logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_2.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_3.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_3.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_3.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) thoroughbreds_strategy = FlatKashModel ( market_filter = streaming_market_filter ( event_type_ids = [ \"7\" ], # Horse Racing country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_order_exposure = 50 , # Max bet sizes of $50 max_trade_count = 1 , # Max of trade/bet attempt per selection max_live_trade_count = 1 , # Max of 1 unmatched Bet per selection ) greyhounds_strategy = FlatIggyModel ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhound Racing country_codes = [ \"AU\" ], # Australian Markets market_types = [ \"WIN\" ], # Win Markets ), max_order_exposure = 50 , # Max bet sizes of $50 max_trade_count = 1 , # Max of trade/bet attempt per selection max_live_trade_count = 1 , # Max of 1 unmatched Bet per selection ) framework . add_strategy ( thoroughbreds_strategy ) # Add horse racing strategy to our framework framework . add_strategy ( greyhounds_strategy ) # Add greyhound racing strategy to our framework # Add the stopped to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) framework . add_logging_control ( LiveLoggingControl () ) framework . run () # run all our strategies","title":"Complete code"},{"location":"api/How_to_Automate_3/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"api/How_to_Automate_4/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); How to Automate IV: Automate your own Model For this tutorial we will be automating the model that Bruno taught us how to make in the Greyhound Modelling Tutorial . This tutorial also follows on logically from How to Automate III . So, if you haven't already, make sure you take a look at those before continuing here! Saving and loading in our model To generate our predictions, we have two options: we can generate our predictions using the same notebook used to train our model then read those predictions into this notebook, or we can save the model and read that model into this notebook. For this tutorial we have chosen to save the model, as it becomes a bit less confusing and easier to manage, although there are some pieces of code we may have to write twice (copy and paste). So first we will need to run the code from the tutorial and then save the model. This is super as simple we can just copy and paste the complete code provided at the end of the tutorial or download from Github . Then we can just run this extra line code (which I have copied from the documentation page ) at the end of the notebook to save the model. from joblib import dump dump(models['LogisticRegression'], 'logistic_regression.joblib') Now that the file is saved, let's read it into this note book: from joblib import load brunos_model = load ( 'logistic_regression.joblib' ) brunos_model LogisticRegression(n_jobs=-1, solver='saga') Generating predictions for today Now that we have the model loaded in, we need the data, to generate our predictions for today's races! # Import libraries required to download today's races import os import sys # Allow imports from src folder module_path = os . path . abspath ( os . path . join ( '../src' )) if module_path not in sys . path : sys . path . append ( module_path ) from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from dateutil import tz from pandas.tseries.offsets import MonthEnd from sklearn.preprocessing import MinMaxScaler import itertools import numpy as np import pandas as pd from nltk.tokenize import regexp_tokenize # settings to display all columns pd . set_option ( \"display.max_columns\" , None ) import fasttrack as ft from dotenv import load_dotenv load_dotenv () True # Validate FastTrack API connection api_key = os . getenv ( 'FAST_TRACK_API_KEY' ,) client = ft . Fasttrack ( api_key ) track_codes = client . listTracks () Valid Security Key # Import race data excluding NZ races au_tracks_filter = list ( track_codes [ track_codes [ 'state' ] != 'NZ' ][ 'track_code' ]) # Time window to import data # First day of the month 46 months back from now date_from = ( datetime . today () - relativedelta ( months = 46 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # First day of previous month date_to = ( datetime . today () - relativedelta ( months = 1 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # Dataframes to populate data with race_details = pd . DataFrame () dog_results = pd . DataFrame () # For each month, either fetch data from API or use local CSV file if we already have downloaded it for start in pd . date_range ( date_from , date_to , freq = 'MS' ): start_date = start . strftime ( \"%Y-%m- %d \" ) end_date = ( start + MonthEnd ( 1 )) . strftime ( \"%Y-%m- %d \" ) try : filename_races = f 'FT_AU_RACES_ { start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { start_date } .csv' filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' print ( f 'Loading data from { start_date } to { end_date } ' ) if os . path . isfile ( filepath_races ): # Load local CSV file month_race_details = pd . read_csv ( filepath_races ) month_dog_results = pd . read_csv ( filepath_dogs ) else : # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( start_date , end_date , au_tracks_filter ) month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) # Combine monthly data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) except : print ( f 'Could not load data from { start_date } to { end_date } ' ) Loading data from 2018-09-01 to 2018-09-30 Loading data from 2018-10-01 to 2018-10-31 Loading data from 2018-11-01 to 2018-11-30 Loading data from 2018-12-01 to 2018-12-31 Loading data from 2019-01-01 to 2019-01-31 Loading data from 2019-02-01 to 2019-02-28 Loading data from 2019-03-01 to 2019-03-31 Loading data from 2019-04-01 to 2019-04-30 Loading data from 2019-05-01 to 2019-05-31 Loading data from 2019-06-01 to 2019-06-30 Loading data from 2019-07-01 to 2019-07-31 Loading data from 2019-08-01 to 2019-08-31 Loading data from 2019-09-01 to 2019-09-30 Loading data from 2019-10-01 to 2019-10-31 Loading data from 2019-11-01 to 2019-11-30 Loading data from 2019-12-01 to 2019-12-31 Loading data from 2020-01-01 to 2020-01-31 Loading data from 2020-02-01 to 2020-02-29 Loading data from 2020-03-01 to 2020-03-31 Loading data from 2020-04-01 to 2020-04-30 Loading data from 2020-05-01 to 2020-05-31 Loading data from 2020-06-01 to 2020-06-30 Loading data from 2020-07-01 to 2020-07-31 Loading data from 2020-08-01 to 2020-08-31 Loading data from 2020-09-01 to 2020-09-30 Loading data from 2020-10-01 to 2020-10-31 Loading data from 2020-11-01 to 2020-11-30 Loading data from 2020-12-01 to 2020-12-31 Loading data from 2021-01-01 to 2021-01-31 Loading data from 2021-02-01 to 2021-02-28 Loading data from 2021-03-01 to 2021-03-31 Loading data from 2021-04-01 to 2021-04-30 Loading data from 2021-05-01 to 2021-05-31 Loading data from 2021-06-01 to 2021-06-30 Loading data from 2021-07-01 to 2021-07-31 Loading data from 2021-08-01 to 2021-08-31 Loading data from 2021-09-01 to 2021-09-30 Loading data from 2021-10-01 to 2021-10-31 Loading data from 2021-11-01 to 2021-11-30 Loading data from 2021-12-01 to 2021-12-31 Loading data from 2022-01-01 to 2022-01-31 c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) Loading data from 2022-02-01 to 2022-02-28 Loading data from 2022-03-01 to 2022-03-31 Loading data from 2022-04-01 to 2022-04-30 Loading data from 2022-05-01 to 2022-05-31 Loading data from 2022-06-01 to 2022-06-30 This piece of code we copied and pasted from the Greyhound Modelling Tutorial is fantastic! It has downloaded/read-in a ton of historic data! There is an issue though! We don't have the data for today's races, and also for any races that has occurred this month. This is because the code above only downloaded data up until the end of last month. For example, if we are in the middle of June, then any races in the first two weeks of June won't be downloaded by the chunk of code above. An issue is that if we download it now, when tomorrow rolls around it won't include the extra races that have finished today. So, the simple but inefficient solution is that every single day we redownload all the races that have already concluded this month. (Ideally you have some sort of database set up or you store and download your data in a daily format instead of the monthly format) race_details . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime Distance RaceGrade Track date 88510 792243395 6 SKY RACING (N/P) STAKE 01:27PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 88511 792243396 7 KURT DONSBERG PHOTOGRAPHY MIXED STAKE 01:44PM 300m Mixed 4/5 Murray Bridge (MBS) 07 Jun 22 88512 792243397 8 GREYHOUNDS AS PETS 02:04PM 300m Grade 5 Final Murray Bridge (MBS) 07 Jun 22 88513 792243398 9 @THEDOGSSA (N/P) STAKE 02:19PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 88514 792243399 10 FOLLOW THEDOGSSA ON TWITTER (N/P) STAKE 02:39PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 current_month_start_date = pd . Timestamp . now () . replace ( day = 1 ) . strftime ( \"%Y-%m- %d \" ) current_month_end_date = ( pd . Timestamp . now () . replace ( day = 1 ) + MonthEnd ( 1 )) current_month_end_date = ( current_month_end_date - pd . Timedelta ( '1 day' )) . strftime ( \"%Y-%m- %d \" ) print ( f 'Start date: { current_month_start_date } ' ) print ( f 'End Date: { current_month_end_date } ' ) Start date: 2022-07-01 End Date: 2022-07-30 # Download data for races that have concluded this current month up untill today # Start and end dates for current month current_month_start_date = pd . Timestamp . now () . replace ( day = 1 ) . strftime ( \"%Y-%m- %d \" ) current_month_end_date = ( pd . Timestamp . now () . replace ( day = 1 ) + MonthEnd ( 1 )) current_month_end_date = ( current_month_end_date - pd . Timedelta ( '1 day' )) . strftime ( \"%Y-%m- %d \" ) # Files names filename_races = f 'FT_AU_RACES_ { current_month_start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { current_month_start_date } .csv' # Where to store files locally filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( current_month_start_date , current_month_end_date , au_tracks_filter ) # Save the files locally and replace any out of date fields month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) Getting meets for each date .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:14<00:00, 2.01it/s] Getting historic results details .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162/162 [01:30<00:00, 1.80it/s] dog_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 114215500 1 DR. MURPHY 7.0 10 29.7 $4.10 NaN 4.24 NaN Q/111 0 NaN 4.70 22.84 NaN 356387352 107925 W McMahon 1 131737955 2 MOLLY SPOLLY 8.0 8 27.3 $2.20F NaN 4.24 4.24 M/222 0 NaN 4.72 23.14 NaN 356387352 199516 K Leviston 2 204414097 3 ASTON NARITA 2.0 2 29.2 $4.50 NaN 4.94 0.70 M/343 2 NaN 4.88 23.19 NaN 356387352 101224 K Gorman 3 126744995 4 ONI 6.0 6 25.0 $15.50 NaN 5.70 0.76 S/674 0 NaN 4.95 23.24 NaN 356387352 107925 W McMahon 4 120958941 5 DARCON FLASH 1.0 1 29.3 $31.00 NaN 6.54 0.84 M/765 8 NaN 4.96 23.30 NaN 356387352 125087 R Conway ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 749514 415996416 1 WEBLEC WHIRL 2.0 2 27.6 $4.20 NaN 0.10 NaN 0 0 NaN 4.38 16.75 510.0 792243399 76598 N Loechel 749515 557002281 2 UP THERE BILLY 1.0 1 32.7 $1.80F NaN 0.10 0.14 NaN 0 NaN 4.43 16.76 175.0 792243399 327728 J Trengove 749516 529022935 3 WEBLEC FLAME 6.0 6 31.8 $5.50 NaN 4.25 4.00 NaN 0 NaN 4.48 17.04 140.0 792243399 76598 N Loechel 749517 383604709 4 STRAIGHT BLAZE 8.0 8 34.7 $23.00 NaN 5.00 0.86 NaN 0 NaN 4.61 17.10 115.0 792243399 123529 D Johnstone 749518 529022943 5 WEBLEC MIST 4.0 4 27.6 $7.00 NaN 15.00 10.14 0 0 VINJ(21) 4.67 17.81 0.0 792243399 76598 N Loechel 749519 rows \u00d7 19 columns # This is super important I have spent literally hours before I found out this was causing errors dog_results [ '@id' ] = pd . to_numeric ( dog_results [ '@id' ]) # Append the extra data to our data frames race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) What we are really interested in are races that are scheduled for today as we want to use our model to predict their ratings. So, let's write some code we can run in the morning that will download the data for the day: # Download the data for todays races todays_date = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) todays_races , todays_dogs = client . getFullFormat ( dt = todays_date , tracks = au_tracks_filter ) display ( todays_races . head ( 1 ), todays_dogs . head ( 1 )) Getting meets for each date .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 1.89it/s] Getting dog lineups .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:13<00:00, 1.14s/it] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 PrizeMoney4 PrizeMoney5 PrizeMoney6 PrizeMoney7 PrizeMoney8 GOBIS Hurdle Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips 0 801896110 1 GPP LASER3300 06:44PM 04 Jul 22 08:44AM 385m Maiden $1600 $460 $230 $115 None None None None None None None TRI/QUIN R/D EXACTA PICK4 M None \"KASUMI BERRY (5) is a well bred type and her ... Shepparton 04 Jul 22 None Box Quinella 1,2,4,7 ($10 for 166.67%) 4, 7, 1, 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceBox DogName BestTime DogHandicap Odds Rating Speed DogComment StartsTOT StartsTTD Suburb Owner Colour Sex Whelped DogGrade DogGOBIS DogPRIZE AgedPrizeMoney Form DamId DamName SireId SireName TrainerId TrainerName RaceId 0 536196758 1 HAVE A SHIRAZ FSH None $4.60 0 None Dam produced the highly talented Flaming rush Starts 0-0-0-0 Trk/Dst 0-0-0-0 Heathcote Paul Ellis BK B 03 Dec 19 M N 0 0 [None, None, None, None, None] 1550070039 Pepper Shiraz -710494 Barcia Bale 117228 Jason Formosa 801896110 # It seems that the todays_races dataframe doesn't have the date column, so let's add that on todays_races [ 'date' ] = pd . Timestamp . now () . strftime ( ' %d %b %y' ) todays_races . head ( 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 PrizeMoney4 PrizeMoney5 PrizeMoney6 PrizeMoney7 PrizeMoney8 GOBIS Hurdle Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips date 0 801896110 1 GPP LASER3300 06:44PM 04 Jul 22 08:44AM 385m Maiden $1600 $460 $230 $115 None None None None None None None TRI/QUIN R/D EXACTA PICK4 M None \"KASUMI BERRY (5) is a well bred type and her ... Shepparton 04 Jul 22 None Box Quinella 1,2,4,7 ($10 for 166.67%) 4, 7, 1, 2 04 Jul 22 # It also seems that in todays_dogs dataframe Box is labeled as RaceBox instead, so let's rename it # We can also see that there are some specific dogs that have \"Res.\" as a suffix of their name, i.e. they are reserve dogs, # We will treat this later todays_dogs = todays_dogs . rename ( columns = { \"RaceBox\" : \"Box\" }) todays_dogs . tail ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id Box DogName BestTime DogHandicap Odds Rating Speed DogComment StartsTOT StartsTTD Suburb Owner Colour Sex Whelped DogGrade DogGOBIS DogPRIZE AgedPrizeMoney Form DamId DamName SireId SireName TrainerId TrainerName RaceId 1061 400500428 5 TEQUILA TALKING 22.63 None None 100 66.425 None Starts 58-11-7-12 Trk/Dst 28-6-5-5 Wolffdene Fives Alive Synd D Wolff,N Brauer BE D 19 Oct 18 4 N 28855 None [{'Place': '2nd', 'FormBox': '5', 'Weight': '3... 255840075 Sivamet -737547 Hostile 93322 Michael Brauer 801490825 1062 525622257 7 REFERRAL FSTD None None 81 61.343 None Starts 51-4-7-6 Trk/Dst 0-0-0-0 Laidley Heights Bad Decisions Synd P O'Reilly,A Pearce,D Henery BD D 22 Oct 19 5 N 13945 None [{'Place': '5th', 'FormBox': '7', 'Weight': '3... 257880044 Lovelace 792880037 Sh Avatar 313314 Andrew Pearce 801490825 1063 566347962 8 STARDUST DREAMS NBT None None 92 61.271 None Starts 22-3-4-2 Trk/Dst 2-0-1-0 Park Ridge Kerri-Lyn Harkness BK D 07 Mar 20 5 N 8240 None [{'Place': '2nd', 'FormBox': '8', 'Weight': '3... 118703516 Ellie Belles 141317074 My Redeemer 127311 Stephen Woods 801490825 # Appending todays data to this months data month_dog_results = pd . concat ([ month_dog_results , todays_dogs ], join = 'outer' )[ month_dog_results . columns ] month_race_details = pd . concat ([ month_race_details , todays_races ], join = 'outer' )[ month_race_details . columns ] # Appending this months data to the rest of our historical data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) Cleaning our data and feature creation Originally I thought that since we now that we have all the data we can easily copy and paste the code used in the greyhound modelling tutorial to clean our data and create the features. But after staring at weird predictions and spending hours trying to work out why some things weren't working I realised that for the most part we can copy and paste code, but when working with the live data we do need to make a few changes. I'll point them out when we get to it, but the main things that tripped me up is the data types the FastTrack API gives and that we need a system to work around reserve dogs race_details .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 356387352 1 RUTTER'S BUTCHERY & POULTRY 05:29PM 395m Mixed 6/7 Traralgon 01 Sep 18 1 356387359 2 TAB - WE LOVE A BET 05:47PM 395m Grade 5 Traralgon 01 Sep 18 2 356387358 3 HALEY CONCRETING 06:05PM 395m Grade 5 Traralgon 01 Sep 18 3 356387355 4 R.W & A.R INGLIS ELECTRICIANS 06:29PM 395m Free For All Traralgon 01 Sep 18 4 356387363 5 PRINTMAC 06:45PM 525m Grade 5 Traralgon 01 Sep 18 ... ... ... ... ... ... ... ... ... 89359 801490821 5 SENNACHIE @ STUD - STEVE WHITE 08:13PM 520m Grade 5 Heat Albion Park 04 Jul 22 89360 801490822 6 ORSON ALLEN @ METICULOUS LODGE 08:35PM 520m Grade 5 Heat Albion Park 04 Jul 22 89361 801490823 7 SKY RACING 08:53PM 600m Mixed 4/5 Albion Park 04 Jul 22 89362 801490824 8 BORGBET TIPPING SERVICE 09:15PM 520m Mixed 3/4 Albion Park 04 Jul 22 89363 801490825 9 TIGGERLONG TONK @ STUD 09:37PM 395m Mixed 4/5 Albion Park 04 Jul 22 89364 rows \u00d7 8 columns The first thing that tripped me up was when FastTrack_DogId for live data was in a string format, and because everything looks like it works, it took ages to find this error. So, let's make sure we deal with it here using: dog_results['FastTrack_DogId'] = pd.to_numeric(dog_results['FastTrack_DogId']) ## Cleanse and normalise the data # Clean up the race dataset race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) # Clean up the dogs results dataset dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) # New line of code (rest of this code chunk is copied from bruno's code) dog_results [ 'FastTrack_DogId' ] = pd . to_numeric ( dog_results [ 'FastTrack_DogId' ]) # Combine dogs results with race attributes dog_results = dog_results . merge ( race_details , how = 'left' , on = 'FastTrack_RaceId' ) # Convert StartPrice to probability dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x is None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' )) if isinstance ( x , str ) else x ) dog_results [ 'StartPrice_probability' ] = ( 1 / dog_results [ 'StartPrice' ]) . fillna ( 0 ) dog_results [ 'StartPrice_probability' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x / x . sum ()) # Discard entries without results (scratched or did not finish) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results [ 'Box' ] = dog_results [ 'Box' ] . astype ( int ) # Clean up other attributes dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'SplitMargin' ] = dog_results [ 'SplitMargin' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) . fillna ( 0 ) dog_results [ 'Place' ] = pd . to_numeric ( dog_results [ 'Place' ] . apply ( lambda x : x . replace ( \"=\" , \"\" ) if isinstance ( x , str ) else 0 ), errors = 'coerce' ) . fillna ( 0 ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x == 1 else 0 ) # Normalise some of the raw values dog_results [ 'Prizemoney_norm' ] = np . log10 ( dog_results [ 'Prizemoney' ] + 1 ) / 12 dog_results [ 'Place_inv' ] = ( 1 / dog_results [ 'Place' ]) . fillna ( 0 ) dog_results [ 'Place_log' ] = np . log10 ( dog_results [ 'Place' ] + 1 ) . fillna ( 0 ) dog_results [ 'RunSpeed' ] = ( dog_results [ 'RunTime' ] / dog_results [ 'Distance' ]) . fillna ( 0 ) ## Generate features using raw data # Calculate median winner time per track/distance win_results = dog_results [ dog_results [ 'win' ] == 1 ] median_win_time = pd . DataFrame ( data = win_results [ win_results [ 'RunTime' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'RunTime' ] . median ()) . rename ( columns = { \"RunTime\" : \"RunTime_median\" }) . reset_index () median_win_split_time = pd . DataFrame ( data = win_results [ win_results [ 'SplitMargin' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'SplitMargin' ] . median ()) . rename ( columns = { \"SplitMargin\" : \"SplitMargin_median\" }) . reset_index () median_win_time . head () # Calculate track speed index median_win_time [ 'speed_index' ] = ( median_win_time [ 'RunTime_median' ] / median_win_time [ 'Distance' ]) median_win_time [ 'speed_index' ] = MinMaxScaler () . fit_transform ( median_win_time [[ 'speed_index' ]]) median_win_time . head () # Compare dogs finish time with median winner time dog_results = dog_results . merge ( median_win_time , on = [ 'Track' , 'Distance' ], how = 'left' ) dog_results = dog_results . merge ( median_win_split_time , on = [ 'Track' , 'Distance' ], how = 'left' ) # Normalise time comparison dog_results [ 'RunTime_norm' ] = ( dog_results [ 'RunTime_median' ] / dog_results [ 'RunTime' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'RunTime_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'RunTime_norm' ]]) dog_results [ 'SplitMargin_norm' ] = ( dog_results [ 'SplitMargin_median' ] / dog_results [ 'SplitMargin' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'SplitMargin_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'SplitMargin_norm' ]]) dog_results . head () # Calculate box winning percentage for each track/distance box_win_percent = pd . DataFrame ( data = dog_results . groupby ([ 'Track' , 'Distance' , 'Box' ])[ 'win' ] . mean ()) . rename ( columns = { \"win\" : \"box_win_percent\" }) . reset_index () # Add to dog results dataframe dog_results = dog_results . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) # Display example of barrier winning probabilities print ( box_win_percent . head ( 8 )) Track Distance Box box_win_percent 0 Albion Park 331 1 0.198089 1 Albion Park 331 2 0.152116 2 Albion Park 331 3 0.127354 3 Albion Park 331 4 0.126605 4 Albion Park 331 5 0.111058 5 Albion Park 331 6 0.109304 6 Albion Park 331 7 0.105310 7 Albion Park 331 8 0.115146 The second thing that we need to add is related to reserve dogs, and this took me ages to come to this solution, but if you have a better one, please submit a pull request. Basically, a single greyhound can be a reserve dog for multiple races on the same day. They each appear as a new row in our data frame. For example, 'COACH BEARD' is a reserve dog for three different races today (2022-07-04): dog_results [ dog_results [ 'FastTrack_DogId' ] == 592253143 ] . tail ()[[ 'date_dt' , 'Place' , 'DogName' , 'RaceNum' , 'Track' , 'Distance' , 'win' , 'Prizemoney_norm' , 'Place_inv' , 'Place_log' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date_dt Place DogName RaceNum Track Distance win Prizemoney_norm Place_inv Place_log 616959 2022-05-04 1.0 COACH BEARD 3 Bendigo 425 1 0.0 1.0 0.301030 627625 2022-05-16 2.0 COACH BEARD 5 Shepparton 450 0 0.0 0.5 0.477121 654587 2022-07-04 0.0 COACH BEARD 8 Ballarat 450 0 0.0 inf 0.000000 654588 2022-07-04 0.0 COACH BEARD 8 Ballarat 450 0 0.0 inf 0.000000 654589 2022-07-04 0.0 COACH BEARD 8 Ballarat 450 0 0.0 inf 0.000000 When we try lag our data by using .shift(1) like in Bruno's original code it will produce the wrong values for our features. In the above example only the first race today (2022-06-07) Townsville Race 4 (the second row) will have correct data but all the rows under it will have incorrectly calculated features. We need each of the following rows to be the same as the second row. The solution that I have come up with is a little bit complicated, but it gets the job done: # Please submit a pull request if you have a better solution temp = rolling_result.reset_index() temp = temp[temp['date_dt'] == pd.Timestamp.now().normalize()] temp.groupby(['FastTrack_DogId','date_dt']).first() rolling_result.loc[pd.IndexSlice[:, pd.Timestamp.now().normalize()], :] = temp.groupby(['FastTrack_DogId','date_dt']).first() Basically, for each greyhound we can just take the first row of data (which is correct) and set the rest of today's races to have the same value # Generate rolling window features dataset = dog_results . copy () dataset = dataset . set_index ([ 'FastTrack_DogId' , 'date_dt' ]) . sort_index () # Use rolling window of 28, 91 and 365 days rolling_windows = [ '28D' , '91D' , '365D' ] # Features to use for rolling windows calculation features = [ 'RunTime_norm' , 'SplitMargin_norm' , 'Place_inv' , 'Place_log' , 'Prizemoney_norm' ] # Aggregation functions to apply aggregates = [ 'min' , 'max' , 'mean' , 'median' , 'std' ] # Keep track of generated feature names feature_cols = [ 'speed_index' , 'box_win_percent' ] for rolling_window in rolling_windows : print ( f 'Processing rolling window { rolling_window } ' ) rolling_result = ( dataset . reset_index ( level = 0 ) . sort_index () . groupby ( 'FastTrack_DogId' )[ features ] . rolling ( rolling_window ) . agg ( aggregates ) . groupby ( level = 0 ) # Thanks to Brett for finding this! . shift ( 1 ) ) # My own dodgey code to work with reserve dogs temp = rolling_result . reset_index () temp = temp [ temp [ 'date_dt' ] == pd . Timestamp . now () . normalize ()] temp . groupby ([ 'FastTrack_DogId' , 'date_dt' ]) . first () rolling_result . loc [ pd . IndexSlice [:, pd . Timestamp . now () . normalize ()], :] = temp . groupby ([ 'FastTrack_DogId' , 'date_dt' ]) . first () # Generate list of rolling window feature names (eg: RunTime_norm_min_365D) agg_features_cols = [ f ' { f } _ { a } _ { rolling_window } ' for f , a in itertools . product ( features , aggregates )] # Add features to dataset dataset [ agg_features_cols ] = rolling_result # Keep track of generated feature names feature_cols . extend ( agg_features_cols ) Processing rolling window 28D c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) Processing rolling window 91D c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) Processing rolling window 365D c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) # Replace missing values with 0 dataset . fillna ( 0 , inplace = True ) display ( dataset . head ( 8 )) # Only keep data after 2018-12-01 model_df = dataset . reset_index () feature_cols = np . unique ( feature_cols ) . tolist () model_df = model_df [ model_df [ 'date_dt' ] >= '2018-12-01' ] # This line was originally part of Bruno's tutorial, but we don't run it in this script # model_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D FastTrack_DogId date_dt -2143477291 2018-09-02 7.0 YOU TELAM ANFY 1 1 31.2 5.2 0.0 7.0 0.57 8 0 8.0 7.48 19.85 0.0 354469749 8462 A Bunney 12 BRISGREYS.COM 09:01PM 331 GRADE 5 PATHWAY NON-PENALTY Albion Park 02 Sep 18 0.161184 0 0.0 0.142857 0.903090 0.059970 19.17 0.600092 7.18 0.328715 0.299465 0.198089 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 2018-09-16 4.0 YOU TELAM ANFY 1 1 31.0 61.0 0.0 10.0 5.0 8 0 8.0 7.43 19.75 0.0 360928569 8462 A Bunney 11 SKY RACING 05:24PM 331 Grade 5 Albion Park 16 Sep 18 0.013847 0 0.0 0.250000 0.698970 0.059668 19.17 0.600092 7.18 0.353165 0.331763 0.198089 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.903090 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.903090 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.90309 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 2018-10-07 7.0 YOU TELAM ANFY 1 1 30.5 71.0 0.0 8.25 4.0 7 0 7.0 7.42 19.71 0.0 367774713 8462 A Bunney 11 SKY RACING 08:27PM 331 Grade 5 Albion Park 07 Oct 18 0.011604 0 0.0 0.142857 0.903090 0.059547 19.17 0.600092 7.18 0.363014 0.338275 0.198089 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.69897 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 2018-10-21 5.0 YOU TELAM ANFY 7 9 29.8 26.0 0.0 5.25 0.29 6 0 6.0 7.46 19.85 0.0 370420123 8462 A Bunney 12 ZILLMERE SPORTS 08:55PM 331 GRADE 5 PATHWAY NON-PENALTY Albion Park 21 Oct 18 0.032235 0 0.0 0.200000 0.778151 0.059970 19.17 0.600092 7.18 0.328715 0.312332 0.105310 0.353165 0.363014 0.358089 0.358089 0.006964 0.331763 0.338275 0.335019 0.335019 0.004605 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.348298 0.353165 0.017659 0.299465 0.338275 0.323168 0.331763 0.020784 0.142857 0.250000 0.178571 0.142857 0.061859 0.698970 0.903090 0.835050 0.903090 0.117849 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.348298 0.353165 0.017659 0.299465 0.338275 0.323168 0.331763 0.020784 0.142857 0.250000 0.178571 0.142857 0.061859 0.69897 0.903090 0.835050 0.903090 0.117849 0.0 0.0 0.0 0.0 0.0 2018-11-18 5.0 YOU TELAM ANFY 1 1 30.2 41.0 0.0 6.25 3.14 6 0 6.0 7.43 19.86 0.0 378695693 8462 A Bunney 10 ZILLMERE SPORTS 08:16PM 331 Grade 5 Albion Park 18 Nov 18 0.020215 0 0.0 0.200000 0.778151 0.060000 19.17 0.600092 7.18 0.326284 0.331763 0.198089 0.328715 0.363014 0.345865 0.345865 0.024253 0.312332 0.338275 0.325304 0.325304 0.018344 0.142857 0.200000 0.171429 0.171429 0.040406 0.778151 0.903090 0.840621 0.840621 0.088345 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.343402 0.340940 0.017429 0.299465 0.338275 0.320459 0.322048 0.017814 0.142857 0.250000 0.183929 0.171429 0.051632 0.698970 0.903090 0.820825 0.840621 0.100341 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.343402 0.340940 0.017429 0.299465 0.338275 0.320459 0.322048 0.017814 0.142857 0.250000 0.183929 0.171429 0.051632 0.69897 0.903090 0.820825 0.840621 0.100341 0.0 0.0 0.0 0.0 0.0 2019-06-23 5.0 YOU TELAM ANFY 1 1 29.6 51.0 0.0 5.75 0.86 8 0 8.0 7.44 19.66 0.0 445056131 8462 A Bunney 9 GREYHOUND ADOPTION PROGRAM 07:54PM 331 Grade 5 Albion Park 23 Jun 19 0.016271 0 0.0 0.200000 0.778151 0.059396 19.17 0.600092 7.18 0.375381 0.325269 0.198089 0.326284 0.326284 0.326284 0.326284 0.000000 0.331763 0.331763 0.331763 0.331763 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.326284 0.363014 0.339979 0.328715 0.016924 0.299465 0.338275 0.322720 0.331763 0.016234 0.142857 0.250000 0.187143 0.200000 0.045288 0.698970 0.903090 0.812290 0.778151 0.088969 0.0 0.0 0.0 0.0 0.0 0.326284 0.363014 0.339979 0.328715 0.016924 0.299465 0.338275 0.322720 0.331763 0.016234 0.142857 0.250000 0.187143 0.200000 0.045288 0.69897 0.903090 0.812290 0.778151 0.088969 0.0 0.0 0.0 0.0 0.0 2019-06-30 8.0 YOU TELAM ANFY 4 10 29.5 51.0 0.0 17.25 2.0 7 0 7.0 11.38 24.16 0.0 448789428 8462 A Bunney 7 SKY RACING 07:45PM 395 Open Albion Park 30 Jun 19 0.015995 0 0.0 0.125000 0.954243 0.061165 22.85 0.588509 10.58 0.228891 0.148506 0.130206 0.375381 0.375381 0.375381 0.375381 0.000000 0.325269 0.325269 0.325269 0.325269 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.375381 0.375381 0.375381 0.375381 0.000000 0.325269 0.325269 0.325269 0.325269 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.326284 0.375381 0.345879 0.340940 0.020929 0.299465 0.338275 0.323145 0.328516 0.014558 0.142857 0.250000 0.189286 0.200000 0.040846 0.69897 0.903090 0.806601 0.778151 0.080787 0.0 0.0 0.0 0.0 0.0 2019-08-25 6.0 YOU TELAM ANFY 4 4 29.5 14.0 0.0 5.0 0.57 3 0 3.0 7.33 19.72 0.0 465432748 8462 A Bunney 9 FABREGAS @ METICULOUS LODGE 07:40PM 331 Masters Grade 5 Albion Park 25 Aug 19 0.058684 0 0.0 0.166667 0.845098 0.059577 19.17 0.600092 7.18 0.360548 0.397681 0.126605 0.228891 0.375381 0.302136 0.302136 0.103585 0.148506 0.325269 0.236887 0.236887 0.124990 0.125000 0.200000 0.162500 0.162500 0.053033 0.778151 0.954243 0.866197 0.866197 0.124515 0.0 0.0 0.0 0.0 0.0 0.228891 0.375381 0.302136 0.302136 0.103585 0.148506 0.325269 0.236887 0.236887 0.124990 0.125000 0.200000 0.162500 0.162500 0.053033 0.778151 0.954243 0.866197 0.866197 0.124515 0.0 0.0 0.0 0.0 0.0 0.228891 0.375381 0.329166 0.328715 0.048169 0.148506 0.338275 0.298196 0.325269 0.067332 0.125000 0.250000 0.180102 0.200000 0.044505 0.69897 0.954243 0.827692 0.778151 0.092481 0.0 0.0 0.0 0.0 0.0 Generate predictions Now this is the part that gets a bit hairy, so I am going to split it up into two parts. The good thing is that the coding will remain relatively simple. The two things that I want to do is place live bets and save our predictions so that we can use them in a simulator we will create in the Part V . Let's save our historical ratings for our simulator first as its quick and straight forward and then move on to placing live bets: Getting data ready for our simulator Feeding our predictions through the simulator is entirely optional, but, in my opinion it is where the real sauce is made. The idea is that if we are testing our model live, we can also use the simulator to test what would happen if we tested different staking methodologies, market timings and bet placement to optimise our model. This way you can have a model but test out different strategies to optimise model performance. The thing is, I have had a play with the simulator already and we can't simulate market_catalogue unless you have recorded it yourself (which is what I'll be using to get market_id and selection_id to place live bets). The simulator we will use later on will only take your ratings, market_id and selection_id, so we need our data in a similar format to what we had in how automate III . In other words, since we don't have market_catalogue in the simulator, we need another way to get the market_id and selection_id. My hacky work around is to generate the probabilities like normal (since the data is historical), we don't need to deal with reserve dogs and scratching's, then get the market_id and selection_id from the Betfair datascience greyhound model by merging on DogName and date. We can take the code we wrote in how to automate III that downloads the greyhound ratings and convert that into a function that downloads the ratings for a date range. # Generate predictions like normal # Range of dates that we want to simulate later '2022-03-01' to '2022-04-01' todays_data = model_df [( model_df [ 'date_dt' ] >= pd . Timestamp ( '2022-03-01' ) . strftime ( '%Y-%m- %d ' )) & ( model_df [ 'date_dt' ] < pd . Timestamp ( '2022-04-01' ) . strftime ( '%Y-%m- %d ' ))] dog_win_probabilities = brunos_model . predict_proba ( todays_data [ feature_cols ])[:, 1 ] todays_data [ 'prob_LogisticRegression' ] = dog_win_probabilities todays_data [ 'renormalise_prob' ] = todays_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x / x . sum ()) todays_data [ 'rating' ] = 1 / todays_data [ 'renormalise_prob' ] todays_data = todays_data . sort_values ( by = 'date_dt' ) todays_data C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['prob_LogisticRegression'] = dog_win_probabilities C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum()) C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['rating'] = 1/todays_data['renormalise_prob'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression renormalise_prob rating 526490 523685389 2022-03-01 1.0 JOSEPH RUMBLE 8 8 30.1 2.5 0.0 8.25 0 0 0 0 0.00 21.83 1365.0 764579619 91264 B Belford 1 TAB 06:55PM 380 Novice Non Penalty Townsville 01 Mar 22 0.318829 1 0.261288 1.000000 0.301030 0.057447 22.10 0.641825 7.56 0.561842 0.000000 0.123070 0.514985 0.514985 0.514985 0.514985 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 1.00 1.000000 1.000000 0.000000 0.301030 0.301030 0.301030 0.301030 0.000000 0.264578 0.264578 2.645776e-01 0.264578 0.000000 0.386805 0.514985 0.450895 0.450895 0.090637 0.482301 0.482301 0.482301 0.482301 0.000000 0.500000 1.00 0.750000 0.750000 0.353553 0.30103 0.477121 0.389076 0.389076 0.124515 0.238161 0.264578 0.251369 0.251369 0.018679 0.386805 0.514985 0.450895 0.450895 0.090637 0.482301 0.482301 0.482301 0.482301 0.000000 0.500000 1.00 0.750000 0.750000 0.353553 0.30103 0.477121 0.389076 0.389076 0.124515 0.238161 0.264578 0.251369 0.251369 0.018679 0.360558 0.321076 3.114527 494469 482776246 2022-03-01 1.0 BLAZING NENNA 4 4 25.7 2.3 0.0 3.31 0 Q/11 0 0 10.27 23.30 0.0 764625202 115912 M Delbridge 6 CHS GROUP HT3 05:37PM 410 Grade 5 Heat Horsham 01 Mar 22 0.365634 1 0.000000 1.000000 0.301030 0.056829 23.46 0.480327 10.39 0.534335 0.558423 0.131261 0.500000 0.513187 0.504396 0.500000 0.007613 0.464830 0.633333 0.570108 0.612161 0.091786 0.250000 1.00 0.500000 0.250000 0.433013 0.301030 0.698970 0.566323 0.698970 0.229751 0.000000 0.000000 1.628327e-15 0.000000 0.000000 0.307377 0.609439 0.508348 0.524719 0.074167 0.373358 0.651515 0.543480 0.584034 0.093516 0.142857 1.00 0.535714 0.500000 0.324194 0.30103 0.903090 0.528325 0.477121 0.198314 0.000000 0.000000 0.000000 0.000000 0.000000 0.233840 0.609439 0.493044 0.512184 0.074069 0.000000 0.651515 0.515072 0.539757 0.120687 0.125000 1.00 0.443328 0.333333 0.314640 0.30103 0.954243 0.607971 0.602060 0.219526 0.000000 0.000000 0.000000 0.000000 0.000000 0.204496 0.145077 6.892877 583640 578899991 2022-03-01 4.0 RIVER RAGING 2 2 30.3 13.8 0.0 3.71 1.13 M/3 0 0 6.89 20.36 0.0 764592641 283109 L Dalziel 1 FOLLOW @GRV_NEWS ON TWITTER 11:08AM 350 Maiden Healesville 01 Mar 22 0.061390 0 0.000000 0.250000 0.698970 0.058171 19.56 0.250778 6.64 0.303536 0.318578 0.138427 0.312992 0.312992 0.312992 0.312992 0.000000 0.346715 0.346715 0.346715 0.346715 0.000000 0.250000 0.25 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000 0.241288 0.312992 0.285038 0.303536 0.033489 0.251704 0.346715 0.290635 0.290765 0.038193 0.142857 0.25 0.201905 0.200000 0.048369 0.69897 0.903090 0.784856 0.778151 0.090009 0.000000 0.000000 0.000000 0.000000 0.000000 0.241288 0.312992 0.285038 0.303536 0.033489 0.251704 0.346715 0.290635 0.290765 0.038193 0.142857 0.25 0.201905 0.200000 0.048369 0.69897 0.903090 0.784856 0.778151 0.090009 0.000000 0.000000 0.000000 0.000000 0.000000 0.065977 0.102443 9.761504 385698 419530408 2022-03-01 5.0 FREENEY 8 8 25.1 14.0 0.0 8.25 1.14 0 0 0 0.00 22.47 20.0 764579628 63422 R Lound 10 BURDEKIN VET CLINIC 09:55PM 380 Grade 5 Townsville 01 Mar 22 0.058903 0 0.110185 0.200000 0.778151 0.059132 22.10 0.641825 7.56 0.417668 0.000000 0.123070 0.389381 0.406750 0.398065 0.398065 0.012282 0.519920 0.519920 0.519920 0.519920 0.000000 0.125000 0.25 0.187500 0.187500 0.088388 0.698970 0.954243 0.826606 0.826606 0.180505 0.110185 0.161208 1.356966e-01 0.135697 0.036079 0.378587 0.520445 0.438208 0.435239 0.050906 0.519920 0.519920 0.519920 0.519920 0.000000 0.125000 1.00 0.398810 0.250000 0.304154 0.30103 0.954243 0.636079 0.698970 0.229354 0.086783 0.256326 0.171119 0.161208 0.059824 0.277345 0.547967 0.448106 0.459606 0.067232 0.486807 0.519920 0.509978 0.516591 0.015762 0.125000 1.00 0.437302 0.333333 0.287424 0.30103 0.954243 0.595411 0.602060 0.199176 0.000000 0.256326 0.161721 0.181581 0.080195 0.148800 0.136846 7.307467 453065 451768903 2022-03-01 5.0 ENCOURAGING 2 2 22.1 15.0 0.0 6.0 0.29 455 0 455 10.35 22.57 0.0 764579685 70111 B Young 12 GOSSIE TIGERS GOOD TIMES 10:39PM 388 Non Graded Gosford 01 Mar 22 0.058204 0 0.000000 0.200000 0.778151 0.058170 22.39 0.564085 10.19 0.460124 0.422705 0.166667 0.462217 0.582400 0.498498 0.474688 0.056299 0.413211 0.605769 0.539241 0.568992 0.086316 0.200000 0.50 0.300000 0.250000 0.135401 0.000000 0.778151 0.530643 0.698970 0.317165 0.000000 0.227091 4.541824e-02 0.000000 0.101558 0.435567 0.582400 0.510280 0.506062 0.043547 0.335526 0.605769 0.507137 0.513845 0.080824 0.200000 1.00 0.559375 0.500000 0.321914 0.00000 0.778151 0.476169 0.477121 0.204293 0.000000 0.269225 0.170899 0.227091 0.115477 0.435567 0.620208 0.519683 0.508949 0.050995 0.335526 0.676056 0.528287 0.534247 0.085502 0.200000 1.00 0.592857 0.500000 0.310625 0.00000 0.778151 0.460377 0.477121 0.185631 0.000000 0.269225 0.187400 0.227091 0.105954 0.310750 0.189764 5.269698 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 269364 328634961 2022-03-31 6.0 LUNARAY 7 7 28.8 18.8 0.0 6.96 0.24 M/766 0 0 6.99 25.92 0.0 771803534 117062 V Mileto 6 SHEPPARTON WORKWEAR & SAFETY 01:19PM 450 Grade 5 T3 Shepparton 31 Mar 22 0.044487 0 0.000000 0.166667 0.845098 0.057600 25.55 0.404304 6.70 0.428627 0.292561 0.118601 0.370377 0.461165 0.415771 0.415771 0.064197 0.320144 0.341040 0.330592 0.330592 0.014776 0.200000 0.25 0.225000 0.225000 0.035355 0.698970 0.778151 0.738561 0.738561 0.055990 0.000000 0.000000 2.720046e-15 0.000000 0.000000 0.282853 0.461165 0.399299 0.415352 0.060173 0.000000 0.397661 0.280607 0.320144 0.129212 0.125000 1.00 0.379762 0.250000 0.297826 0.30103 0.954243 0.644364 0.698970 0.211158 0.000000 0.159040 0.022720 0.000000 0.060112 0.280719 0.529528 0.410459 0.414407 0.058641 0.000000 0.441003 0.297743 0.312132 0.111641 0.125000 1.00 0.324033 0.250000 0.245639 0.30103 0.954243 0.687225 0.698970 0.181101 0.000000 0.177795 0.010526 0.000000 0.041488 0.075047 0.097817 10.223167 538082 534921234 2022-03-31 6.0 QUINNLEY BALE 6 6 26.7 18.0 0.0 7.5 1.14 56 0 56 5.47 17.83 0.0 771810563 98781 S Rhodes 12 WATCH LIVE ON SPORTSBET 10:46PM 297 Grade 5 Dapto 31 Mar 22 0.046607 0 0.000000 0.166667 0.845098 0.060034 17.19 0.593790 5.37 0.320527 0.408592 0.120301 0.288301 0.347716 0.318008 0.318008 0.042013 0.303220 0.390710 0.346965 0.346965 0.061865 0.142857 0.50 0.321429 0.321429 0.252538 0.477121 0.903090 0.690106 0.690106 0.301205 0.000000 0.221975 1.109875e-01 0.110988 0.156960 0.288301 0.471082 0.384562 0.389432 0.082219 0.303220 0.426606 0.368411 0.371909 0.052814 0.142857 1.00 0.473214 0.375000 0.381742 0.30103 0.903090 0.595053 0.588046 0.262071 0.000000 0.264698 0.121668 0.110988 0.141569 0.236247 0.500000 0.387665 0.390897 0.075799 0.000000 0.481447 0.342572 0.353107 0.117160 0.125000 1.00 0.374454 0.250000 0.278973 0.00000 0.954243 0.605624 0.650515 0.270858 0.000000 0.264698 0.053752 0.000000 0.100704 0.068536 0.065307 15.312345 363059 403640676 2022-03-31 3.0 SAINT CHARLOTTE 5 5 27.7 18.0 0.0 4.5 0.14 0 0 0 10.87 23.59 140.0 771539517 67189 W Wilson 8 EXCHANGE PRINTERS (N/P) STAKE 01:52PM 400 Restricted Win Mount Gambier 31 Mar 22 0.041828 0 0.179102 0.333333 0.602060 0.058975 23.39 0.696399 10.98 0.457609 0.550598 0.147799 0.401509 0.534438 0.467974 0.467974 0.093995 0.328496 0.527473 0.427984 0.427984 0.140698 0.125000 1.00 0.562500 0.562500 0.618718 0.301030 0.954243 0.627636 0.627636 0.461891 0.000000 0.225702 1.128509e-01 0.112851 0.159595 0.401509 0.534438 0.475213 0.486146 0.041384 0.328496 0.564576 0.467640 0.504558 0.078033 0.125000 1.00 0.440833 0.333333 0.320552 0.30103 0.954243 0.603688 0.602060 0.220083 0.000000 0.225702 0.135590 0.179102 0.095348 0.257933 0.534438 0.457113 0.477655 0.064917 0.212446 0.564576 0.443305 0.445950 0.089104 0.125000 1.00 0.358408 0.250000 0.274086 0.30103 0.954243 0.660966 0.698970 0.195529 0.000000 0.225702 0.106690 0.172038 0.098393 0.155050 0.100326 9.967472 362095 403093108 2022-03-31 3.0 SILVER SANDALS 5 5 27.0 10.0 0.0 2.5 2.29 42 0 42 5.58 31.67 800.0 771539501 87148 S Lawrance 2 SKY RACING 06:40PM 520 Masters Grade 5 Ipswich 31 Mar 22 0.083134 0 0.241969 0.333333 0.602060 0.060904 30.79 0.823159 5.42 0.361067 0.356631 0.095370 0.435748 0.439595 0.437671 0.437671 0.002720 0.354196 0.368046 0.361121 0.361121 0.009793 0.333333 1.00 0.666667 0.666667 0.471405 0.301030 0.602060 0.451545 0.451545 0.212860 0.207730 0.275374 2.415521e-01 0.241552 0.047832 0.330867 0.507902 0.426973 0.435748 0.052271 0.198046 0.460029 0.340578 0.340419 0.073412 0.166667 1.00 0.498485 0.333333 0.340462 0.30103 0.845098 0.560166 0.602060 0.203530 0.110185 0.275374 0.199906 0.207730 0.064087 0.297445 0.507902 0.398206 0.402791 0.056453 0.156690 0.460029 0.325128 0.317851 0.074465 0.125000 1.00 0.328665 0.225000 0.260181 0.30103 0.954243 0.694669 0.738561 0.197928 0.000000 0.275374 0.127103 0.138606 0.097759 0.081817 0.110114 9.081527 565804 556974861 2022-03-31 2.0 ORSON LAURIE 3 3 30.4 3.1 0.0 0.75 0.86 112 0 112 8.32 23.04 530.0 771810580 125472 E Harris 4 RIVERINA STOCKFEEDS 08:04PM 411 Free For All Casino 31 Mar 22 0.275736 0 0.227091 0.500000 0.477121 0.056058 23.37 0.418680 8.62 0.571615 0.680288 0.062500 0.351105 0.583040 0.453627 0.445632 0.092450 0.487805 0.651134 0.562563 0.574442 0.064323 0.142857 1.00 0.362698 0.183333 0.339607 0.000000 0.903090 0.592798 0.778151 0.343479 0.000000 0.267033 7.058912e-02 0.000000 0.121104 0.351105 0.588161 0.473661 0.483607 0.085113 0.465422 0.664141 0.554578 0.574442 0.067472 0.142857 1.00 0.543492 0.500000 0.401886 0.00000 0.903090 0.545322 0.477121 0.298264 0.000000 0.275104 0.142790 0.167586 0.124768 0.351105 0.588161 0.479065 0.494281 0.085020 0.465422 0.664141 0.557285 0.574442 0.065012 0.142857 1.00 0.572024 0.500000 0.404685 0.00000 0.903090 0.530952 0.477121 0.294808 0.000000 0.275104 0.149961 0.224986 0.124372 0.150460 0.123089 8.124181 26438 rows \u00d7 117 columns def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" , \"meetings.races.number\" : \"RaceNum\" , \"meetings.name\" : \"Track\" , \"meetings.races.runners.name\" : \"DogName\" } ) # iggy_df = iggy_df[['market_id','selection_id','rating']] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) iggy_df [ 'date_dt' ] = date # Set market_id and selection_id as index for easy referencing # iggy_df = iggy_df.set_index(['market_id','selection_id']) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022-03-01' , end = '2022-04-01' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) iggy_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Track meetings.bfExchangeEventId meetings.races.name RaceNum market_id meetings.races.comment selection_id meetings.races.runners.number DogName rating date_dt 0 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154446 1 Magic Rogue 11.07 2022-03-01 1 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154447 2 Youre Off 6.10 2022-03-01 2 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 42352031 3 Castle Town 6.60 2022-03-01 3 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154448 4 Buckle Up Aumond 8.02 2022-03-01 4 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 42413752 5 Was That Then 17.14 2022-03-01 ... ... ... ... ... ... ... ... ... ... ... ... 906 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 27692794 3 Mercator Closer 24.49 2022-04-01 907 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 36057267 4 Kooringa Lucy 2.61 2022-04-01 908 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 25540155 6 Shanjo Prince 213.53 2022-04-01 909 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 39788790 7 Lots Of Chatter 2.10 2022-04-01 910 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 30681833 8 Zipping Brady 18.25 2022-04-01 27346 rows \u00d7 11 columns # format DogNames to merge todays_data [ 'DogName' ] = todays_data [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) iggy_df [ 'DogName' ] = iggy_df [ 'DogName' ] . str . upper () # Merge backtest = iggy_df [[ 'market_id' , 'selection_id' , 'DogName' , 'date_dt' ]] . merge ( todays_data [[ 'rating' , 'DogName' , 'date_dt' ]], how = 'inner' , on = [ 'DogName' , 'date_dt' ]) backtest .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id DogName date_dt rating 0 1.195395419 43154446 MAGIC ROGUE 2022-03-01 8.137725 1 1.195395419 43154447 YOURE OFF 2022-03-01 6.051752 2 1.195395419 42352031 CASTLE TOWN 2022-03-01 9.768546 3 1.195395419 43154448 BUCKLE UP AUMOND 2022-03-01 13.656089 4 1.195395419 42413752 WAS THAT THEN 2022-03-01 11.941057 ... ... ... ... ... ... 25540 1.196921144 39309141 RANAKO BALE 2022-03-31 3.975601 25541 1.196921144 39348645 NEVAEH BALE 2022-03-31 3.917458 25542 1.196921144 26870111 INGA MIA 2022-03-31 7.459386 25543 1.196921144 42472271 WINNIE COASTER 2022-03-31 7.046953 25544 1.196921144 40022831 ASTON HEBE 2022-03-31 4.603341 25545 rows \u00d7 5 columns # Save predictions for if we want to backtest/simulate it later backtest . to_csv ( 'backtest.csv' , index = False ) # Csv format # backtest.to_pickle('backtest.pkl') # pickle format (faster, but can't open in excel) Perfect, with our hacky solution we have managed to merge around a months' worth of data relatively quickly and saved it in a csv format. With all the merging it seems we have only lost around 1000 - 2000 rows of data out of 27,000 rows of data, which seems only a small price to pay. Getting data ready for placing live bets Placing live bets is pretty simple but we have one issue. FastTrack Data alone is unable to tell us how many greyhounds will run in the race. For example, this race later today (2022-07-04) has 8 runners + 2 reserves: todays_data [ todays_data [ 'FastTrack_RaceId' ] == '798906744' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceName RaceTime Distance RaceGrade date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression DogName_bf Track RaceNum YOU SEE LINA Cannington 1 530411826 2022-07-04 0.0 YOU SEE LINA 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 10408 Michael McLennan FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.167785 0.363135 0.419607 0.395672 0.404274 0.029202 0.395833 0.432534 0.410499 0.403130 0.019428 0.200000 0.500000 0.300000 0.200000 0.173205 0.477121 0.778151 0.677808 0.778151 0.173800 0.0 0.213623 7.120781e-02 0.000000 0.123336 0.184971 0.419607 0.324576 0.346645 0.092382 0.220812 0.432534 0.338647 0.353089 0.084555 0.166667 0.500000 0.316667 0.266667 0.153116 0.477121 0.845098 0.659617 0.690106 0.162743 0.0 0.213623 0.101436 0.092505 0.111554 0.179269 0.419607 0.303822 0.326906 0.075138 0.133803 0.432534 0.315318 0.310345 0.082022 0.125000 0.500000 0.247937 0.200000 0.121737 0.477121 0.954243 0.743299 0.778151 0.156196 0.0 0.213623 0.085118 0.000000 0.095461 0.115534 BELLA LINA Cannington 1 547605028 2022-07-04 0.0 BELLA LINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 83951 Rodney Noden FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.111111 0.228705 0.307236 0.273491 0.284534 0.040413 0.220812 0.285592 0.245073 0.228814 0.035318 0.166667 0.200000 0.188889 0.200000 0.019245 0.778151 0.845098 0.800467 0.778151 0.038652 0.0 0.000000 3.700743e-16 0.000000 0.000000 0.158046 0.307236 0.246575 0.264844 0.050600 0.029221 0.285592 0.203061 0.228814 0.091884 0.142857 0.200000 0.187075 0.200000 0.023119 0.778151 0.903090 0.805563 0.778151 0.049718 0.0 0.000000 0.000000 0.000000 0.000000 0.084276 0.307236 0.225040 0.234235 0.061784 0.029221 0.366864 0.240440 0.236842 0.088850 0.125000 0.250000 0.174702 0.166667 0.034400 0.698970 0.954243 0.835377 0.845098 0.073324 0.0 0.142298 0.008894 0.000000 0.035574 0.031221 PENNY KEEPING Cannington 1 561780971 2022-07-04 0.0 PENNY KEEPING 8 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 68481 Bradley Cook FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.100671 0.076736 0.356220 0.228759 0.253321 0.141352 0.424645 0.513538 0.474906 0.486535 0.045573 0.125000 0.500000 0.250000 0.125000 0.216506 0.477121 0.954243 0.795202 0.954243 0.275466 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.076736 0.356220 0.236342 0.256207 0.116406 0.424645 0.513538 0.467291 0.465490 0.040207 0.125000 0.500000 0.223214 0.133929 0.184716 0.477121 0.954243 0.822174 0.928666 0.231296 0.0 0.000000 0.000000 0.000000 0.000000 0.076736 0.356220 0.236342 0.256207 0.116406 0.424645 0.513538 0.467291 0.465490 0.040207 0.125000 0.500000 0.223214 0.133929 0.184716 0.477121 0.954243 0.822174 0.928666 0.231296 0.0 0.000000 0.000000 0.000000 0.000000 0.049673 WHAT A PHOENIX Cannington 1 603189486 2022-07-04 0.0 WHAT A PHOENIX 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 10408 Michael McLennan FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.108392 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000 0.000000 0.000000 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000 0.000000 0.000000 0.040679 WHAT A QUIZ Cannington 1 603189487 2022-07-04 0.0 WHAT A QUIZ 2 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 72510 Barry McPherson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.141892 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.043975 WHAT A SHAKER Cannington 1 603189986 2022-07-04 0.0 WHAT A SHAKER 4 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 72510 Barry McPherson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.125000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.059174 WIZARDS LEGEND Cannington 1 614056673 2022-07-04 0.0 WIZARD'S LEGEND Res. 10 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 127397 Colin Bainbridge FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.000000 0.307944 0.341758 0.326417 0.327983 0.015946 0.206724 0.288937 0.259703 0.271576 0.036365 0.166667 0.250000 0.204167 0.200000 0.034359 0.698970 0.845098 0.775093 0.778151 0.059761 0.0 0.151629 3.790717e-02 0.000000 0.075814 0.089468 0.341758 0.269683 0.313202 0.093934 0.103960 0.377622 0.240904 0.244173 0.081242 0.125000 0.250000 0.194792 0.200000 0.042477 0.698970 0.954243 0.797104 0.778151 0.084209 0.0 0.151629 0.037164 0.000000 0.068832 0.089468 0.341758 0.275533 0.303754 0.077507 0.103960 0.377622 0.242702 0.244173 0.071625 0.125000 0.333333 0.204266 0.200000 0.058218 0.602060 0.954243 0.785504 0.778151 0.099650 0.0 0.151629 0.037412 0.000000 0.067696 0.019388 WIZARDS DRAMA Cannington 1 614057677 2022-07-04 0.0 WIZARD'S DRAMA Res. 9 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 127397 Colin Bainbridge FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.000000 0.225948 0.344591 0.271951 0.245316 0.063648 0.150000 0.269231 0.221376 0.244898 0.063000 0.142857 0.500000 0.280952 0.200000 0.191840 0.477121 0.903090 0.719454 0.778151 0.218967 0.0 0.209986 6.999522e-02 0.000000 0.121235 0.086870 0.344591 0.234575 0.238391 0.084399 0.088816 0.269231 0.188587 0.189288 0.067980 0.142857 0.500000 0.229762 0.171429 0.139270 0.477121 0.903090 0.777252 0.840621 0.169536 0.0 0.209986 0.059278 0.000000 0.094057 0.036656 0.344591 0.200202 0.228706 0.100907 0.069444 0.269231 0.170954 0.162215 0.071007 0.125000 0.500000 0.229613 0.171429 0.130210 0.477121 0.954243 0.777477 0.840621 0.171435 0.0 0.209986 0.044459 0.000000 0.084096 0.014393 DASHING ONYX Cannington 1 626191408 2022-07-04 0.0 DASHING ONYX 6 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 67839 Graeme Hall FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.144876 0.262045 0.359113 0.314231 0.321535 0.048944 0.244898 0.377622 0.319292 0.335355 0.067805 0.166667 0.333333 0.277778 0.333333 0.096225 0.602060 0.845098 0.683073 0.602060 0.140318 0.0 0.185009 1.233393e-01 0.185009 0.106815 0.229498 0.359113 0.293047 0.291790 0.058241 0.244898 0.377622 0.318715 0.326170 0.055374 0.142857 0.333333 0.244048 0.250000 0.103555 0.602060 0.903090 0.738077 0.723579 0.158833 0.0 0.185009 0.092505 0.092505 0.106815 0.229498 0.359113 0.293047 0.291790 0.058241 0.244898 0.377622 0.318715 0.326170 0.055374 0.142857 0.333333 0.244048 0.250000 0.103555 0.602060 0.903090 0.738077 0.723579 0.158833 0.0 0.185009 0.092505 0.092505 0.106815 0.096959 WINTER RAIN Cannington 1 637972981 2022-07-04 0.0 WINTER RAIN 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 27464 Jennifer Thompson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.101754 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000 0.000000 0.000000 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000 0.000000 0.000000 0.101402 If we predict probabilities and renormalise now, we will calculate incorrect probabilities. I've spent a really long time thinking about this and testing different methods that didn't work or weren't optimal. The best solution (and least complicated) that I have come up with is to predict probabilities on the FastTrack data first. Then a few minutes before the jump when all the lineups have been confirmed we use market_catalogue from the Betfair API to merge our predicted probabilities, merging on DogName , Track and RaceNum . If we merge on these three fields, it will bypass any issues with reserve dogs and scratchings. Then we can renormalise probabilities live within Flumine. # Select todays data todays_data = model_df [ model_df [ 'date_dt' ] == pd . Timestamp . now () . strftime ( '%Y-%m- %d ' )] # Generate runner win predictions dog_win_probabilities = brunos_model . predict_proba ( todays_data [ feature_cols ])[:, 1 ] todays_data [ 'prob_LogisticRegression' ] = dog_win_probabilities # We no longer renomralise probability in this chunk of code, do it in Flumine instead # todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum()) # todays_data['rating'] = 1/todays_data['renormalise_prob'] # todays_data = todays_data.sort_values(by = 'date_dt') todays_data C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/2638603781.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['prob_LogisticRegression'] = dog_win_probabilities .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression 44514 148673258 2022-07-04 0.0 SPEEDY MARINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455740 65928 Dawn Lee 5 LADBROKES BLENDED BETS 1-3 WIN 04:37PM 307 Grade 5 Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.136986 0.317232 0.317232 0.317232 0.317232 0.000000 0.173267 0.173267 0.173267 0.173267 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 6.020600e-01 0.602060 0.000000 0.212109 0.212109 2.121089e-01 0.212109 0.000000e+00 0.317232 0.332373 0.324803 0.324803 0.010706 0.173267 0.210579 0.191923 0.191923 0.026383 0.166667 0.333333 0.250000 0.250000 0.117851 0.602060 0.845098 7.235790e-01 0.723579 0.171854 0.000000 0.212109 0.106054 0.106054 0.149984 0.236982 0.371585 0.306603 0.317232 0.052095 0.173267 0.406600 0.242140 0.210579 0.096894 0.125000 0.333333 0.186905 0.166667 0.083715 0.602060 0.954243 8.299177e-01 0.845098 0.135269 0.000000 0.212109 0.042422 0.000000 0.094858 0.073607 54463 161977365 2022-07-04 0.0 FILTHY PHANTOM 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 110385 Tony Hinrichsen 5 GIDDY-UP (N/P) STAKE 07:27PM 342 Masters Angle Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.124590 0.386907 0.455919 0.427165 0.420839 0.028492 0.314004 0.500000 0.408769 0.394027 0.082256 0.166667 0.333333 0.220000 0.200000 0.064979 0.602060 0.845098 7.563224e-01 0.778151 0.090977 0.000000 0.182760 3.655208e-02 0.000000 8.173293e-02 0.386907 0.572783 0.449667 0.436026 0.053935 0.210921 0.551665 0.403088 0.394027 0.092070 0.142857 1.000000 0.291209 0.200000 0.235613 0.000000 0.903090 6.692431e-01 0.778151 0.257193 0.000000 0.249855 0.057150 0.000000 0.095131 0.277002 0.572783 0.459854 0.456897 0.059514 0.210921 0.616496 0.443040 0.443820 0.089192 0.142857 1.000000 0.464354 0.333333 0.320517 0.000000 0.903090 5.716763e-01 0.602060 0.227821 0.000000 0.249855 0.121936 0.180363 0.106520 0.128265 77950 196384049 2022-07-04 0.0 HOUND 'EM DOWN 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490787 313281 Steven Winstanley 3 ZIPPING GARTH @ STUD 0-2 WIN 07:36PM 565 Mixed Maiden and Grade Five Maitland 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 31.875 0.342029 13.795 0.0 0.0 0.200000 0.233442 0.277637 0.261405 0.273136 0.024321 0.218310 0.316690 0.270554 0.276662 0.049474 0.142857 0.200000 0.161905 0.142857 0.032991 0.778151 0.903090 8.614437e-01 0.903090 0.072133 0.000000 0.000000 4.440892e-16 0.000000 0.000000e+00 0.194132 0.355912 0.279655 0.277637 0.046001 0.218310 0.316690 0.271078 0.275180 0.032574 0.125000 0.200000 0.158862 0.142857 0.026641 0.778151 0.954243 8.681223e-01 0.903090 0.060784 0.000000 0.000000 0.000000 0.000000 0.000000 0.194132 0.462627 0.328446 0.316396 0.054098 0.218310 0.346154 0.292413 0.296715 0.036166 0.125000 0.333333 0.187245 0.166667 0.054481 0.000000 0.954243 7.520173e-01 0.845098 0.239572 0.000000 0.212109 0.014373 0.000000 0.050118 0.070298 121171 230053393 2022-07-04 0.0 CAWBOURNE CROSS 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 104699 Lisa Rasmussen 5 GIDDY-UP (N/P) STAKE 07:27PM 342 Masters Angle Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.169811 0.336395 0.413836 0.386878 0.410402 0.043753 0.500000 0.500000 0.500000 0.500000 0.000000 0.142857 0.250000 0.186508 0.166667 0.056260 0.698970 0.903090 8.157193e-01 0.845098 0.105184 0.000000 0.000000 3.700743e-16 0.000000 2.533726e-17 0.336395 0.491121 0.420107 0.412119 0.059452 0.401747 0.506477 0.480510 0.500000 0.044239 0.142857 0.500000 0.273810 0.250000 0.129975 0.477121 0.903090 7.042182e-01 0.698970 0.155860 0.000000 0.188140 0.058566 0.000000 0.091070 0.280126 0.505631 0.403460 0.405048 0.064662 0.329857 0.610664 0.472084 0.500000 0.079724 0.142857 0.500000 0.269048 0.225000 0.125458 0.477121 0.903090 7.115827e-01 0.738561 0.144209 0.000000 0.205324 0.060368 0.000000 0.090871 0.179535 142478 243599770 2022-07-04 0.0 SKAIKRU 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455741 83214 Robert Sonter 6 BATHURST RSL CLUB 04:59PM 450 Grade 5 Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.159574 0.392354 0.392354 0.392354 0.392354 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 1.998401e-15 0.000000 0.000000e+00 0.293318 0.392354 0.329286 0.302186 0.054798 0.238956 0.238956 0.238956 0.238956 0.000000 0.142857 0.333333 0.242063 0.250000 0.095486 0.602060 0.903090 7.347067e-01 0.698970 0.153664 0.000000 0.167027 0.055676 0.000000 0.096433 0.285293 0.392354 0.317943 0.303341 0.036066 0.162722 0.258454 0.223927 0.237266 0.042031 0.125000 0.333333 0.202551 0.200000 0.070985 0.602060 0.954243 7.942519e-01 0.778151 0.120113 0.000000 0.167027 0.023861 0.000000 0.063130 0.092948 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 599603 673011364 2022-07-04 0.0 FERAL AGENT 8 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 96421 Derek Kerr 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.177215 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 1.478939e-01 0.212109 1.284491e-01 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 0.147894 0.212109 0.128449 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 0.147894 0.212109 0.128449 0.194287 599956 694776805 2022-07-04 0.0 WENDY MAREE 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490818 307271 Brian Baker 2 GARRARD'S HORSE AND HOUND 07:11PM 520 Novice Non Penalty Albion Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 30.220 0.634509 5.630 0.0 0.0 0.136417 0.325934 0.410627 0.368281 0.368281 0.059887 0.168325 0.336770 0.252547 0.252547 0.119108 0.200000 0.250000 0.225000 0.225000 0.035355 0.698970 0.778151 7.385606e-01 0.738561 0.055990 0.110185 0.193690 1.519376e-01 0.151938 5.904714e-02 0.325934 0.410627 0.366778 0.363772 0.042426 0.168325 0.344322 0.283139 0.336770 0.099504 0.200000 0.250000 0.216667 0.200000 0.028868 0.698970 0.778151 7.517575e-01 0.778151 0.045715 0.110185 0.193690 0.138020 0.110185 0.048212 0.305980 0.410627 0.355981 0.361146 0.036561 0.168325 0.344322 0.274549 0.279411 0.067105 0.142857 0.333333 0.221032 0.200000 0.064636 0.602060 0.903090 7.564290e-01 0.778151 0.100056 0.110185 0.218690 0.142187 0.110185 0.050203 0.123793 600100 707214702 2022-07-04 0.0 WARDEN JODIE Res. 10 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 75264 Sam Simonetta 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.000000 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000 0.000000 0.000000 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000 0.000000 0.000000 0.014319 600105 707215693 2022-07-04 0.0 MYSTERY ANNE 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 75264 Sam Simonetta 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.125874 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.090248 600106 707215696 2022-07-04 0.0 BROKEN PROMISES 2 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455736 75264 Sam Simonetta 1 WELCOME GBOTA MAIDEN 03:07PM 450 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.139785 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.708944e-14 0.000000 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.642331e-14 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.842171e-14 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.060767 1704 rows \u00d7 115 columns Before we merge, let's do some minor formatting changes to the FastTrack names so we can match onto the Betfair names. Betfair excludes all apostrophes and full stops in their naming convention, so we'll create a Betfair equivalent dog name on the dataset removing these characters. We also need to do this for the tracks, sometimes FastTrack will name tracks differently to Betfair e.g., Sandown Park from Betfair is known as Sandown (SAP) in the FastTrack database. # Prepare data for easy reference in flumine todays_data [ 'DogName_bf' ] = todays_data [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) todays_data . replace ({ 'Sandown (SAP)' : 'Sandown Park' }, regex = True , inplace = True ) todays_data = todays_data . set_index ([ 'DogName_bf' , 'Track' , 'RaceNum' ]) todays_data . head () C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/90992895.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['DogName_bf'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceName RaceTime Distance RaceGrade date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression DogName_bf Track RaceNum SPEEDY MARINA Bathurst 5 148673258 2022-07-04 0.0 SPEEDY MARINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455740 65928 Dawn Lee LADBROKES BLENDED BETS 1-3 WIN 04:37PM 307 Grade 5 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.136986 0.317232 0.317232 0.317232 0.317232 0.000000 0.173267 0.173267 0.173267 0.173267 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.212109 0.212109 2.121089e-01 0.212109 0.000000e+00 0.317232 0.332373 0.324803 0.324803 0.010706 0.173267 0.210579 0.191923 0.191923 0.026383 0.166667 0.333333 0.250000 0.250000 0.117851 0.602060 0.845098 0.723579 0.723579 0.171854 0.0 0.212109 0.106054 0.106054 0.149984 0.236982 0.371585 0.306603 0.317232 0.052095 0.173267 0.406600 0.242140 0.210579 0.096894 0.125000 0.333333 0.186905 0.166667 0.083715 0.602060 0.954243 0.829918 0.845098 0.135269 0.0 0.212109 0.042422 0.000000 0.094858 0.073607 FILTHY PHANTOM Angle Park 5 161977365 2022-07-04 0.0 FILTHY PHANTOM 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 110385 Tony Hinrichsen GIDDY-UP (N/P) STAKE 07:27PM 342 Masters 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.124590 0.386907 0.455919 0.427165 0.420839 0.028492 0.314004 0.500000 0.408769 0.394027 0.082256 0.166667 0.333333 0.220000 0.200000 0.064979 0.602060 0.845098 0.756322 0.778151 0.090977 0.000000 0.182760 3.655208e-02 0.000000 8.173293e-02 0.386907 0.572783 0.449667 0.436026 0.053935 0.210921 0.551665 0.403088 0.394027 0.092070 0.142857 1.000000 0.291209 0.200000 0.235613 0.000000 0.903090 0.669243 0.778151 0.257193 0.0 0.249855 0.057150 0.000000 0.095131 0.277002 0.572783 0.459854 0.456897 0.059514 0.210921 0.616496 0.443040 0.443820 0.089192 0.142857 1.000000 0.464354 0.333333 0.320517 0.000000 0.903090 0.571676 0.602060 0.227821 0.0 0.249855 0.121936 0.180363 0.106520 0.128265 HOUND EM DOWN Maitland 3 196384049 2022-07-04 0.0 HOUND 'EM DOWN 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490787 313281 Steven Winstanley ZIPPING GARTH @ STUD 0-2 WIN 07:36PM 565 Mixed Maiden and Grade Five 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 31.875 0.342029 13.795 0.0 0.0 0.200000 0.233442 0.277637 0.261405 0.273136 0.024321 0.218310 0.316690 0.270554 0.276662 0.049474 0.142857 0.200000 0.161905 0.142857 0.032991 0.778151 0.903090 0.861444 0.903090 0.072133 0.000000 0.000000 4.440892e-16 0.000000 0.000000e+00 0.194132 0.355912 0.279655 0.277637 0.046001 0.218310 0.316690 0.271078 0.275180 0.032574 0.125000 0.200000 0.158862 0.142857 0.026641 0.778151 0.954243 0.868122 0.903090 0.060784 0.0 0.000000 0.000000 0.000000 0.000000 0.194132 0.462627 0.328446 0.316396 0.054098 0.218310 0.346154 0.292413 0.296715 0.036166 0.125000 0.333333 0.187245 0.166667 0.054481 0.000000 0.954243 0.752017 0.845098 0.239572 0.0 0.212109 0.014373 0.000000 0.050118 0.070298 CAWBOURNE CROSS Angle Park 5 230053393 2022-07-04 0.0 CAWBOURNE CROSS 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 104699 Lisa Rasmussen GIDDY-UP (N/P) STAKE 07:27PM 342 Masters 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.169811 0.336395 0.413836 0.386878 0.410402 0.043753 0.500000 0.500000 0.500000 0.500000 0.000000 0.142857 0.250000 0.186508 0.166667 0.056260 0.698970 0.903090 0.815719 0.845098 0.105184 0.000000 0.000000 3.700743e-16 0.000000 2.533726e-17 0.336395 0.491121 0.420107 0.412119 0.059452 0.401747 0.506477 0.480510 0.500000 0.044239 0.142857 0.500000 0.273810 0.250000 0.129975 0.477121 0.903090 0.704218 0.698970 0.155860 0.0 0.188140 0.058566 0.000000 0.091070 0.280126 0.505631 0.403460 0.405048 0.064662 0.329857 0.610664 0.472084 0.500000 0.079724 0.142857 0.500000 0.269048 0.225000 0.125458 0.477121 0.903090 0.711583 0.738561 0.144209 0.0 0.205324 0.060368 0.000000 0.090871 0.179535 SKAIKRU Bathurst 6 243599770 2022-07-04 0.0 SKAIKRU 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455741 83214 Robert Sonter BATHURST RSL CLUB 04:59PM 450 Grade 5 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.159574 0.392354 0.392354 0.392354 0.392354 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.000000 0.000000 1.998401e-15 0.000000 0.000000e+00 0.293318 0.392354 0.329286 0.302186 0.054798 0.238956 0.238956 0.238956 0.238956 0.000000 0.142857 0.333333 0.242063 0.250000 0.095486 0.602060 0.903090 0.734707 0.698970 0.153664 0.0 0.167027 0.055676 0.000000 0.096433 0.285293 0.392354 0.317943 0.303341 0.036066 0.162722 0.258454 0.223927 0.237266 0.042031 0.125000 0.333333 0.202551 0.200000 0.070985 0.602060 0.954243 0.794252 0.778151 0.120113 0.0 0.167027 0.023861 0.000000 0.063130 0.092948 If you look closely at the data frame above you might notice that for reserve dogs, they will have a Box number of 9 or 10. I personally don't really watch greyhound racing (I just like numbers), but I'm pretty confident that there is only ever a max of around 8 greyhounds per race. In which case we will need to adjust it somehow. I didn't notice this issue for quite a while, but the good thing is the website gives us the info we need to adjust: We can see that Rhinestone Ash is a reserve dog and has the number 9, if you click on rules, you can see what Box it is starting from: The problem is, my webscraping is pretty poor, and it would take significant time for me to learn it. But after going through the documentation again, changes to boxes are actually available through the API under the clarifications attribute of marketDescription . You will be able to access this within Flumine as market.market_catalogue.description.clarifications , but it's a bit weird. It returns box changes as a string that looks like this: Originally I had planned to leave this article as it is since, I've never worked with anything like this before and its already getting pretty long, however huge shoutout to Betfair Quants community and especially Brett who provided his solution to working with box changes. Brett's solution is amazing, there is only one problem, currently our code is structured so that we generate our predictions in the morning well before the race starts. To implement the above fix, we need to generate our predictions just before the race starts to incorporate the Box information. This means we need to write a little bit more code to make it happen, but we are almost there. So now my plan to update the old data and generate probabilities just before the race. So now just before the jump my code structure will look like this: - pull any data on box changes from the Betfair API - convert the box change data into a dataframe named runners_df using the Brett's code - in my original dataframe named todays_data replace any Box data with runners_df data, otherwise leave it untouched - then merge the box_win_percent dataframe back onto the todays_data dataframe - now we can predict probabilities again and then renormalise them It may sound a little complicated but as we already have Brett's code there is only a few extra lines of code we need to write. This is what we will add into our Flumine strategy along with Brett's code: # Running Brett's code gives us a nice dataframe named runners_df that we can work with # Replace any old Box info in our original dataframe with data available in runners_df = runners_df.set_index('runner_name') todays_data.loc[(runners_df.index[runners_df.index.isin(dog_names)],track,race_number),'Box'] = runners_df.loc[runners_df.index.isin(dog_names),'Box'].to_list() # Merge box_win_percent data onto todays_data todays_data = todays_data.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left') # Merge box_win_percentage back on: todays_data = todays_data.drop(columns = 'box_win_percentage', axis = 1) todays_data = todays_data.merge(box_win_percent, on = ['Track', 'Distance','Box'], how = 'left') # Generate probabilities using Bruno's model todays_data.loc[(dog_names,track,race_number),'prob_LogisticRegression'] = brunos_model.predict_proba(todays_data.loc[(dog_names,track,race_number)][feature_cols])[:,1] # renomalise probabilities probabilities = todays_data.loc[dog_names,track,race_number]['prob_LogisticRegression'] todays_data.loc[(dog_names,track,race_number),'renormalised_prob'] = probabilities/probabilities.sum() # convert probaiblities to ratings todays_data.loc[(dog_names,track,race_number),'rating'] = 1/todays_data.loc[dog_names,track,race_number]['renormalised_prob'] Now everything is done, and we can finally move onto placing our bets Automating our predictions Now that we have our data nicely set up. We can reference our probabilities by getting the DogName, Track and RaceNum from the Betfair polling API and then renormalised probabilities to calculate ratings with only a few lines of code. Then the rest is the same as How to Automate III # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) # Import libraries and logging from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import re import pandas as pd import numpy as np import datetime import logging logging . basicConfig ( filename = 'how_to_automate_4.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) Let's create a new class for our strategy called FlatBetting that finds the best available to back and lay price 60 seconds before the jump. If any of those prices have value, we will place a flat bet for /$/5 at those prices. This code is almost the same as How to Automate III Since we are now editing our todays_data dataframe inside our Flumine strategy we will also need to convert todays_data to a global variable which is a simple one liner: global todays_data I also wanted to call out one gotcha that, Brett found that is almost impossible to find unless you are keeping a close eye on your logs. Sometimes the polling API and streaming API doesn't match up when there are scratchings, so we need to check if it does: # Check the polling API and streaming API matches up (sometimes it doesn't) if runner_cata.selection_id == runner.selection_id: class FlatBetting ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Convert dataframe to a global variable global todays_data # At the 60 second mark: if market . seconds_to_start < 60 and market_book . inplay == False : # get the list of dog_names, name of the track/venue and race_number/RaceNum from Betfair Polling API dog_names = [] track = market . market_catalogue . event . venue race_number = market . market_catalogue . market_name . split ( ' ' , 1 )[ 0 ] # comes out as R1/R2/R3 .. etc race_number = re . sub ( \"[^0-9]\" , \"\" , race_number ) # only keep the numbers for runner_cata in market . market_catalogue . runners : dog_name = runner_cata . runner_name . split ( ' ' , 1 )[ 1 ] . upper () dog_names . append ( dog_name ) # Check if there are box changes, if there are then use Brett's code if market . market_catalogue . description . clarifications != None : # Brett's code to get Box changes: my_string = market . market_catalogue . description . clarifications . replace ( \"<br> Dog\" , \"<br>Dog\" ) pattern1 = r '(?<=<br>Dog ).+?(?= starts)' pattern2 = r \"(?<=\\bbox no. )(\\w+)\" runners_df = pd . DataFrame ( regexp_tokenize ( my_string , pattern1 ), columns = [ 'runner_name' ]) runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . astype ( str ) # Remove dog name from runner_number runners_df [ 'runner_number' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [:( x . find ( \" \" ) - 1 )] . upper ()) # Remove dog number from runner_name runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [( x . find ( \" \" ) + 1 ):] . upper ()) runners_df [ 'Box' ] = regexp_tokenize ( my_string , pattern2 ) # Replace any old Box info in our original dataframe with data available in runners_df runners_df = runners_df . set_index ( 'runner_name' ) todays_data . loc [( runners_df . index [ runners_df . index . isin ( dog_names )], track , race_number ), 'Box' ] = runners_df . loc [ runners_df . index . isin ( dog_names ), 'Box' ] . to_list () # Merge box_win_percentage back on: todays_data = todays_data . drop ( columns = 'box_win_percentage' , axis = 1 ) todays_data = todays_data . reset_index () . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) . set_index ([ 'DogName_bf' , 'Track' , 'RaceNum' ]) # Generate probabilities using Bruno's model todays_data . loc [( dog_names , track , race_number ), 'prob_LogisticRegression' ] = brunos_model . predict_proba ( todays_data . loc [( dog_names , track , race_number )][ feature_cols ])[:, 1 ] # renomalise probabilities probabilities = todays_data . loc [ dog_names , track , race_number ][ 'prob_LogisticRegression' ] todays_data . loc [( dog_names , track , race_number ), 'renormalised_prob' ] = probabilities / probabilities . sum () # convert probaiblities to ratings todays_data . loc [( dog_names , track , race_number ), 'rating' ] = 1 / todays_data . loc [ dog_names , track , race_number ][ 'renormalised_prob' ] # Use both the polling api (market.catalogue) and the streaming api at once: for runner_cata , runner in zip ( market . market_catalogue . runners , market_book . runners ): # Check the polling api and streaming api matches up (sometimes it doesn't) if runner_cata . selection_id == runner . selection_id : # Get the dog_name from polling api then reference our data for our model rating dog_name = runner_cata . runner_name . split ( ' ' , 1 )[ 1 ] . upper () # Rest is the same as How to Automate III model_price = todays_data . loc [ dog_name , track , race_number ][ 'rating' ] ### If you have an issue such as: # Unknown error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). # Then do model_price = todays_data.loc[dog_name,track,race_number]['rating'].item() # Log info before placing bets logging . info ( f 'dog_name: { dog_name } ' ) logging . info ( f 'model_price: { model_price } ' ) logging . info ( f 'market_id: { market_book . market_id } ' ) logging . info ( f 'selection_id: { runner . selection_id } ' ) # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) As the model we have built is a greyhound model for Australian racing let's point our strategy to Australian greyhound win markets greyhounds_strategy = FlatBetting ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds markets country_codes = [ \"AU\" ], # Australian markets market_types = [ \"WIN\" ], # Win markets ), max_order_exposure = 50 , # Max exposure per order = 50 max_trade_count = 1 , # Max 1 trade per selection max_live_trade_count = 1 , # Max 1 unmatched trade per selection ) framework . add_strategy ( greyhounds_strategy ) And add our auto-terminate and bet logging from the previous tutorials: # import logging import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent # logger = logging.getLogger(__name__) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : # logger.info(\"No more markets available, terminating framework\") flumine . handler_queue . put ( TerminationEvent ( flumine )) # Add the stopped to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) import os import csv import logging from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_4.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_4.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_4.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_4.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) framework . add_logging_control ( LiveLoggingControl () ) framework . run () Conclusion and next steps Boom! We now have an automated script that will downloads all the data we need in the morning, generates a set of predictions, place flat stakes bets, logs all bets and switches itself off at the end of the day. All we need to do is hit play in the morning! We have now written code automation code for three different strategies, however we haven't actually backtested any of our strategies or models yet. So for the final part of the How to Automate series we will be writing code to How to simulate the Exchange to backtest and optimise our strategies . Make sure not to miss it as this is where I believe the sauce is made (not that I have made significant sauce). Complete code Run the code from your ide by using py <filename> .py, making sure you amend the path to point to your input data. Download from Github from joblib import load import os import sys # Allow imports from src folder module_path = os . path . abspath ( os . path . join ( '../src' )) if module_path not in sys . path : sys . path . append ( module_path ) from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from dateutil import tz from pandas.tseries.offsets import MonthEnd from sklearn.preprocessing import MinMaxScaler import itertools import numpy as np import pandas as pd from nltk.tokenize import regexp_tokenize # settings to display all columns pd . set_option ( \"display.max_columns\" , None ) import fasttrack as ft from dotenv import load_dotenv load_dotenv () # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Import libraries and logging from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import re import pandas as pd import numpy as np import datetime import logging logging . basicConfig ( filename = 'how_to_automate_4.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # import logging from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent import csv from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logger = logging . getLogger ( __name__ ) brunos_model = load ( 'logistic_regression.joblib' ) brunos_model # Validate FastTrack API connection api_key = os . getenv ( 'FAST_TRACK_API_KEY' ,) client = ft . Fasttrack ( api_key ) track_codes = client . listTracks () # Import race data excluding NZ races au_tracks_filter = list ( track_codes [ track_codes [ 'state' ] != 'NZ' ][ 'track_code' ]) # Time window to import data # First day of the month 46 months back from now date_from = ( datetime . today () - relativedelta ( months = 46 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # First day of previous month date_to = ( datetime . today () - relativedelta ( months = 1 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # Dataframes to populate data with race_details = pd . DataFrame () dog_results = pd . DataFrame () # For each month, either fetch data from API or use local CSV file if we already have downloaded it for start in pd . date_range ( date_from , date_to , freq = 'MS' ): start_date = start . strftime ( \"%Y-%m- %d \" ) end_date = ( start + MonthEnd ( 1 )) . strftime ( \"%Y-%m- %d \" ) try : filename_races = f 'FT_AU_RACES_ { start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { start_date } .csv' filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' print ( f 'Loading data from { start_date } to { end_date } ' ) if os . path . isfile ( filepath_races ): # Load local CSV file month_race_details = pd . read_csv ( filepath_races ) month_dog_results = pd . read_csv ( filepath_dogs ) else : # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( start_date , end_date , au_tracks_filter ) month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) # Combine monthly data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) except : print ( f 'Could not load data from { start_date } to { end_date } ' ) race_details . tail () current_month_start_date = pd . Timestamp . now () . replace ( day = 1 ) . strftime ( \"%Y-%m- %d \" ) current_month_end_date = ( pd . Timestamp . now () . replace ( day = 1 ) + MonthEnd ( 1 )) current_month_end_date = ( current_month_end_date - pd . Timedelta ( '1 day' )) . strftime ( \"%Y-%m- %d \" ) print ( f 'Start date: { current_month_start_date } ' ) print ( f 'End Date: { current_month_end_date } ' ) # Download data for races that have concluded this current month up untill today # Start and end dates for current month current_month_start_date = pd . Timestamp . now () . replace ( day = 1 ) . strftime ( \"%Y-%m- %d \" ) current_month_end_date = ( pd . Timestamp . now () . replace ( day = 1 ) + MonthEnd ( 1 )) current_month_end_date = ( current_month_end_date - pd . Timedelta ( '1 day' )) . strftime ( \"%Y-%m- %d \" ) # Files names filename_races = f 'FT_AU_RACES_ { current_month_start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { current_month_start_date } .csv' # Where to store files locally filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( current_month_start_date , current_month_end_date , au_tracks_filter ) # Save the files locally and replace any out of date fields month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) dog_results # This is super important I have spent literally hours before I found out this was causing errors dog_results [ '@id' ] = pd . to_numeric ( dog_results [ '@id' ]) # Append the extra data to our data frames race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) # Download the data for todays races todays_date = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) todays_races , todays_dogs = client . getFullFormat ( dt = todays_date , tracks = au_tracks_filter ) # display is for ipython notebooks only # display(todays_races.head(1), todays_dogs.head(1)) # It seems that the todays_races dataframe doesn't have the date column, so let's add that on todays_races [ 'date' ] = pd . Timestamp . now () . strftime ( ' %d %b %y' ) todays_races . head ( 1 ) # It also seems that in todays_dogs dataframe Box is labeled as RaceBox instead, so let's rename it # We can also see that there are some specific dogs that have \"Res.\" as a suffix of their name, i.e. they are reserve dogs, # We will treat this later todays_dogs = todays_dogs . rename ( columns = { \"RaceBox\" : \"Box\" }) todays_dogs . tail ( 3 ) # Appending todays data to this months data month_dog_results = pd . concat ([ month_dog_results , todays_dogs ], join = 'outer' )[ month_dog_results . columns ] month_race_details = pd . concat ([ month_race_details , todays_races ], join = 'outer' )[ month_race_details . columns ] # Appending this months data to the rest of our historical data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) race_details ## Cleanse and normalise the data # Clean up the race dataset race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) # Clean up the dogs results dataset dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) # New line of code (rest of this code chunk is copied from bruno's code) dog_results [ 'FastTrack_DogId' ] = pd . to_numeric ( dog_results [ 'FastTrack_DogId' ]) # Combine dogs results with race attributes dog_results = dog_results . merge ( race_details , how = 'left' , on = 'FastTrack_RaceId' ) # Convert StartPrice to probability dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x is None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' )) if isinstance ( x , str ) else x ) dog_results [ 'StartPrice_probability' ] = ( 1 / dog_results [ 'StartPrice' ]) . fillna ( 0 ) dog_results [ 'StartPrice_probability' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x / x . sum ()) # Discard entries without results (scratched or did not finish) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results [ 'Box' ] = dog_results [ 'Box' ] . astype ( int ) # Clean up other attributes dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'SplitMargin' ] = dog_results [ 'SplitMargin' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) . fillna ( 0 ) dog_results [ 'Place' ] = pd . to_numeric ( dog_results [ 'Place' ] . apply ( lambda x : x . replace ( \"=\" , \"\" ) if isinstance ( x , str ) else 0 ), errors = 'coerce' ) . fillna ( 0 ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x == 1 else 0 ) # Normalise some of the raw values dog_results [ 'Prizemoney_norm' ] = np . log10 ( dog_results [ 'Prizemoney' ] + 1 ) / 12 dog_results [ 'Place_inv' ] = ( 1 / dog_results [ 'Place' ]) . fillna ( 0 ) dog_results [ 'Place_log' ] = np . log10 ( dog_results [ 'Place' ] + 1 ) . fillna ( 0 ) dog_results [ 'RunSpeed' ] = ( dog_results [ 'RunTime' ] / dog_results [ 'Distance' ]) . fillna ( 0 ) ## Generate features using raw data # Calculate median winner time per track/distance win_results = dog_results [ dog_results [ 'win' ] == 1 ] median_win_time = pd . DataFrame ( data = win_results [ win_results [ 'RunTime' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'RunTime' ] . median ()) . rename ( columns = { \"RunTime\" : \"RunTime_median\" }) . reset_index () median_win_split_time = pd . DataFrame ( data = win_results [ win_results [ 'SplitMargin' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'SplitMargin' ] . median ()) . rename ( columns = { \"SplitMargin\" : \"SplitMargin_median\" }) . reset_index () median_win_time . head () # Calculate track speed index median_win_time [ 'speed_index' ] = ( median_win_time [ 'RunTime_median' ] / median_win_time [ 'Distance' ]) median_win_time [ 'speed_index' ] = MinMaxScaler () . fit_transform ( median_win_time [[ 'speed_index' ]]) median_win_time . head () # Compare dogs finish time with median winner time dog_results = dog_results . merge ( median_win_time , on = [ 'Track' , 'Distance' ], how = 'left' ) dog_results = dog_results . merge ( median_win_split_time , on = [ 'Track' , 'Distance' ], how = 'left' ) # Normalise time comparison dog_results [ 'RunTime_norm' ] = ( dog_results [ 'RunTime_median' ] / dog_results [ 'RunTime' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'RunTime_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'RunTime_norm' ]]) dog_results [ 'SplitMargin_norm' ] = ( dog_results [ 'SplitMargin_median' ] / dog_results [ 'SplitMargin' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'SplitMargin_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'SplitMargin_norm' ]]) dog_results . head () # Calculate box winning percentage for each track/distance box_win_percent = pd . DataFrame ( data = dog_results . groupby ([ 'Track' , 'Distance' , 'Box' ])[ 'win' ] . mean ()) . rename ( columns = { \"win\" : \"box_win_percent\" }) . reset_index () # Add to dog results dataframe dog_results = dog_results . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) # Display example of barrier winning probabilities print ( box_win_percent . head ( 8 )) dog_results [ dog_results [ 'FastTrack_DogId' ] == 592253143 ] . tail ()[[ 'date_dt' , 'Place' , 'DogName' , 'RaceNum' , 'Track' , 'Distance' , 'win' , 'Prizemoney_norm' , 'Place_inv' , 'Place_log' ]] # Generate rolling window features dataset = dog_results . copy () dataset = dataset . set_index ([ 'FastTrack_DogId' , 'date_dt' ]) . sort_index () # Use rolling window of 28, 91 and 365 days rolling_windows = [ '28D' , '91D' , '365D' ] # Features to use for rolling windows calculation features = [ 'RunTime_norm' , 'SplitMargin_norm' , 'Place_inv' , 'Place_log' , 'Prizemoney_norm' ] # Aggregation functions to apply aggregates = [ 'min' , 'max' , 'mean' , 'median' , 'std' ] # Keep track of generated feature names feature_cols = [ 'speed_index' , 'box_win_percent' ] for rolling_window in rolling_windows : print ( f 'Processing rolling window { rolling_window } ' ) rolling_result = ( dataset . reset_index ( level = 0 ) . sort_index () . groupby ( 'FastTrack_DogId' )[ features ] . rolling ( rolling_window ) . agg ( aggregates ) . groupby ( level = 0 ) # Thanks to Brett for finding this! . shift ( 1 ) ) # My own dodgey code to work with reserve dogs temp = rolling_result . reset_index () temp = temp [ temp [ 'date_dt' ] == pd . Timestamp . now () . normalize ()] temp . groupby ([ 'FastTrack_DogId' , 'date_dt' ]) . first () rolling_result . loc [ pd . IndexSlice [:, pd . Timestamp . now () . normalize ()], :] = temp . groupby ([ 'FastTrack_DogId' , 'date_dt' ]) . first () # Generate list of rolling window feature names (eg: RunTime_norm_min_365D) agg_features_cols = [ f ' { f } _ { a } _ { rolling_window } ' for f , a in itertools . product ( features , aggregates )] # Add features to dataset dataset [ agg_features_cols ] = rolling_result # Keep track of generated feature names feature_cols . extend ( agg_features_cols ) # Replace missing values with 0 dataset . fillna ( 0 , inplace = True ) # display(dataset.head(8)) # display is only for ipython notebooks # Only keep data after 2018-12-01 model_df = dataset . reset_index () feature_cols = np . unique ( feature_cols ) . tolist () model_df = model_df [ model_df [ 'date_dt' ] >= '2018-12-01' ] # This line was originally part of Bruno's tutorial, but we don't run it in this script # model_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] # Generate predictions like normal # Range of dates that we want to simulate later '2022-03-01' to '2022-04-01' todays_data = model_df [( model_df [ 'date_dt' ] >= pd . Timestamp ( '2022-03-01' ) . strftime ( '%Y-%m- %d ' )) & ( model_df [ 'date_dt' ] < pd . Timestamp ( '2022-04-01' ) . strftime ( '%Y-%m- %d ' ))] dog_win_probabilities = brunos_model . predict_proba ( todays_data [ feature_cols ])[:, 1 ] todays_data [ 'prob_LogisticRegression' ] = dog_win_probabilities todays_data [ 'renormalise_prob' ] = todays_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x / x . sum ()) todays_data [ 'rating' ] = 1 / todays_data [ 'renormalise_prob' ] todays_data = todays_data . sort_values ( by = 'date_dt' ) todays_data def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" , \"meetings.races.number\" : \"RaceNum\" , \"meetings.name\" : \"Track\" , \"meetings.races.runners.name\" : \"DogName\" } ) # iggy_df = iggy_df[['market_id','selection_id','rating']] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) iggy_df [ 'date_dt' ] = date # Set market_id and selection_id as index for easy referencing # iggy_df = iggy_df.set_index(['market_id','selection_id']) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022-03-01' , end = '2022-04-01' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) iggy_df # format DogNames to merge todays_data [ 'DogName' ] = todays_data [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) iggy_df [ 'DogName' ] = iggy_df [ 'DogName' ] . str . upper () # Merge backtest = iggy_df [[ 'market_id' , 'selection_id' , 'DogName' , 'date_dt' ]] . merge ( todays_data [[ 'rating' , 'DogName' , 'date_dt' ]], how = 'inner' , on = [ 'DogName' , 'date_dt' ]) backtest # Save predictions for if we want to backtest/simulate it later backtest . to_csv ( 'backtest.csv' , index = False ) # Csv format # backtest.to_pickle('backtest.pkl') # pickle format (faster, but can't open in excel) todays_data [ todays_data [ 'FastTrack_RaceId' ] == '798906744' ] # Select todays data todays_data = model_df [ model_df [ 'date_dt' ] == pd . Timestamp . now () . strftime ( '%Y-%m- %d ' )] # Generate runner win predictions dog_win_probabilities = brunos_model . predict_proba ( todays_data [ feature_cols ])[:, 1 ] todays_data [ 'prob_LogisticRegression' ] = dog_win_probabilities # We no longer renomralise probability in this chunk of code, do it in Flumine instead # todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum()) # todays_data['rating'] = 1/todays_data['renormalise_prob'] # todays_data = todays_data.sort_values(by = 'date_dt') todays_data # Prepare data for easy reference in flumine todays_data [ 'DogName_bf' ] = todays_data [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) todays_data . replace ({ 'Sandown (SAP)' : 'Sandown Park' }, regex = True , inplace = True ) todays_data = todays_data . set_index ([ 'DogName_bf' , 'Track' , 'RaceNum' ]) todays_data . head () # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) class FlatBetting ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Convert dataframe to a global variable global todays_data # At the 60 second mark: if market . seconds_to_start < 60 and market_book . inplay == False : # get the list of dog_names, name of the track/venue and race_number/RaceNum from Betfair Polling API dog_names = [] track = market . market_catalogue . event . venue race_number = market . market_catalogue . market_name . split ( ' ' , 1 )[ 0 ] # comes out as R1/R2/R3 .. etc race_number = re . sub ( \"[^0-9]\" , \"\" , race_number ) # only keep the numbers for runner_cata in market . market_catalogue . runners : dog_name = runner_cata . runner_name . split ( ' ' , 1 )[ 1 ] . upper () dog_names . append ( dog_name ) # Check if there are box changes, if there are then use Brett's code if market . market_catalogue . description . clarifications != None : # Brett's code to get Box changes: my_string = market . market_catalogue . description . clarifications . replace ( \"<br> Dog\" , \"<br>Dog\" ) pattern1 = r '(?<=<br>Dog ).+?(?= starts)' pattern2 = r \"(?<=\\bbox no. )(\\w+)\" runners_df = pd . DataFrame ( regexp_tokenize ( my_string , pattern1 ), columns = [ 'runner_name' ]) runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . astype ( str ) # Remove dog name from runner_number runners_df [ 'runner_number' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [:( x . find ( \" \" ) - 1 )] . upper ()) # Remove dog number from runner_name runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [( x . find ( \" \" ) + 1 ):] . upper ()) runners_df [ 'Box' ] = regexp_tokenize ( my_string , pattern2 ) # Replace any old Box info in our original dataframe with data available in runners_df runners_df = runners_df . set_index ( 'runner_name' ) todays_data . loc [( runners_df . index [ runners_df . index . isin ( dog_names )], track , race_number ), 'Box' ] = runners_df . loc [ runners_df . index . isin ( dog_names ), 'Box' ] . to_list () # Merge box_win_percentage back on: todays_data = todays_data . drop ( columns = 'box_win_percentage' , axis = 1 ) todays_data = todays_data . reset_index () . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) . set_index ([ 'DogName_bf' , 'Track' , 'RaceNum' ]) # Generate probabilities using Bruno's model todays_data . loc [( dog_names , track , race_number ), 'prob_LogisticRegression' ] = brunos_model . predict_proba ( todays_data . loc [( dog_names , track , race_number )][ feature_cols ])[:, 1 ] # renomalise probabilities probabilities = todays_data . loc [ dog_names , track , race_number ][ 'prob_LogisticRegression' ] todays_data . loc [( dog_names , track , race_number ), 'renormalised_prob' ] = probabilities / probabilities . sum () # convert probaiblities to ratings todays_data . loc [( dog_names , track , race_number ), 'rating' ] = 1 / todays_data . loc [ dog_names , track , race_number ][ 'renormalised_prob' ] # Use both the polling api (market.catalogue) and the streaming api at once: for runner_cata , runner in zip ( market . market_catalogue . runners , market_book . runners ): # Check the polling api and streaming api matches up (sometimes it doesn't) if runner_cata . selection_id == runner . selection_id : # Get the dog_name from polling api then reference our data for our model rating dog_name = runner_cata . runner_name . split ( ' ' , 1 )[ 1 ] . upper () # Rest is the same as How to Automate III model_price = todays_data . loc [ dog_name , track , race_number ][ 'rating' ] ### If you have an issue such as: # Unknown error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). # Then do model_price = todays_data.loc[dog_name,track,race_number]['rating'].item() # Log info before placing bets logging . info ( f 'dog_name: { dog_name } ' ) logging . info ( f 'model_price: { model_price } ' ) logging . info ( f 'market_id: { market_book . market_id } ' ) logging . info ( f 'selection_id: { runner . selection_id } ' ) # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) greyhounds_strategy = FlatBetting ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds markets country_codes = [ \"AU\" ], # Australian markets market_types = [ \"WIN\" ], # Win markets ), max_order_exposure = 50 , # Max exposure per order = 50 max_trade_count = 1 , # Max 1 trade per selection max_live_trade_count = 1 , # Max 1 unmatched trade per selection ) framework . add_strategy ( greyhounds_strategy ) # logger = logging.getLogger(__name__) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : # logger.info(\"No more markets available, terminating framework\") flumine . handler_queue . put ( TerminationEvent ( flumine )) # Add the stopped to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_4.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_4.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_4.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_4.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) framework . add_logging_control ( LiveLoggingControl () ) framework . run () Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"How to Automate 4"},{"location":"api/How_to_Automate_4/#how-to-automate-iv-automate-your-own-model","text":"For this tutorial we will be automating the model that Bruno taught us how to make in the Greyhound Modelling Tutorial . This tutorial also follows on logically from How to Automate III . So, if you haven't already, make sure you take a look at those before continuing here!","title":"How to Automate IV: Automate your own Model"},{"location":"api/How_to_Automate_4/#saving-and-loading-in-our-model","text":"To generate our predictions, we have two options: we can generate our predictions using the same notebook used to train our model then read those predictions into this notebook, or we can save the model and read that model into this notebook. For this tutorial we have chosen to save the model, as it becomes a bit less confusing and easier to manage, although there are some pieces of code we may have to write twice (copy and paste). So first we will need to run the code from the tutorial and then save the model. This is super as simple we can just copy and paste the complete code provided at the end of the tutorial or download from Github . Then we can just run this extra line code (which I have copied from the documentation page ) at the end of the notebook to save the model. from joblib import dump dump(models['LogisticRegression'], 'logistic_regression.joblib') Now that the file is saved, let's read it into this note book: from joblib import load brunos_model = load ( 'logistic_regression.joblib' ) brunos_model LogisticRegression(n_jobs=-1, solver='saga')","title":"Saving and loading in our model"},{"location":"api/How_to_Automate_4/#generating-predictions-for-today","text":"Now that we have the model loaded in, we need the data, to generate our predictions for today's races! # Import libraries required to download today's races import os import sys # Allow imports from src folder module_path = os . path . abspath ( os . path . join ( '../src' )) if module_path not in sys . path : sys . path . append ( module_path ) from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from dateutil import tz from pandas.tseries.offsets import MonthEnd from sklearn.preprocessing import MinMaxScaler import itertools import numpy as np import pandas as pd from nltk.tokenize import regexp_tokenize # settings to display all columns pd . set_option ( \"display.max_columns\" , None ) import fasttrack as ft from dotenv import load_dotenv load_dotenv () True # Validate FastTrack API connection api_key = os . getenv ( 'FAST_TRACK_API_KEY' ,) client = ft . Fasttrack ( api_key ) track_codes = client . listTracks () Valid Security Key # Import race data excluding NZ races au_tracks_filter = list ( track_codes [ track_codes [ 'state' ] != 'NZ' ][ 'track_code' ]) # Time window to import data # First day of the month 46 months back from now date_from = ( datetime . today () - relativedelta ( months = 46 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # First day of previous month date_to = ( datetime . today () - relativedelta ( months = 1 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # Dataframes to populate data with race_details = pd . DataFrame () dog_results = pd . DataFrame () # For each month, either fetch data from API or use local CSV file if we already have downloaded it for start in pd . date_range ( date_from , date_to , freq = 'MS' ): start_date = start . strftime ( \"%Y-%m- %d \" ) end_date = ( start + MonthEnd ( 1 )) . strftime ( \"%Y-%m- %d \" ) try : filename_races = f 'FT_AU_RACES_ { start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { start_date } .csv' filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' print ( f 'Loading data from { start_date } to { end_date } ' ) if os . path . isfile ( filepath_races ): # Load local CSV file month_race_details = pd . read_csv ( filepath_races ) month_dog_results = pd . read_csv ( filepath_dogs ) else : # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( start_date , end_date , au_tracks_filter ) month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) # Combine monthly data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) except : print ( f 'Could not load data from { start_date } to { end_date } ' ) Loading data from 2018-09-01 to 2018-09-30 Loading data from 2018-10-01 to 2018-10-31 Loading data from 2018-11-01 to 2018-11-30 Loading data from 2018-12-01 to 2018-12-31 Loading data from 2019-01-01 to 2019-01-31 Loading data from 2019-02-01 to 2019-02-28 Loading data from 2019-03-01 to 2019-03-31 Loading data from 2019-04-01 to 2019-04-30 Loading data from 2019-05-01 to 2019-05-31 Loading data from 2019-06-01 to 2019-06-30 Loading data from 2019-07-01 to 2019-07-31 Loading data from 2019-08-01 to 2019-08-31 Loading data from 2019-09-01 to 2019-09-30 Loading data from 2019-10-01 to 2019-10-31 Loading data from 2019-11-01 to 2019-11-30 Loading data from 2019-12-01 to 2019-12-31 Loading data from 2020-01-01 to 2020-01-31 Loading data from 2020-02-01 to 2020-02-29 Loading data from 2020-03-01 to 2020-03-31 Loading data from 2020-04-01 to 2020-04-30 Loading data from 2020-05-01 to 2020-05-31 Loading data from 2020-06-01 to 2020-06-30 Loading data from 2020-07-01 to 2020-07-31 Loading data from 2020-08-01 to 2020-08-31 Loading data from 2020-09-01 to 2020-09-30 Loading data from 2020-10-01 to 2020-10-31 Loading data from 2020-11-01 to 2020-11-30 Loading data from 2020-12-01 to 2020-12-31 Loading data from 2021-01-01 to 2021-01-31 Loading data from 2021-02-01 to 2021-02-28 Loading data from 2021-03-01 to 2021-03-31 Loading data from 2021-04-01 to 2021-04-30 Loading data from 2021-05-01 to 2021-05-31 Loading data from 2021-06-01 to 2021-06-30 Loading data from 2021-07-01 to 2021-07-31 Loading data from 2021-08-01 to 2021-08-31 Loading data from 2021-09-01 to 2021-09-30 Loading data from 2021-10-01 to 2021-10-31 Loading data from 2021-11-01 to 2021-11-30 Loading data from 2021-12-01 to 2021-12-31 Loading data from 2022-01-01 to 2022-01-31 c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) Loading data from 2022-02-01 to 2022-02-28 Loading data from 2022-03-01 to 2022-03-31 Loading data from 2022-04-01 to 2022-04-30 Loading data from 2022-05-01 to 2022-05-31 Loading data from 2022-06-01 to 2022-06-30 This piece of code we copied and pasted from the Greyhound Modelling Tutorial is fantastic! It has downloaded/read-in a ton of historic data! There is an issue though! We don't have the data for today's races, and also for any races that has occurred this month. This is because the code above only downloaded data up until the end of last month. For example, if we are in the middle of June, then any races in the first two weeks of June won't be downloaded by the chunk of code above. An issue is that if we download it now, when tomorrow rolls around it won't include the extra races that have finished today. So, the simple but inefficient solution is that every single day we redownload all the races that have already concluded this month. (Ideally you have some sort of database set up or you store and download your data in a daily format instead of the monthly format) race_details . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime Distance RaceGrade Track date 88510 792243395 6 SKY RACING (N/P) STAKE 01:27PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 88511 792243396 7 KURT DONSBERG PHOTOGRAPHY MIXED STAKE 01:44PM 300m Mixed 4/5 Murray Bridge (MBS) 07 Jun 22 88512 792243397 8 GREYHOUNDS AS PETS 02:04PM 300m Grade 5 Final Murray Bridge (MBS) 07 Jun 22 88513 792243398 9 @THEDOGSSA (N/P) STAKE 02:19PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 88514 792243399 10 FOLLOW THEDOGSSA ON TWITTER (N/P) STAKE 02:39PM 300m Restricted Win Murray Bridge (MBS) 07 Jun 22 current_month_start_date = pd . Timestamp . now () . replace ( day = 1 ) . strftime ( \"%Y-%m- %d \" ) current_month_end_date = ( pd . Timestamp . now () . replace ( day = 1 ) + MonthEnd ( 1 )) current_month_end_date = ( current_month_end_date - pd . Timedelta ( '1 day' )) . strftime ( \"%Y-%m- %d \" ) print ( f 'Start date: { current_month_start_date } ' ) print ( f 'End Date: { current_month_end_date } ' ) Start date: 2022-07-01 End Date: 2022-07-30 # Download data for races that have concluded this current month up untill today # Start and end dates for current month current_month_start_date = pd . Timestamp . now () . replace ( day = 1 ) . strftime ( \"%Y-%m- %d \" ) current_month_end_date = ( pd . Timestamp . now () . replace ( day = 1 ) + MonthEnd ( 1 )) current_month_end_date = ( current_month_end_date - pd . Timedelta ( '1 day' )) . strftime ( \"%Y-%m- %d \" ) # Files names filename_races = f 'FT_AU_RACES_ { current_month_start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { current_month_start_date } .csv' # Where to store files locally filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( current_month_start_date , current_month_end_date , au_tracks_filter ) # Save the files locally and replace any out of date fields month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) Getting meets for each date .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:14<00:00, 2.01it/s] Getting historic results details .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162/162 [01:30<00:00, 1.80it/s] dog_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 114215500 1 DR. MURPHY 7.0 10 29.7 $4.10 NaN 4.24 NaN Q/111 0 NaN 4.70 22.84 NaN 356387352 107925 W McMahon 1 131737955 2 MOLLY SPOLLY 8.0 8 27.3 $2.20F NaN 4.24 4.24 M/222 0 NaN 4.72 23.14 NaN 356387352 199516 K Leviston 2 204414097 3 ASTON NARITA 2.0 2 29.2 $4.50 NaN 4.94 0.70 M/343 2 NaN 4.88 23.19 NaN 356387352 101224 K Gorman 3 126744995 4 ONI 6.0 6 25.0 $15.50 NaN 5.70 0.76 S/674 0 NaN 4.95 23.24 NaN 356387352 107925 W McMahon 4 120958941 5 DARCON FLASH 1.0 1 29.3 $31.00 NaN 6.54 0.84 M/765 8 NaN 4.96 23.30 NaN 356387352 125087 R Conway ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 749514 415996416 1 WEBLEC WHIRL 2.0 2 27.6 $4.20 NaN 0.10 NaN 0 0 NaN 4.38 16.75 510.0 792243399 76598 N Loechel 749515 557002281 2 UP THERE BILLY 1.0 1 32.7 $1.80F NaN 0.10 0.14 NaN 0 NaN 4.43 16.76 175.0 792243399 327728 J Trengove 749516 529022935 3 WEBLEC FLAME 6.0 6 31.8 $5.50 NaN 4.25 4.00 NaN 0 NaN 4.48 17.04 140.0 792243399 76598 N Loechel 749517 383604709 4 STRAIGHT BLAZE 8.0 8 34.7 $23.00 NaN 5.00 0.86 NaN 0 NaN 4.61 17.10 115.0 792243399 123529 D Johnstone 749518 529022943 5 WEBLEC MIST 4.0 4 27.6 $7.00 NaN 15.00 10.14 0 0 VINJ(21) 4.67 17.81 0.0 792243399 76598 N Loechel 749519 rows \u00d7 19 columns # This is super important I have spent literally hours before I found out this was causing errors dog_results [ '@id' ] = pd . to_numeric ( dog_results [ '@id' ]) # Append the extra data to our data frames race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) What we are really interested in are races that are scheduled for today as we want to use our model to predict their ratings. So, let's write some code we can run in the morning that will download the data for the day: # Download the data for todays races todays_date = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) todays_races , todays_dogs = client . getFullFormat ( dt = todays_date , tracks = au_tracks_filter ) display ( todays_races . head ( 1 ), todays_dogs . head ( 1 )) Getting meets for each date .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 1.89it/s] Getting dog lineups .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:13<00:00, 1.14s/it] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 PrizeMoney4 PrizeMoney5 PrizeMoney6 PrizeMoney7 PrizeMoney8 GOBIS Hurdle Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips 0 801896110 1 GPP LASER3300 06:44PM 04 Jul 22 08:44AM 385m Maiden $1600 $460 $230 $115 None None None None None None None TRI/QUIN R/D EXACTA PICK4 M None \"KASUMI BERRY (5) is a well bred type and her ... Shepparton 04 Jul 22 None Box Quinella 1,2,4,7 ($10 for 166.67%) 4, 7, 1, 2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceBox DogName BestTime DogHandicap Odds Rating Speed DogComment StartsTOT StartsTTD Suburb Owner Colour Sex Whelped DogGrade DogGOBIS DogPRIZE AgedPrizeMoney Form DamId DamName SireId SireName TrainerId TrainerName RaceId 0 536196758 1 HAVE A SHIRAZ FSH None $4.60 0 None Dam produced the highly talented Flaming rush Starts 0-0-0-0 Trk/Dst 0-0-0-0 Heathcote Paul Ellis BK B 03 Dec 19 M N 0 0 [None, None, None, None, None] 1550070039 Pepper Shiraz -710494 Barcia Bale 117228 Jason Formosa 801896110 # It seems that the todays_races dataframe doesn't have the date column, so let's add that on todays_races [ 'date' ] = pd . Timestamp . now () . strftime ( ' %d %b %y' ) todays_races . head ( 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 PrizeMoney4 PrizeMoney5 PrizeMoney6 PrizeMoney7 PrizeMoney8 GOBIS Hurdle Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips date 0 801896110 1 GPP LASER3300 06:44PM 04 Jul 22 08:44AM 385m Maiden $1600 $460 $230 $115 None None None None None None None TRI/QUIN R/D EXACTA PICK4 M None \"KASUMI BERRY (5) is a well bred type and her ... Shepparton 04 Jul 22 None Box Quinella 1,2,4,7 ($10 for 166.67%) 4, 7, 1, 2 04 Jul 22 # It also seems that in todays_dogs dataframe Box is labeled as RaceBox instead, so let's rename it # We can also see that there are some specific dogs that have \"Res.\" as a suffix of their name, i.e. they are reserve dogs, # We will treat this later todays_dogs = todays_dogs . rename ( columns = { \"RaceBox\" : \"Box\" }) todays_dogs . tail ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id Box DogName BestTime DogHandicap Odds Rating Speed DogComment StartsTOT StartsTTD Suburb Owner Colour Sex Whelped DogGrade DogGOBIS DogPRIZE AgedPrizeMoney Form DamId DamName SireId SireName TrainerId TrainerName RaceId 1061 400500428 5 TEQUILA TALKING 22.63 None None 100 66.425 None Starts 58-11-7-12 Trk/Dst 28-6-5-5 Wolffdene Fives Alive Synd D Wolff,N Brauer BE D 19 Oct 18 4 N 28855 None [{'Place': '2nd', 'FormBox': '5', 'Weight': '3... 255840075 Sivamet -737547 Hostile 93322 Michael Brauer 801490825 1062 525622257 7 REFERRAL FSTD None None 81 61.343 None Starts 51-4-7-6 Trk/Dst 0-0-0-0 Laidley Heights Bad Decisions Synd P O'Reilly,A Pearce,D Henery BD D 22 Oct 19 5 N 13945 None [{'Place': '5th', 'FormBox': '7', 'Weight': '3... 257880044 Lovelace 792880037 Sh Avatar 313314 Andrew Pearce 801490825 1063 566347962 8 STARDUST DREAMS NBT None None 92 61.271 None Starts 22-3-4-2 Trk/Dst 2-0-1-0 Park Ridge Kerri-Lyn Harkness BK D 07 Mar 20 5 N 8240 None [{'Place': '2nd', 'FormBox': '8', 'Weight': '3... 118703516 Ellie Belles 141317074 My Redeemer 127311 Stephen Woods 801490825 # Appending todays data to this months data month_dog_results = pd . concat ([ month_dog_results , todays_dogs ], join = 'outer' )[ month_dog_results . columns ] month_race_details = pd . concat ([ month_race_details , todays_races ], join = 'outer' )[ month_race_details . columns ] # Appending this months data to the rest of our historical data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True )","title":"Generating predictions for today"},{"location":"api/How_to_Automate_4/#cleaning-our-data-and-feature-creation","text":"Originally I thought that since we now that we have all the data we can easily copy and paste the code used in the greyhound modelling tutorial to clean our data and create the features. But after staring at weird predictions and spending hours trying to work out why some things weren't working I realised that for the most part we can copy and paste code, but when working with the live data we do need to make a few changes. I'll point them out when we get to it, but the main things that tripped me up is the data types the FastTrack API gives and that we need a system to work around reserve dogs race_details .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 356387352 1 RUTTER'S BUTCHERY & POULTRY 05:29PM 395m Mixed 6/7 Traralgon 01 Sep 18 1 356387359 2 TAB - WE LOVE A BET 05:47PM 395m Grade 5 Traralgon 01 Sep 18 2 356387358 3 HALEY CONCRETING 06:05PM 395m Grade 5 Traralgon 01 Sep 18 3 356387355 4 R.W & A.R INGLIS ELECTRICIANS 06:29PM 395m Free For All Traralgon 01 Sep 18 4 356387363 5 PRINTMAC 06:45PM 525m Grade 5 Traralgon 01 Sep 18 ... ... ... ... ... ... ... ... ... 89359 801490821 5 SENNACHIE @ STUD - STEVE WHITE 08:13PM 520m Grade 5 Heat Albion Park 04 Jul 22 89360 801490822 6 ORSON ALLEN @ METICULOUS LODGE 08:35PM 520m Grade 5 Heat Albion Park 04 Jul 22 89361 801490823 7 SKY RACING 08:53PM 600m Mixed 4/5 Albion Park 04 Jul 22 89362 801490824 8 BORGBET TIPPING SERVICE 09:15PM 520m Mixed 3/4 Albion Park 04 Jul 22 89363 801490825 9 TIGGERLONG TONK @ STUD 09:37PM 395m Mixed 4/5 Albion Park 04 Jul 22 89364 rows \u00d7 8 columns The first thing that tripped me up was when FastTrack_DogId for live data was in a string format, and because everything looks like it works, it took ages to find this error. So, let's make sure we deal with it here using: dog_results['FastTrack_DogId'] = pd.to_numeric(dog_results['FastTrack_DogId']) ## Cleanse and normalise the data # Clean up the race dataset race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) # Clean up the dogs results dataset dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) # New line of code (rest of this code chunk is copied from bruno's code) dog_results [ 'FastTrack_DogId' ] = pd . to_numeric ( dog_results [ 'FastTrack_DogId' ]) # Combine dogs results with race attributes dog_results = dog_results . merge ( race_details , how = 'left' , on = 'FastTrack_RaceId' ) # Convert StartPrice to probability dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x is None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' )) if isinstance ( x , str ) else x ) dog_results [ 'StartPrice_probability' ] = ( 1 / dog_results [ 'StartPrice' ]) . fillna ( 0 ) dog_results [ 'StartPrice_probability' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x / x . sum ()) # Discard entries without results (scratched or did not finish) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results [ 'Box' ] = dog_results [ 'Box' ] . astype ( int ) # Clean up other attributes dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'SplitMargin' ] = dog_results [ 'SplitMargin' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) . fillna ( 0 ) dog_results [ 'Place' ] = pd . to_numeric ( dog_results [ 'Place' ] . apply ( lambda x : x . replace ( \"=\" , \"\" ) if isinstance ( x , str ) else 0 ), errors = 'coerce' ) . fillna ( 0 ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x == 1 else 0 ) # Normalise some of the raw values dog_results [ 'Prizemoney_norm' ] = np . log10 ( dog_results [ 'Prizemoney' ] + 1 ) / 12 dog_results [ 'Place_inv' ] = ( 1 / dog_results [ 'Place' ]) . fillna ( 0 ) dog_results [ 'Place_log' ] = np . log10 ( dog_results [ 'Place' ] + 1 ) . fillna ( 0 ) dog_results [ 'RunSpeed' ] = ( dog_results [ 'RunTime' ] / dog_results [ 'Distance' ]) . fillna ( 0 ) ## Generate features using raw data # Calculate median winner time per track/distance win_results = dog_results [ dog_results [ 'win' ] == 1 ] median_win_time = pd . DataFrame ( data = win_results [ win_results [ 'RunTime' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'RunTime' ] . median ()) . rename ( columns = { \"RunTime\" : \"RunTime_median\" }) . reset_index () median_win_split_time = pd . DataFrame ( data = win_results [ win_results [ 'SplitMargin' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'SplitMargin' ] . median ()) . rename ( columns = { \"SplitMargin\" : \"SplitMargin_median\" }) . reset_index () median_win_time . head () # Calculate track speed index median_win_time [ 'speed_index' ] = ( median_win_time [ 'RunTime_median' ] / median_win_time [ 'Distance' ]) median_win_time [ 'speed_index' ] = MinMaxScaler () . fit_transform ( median_win_time [[ 'speed_index' ]]) median_win_time . head () # Compare dogs finish time with median winner time dog_results = dog_results . merge ( median_win_time , on = [ 'Track' , 'Distance' ], how = 'left' ) dog_results = dog_results . merge ( median_win_split_time , on = [ 'Track' , 'Distance' ], how = 'left' ) # Normalise time comparison dog_results [ 'RunTime_norm' ] = ( dog_results [ 'RunTime_median' ] / dog_results [ 'RunTime' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'RunTime_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'RunTime_norm' ]]) dog_results [ 'SplitMargin_norm' ] = ( dog_results [ 'SplitMargin_median' ] / dog_results [ 'SplitMargin' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'SplitMargin_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'SplitMargin_norm' ]]) dog_results . head () # Calculate box winning percentage for each track/distance box_win_percent = pd . DataFrame ( data = dog_results . groupby ([ 'Track' , 'Distance' , 'Box' ])[ 'win' ] . mean ()) . rename ( columns = { \"win\" : \"box_win_percent\" }) . reset_index () # Add to dog results dataframe dog_results = dog_results . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) # Display example of barrier winning probabilities print ( box_win_percent . head ( 8 )) Track Distance Box box_win_percent 0 Albion Park 331 1 0.198089 1 Albion Park 331 2 0.152116 2 Albion Park 331 3 0.127354 3 Albion Park 331 4 0.126605 4 Albion Park 331 5 0.111058 5 Albion Park 331 6 0.109304 6 Albion Park 331 7 0.105310 7 Albion Park 331 8 0.115146 The second thing that we need to add is related to reserve dogs, and this took me ages to come to this solution, but if you have a better one, please submit a pull request. Basically, a single greyhound can be a reserve dog for multiple races on the same day. They each appear as a new row in our data frame. For example, 'COACH BEARD' is a reserve dog for three different races today (2022-07-04): dog_results [ dog_results [ 'FastTrack_DogId' ] == 592253143 ] . tail ()[[ 'date_dt' , 'Place' , 'DogName' , 'RaceNum' , 'Track' , 'Distance' , 'win' , 'Prizemoney_norm' , 'Place_inv' , 'Place_log' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date_dt Place DogName RaceNum Track Distance win Prizemoney_norm Place_inv Place_log 616959 2022-05-04 1.0 COACH BEARD 3 Bendigo 425 1 0.0 1.0 0.301030 627625 2022-05-16 2.0 COACH BEARD 5 Shepparton 450 0 0.0 0.5 0.477121 654587 2022-07-04 0.0 COACH BEARD 8 Ballarat 450 0 0.0 inf 0.000000 654588 2022-07-04 0.0 COACH BEARD 8 Ballarat 450 0 0.0 inf 0.000000 654589 2022-07-04 0.0 COACH BEARD 8 Ballarat 450 0 0.0 inf 0.000000 When we try lag our data by using .shift(1) like in Bruno's original code it will produce the wrong values for our features. In the above example only the first race today (2022-06-07) Townsville Race 4 (the second row) will have correct data but all the rows under it will have incorrectly calculated features. We need each of the following rows to be the same as the second row. The solution that I have come up with is a little bit complicated, but it gets the job done: # Please submit a pull request if you have a better solution temp = rolling_result.reset_index() temp = temp[temp['date_dt'] == pd.Timestamp.now().normalize()] temp.groupby(['FastTrack_DogId','date_dt']).first() rolling_result.loc[pd.IndexSlice[:, pd.Timestamp.now().normalize()], :] = temp.groupby(['FastTrack_DogId','date_dt']).first() Basically, for each greyhound we can just take the first row of data (which is correct) and set the rest of today's races to have the same value # Generate rolling window features dataset = dog_results . copy () dataset = dataset . set_index ([ 'FastTrack_DogId' , 'date_dt' ]) . sort_index () # Use rolling window of 28, 91 and 365 days rolling_windows = [ '28D' , '91D' , '365D' ] # Features to use for rolling windows calculation features = [ 'RunTime_norm' , 'SplitMargin_norm' , 'Place_inv' , 'Place_log' , 'Prizemoney_norm' ] # Aggregation functions to apply aggregates = [ 'min' , 'max' , 'mean' , 'median' , 'std' ] # Keep track of generated feature names feature_cols = [ 'speed_index' , 'box_win_percent' ] for rolling_window in rolling_windows : print ( f 'Processing rolling window { rolling_window } ' ) rolling_result = ( dataset . reset_index ( level = 0 ) . sort_index () . groupby ( 'FastTrack_DogId' )[ features ] . rolling ( rolling_window ) . agg ( aggregates ) . groupby ( level = 0 ) # Thanks to Brett for finding this! . shift ( 1 ) ) # My own dodgey code to work with reserve dogs temp = rolling_result . reset_index () temp = temp [ temp [ 'date_dt' ] == pd . Timestamp . now () . normalize ()] temp . groupby ([ 'FastTrack_DogId' , 'date_dt' ]) . first () rolling_result . loc [ pd . IndexSlice [:, pd . Timestamp . now () . normalize ()], :] = temp . groupby ([ 'FastTrack_DogId' , 'date_dt' ]) . first () # Generate list of rolling window feature names (eg: RunTime_norm_min_365D) agg_features_cols = [ f ' { f } _ { a } _ { rolling_window } ' for f , a in itertools . product ( features , aggregates )] # Add features to dataset dataset [ agg_features_cols ] = rolling_result # Keep track of generated feature names feature_cols . extend ( agg_features_cols ) Processing rolling window 28D c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) Processing rolling window 91D c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) Processing rolling window 365D c:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance. obj = obj._drop_axis(labels, axis, level=level, errors=errors) # Replace missing values with 0 dataset . fillna ( 0 , inplace = True ) display ( dataset . head ( 8 )) # Only keep data after 2018-12-01 model_df = dataset . reset_index () feature_cols = np . unique ( feature_cols ) . tolist () model_df = model_df [ model_df [ 'date_dt' ] >= '2018-12-01' ] # This line was originally part of Bruno's tutorial, but we don't run it in this script # model_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D FastTrack_DogId date_dt -2143477291 2018-09-02 7.0 YOU TELAM ANFY 1 1 31.2 5.2 0.0 7.0 0.57 8 0 8.0 7.48 19.85 0.0 354469749 8462 A Bunney 12 BRISGREYS.COM 09:01PM 331 GRADE 5 PATHWAY NON-PENALTY Albion Park 02 Sep 18 0.161184 0 0.0 0.142857 0.903090 0.059970 19.17 0.600092 7.18 0.328715 0.299465 0.198089 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 2018-09-16 4.0 YOU TELAM ANFY 1 1 31.0 61.0 0.0 10.0 5.0 8 0 8.0 7.43 19.75 0.0 360928569 8462 A Bunney 11 SKY RACING 05:24PM 331 Grade 5 Albion Park 16 Sep 18 0.013847 0 0.0 0.250000 0.698970 0.059668 19.17 0.600092 7.18 0.353165 0.331763 0.198089 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.903090 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.903090 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 0.328715 0.328715 0.328715 0.328715 0.000000 0.299465 0.299465 0.299465 0.299465 0.000000 0.142857 0.142857 0.142857 0.142857 0.000000 0.90309 0.903090 0.903090 0.903090 0.000000 0.0 0.0 0.0 0.0 0.0 2018-10-07 7.0 YOU TELAM ANFY 1 1 30.5 71.0 0.0 8.25 4.0 7 0 7.0 7.42 19.71 0.0 367774713 8462 A Bunney 11 SKY RACING 08:27PM 331 Grade 5 Albion Park 07 Oct 18 0.011604 0 0.0 0.142857 0.903090 0.059547 19.17 0.600092 7.18 0.363014 0.338275 0.198089 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.353165 0.340940 0.340940 0.017288 0.299465 0.331763 0.315614 0.315614 0.022838 0.142857 0.250000 0.196429 0.196429 0.075761 0.69897 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 2018-10-21 5.0 YOU TELAM ANFY 7 9 29.8 26.0 0.0 5.25 0.29 6 0 6.0 7.46 19.85 0.0 370420123 8462 A Bunney 12 ZILLMERE SPORTS 08:55PM 331 GRADE 5 PATHWAY NON-PENALTY Albion Park 21 Oct 18 0.032235 0 0.0 0.200000 0.778151 0.059970 19.17 0.600092 7.18 0.328715 0.312332 0.105310 0.353165 0.363014 0.358089 0.358089 0.006964 0.331763 0.338275 0.335019 0.335019 0.004605 0.142857 0.250000 0.196429 0.196429 0.075761 0.698970 0.903090 0.801030 0.801030 0.144335 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.348298 0.353165 0.017659 0.299465 0.338275 0.323168 0.331763 0.020784 0.142857 0.250000 0.178571 0.142857 0.061859 0.698970 0.903090 0.835050 0.903090 0.117849 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.348298 0.353165 0.017659 0.299465 0.338275 0.323168 0.331763 0.020784 0.142857 0.250000 0.178571 0.142857 0.061859 0.69897 0.903090 0.835050 0.903090 0.117849 0.0 0.0 0.0 0.0 0.0 2018-11-18 5.0 YOU TELAM ANFY 1 1 30.2 41.0 0.0 6.25 3.14 6 0 6.0 7.43 19.86 0.0 378695693 8462 A Bunney 10 ZILLMERE SPORTS 08:16PM 331 Grade 5 Albion Park 18 Nov 18 0.020215 0 0.0 0.200000 0.778151 0.060000 19.17 0.600092 7.18 0.326284 0.331763 0.198089 0.328715 0.363014 0.345865 0.345865 0.024253 0.312332 0.338275 0.325304 0.325304 0.018344 0.142857 0.200000 0.171429 0.171429 0.040406 0.778151 0.903090 0.840621 0.840621 0.088345 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.343402 0.340940 0.017429 0.299465 0.338275 0.320459 0.322048 0.017814 0.142857 0.250000 0.183929 0.171429 0.051632 0.698970 0.903090 0.820825 0.840621 0.100341 0.0 0.0 0.0 0.0 0.0 0.328715 0.363014 0.343402 0.340940 0.017429 0.299465 0.338275 0.320459 0.322048 0.017814 0.142857 0.250000 0.183929 0.171429 0.051632 0.69897 0.903090 0.820825 0.840621 0.100341 0.0 0.0 0.0 0.0 0.0 2019-06-23 5.0 YOU TELAM ANFY 1 1 29.6 51.0 0.0 5.75 0.86 8 0 8.0 7.44 19.66 0.0 445056131 8462 A Bunney 9 GREYHOUND ADOPTION PROGRAM 07:54PM 331 Grade 5 Albion Park 23 Jun 19 0.016271 0 0.0 0.200000 0.778151 0.059396 19.17 0.600092 7.18 0.375381 0.325269 0.198089 0.326284 0.326284 0.326284 0.326284 0.000000 0.331763 0.331763 0.331763 0.331763 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.326284 0.363014 0.339979 0.328715 0.016924 0.299465 0.338275 0.322720 0.331763 0.016234 0.142857 0.250000 0.187143 0.200000 0.045288 0.698970 0.903090 0.812290 0.778151 0.088969 0.0 0.0 0.0 0.0 0.0 0.326284 0.363014 0.339979 0.328715 0.016924 0.299465 0.338275 0.322720 0.331763 0.016234 0.142857 0.250000 0.187143 0.200000 0.045288 0.69897 0.903090 0.812290 0.778151 0.088969 0.0 0.0 0.0 0.0 0.0 2019-06-30 8.0 YOU TELAM ANFY 4 10 29.5 51.0 0.0 17.25 2.0 7 0 7.0 11.38 24.16 0.0 448789428 8462 A Bunney 7 SKY RACING 07:45PM 395 Open Albion Park 30 Jun 19 0.015995 0 0.0 0.125000 0.954243 0.061165 22.85 0.588509 10.58 0.228891 0.148506 0.130206 0.375381 0.375381 0.375381 0.375381 0.000000 0.325269 0.325269 0.325269 0.325269 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.375381 0.375381 0.375381 0.375381 0.000000 0.325269 0.325269 0.325269 0.325269 0.000000 0.200000 0.200000 0.200000 0.200000 0.000000 0.778151 0.778151 0.778151 0.778151 0.000000 0.0 0.0 0.0 0.0 0.0 0.326284 0.375381 0.345879 0.340940 0.020929 0.299465 0.338275 0.323145 0.328516 0.014558 0.142857 0.250000 0.189286 0.200000 0.040846 0.69897 0.903090 0.806601 0.778151 0.080787 0.0 0.0 0.0 0.0 0.0 2019-08-25 6.0 YOU TELAM ANFY 4 4 29.5 14.0 0.0 5.0 0.57 3 0 3.0 7.33 19.72 0.0 465432748 8462 A Bunney 9 FABREGAS @ METICULOUS LODGE 07:40PM 331 Masters Grade 5 Albion Park 25 Aug 19 0.058684 0 0.0 0.166667 0.845098 0.059577 19.17 0.600092 7.18 0.360548 0.397681 0.126605 0.228891 0.375381 0.302136 0.302136 0.103585 0.148506 0.325269 0.236887 0.236887 0.124990 0.125000 0.200000 0.162500 0.162500 0.053033 0.778151 0.954243 0.866197 0.866197 0.124515 0.0 0.0 0.0 0.0 0.0 0.228891 0.375381 0.302136 0.302136 0.103585 0.148506 0.325269 0.236887 0.236887 0.124990 0.125000 0.200000 0.162500 0.162500 0.053033 0.778151 0.954243 0.866197 0.866197 0.124515 0.0 0.0 0.0 0.0 0.0 0.228891 0.375381 0.329166 0.328715 0.048169 0.148506 0.338275 0.298196 0.325269 0.067332 0.125000 0.250000 0.180102 0.200000 0.044505 0.69897 0.954243 0.827692 0.778151 0.092481 0.0 0.0 0.0 0.0 0.0","title":"Cleaning our data and feature creation"},{"location":"api/How_to_Automate_4/#generate-predictions","text":"Now this is the part that gets a bit hairy, so I am going to split it up into two parts. The good thing is that the coding will remain relatively simple. The two things that I want to do is place live bets and save our predictions so that we can use them in a simulator we will create in the Part V . Let's save our historical ratings for our simulator first as its quick and straight forward and then move on to placing live bets:","title":"Generate predictions"},{"location":"api/How_to_Automate_4/#getting-data-ready-for-our-simulator","text":"Feeding our predictions through the simulator is entirely optional, but, in my opinion it is where the real sauce is made. The idea is that if we are testing our model live, we can also use the simulator to test what would happen if we tested different staking methodologies, market timings and bet placement to optimise our model. This way you can have a model but test out different strategies to optimise model performance. The thing is, I have had a play with the simulator already and we can't simulate market_catalogue unless you have recorded it yourself (which is what I'll be using to get market_id and selection_id to place live bets). The simulator we will use later on will only take your ratings, market_id and selection_id, so we need our data in a similar format to what we had in how automate III . In other words, since we don't have market_catalogue in the simulator, we need another way to get the market_id and selection_id. My hacky work around is to generate the probabilities like normal (since the data is historical), we don't need to deal with reserve dogs and scratching's, then get the market_id and selection_id from the Betfair datascience greyhound model by merging on DogName and date. We can take the code we wrote in how to automate III that downloads the greyhound ratings and convert that into a function that downloads the ratings for a date range. # Generate predictions like normal # Range of dates that we want to simulate later '2022-03-01' to '2022-04-01' todays_data = model_df [( model_df [ 'date_dt' ] >= pd . Timestamp ( '2022-03-01' ) . strftime ( '%Y-%m- %d ' )) & ( model_df [ 'date_dt' ] < pd . Timestamp ( '2022-04-01' ) . strftime ( '%Y-%m- %d ' ))] dog_win_probabilities = brunos_model . predict_proba ( todays_data [ feature_cols ])[:, 1 ] todays_data [ 'prob_LogisticRegression' ] = dog_win_probabilities todays_data [ 'renormalise_prob' ] = todays_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x / x . sum ()) todays_data [ 'rating' ] = 1 / todays_data [ 'renormalise_prob' ] todays_data = todays_data . sort_values ( by = 'date_dt' ) todays_data C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['prob_LogisticRegression'] = dog_win_probabilities C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum()) C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/3121846001.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['rating'] = 1/todays_data['renormalise_prob'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression renormalise_prob rating 526490 523685389 2022-03-01 1.0 JOSEPH RUMBLE 8 8 30.1 2.5 0.0 8.25 0 0 0 0 0.00 21.83 1365.0 764579619 91264 B Belford 1 TAB 06:55PM 380 Novice Non Penalty Townsville 01 Mar 22 0.318829 1 0.261288 1.000000 0.301030 0.057447 22.10 0.641825 7.56 0.561842 0.000000 0.123070 0.514985 0.514985 0.514985 0.514985 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 1.00 1.000000 1.000000 0.000000 0.301030 0.301030 0.301030 0.301030 0.000000 0.264578 0.264578 2.645776e-01 0.264578 0.000000 0.386805 0.514985 0.450895 0.450895 0.090637 0.482301 0.482301 0.482301 0.482301 0.000000 0.500000 1.00 0.750000 0.750000 0.353553 0.30103 0.477121 0.389076 0.389076 0.124515 0.238161 0.264578 0.251369 0.251369 0.018679 0.386805 0.514985 0.450895 0.450895 0.090637 0.482301 0.482301 0.482301 0.482301 0.000000 0.500000 1.00 0.750000 0.750000 0.353553 0.30103 0.477121 0.389076 0.389076 0.124515 0.238161 0.264578 0.251369 0.251369 0.018679 0.360558 0.321076 3.114527 494469 482776246 2022-03-01 1.0 BLAZING NENNA 4 4 25.7 2.3 0.0 3.31 0 Q/11 0 0 10.27 23.30 0.0 764625202 115912 M Delbridge 6 CHS GROUP HT3 05:37PM 410 Grade 5 Heat Horsham 01 Mar 22 0.365634 1 0.000000 1.000000 0.301030 0.056829 23.46 0.480327 10.39 0.534335 0.558423 0.131261 0.500000 0.513187 0.504396 0.500000 0.007613 0.464830 0.633333 0.570108 0.612161 0.091786 0.250000 1.00 0.500000 0.250000 0.433013 0.301030 0.698970 0.566323 0.698970 0.229751 0.000000 0.000000 1.628327e-15 0.000000 0.000000 0.307377 0.609439 0.508348 0.524719 0.074167 0.373358 0.651515 0.543480 0.584034 0.093516 0.142857 1.00 0.535714 0.500000 0.324194 0.30103 0.903090 0.528325 0.477121 0.198314 0.000000 0.000000 0.000000 0.000000 0.000000 0.233840 0.609439 0.493044 0.512184 0.074069 0.000000 0.651515 0.515072 0.539757 0.120687 0.125000 1.00 0.443328 0.333333 0.314640 0.30103 0.954243 0.607971 0.602060 0.219526 0.000000 0.000000 0.000000 0.000000 0.000000 0.204496 0.145077 6.892877 583640 578899991 2022-03-01 4.0 RIVER RAGING 2 2 30.3 13.8 0.0 3.71 1.13 M/3 0 0 6.89 20.36 0.0 764592641 283109 L Dalziel 1 FOLLOW @GRV_NEWS ON TWITTER 11:08AM 350 Maiden Healesville 01 Mar 22 0.061390 0 0.000000 0.250000 0.698970 0.058171 19.56 0.250778 6.64 0.303536 0.318578 0.138427 0.312992 0.312992 0.312992 0.312992 0.000000 0.346715 0.346715 0.346715 0.346715 0.000000 0.250000 0.25 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000 0.241288 0.312992 0.285038 0.303536 0.033489 0.251704 0.346715 0.290635 0.290765 0.038193 0.142857 0.25 0.201905 0.200000 0.048369 0.69897 0.903090 0.784856 0.778151 0.090009 0.000000 0.000000 0.000000 0.000000 0.000000 0.241288 0.312992 0.285038 0.303536 0.033489 0.251704 0.346715 0.290635 0.290765 0.038193 0.142857 0.25 0.201905 0.200000 0.048369 0.69897 0.903090 0.784856 0.778151 0.090009 0.000000 0.000000 0.000000 0.000000 0.000000 0.065977 0.102443 9.761504 385698 419530408 2022-03-01 5.0 FREENEY 8 8 25.1 14.0 0.0 8.25 1.14 0 0 0 0.00 22.47 20.0 764579628 63422 R Lound 10 BURDEKIN VET CLINIC 09:55PM 380 Grade 5 Townsville 01 Mar 22 0.058903 0 0.110185 0.200000 0.778151 0.059132 22.10 0.641825 7.56 0.417668 0.000000 0.123070 0.389381 0.406750 0.398065 0.398065 0.012282 0.519920 0.519920 0.519920 0.519920 0.000000 0.125000 0.25 0.187500 0.187500 0.088388 0.698970 0.954243 0.826606 0.826606 0.180505 0.110185 0.161208 1.356966e-01 0.135697 0.036079 0.378587 0.520445 0.438208 0.435239 0.050906 0.519920 0.519920 0.519920 0.519920 0.000000 0.125000 1.00 0.398810 0.250000 0.304154 0.30103 0.954243 0.636079 0.698970 0.229354 0.086783 0.256326 0.171119 0.161208 0.059824 0.277345 0.547967 0.448106 0.459606 0.067232 0.486807 0.519920 0.509978 0.516591 0.015762 0.125000 1.00 0.437302 0.333333 0.287424 0.30103 0.954243 0.595411 0.602060 0.199176 0.000000 0.256326 0.161721 0.181581 0.080195 0.148800 0.136846 7.307467 453065 451768903 2022-03-01 5.0 ENCOURAGING 2 2 22.1 15.0 0.0 6.0 0.29 455 0 455 10.35 22.57 0.0 764579685 70111 B Young 12 GOSSIE TIGERS GOOD TIMES 10:39PM 388 Non Graded Gosford 01 Mar 22 0.058204 0 0.000000 0.200000 0.778151 0.058170 22.39 0.564085 10.19 0.460124 0.422705 0.166667 0.462217 0.582400 0.498498 0.474688 0.056299 0.413211 0.605769 0.539241 0.568992 0.086316 0.200000 0.50 0.300000 0.250000 0.135401 0.000000 0.778151 0.530643 0.698970 0.317165 0.000000 0.227091 4.541824e-02 0.000000 0.101558 0.435567 0.582400 0.510280 0.506062 0.043547 0.335526 0.605769 0.507137 0.513845 0.080824 0.200000 1.00 0.559375 0.500000 0.321914 0.00000 0.778151 0.476169 0.477121 0.204293 0.000000 0.269225 0.170899 0.227091 0.115477 0.435567 0.620208 0.519683 0.508949 0.050995 0.335526 0.676056 0.528287 0.534247 0.085502 0.200000 1.00 0.592857 0.500000 0.310625 0.00000 0.778151 0.460377 0.477121 0.185631 0.000000 0.269225 0.187400 0.227091 0.105954 0.310750 0.189764 5.269698 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 269364 328634961 2022-03-31 6.0 LUNARAY 7 7 28.8 18.8 0.0 6.96 0.24 M/766 0 0 6.99 25.92 0.0 771803534 117062 V Mileto 6 SHEPPARTON WORKWEAR & SAFETY 01:19PM 450 Grade 5 T3 Shepparton 31 Mar 22 0.044487 0 0.000000 0.166667 0.845098 0.057600 25.55 0.404304 6.70 0.428627 0.292561 0.118601 0.370377 0.461165 0.415771 0.415771 0.064197 0.320144 0.341040 0.330592 0.330592 0.014776 0.200000 0.25 0.225000 0.225000 0.035355 0.698970 0.778151 0.738561 0.738561 0.055990 0.000000 0.000000 2.720046e-15 0.000000 0.000000 0.282853 0.461165 0.399299 0.415352 0.060173 0.000000 0.397661 0.280607 0.320144 0.129212 0.125000 1.00 0.379762 0.250000 0.297826 0.30103 0.954243 0.644364 0.698970 0.211158 0.000000 0.159040 0.022720 0.000000 0.060112 0.280719 0.529528 0.410459 0.414407 0.058641 0.000000 0.441003 0.297743 0.312132 0.111641 0.125000 1.00 0.324033 0.250000 0.245639 0.30103 0.954243 0.687225 0.698970 0.181101 0.000000 0.177795 0.010526 0.000000 0.041488 0.075047 0.097817 10.223167 538082 534921234 2022-03-31 6.0 QUINNLEY BALE 6 6 26.7 18.0 0.0 7.5 1.14 56 0 56 5.47 17.83 0.0 771810563 98781 S Rhodes 12 WATCH LIVE ON SPORTSBET 10:46PM 297 Grade 5 Dapto 31 Mar 22 0.046607 0 0.000000 0.166667 0.845098 0.060034 17.19 0.593790 5.37 0.320527 0.408592 0.120301 0.288301 0.347716 0.318008 0.318008 0.042013 0.303220 0.390710 0.346965 0.346965 0.061865 0.142857 0.50 0.321429 0.321429 0.252538 0.477121 0.903090 0.690106 0.690106 0.301205 0.000000 0.221975 1.109875e-01 0.110988 0.156960 0.288301 0.471082 0.384562 0.389432 0.082219 0.303220 0.426606 0.368411 0.371909 0.052814 0.142857 1.00 0.473214 0.375000 0.381742 0.30103 0.903090 0.595053 0.588046 0.262071 0.000000 0.264698 0.121668 0.110988 0.141569 0.236247 0.500000 0.387665 0.390897 0.075799 0.000000 0.481447 0.342572 0.353107 0.117160 0.125000 1.00 0.374454 0.250000 0.278973 0.00000 0.954243 0.605624 0.650515 0.270858 0.000000 0.264698 0.053752 0.000000 0.100704 0.068536 0.065307 15.312345 363059 403640676 2022-03-31 3.0 SAINT CHARLOTTE 5 5 27.7 18.0 0.0 4.5 0.14 0 0 0 10.87 23.59 140.0 771539517 67189 W Wilson 8 EXCHANGE PRINTERS (N/P) STAKE 01:52PM 400 Restricted Win Mount Gambier 31 Mar 22 0.041828 0 0.179102 0.333333 0.602060 0.058975 23.39 0.696399 10.98 0.457609 0.550598 0.147799 0.401509 0.534438 0.467974 0.467974 0.093995 0.328496 0.527473 0.427984 0.427984 0.140698 0.125000 1.00 0.562500 0.562500 0.618718 0.301030 0.954243 0.627636 0.627636 0.461891 0.000000 0.225702 1.128509e-01 0.112851 0.159595 0.401509 0.534438 0.475213 0.486146 0.041384 0.328496 0.564576 0.467640 0.504558 0.078033 0.125000 1.00 0.440833 0.333333 0.320552 0.30103 0.954243 0.603688 0.602060 0.220083 0.000000 0.225702 0.135590 0.179102 0.095348 0.257933 0.534438 0.457113 0.477655 0.064917 0.212446 0.564576 0.443305 0.445950 0.089104 0.125000 1.00 0.358408 0.250000 0.274086 0.30103 0.954243 0.660966 0.698970 0.195529 0.000000 0.225702 0.106690 0.172038 0.098393 0.155050 0.100326 9.967472 362095 403093108 2022-03-31 3.0 SILVER SANDALS 5 5 27.0 10.0 0.0 2.5 2.29 42 0 42 5.58 31.67 800.0 771539501 87148 S Lawrance 2 SKY RACING 06:40PM 520 Masters Grade 5 Ipswich 31 Mar 22 0.083134 0 0.241969 0.333333 0.602060 0.060904 30.79 0.823159 5.42 0.361067 0.356631 0.095370 0.435748 0.439595 0.437671 0.437671 0.002720 0.354196 0.368046 0.361121 0.361121 0.009793 0.333333 1.00 0.666667 0.666667 0.471405 0.301030 0.602060 0.451545 0.451545 0.212860 0.207730 0.275374 2.415521e-01 0.241552 0.047832 0.330867 0.507902 0.426973 0.435748 0.052271 0.198046 0.460029 0.340578 0.340419 0.073412 0.166667 1.00 0.498485 0.333333 0.340462 0.30103 0.845098 0.560166 0.602060 0.203530 0.110185 0.275374 0.199906 0.207730 0.064087 0.297445 0.507902 0.398206 0.402791 0.056453 0.156690 0.460029 0.325128 0.317851 0.074465 0.125000 1.00 0.328665 0.225000 0.260181 0.30103 0.954243 0.694669 0.738561 0.197928 0.000000 0.275374 0.127103 0.138606 0.097759 0.081817 0.110114 9.081527 565804 556974861 2022-03-31 2.0 ORSON LAURIE 3 3 30.4 3.1 0.0 0.75 0.86 112 0 112 8.32 23.04 530.0 771810580 125472 E Harris 4 RIVERINA STOCKFEEDS 08:04PM 411 Free For All Casino 31 Mar 22 0.275736 0 0.227091 0.500000 0.477121 0.056058 23.37 0.418680 8.62 0.571615 0.680288 0.062500 0.351105 0.583040 0.453627 0.445632 0.092450 0.487805 0.651134 0.562563 0.574442 0.064323 0.142857 1.00 0.362698 0.183333 0.339607 0.000000 0.903090 0.592798 0.778151 0.343479 0.000000 0.267033 7.058912e-02 0.000000 0.121104 0.351105 0.588161 0.473661 0.483607 0.085113 0.465422 0.664141 0.554578 0.574442 0.067472 0.142857 1.00 0.543492 0.500000 0.401886 0.00000 0.903090 0.545322 0.477121 0.298264 0.000000 0.275104 0.142790 0.167586 0.124768 0.351105 0.588161 0.479065 0.494281 0.085020 0.465422 0.664141 0.557285 0.574442 0.065012 0.142857 1.00 0.572024 0.500000 0.404685 0.00000 0.903090 0.530952 0.477121 0.294808 0.000000 0.275104 0.149961 0.224986 0.124372 0.150460 0.123089 8.124181 26438 rows \u00d7 117 columns def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" , \"meetings.races.number\" : \"RaceNum\" , \"meetings.name\" : \"Track\" , \"meetings.races.runners.name\" : \"DogName\" } ) # iggy_df = iggy_df[['market_id','selection_id','rating']] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) iggy_df [ 'date_dt' ] = date # Set market_id and selection_id as index for easy referencing # iggy_df = iggy_df.set_index(['market_id','selection_id']) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022-03-01' , end = '2022-04-01' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) iggy_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Track meetings.bfExchangeEventId meetings.races.name RaceNum market_id meetings.races.comment selection_id meetings.races.runners.number DogName rating date_dt 0 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154446 1 Magic Rogue 11.07 2022-03-01 1 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154447 2 Youre Off 6.10 2022-03-01 2 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 42352031 3 Castle Town 6.60 2022-03-01 3 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 43154448 4 Buckle Up Aumond 8.02 2022-03-01 4 Devonport 3 R1 452m Gr6 1 1.195395419 NaN 42413752 5 Was That Then 17.14 2022-03-01 ... ... ... ... ... ... ... ... ... ... ... ... 906 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 27692794 3 Mercator Closer 24.49 2022-04-01 907 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 36057267 4 Kooringa Lucy 2.61 2022-04-01 908 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 25540155 6 Shanjo Prince 213.53 2022-04-01 909 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 39788790 7 Lots Of Chatter 2.10 2022-04-01 910 Wentworth Park 11 R9 520m Heat 9 1.196971034 NaN 30681833 8 Zipping Brady 18.25 2022-04-01 27346 rows \u00d7 11 columns # format DogNames to merge todays_data [ 'DogName' ] = todays_data [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) iggy_df [ 'DogName' ] = iggy_df [ 'DogName' ] . str . upper () # Merge backtest = iggy_df [[ 'market_id' , 'selection_id' , 'DogName' , 'date_dt' ]] . merge ( todays_data [[ 'rating' , 'DogName' , 'date_dt' ]], how = 'inner' , on = [ 'DogName' , 'date_dt' ]) backtest .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id DogName date_dt rating 0 1.195395419 43154446 MAGIC ROGUE 2022-03-01 8.137725 1 1.195395419 43154447 YOURE OFF 2022-03-01 6.051752 2 1.195395419 42352031 CASTLE TOWN 2022-03-01 9.768546 3 1.195395419 43154448 BUCKLE UP AUMOND 2022-03-01 13.656089 4 1.195395419 42413752 WAS THAT THEN 2022-03-01 11.941057 ... ... ... ... ... ... 25540 1.196921144 39309141 RANAKO BALE 2022-03-31 3.975601 25541 1.196921144 39348645 NEVAEH BALE 2022-03-31 3.917458 25542 1.196921144 26870111 INGA MIA 2022-03-31 7.459386 25543 1.196921144 42472271 WINNIE COASTER 2022-03-31 7.046953 25544 1.196921144 40022831 ASTON HEBE 2022-03-31 4.603341 25545 rows \u00d7 5 columns # Save predictions for if we want to backtest/simulate it later backtest . to_csv ( 'backtest.csv' , index = False ) # Csv format # backtest.to_pickle('backtest.pkl') # pickle format (faster, but can't open in excel) Perfect, with our hacky solution we have managed to merge around a months' worth of data relatively quickly and saved it in a csv format. With all the merging it seems we have only lost around 1000 - 2000 rows of data out of 27,000 rows of data, which seems only a small price to pay.","title":"Getting data ready for our simulator"},{"location":"api/How_to_Automate_4/#getting-data-ready-for-placing-live-bets","text":"Placing live bets is pretty simple but we have one issue. FastTrack Data alone is unable to tell us how many greyhounds will run in the race. For example, this race later today (2022-07-04) has 8 runners + 2 reserves: todays_data [ todays_data [ 'FastTrack_RaceId' ] == '798906744' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceName RaceTime Distance RaceGrade date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression DogName_bf Track RaceNum YOU SEE LINA Cannington 1 530411826 2022-07-04 0.0 YOU SEE LINA 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 10408 Michael McLennan FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.167785 0.363135 0.419607 0.395672 0.404274 0.029202 0.395833 0.432534 0.410499 0.403130 0.019428 0.200000 0.500000 0.300000 0.200000 0.173205 0.477121 0.778151 0.677808 0.778151 0.173800 0.0 0.213623 7.120781e-02 0.000000 0.123336 0.184971 0.419607 0.324576 0.346645 0.092382 0.220812 0.432534 0.338647 0.353089 0.084555 0.166667 0.500000 0.316667 0.266667 0.153116 0.477121 0.845098 0.659617 0.690106 0.162743 0.0 0.213623 0.101436 0.092505 0.111554 0.179269 0.419607 0.303822 0.326906 0.075138 0.133803 0.432534 0.315318 0.310345 0.082022 0.125000 0.500000 0.247937 0.200000 0.121737 0.477121 0.954243 0.743299 0.778151 0.156196 0.0 0.213623 0.085118 0.000000 0.095461 0.115534 BELLA LINA Cannington 1 547605028 2022-07-04 0.0 BELLA LINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 83951 Rodney Noden FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.111111 0.228705 0.307236 0.273491 0.284534 0.040413 0.220812 0.285592 0.245073 0.228814 0.035318 0.166667 0.200000 0.188889 0.200000 0.019245 0.778151 0.845098 0.800467 0.778151 0.038652 0.0 0.000000 3.700743e-16 0.000000 0.000000 0.158046 0.307236 0.246575 0.264844 0.050600 0.029221 0.285592 0.203061 0.228814 0.091884 0.142857 0.200000 0.187075 0.200000 0.023119 0.778151 0.903090 0.805563 0.778151 0.049718 0.0 0.000000 0.000000 0.000000 0.000000 0.084276 0.307236 0.225040 0.234235 0.061784 0.029221 0.366864 0.240440 0.236842 0.088850 0.125000 0.250000 0.174702 0.166667 0.034400 0.698970 0.954243 0.835377 0.845098 0.073324 0.0 0.142298 0.008894 0.000000 0.035574 0.031221 PENNY KEEPING Cannington 1 561780971 2022-07-04 0.0 PENNY KEEPING 8 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 68481 Bradley Cook FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.100671 0.076736 0.356220 0.228759 0.253321 0.141352 0.424645 0.513538 0.474906 0.486535 0.045573 0.125000 0.500000 0.250000 0.125000 0.216506 0.477121 0.954243 0.795202 0.954243 0.275466 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.076736 0.356220 0.236342 0.256207 0.116406 0.424645 0.513538 0.467291 0.465490 0.040207 0.125000 0.500000 0.223214 0.133929 0.184716 0.477121 0.954243 0.822174 0.928666 0.231296 0.0 0.000000 0.000000 0.000000 0.000000 0.076736 0.356220 0.236342 0.256207 0.116406 0.424645 0.513538 0.467291 0.465490 0.040207 0.125000 0.500000 0.223214 0.133929 0.184716 0.477121 0.954243 0.822174 0.928666 0.231296 0.0 0.000000 0.000000 0.000000 0.000000 0.049673 WHAT A PHOENIX Cannington 1 603189486 2022-07-04 0.0 WHAT A PHOENIX 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 10408 Michael McLennan FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.108392 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000 0.000000 0.000000 0.155537 0.318460 0.227087 0.207265 0.083251 0.000000 0.338235 0.150847 0.114306 0.172053 0.125000 0.250000 0.166667 0.125000 0.072169 0.698970 0.954243 0.869152 0.954243 0.147382 0.0 0.000000 0.000000 0.000000 0.000000 0.040679 WHAT A QUIZ Cannington 1 603189487 2022-07-04 0.0 WHAT A QUIZ 2 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 72510 Barry McPherson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.141892 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.233563 0.233563 0.233563 0.233563 0.000000 0.302920 0.302920 0.302920 0.302920 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.043975 WHAT A SHAKER Cannington 1 603189986 2022-07-04 0.0 WHAT A SHAKER 4 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 72510 Barry McPherson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.125000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.282892 0.282892 0.282892 0.282892 0.000000 0.268116 0.268116 0.268116 0.268116 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 0.059174 WIZARDS LEGEND Cannington 1 614056673 2022-07-04 0.0 WIZARD'S LEGEND Res. 10 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 127397 Colin Bainbridge FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.000000 0.307944 0.341758 0.326417 0.327983 0.015946 0.206724 0.288937 0.259703 0.271576 0.036365 0.166667 0.250000 0.204167 0.200000 0.034359 0.698970 0.845098 0.775093 0.778151 0.059761 0.0 0.151629 3.790717e-02 0.000000 0.075814 0.089468 0.341758 0.269683 0.313202 0.093934 0.103960 0.377622 0.240904 0.244173 0.081242 0.125000 0.250000 0.194792 0.200000 0.042477 0.698970 0.954243 0.797104 0.778151 0.084209 0.0 0.151629 0.037164 0.000000 0.068832 0.089468 0.341758 0.275533 0.303754 0.077507 0.103960 0.377622 0.242702 0.244173 0.071625 0.125000 0.333333 0.204266 0.200000 0.058218 0.602060 0.954243 0.785504 0.778151 0.099650 0.0 0.151629 0.037412 0.000000 0.067696 0.019388 WIZARDS DRAMA Cannington 1 614057677 2022-07-04 0.0 WIZARD'S DRAMA Res. 9 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 127397 Colin Bainbridge FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.000000 0.225948 0.344591 0.271951 0.245316 0.063648 0.150000 0.269231 0.221376 0.244898 0.063000 0.142857 0.500000 0.280952 0.200000 0.191840 0.477121 0.903090 0.719454 0.778151 0.218967 0.0 0.209986 6.999522e-02 0.000000 0.121235 0.086870 0.344591 0.234575 0.238391 0.084399 0.088816 0.269231 0.188587 0.189288 0.067980 0.142857 0.500000 0.229762 0.171429 0.139270 0.477121 0.903090 0.777252 0.840621 0.169536 0.0 0.209986 0.059278 0.000000 0.094057 0.036656 0.344591 0.200202 0.228706 0.100907 0.069444 0.269231 0.170954 0.162215 0.071007 0.125000 0.500000 0.229613 0.171429 0.130210 0.477121 0.954243 0.777477 0.840621 0.171435 0.0 0.209986 0.044459 0.000000 0.084096 0.014393 DASHING ONYX Cannington 1 626191408 2022-07-04 0.0 DASHING ONYX 6 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 67839 Graeme Hall FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.144876 0.262045 0.359113 0.314231 0.321535 0.048944 0.244898 0.377622 0.319292 0.335355 0.067805 0.166667 0.333333 0.277778 0.333333 0.096225 0.602060 0.845098 0.683073 0.602060 0.140318 0.0 0.185009 1.233393e-01 0.185009 0.106815 0.229498 0.359113 0.293047 0.291790 0.058241 0.244898 0.377622 0.318715 0.326170 0.055374 0.142857 0.333333 0.244048 0.250000 0.103555 0.602060 0.903090 0.738077 0.723579 0.158833 0.0 0.185009 0.092505 0.092505 0.106815 0.229498 0.359113 0.293047 0.291790 0.058241 0.244898 0.377622 0.318715 0.326170 0.055374 0.142857 0.333333 0.244048 0.250000 0.103555 0.602060 0.903090 0.738077 0.723579 0.158833 0.0 0.185009 0.092505 0.092505 0.106815 0.096959 WINTER RAIN Cannington 1 637972981 2022-07-04 0.0 WINTER RAIN 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 798906744 27464 Jennifer Thompson FREE ENTRY TABTOUCH PARK 01:32PM 275 Maiden 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 16.21 0.777366 5.58 0.0 0.0 0.101754 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000e+00 0.000000 0.000000 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000 0.000000 0.000000 0.239673 0.371004 0.305338 0.305338 0.092865 0.133803 0.350640 0.242221 0.242221 0.153327 0.166667 0.500000 0.333333 0.333333 0.235702 0.477121 0.845098 0.661110 0.661110 0.260199 0.0 0.000000 0.000000 0.000000 0.000000 0.101402 If we predict probabilities and renormalise now, we will calculate incorrect probabilities. I've spent a really long time thinking about this and testing different methods that didn't work or weren't optimal. The best solution (and least complicated) that I have come up with is to predict probabilities on the FastTrack data first. Then a few minutes before the jump when all the lineups have been confirmed we use market_catalogue from the Betfair API to merge our predicted probabilities, merging on DogName , Track and RaceNum . If we merge on these three fields, it will bypass any issues with reserve dogs and scratchings. Then we can renormalise probabilities live within Flumine. # Select todays data todays_data = model_df [ model_df [ 'date_dt' ] == pd . Timestamp . now () . strftime ( '%Y-%m- %d ' )] # Generate runner win predictions dog_win_probabilities = brunos_model . predict_proba ( todays_data [ feature_cols ])[:, 1 ] todays_data [ 'prob_LogisticRegression' ] = dog_win_probabilities # We no longer renomralise probability in this chunk of code, do it in Flumine instead # todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum()) # todays_data['rating'] = 1/todays_data['renormalise_prob'] # todays_data = todays_data.sort_values(by = 'date_dt') todays_data C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/2638603781.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['prob_LogisticRegression'] = dog_win_probabilities .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceNum RaceName RaceTime Distance RaceGrade Track date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression 44514 148673258 2022-07-04 0.0 SPEEDY MARINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455740 65928 Dawn Lee 5 LADBROKES BLENDED BETS 1-3 WIN 04:37PM 307 Grade 5 Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.136986 0.317232 0.317232 0.317232 0.317232 0.000000 0.173267 0.173267 0.173267 0.173267 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 6.020600e-01 0.602060 0.000000 0.212109 0.212109 2.121089e-01 0.212109 0.000000e+00 0.317232 0.332373 0.324803 0.324803 0.010706 0.173267 0.210579 0.191923 0.191923 0.026383 0.166667 0.333333 0.250000 0.250000 0.117851 0.602060 0.845098 7.235790e-01 0.723579 0.171854 0.000000 0.212109 0.106054 0.106054 0.149984 0.236982 0.371585 0.306603 0.317232 0.052095 0.173267 0.406600 0.242140 0.210579 0.096894 0.125000 0.333333 0.186905 0.166667 0.083715 0.602060 0.954243 8.299177e-01 0.845098 0.135269 0.000000 0.212109 0.042422 0.000000 0.094858 0.073607 54463 161977365 2022-07-04 0.0 FILTHY PHANTOM 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 110385 Tony Hinrichsen 5 GIDDY-UP (N/P) STAKE 07:27PM 342 Masters Angle Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.124590 0.386907 0.455919 0.427165 0.420839 0.028492 0.314004 0.500000 0.408769 0.394027 0.082256 0.166667 0.333333 0.220000 0.200000 0.064979 0.602060 0.845098 7.563224e-01 0.778151 0.090977 0.000000 0.182760 3.655208e-02 0.000000 8.173293e-02 0.386907 0.572783 0.449667 0.436026 0.053935 0.210921 0.551665 0.403088 0.394027 0.092070 0.142857 1.000000 0.291209 0.200000 0.235613 0.000000 0.903090 6.692431e-01 0.778151 0.257193 0.000000 0.249855 0.057150 0.000000 0.095131 0.277002 0.572783 0.459854 0.456897 0.059514 0.210921 0.616496 0.443040 0.443820 0.089192 0.142857 1.000000 0.464354 0.333333 0.320517 0.000000 0.903090 5.716763e-01 0.602060 0.227821 0.000000 0.249855 0.121936 0.180363 0.106520 0.128265 77950 196384049 2022-07-04 0.0 HOUND 'EM DOWN 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490787 313281 Steven Winstanley 3 ZIPPING GARTH @ STUD 0-2 WIN 07:36PM 565 Mixed Maiden and Grade Five Maitland 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 31.875 0.342029 13.795 0.0 0.0 0.200000 0.233442 0.277637 0.261405 0.273136 0.024321 0.218310 0.316690 0.270554 0.276662 0.049474 0.142857 0.200000 0.161905 0.142857 0.032991 0.778151 0.903090 8.614437e-01 0.903090 0.072133 0.000000 0.000000 4.440892e-16 0.000000 0.000000e+00 0.194132 0.355912 0.279655 0.277637 0.046001 0.218310 0.316690 0.271078 0.275180 0.032574 0.125000 0.200000 0.158862 0.142857 0.026641 0.778151 0.954243 8.681223e-01 0.903090 0.060784 0.000000 0.000000 0.000000 0.000000 0.000000 0.194132 0.462627 0.328446 0.316396 0.054098 0.218310 0.346154 0.292413 0.296715 0.036166 0.125000 0.333333 0.187245 0.166667 0.054481 0.000000 0.954243 7.520173e-01 0.845098 0.239572 0.000000 0.212109 0.014373 0.000000 0.050118 0.070298 121171 230053393 2022-07-04 0.0 CAWBOURNE CROSS 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 104699 Lisa Rasmussen 5 GIDDY-UP (N/P) STAKE 07:27PM 342 Masters Angle Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.169811 0.336395 0.413836 0.386878 0.410402 0.043753 0.500000 0.500000 0.500000 0.500000 0.000000 0.142857 0.250000 0.186508 0.166667 0.056260 0.698970 0.903090 8.157193e-01 0.845098 0.105184 0.000000 0.000000 3.700743e-16 0.000000 2.533726e-17 0.336395 0.491121 0.420107 0.412119 0.059452 0.401747 0.506477 0.480510 0.500000 0.044239 0.142857 0.500000 0.273810 0.250000 0.129975 0.477121 0.903090 7.042182e-01 0.698970 0.155860 0.000000 0.188140 0.058566 0.000000 0.091070 0.280126 0.505631 0.403460 0.405048 0.064662 0.329857 0.610664 0.472084 0.500000 0.079724 0.142857 0.500000 0.269048 0.225000 0.125458 0.477121 0.903090 7.115827e-01 0.738561 0.144209 0.000000 0.205324 0.060368 0.000000 0.090871 0.179535 142478 243599770 2022-07-04 0.0 SKAIKRU 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455741 83214 Robert Sonter 6 BATHURST RSL CLUB 04:59PM 450 Grade 5 Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.159574 0.392354 0.392354 0.392354 0.392354 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 1.998401e-15 0.000000 0.000000e+00 0.293318 0.392354 0.329286 0.302186 0.054798 0.238956 0.238956 0.238956 0.238956 0.000000 0.142857 0.333333 0.242063 0.250000 0.095486 0.602060 0.903090 7.347067e-01 0.698970 0.153664 0.000000 0.167027 0.055676 0.000000 0.096433 0.285293 0.392354 0.317943 0.303341 0.036066 0.162722 0.258454 0.223927 0.237266 0.042031 0.125000 0.333333 0.202551 0.200000 0.070985 0.602060 0.954243 7.942519e-01 0.778151 0.120113 0.000000 0.167027 0.023861 0.000000 0.063130 0.092948 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 599603 673011364 2022-07-04 0.0 FERAL AGENT 8 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 96421 Derek Kerr 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.177215 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 1.478939e-01 0.212109 1.284491e-01 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 0.147894 0.212109 0.128449 0.409480 0.425999 0.418593 0.420299 0.008391 0.411700 0.431973 0.421836 0.421836 0.014335 0.250000 1.000000 0.527778 0.333333 0.411074 0.301030 0.698970 5.340200e-01 0.602060 0.207512 0.000000 0.231573 0.147894 0.212109 0.128449 0.194287 599956 694776805 2022-07-04 0.0 WENDY MAREE 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490818 307271 Brian Baker 2 GARRARD'S HORSE AND HOUND 07:11PM 520 Novice Non Penalty Albion Park 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 30.220 0.634509 5.630 0.0 0.0 0.136417 0.325934 0.410627 0.368281 0.368281 0.059887 0.168325 0.336770 0.252547 0.252547 0.119108 0.200000 0.250000 0.225000 0.225000 0.035355 0.698970 0.778151 7.385606e-01 0.738561 0.055990 0.110185 0.193690 1.519376e-01 0.151938 5.904714e-02 0.325934 0.410627 0.366778 0.363772 0.042426 0.168325 0.344322 0.283139 0.336770 0.099504 0.200000 0.250000 0.216667 0.200000 0.028868 0.698970 0.778151 7.517575e-01 0.778151 0.045715 0.110185 0.193690 0.138020 0.110185 0.048212 0.305980 0.410627 0.355981 0.361146 0.036561 0.168325 0.344322 0.274549 0.279411 0.067105 0.142857 0.333333 0.221032 0.200000 0.064636 0.602060 0.903090 7.564290e-01 0.778151 0.100056 0.110185 0.218690 0.142187 0.110185 0.050203 0.123793 600100 707214702 2022-07-04 0.0 WARDEN JODIE Res. 10 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 75264 Sam Simonetta 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.000000 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000 0.000000 0.000000 0.146202 0.146202 0.146202 0.146202 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.125000 0.125000 0.125000 0.000000 0.000000 0.954243 1.908485e-01 0.000000 0.426750 0.000000 0.000000 0.000000 0.000000 0.000000 0.014319 600105 707215693 2022-07-04 0.0 MYSTERY ANNE 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455737 75264 Sam Simonetta 2 ZIPPING GARTH @ STUD MAIDEN 03:28PM 307 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.125874 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.331265 0.331265 0.331265 0.331265 0.000000 0.473776 0.473776 0.473776 0.473776 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 6.989700e-01 0.698970 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.090248 600106 707215696 2022-07-04 0.0 BROKEN PROMISES 2 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455736 75264 Sam Simonetta 1 WELCOME GBOTA MAIDEN 03:07PM 450 Maiden Bathurst 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.139785 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.708944e-14 0.000000 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000e+00 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.642331e-14 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2.842171e-14 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.060767 1704 rows \u00d7 115 columns Before we merge, let's do some minor formatting changes to the FastTrack names so we can match onto the Betfair names. Betfair excludes all apostrophes and full stops in their naming convention, so we'll create a Betfair equivalent dog name on the dataset removing these characters. We also need to do this for the tracks, sometimes FastTrack will name tracks differently to Betfair e.g., Sandown Park from Betfair is known as Sandown (SAP) in the FastTrack database. # Prepare data for easy reference in flumine todays_data [ 'DogName_bf' ] = todays_data [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) todays_data . replace ({ 'Sandown (SAP)' : 'Sandown Park' }, regex = True , inplace = True ) todays_data = todays_data . set_index ([ 'DogName_bf' , 'Track' , 'RaceNum' ]) todays_data . head () C:\\Users\\zhoui\\AppData\\Local\\Temp/ipykernel_25584/90992895.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy todays_data['DogName_bf'] = todays_data['DogName'].apply(lambda x: x.replace(\"'\", \"\").replace(\".\", \"\").replace(\"Res\", \"\").strip()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId date_dt Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney FastTrack_RaceId TrainerId TrainerName RaceName RaceTime Distance RaceGrade date StartPrice_probability win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm box_win_percent RunTime_norm_min_28D RunTime_norm_max_28D RunTime_norm_mean_28D RunTime_norm_median_28D RunTime_norm_std_28D SplitMargin_norm_min_28D SplitMargin_norm_max_28D SplitMargin_norm_mean_28D SplitMargin_norm_median_28D SplitMargin_norm_std_28D Place_inv_min_28D Place_inv_max_28D Place_inv_mean_28D Place_inv_median_28D Place_inv_std_28D Place_log_min_28D Place_log_max_28D Place_log_mean_28D Place_log_median_28D Place_log_std_28D Prizemoney_norm_min_28D Prizemoney_norm_max_28D Prizemoney_norm_mean_28D Prizemoney_norm_median_28D Prizemoney_norm_std_28D RunTime_norm_min_91D RunTime_norm_max_91D RunTime_norm_mean_91D RunTime_norm_median_91D RunTime_norm_std_91D SplitMargin_norm_min_91D SplitMargin_norm_max_91D SplitMargin_norm_mean_91D SplitMargin_norm_median_91D SplitMargin_norm_std_91D Place_inv_min_91D Place_inv_max_91D Place_inv_mean_91D Place_inv_median_91D Place_inv_std_91D Place_log_min_91D Place_log_max_91D Place_log_mean_91D Place_log_median_91D Place_log_std_91D Prizemoney_norm_min_91D Prizemoney_norm_max_91D Prizemoney_norm_mean_91D Prizemoney_norm_median_91D Prizemoney_norm_std_91D RunTime_norm_min_365D RunTime_norm_max_365D RunTime_norm_mean_365D RunTime_norm_median_365D RunTime_norm_std_365D SplitMargin_norm_min_365D SplitMargin_norm_max_365D SplitMargin_norm_mean_365D SplitMargin_norm_median_365D SplitMargin_norm_std_365D Place_inv_min_365D Place_inv_max_365D Place_inv_mean_365D Place_inv_median_365D Place_inv_std_365D Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D prob_LogisticRegression DogName_bf Track RaceNum SPEEDY MARINA Bathurst 5 148673258 2022-07-04 0.0 SPEEDY MARINA 5 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455740 65928 Dawn Lee LADBROKES BLENDED BETS 1-3 WIN 04:37PM 307 Grade 5 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 17.830 0.628105 7.880 0.0 0.0 0.136986 0.317232 0.317232 0.317232 0.317232 0.000000 0.173267 0.173267 0.173267 0.173267 0.000000 0.333333 0.333333 0.333333 0.333333 0.000000 0.602060 0.602060 0.602060 0.602060 0.000000 0.212109 0.212109 2.121089e-01 0.212109 0.000000e+00 0.317232 0.332373 0.324803 0.324803 0.010706 0.173267 0.210579 0.191923 0.191923 0.026383 0.166667 0.333333 0.250000 0.250000 0.117851 0.602060 0.845098 0.723579 0.723579 0.171854 0.0 0.212109 0.106054 0.106054 0.149984 0.236982 0.371585 0.306603 0.317232 0.052095 0.173267 0.406600 0.242140 0.210579 0.096894 0.125000 0.333333 0.186905 0.166667 0.083715 0.602060 0.954243 0.829918 0.845098 0.135269 0.0 0.212109 0.042422 0.000000 0.094858 0.073607 FILTHY PHANTOM Angle Park 5 161977365 2022-07-04 0.0 FILTHY PHANTOM 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 110385 Tony Hinrichsen GIDDY-UP (N/P) STAKE 07:27PM 342 Masters 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.124590 0.386907 0.455919 0.427165 0.420839 0.028492 0.314004 0.500000 0.408769 0.394027 0.082256 0.166667 0.333333 0.220000 0.200000 0.064979 0.602060 0.845098 0.756322 0.778151 0.090977 0.000000 0.182760 3.655208e-02 0.000000 8.173293e-02 0.386907 0.572783 0.449667 0.436026 0.053935 0.210921 0.551665 0.403088 0.394027 0.092070 0.142857 1.000000 0.291209 0.200000 0.235613 0.000000 0.903090 0.669243 0.778151 0.257193 0.0 0.249855 0.057150 0.000000 0.095131 0.277002 0.572783 0.459854 0.456897 0.059514 0.210921 0.616496 0.443040 0.443820 0.089192 0.142857 1.000000 0.464354 0.333333 0.320517 0.000000 0.903090 0.571676 0.602060 0.227821 0.0 0.249855 0.121936 0.180363 0.106520 0.128265 HOUND EM DOWN Maitland 3 196384049 2022-07-04 0.0 HOUND 'EM DOWN 7 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801490787 313281 Steven Winstanley ZIPPING GARTH @ STUD 0-2 WIN 07:36PM 565 Mixed Maiden and Grade Five 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 31.875 0.342029 13.795 0.0 0.0 0.200000 0.233442 0.277637 0.261405 0.273136 0.024321 0.218310 0.316690 0.270554 0.276662 0.049474 0.142857 0.200000 0.161905 0.142857 0.032991 0.778151 0.903090 0.861444 0.903090 0.072133 0.000000 0.000000 4.440892e-16 0.000000 0.000000e+00 0.194132 0.355912 0.279655 0.277637 0.046001 0.218310 0.316690 0.271078 0.275180 0.032574 0.125000 0.200000 0.158862 0.142857 0.026641 0.778151 0.954243 0.868122 0.903090 0.060784 0.0 0.000000 0.000000 0.000000 0.000000 0.194132 0.462627 0.328446 0.316396 0.054098 0.218310 0.346154 0.292413 0.296715 0.036166 0.125000 0.333333 0.187245 0.166667 0.054481 0.000000 0.954243 0.752017 0.845098 0.239572 0.0 0.212109 0.014373 0.000000 0.050118 0.070298 CAWBOURNE CROSS Angle Park 5 230053393 2022-07-04 0.0 CAWBOURNE CROSS 3 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801448232 104699 Lisa Rasmussen GIDDY-UP (N/P) STAKE 07:27PM 342 Masters 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 19.675 0.533632 7.730 0.0 0.0 0.169811 0.336395 0.413836 0.386878 0.410402 0.043753 0.500000 0.500000 0.500000 0.500000 0.000000 0.142857 0.250000 0.186508 0.166667 0.056260 0.698970 0.903090 0.815719 0.845098 0.105184 0.000000 0.000000 3.700743e-16 0.000000 2.533726e-17 0.336395 0.491121 0.420107 0.412119 0.059452 0.401747 0.506477 0.480510 0.500000 0.044239 0.142857 0.500000 0.273810 0.250000 0.129975 0.477121 0.903090 0.704218 0.698970 0.155860 0.0 0.188140 0.058566 0.000000 0.091070 0.280126 0.505631 0.403460 0.405048 0.064662 0.329857 0.610664 0.472084 0.500000 0.079724 0.142857 0.500000 0.269048 0.225000 0.125458 0.477121 0.903090 0.711583 0.738561 0.144209 0.0 0.205324 0.060368 0.000000 0.090871 0.179535 SKAIKRU Bathurst 6 243599770 2022-07-04 0.0 SKAIKRU 1 0 0 0.0 0.0 0 0 0 0 0 0.0 0.0 0.0 801455741 83214 Robert Sonter BATHURST RSL CLUB 04:59PM 450 Grade 5 04 Jul 22 0.0 0 0.0 inf 0.0 0.0 26.000 0.576406 15.450 0.0 0.0 0.159574 0.392354 0.392354 0.392354 0.392354 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.250000 0.250000 0.250000 0.250000 0.000000 0.698970 0.698970 0.698970 0.698970 0.000000 0.000000 0.000000 1.998401e-15 0.000000 0.000000e+00 0.293318 0.392354 0.329286 0.302186 0.054798 0.238956 0.238956 0.238956 0.238956 0.000000 0.142857 0.333333 0.242063 0.250000 0.095486 0.602060 0.903090 0.734707 0.698970 0.153664 0.0 0.167027 0.055676 0.000000 0.096433 0.285293 0.392354 0.317943 0.303341 0.036066 0.162722 0.258454 0.223927 0.237266 0.042031 0.125000 0.333333 0.202551 0.200000 0.070985 0.602060 0.954243 0.794252 0.778151 0.120113 0.0 0.167027 0.023861 0.000000 0.063130 0.092948 If you look closely at the data frame above you might notice that for reserve dogs, they will have a Box number of 9 or 10. I personally don't really watch greyhound racing (I just like numbers), but I'm pretty confident that there is only ever a max of around 8 greyhounds per race. In which case we will need to adjust it somehow. I didn't notice this issue for quite a while, but the good thing is the website gives us the info we need to adjust: We can see that Rhinestone Ash is a reserve dog and has the number 9, if you click on rules, you can see what Box it is starting from: The problem is, my webscraping is pretty poor, and it would take significant time for me to learn it. But after going through the documentation again, changes to boxes are actually available through the API under the clarifications attribute of marketDescription . You will be able to access this within Flumine as market.market_catalogue.description.clarifications , but it's a bit weird. It returns box changes as a string that looks like this: Originally I had planned to leave this article as it is since, I've never worked with anything like this before and its already getting pretty long, however huge shoutout to Betfair Quants community and especially Brett who provided his solution to working with box changes. Brett's solution is amazing, there is only one problem, currently our code is structured so that we generate our predictions in the morning well before the race starts. To implement the above fix, we need to generate our predictions just before the race starts to incorporate the Box information. This means we need to write a little bit more code to make it happen, but we are almost there. So now my plan to update the old data and generate probabilities just before the race. So now just before the jump my code structure will look like this: - pull any data on box changes from the Betfair API - convert the box change data into a dataframe named runners_df using the Brett's code - in my original dataframe named todays_data replace any Box data with runners_df data, otherwise leave it untouched - then merge the box_win_percent dataframe back onto the todays_data dataframe - now we can predict probabilities again and then renormalise them It may sound a little complicated but as we already have Brett's code there is only a few extra lines of code we need to write. This is what we will add into our Flumine strategy along with Brett's code: # Running Brett's code gives us a nice dataframe named runners_df that we can work with # Replace any old Box info in our original dataframe with data available in runners_df = runners_df.set_index('runner_name') todays_data.loc[(runners_df.index[runners_df.index.isin(dog_names)],track,race_number),'Box'] = runners_df.loc[runners_df.index.isin(dog_names),'Box'].to_list() # Merge box_win_percent data onto todays_data todays_data = todays_data.merge(box_win_percent, on=['Track', 'Distance', 'Box'], how='left') # Merge box_win_percentage back on: todays_data = todays_data.drop(columns = 'box_win_percentage', axis = 1) todays_data = todays_data.merge(box_win_percent, on = ['Track', 'Distance','Box'], how = 'left') # Generate probabilities using Bruno's model todays_data.loc[(dog_names,track,race_number),'prob_LogisticRegression'] = brunos_model.predict_proba(todays_data.loc[(dog_names,track,race_number)][feature_cols])[:,1] # renomalise probabilities probabilities = todays_data.loc[dog_names,track,race_number]['prob_LogisticRegression'] todays_data.loc[(dog_names,track,race_number),'renormalised_prob'] = probabilities/probabilities.sum() # convert probaiblities to ratings todays_data.loc[(dog_names,track,race_number),'rating'] = 1/todays_data.loc[dog_names,track,race_number]['renormalised_prob'] Now everything is done, and we can finally move onto placing our bets","title":"Getting data ready for placing live bets"},{"location":"api/How_to_Automate_4/#automating-our-predictions","text":"Now that we have our data nicely set up. We can reference our probabilities by getting the DogName, Track and RaceNum from the Betfair polling API and then renormalised probabilities to calculate ratings with only a few lines of code. Then the rest is the same as How to Automate III # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) # Import libraries and logging from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import re import pandas as pd import numpy as np import datetime import logging logging . basicConfig ( filename = 'how_to_automate_4.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) Let's create a new class for our strategy called FlatBetting that finds the best available to back and lay price 60 seconds before the jump. If any of those prices have value, we will place a flat bet for /$/5 at those prices. This code is almost the same as How to Automate III Since we are now editing our todays_data dataframe inside our Flumine strategy we will also need to convert todays_data to a global variable which is a simple one liner: global todays_data I also wanted to call out one gotcha that, Brett found that is almost impossible to find unless you are keeping a close eye on your logs. Sometimes the polling API and streaming API doesn't match up when there are scratchings, so we need to check if it does: # Check the polling API and streaming API matches up (sometimes it doesn't) if runner_cata.selection_id == runner.selection_id: class FlatBetting ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Convert dataframe to a global variable global todays_data # At the 60 second mark: if market . seconds_to_start < 60 and market_book . inplay == False : # get the list of dog_names, name of the track/venue and race_number/RaceNum from Betfair Polling API dog_names = [] track = market . market_catalogue . event . venue race_number = market . market_catalogue . market_name . split ( ' ' , 1 )[ 0 ] # comes out as R1/R2/R3 .. etc race_number = re . sub ( \"[^0-9]\" , \"\" , race_number ) # only keep the numbers for runner_cata in market . market_catalogue . runners : dog_name = runner_cata . runner_name . split ( ' ' , 1 )[ 1 ] . upper () dog_names . append ( dog_name ) # Check if there are box changes, if there are then use Brett's code if market . market_catalogue . description . clarifications != None : # Brett's code to get Box changes: my_string = market . market_catalogue . description . clarifications . replace ( \"<br> Dog\" , \"<br>Dog\" ) pattern1 = r '(?<=<br>Dog ).+?(?= starts)' pattern2 = r \"(?<=\\bbox no. )(\\w+)\" runners_df = pd . DataFrame ( regexp_tokenize ( my_string , pattern1 ), columns = [ 'runner_name' ]) runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . astype ( str ) # Remove dog name from runner_number runners_df [ 'runner_number' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [:( x . find ( \" \" ) - 1 )] . upper ()) # Remove dog number from runner_name runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [( x . find ( \" \" ) + 1 ):] . upper ()) runners_df [ 'Box' ] = regexp_tokenize ( my_string , pattern2 ) # Replace any old Box info in our original dataframe with data available in runners_df runners_df = runners_df . set_index ( 'runner_name' ) todays_data . loc [( runners_df . index [ runners_df . index . isin ( dog_names )], track , race_number ), 'Box' ] = runners_df . loc [ runners_df . index . isin ( dog_names ), 'Box' ] . to_list () # Merge box_win_percentage back on: todays_data = todays_data . drop ( columns = 'box_win_percentage' , axis = 1 ) todays_data = todays_data . reset_index () . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) . set_index ([ 'DogName_bf' , 'Track' , 'RaceNum' ]) # Generate probabilities using Bruno's model todays_data . loc [( dog_names , track , race_number ), 'prob_LogisticRegression' ] = brunos_model . predict_proba ( todays_data . loc [( dog_names , track , race_number )][ feature_cols ])[:, 1 ] # renomalise probabilities probabilities = todays_data . loc [ dog_names , track , race_number ][ 'prob_LogisticRegression' ] todays_data . loc [( dog_names , track , race_number ), 'renormalised_prob' ] = probabilities / probabilities . sum () # convert probaiblities to ratings todays_data . loc [( dog_names , track , race_number ), 'rating' ] = 1 / todays_data . loc [ dog_names , track , race_number ][ 'renormalised_prob' ] # Use both the polling api (market.catalogue) and the streaming api at once: for runner_cata , runner in zip ( market . market_catalogue . runners , market_book . runners ): # Check the polling api and streaming api matches up (sometimes it doesn't) if runner_cata . selection_id == runner . selection_id : # Get the dog_name from polling api then reference our data for our model rating dog_name = runner_cata . runner_name . split ( ' ' , 1 )[ 1 ] . upper () # Rest is the same as How to Automate III model_price = todays_data . loc [ dog_name , track , race_number ][ 'rating' ] ### If you have an issue such as: # Unknown error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). # Then do model_price = todays_data.loc[dog_name,track,race_number]['rating'].item() # Log info before placing bets logging . info ( f 'dog_name: { dog_name } ' ) logging . info ( f 'model_price: { model_price } ' ) logging . info ( f 'market_id: { market_book . market_id } ' ) logging . info ( f 'selection_id: { runner . selection_id } ' ) # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) As the model we have built is a greyhound model for Australian racing let's point our strategy to Australian greyhound win markets greyhounds_strategy = FlatBetting ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds markets country_codes = [ \"AU\" ], # Australian markets market_types = [ \"WIN\" ], # Win markets ), max_order_exposure = 50 , # Max exposure per order = 50 max_trade_count = 1 , # Max 1 trade per selection max_live_trade_count = 1 , # Max 1 unmatched trade per selection ) framework . add_strategy ( greyhounds_strategy ) And add our auto-terminate and bet logging from the previous tutorials: # import logging import datetime from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent # logger = logging.getLogger(__name__) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : # logger.info(\"No more markets available, terminating framework\") flumine . handler_queue . put ( TerminationEvent ( flumine )) # Add the stopped to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) import os import csv import logging from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_4.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_4.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_4.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_4.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) framework . add_logging_control ( LiveLoggingControl () ) framework . run ()","title":"Automating our predictions"},{"location":"api/How_to_Automate_4/#conclusion-and-next-steps","text":"Boom! We now have an automated script that will downloads all the data we need in the morning, generates a set of predictions, place flat stakes bets, logs all bets and switches itself off at the end of the day. All we need to do is hit play in the morning! We have now written code automation code for three different strategies, however we haven't actually backtested any of our strategies or models yet. So for the final part of the How to Automate series we will be writing code to How to simulate the Exchange to backtest and optimise our strategies . Make sure not to miss it as this is where I believe the sauce is made (not that I have made significant sauce).","title":"Conclusion and next steps"},{"location":"api/How_to_Automate_4/#complete-code","text":"Run the code from your ide by using py <filename> .py, making sure you amend the path to point to your input data. Download from Github from joblib import load import os import sys # Allow imports from src folder module_path = os . path . abspath ( os . path . join ( '../src' )) if module_path not in sys . path : sys . path . append ( module_path ) from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from dateutil import tz from pandas.tseries.offsets import MonthEnd from sklearn.preprocessing import MinMaxScaler import itertools import numpy as np import pandas as pd from nltk.tokenize import regexp_tokenize # settings to display all columns pd . set_option ( \"display.max_columns\" , None ) import fasttrack as ft from dotenv import load_dotenv load_dotenv () # Import libraries for logging in import betfairlightweight from flumine import Flumine , clients # Import libraries and logging from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import re import pandas as pd import numpy as np import datetime import logging logging . basicConfig ( filename = 'how_to_automate_4.log' , level = logging . INFO , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) # import logging from flumine.worker import BackgroundWorker from flumine.events.events import TerminationEvent import csv from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes logger = logging . getLogger ( __name__ ) brunos_model = load ( 'logistic_regression.joblib' ) brunos_model # Validate FastTrack API connection api_key = os . getenv ( 'FAST_TRACK_API_KEY' ,) client = ft . Fasttrack ( api_key ) track_codes = client . listTracks () # Import race data excluding NZ races au_tracks_filter = list ( track_codes [ track_codes [ 'state' ] != 'NZ' ][ 'track_code' ]) # Time window to import data # First day of the month 46 months back from now date_from = ( datetime . today () - relativedelta ( months = 46 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # First day of previous month date_to = ( datetime . today () - relativedelta ( months = 1 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # Dataframes to populate data with race_details = pd . DataFrame () dog_results = pd . DataFrame () # For each month, either fetch data from API or use local CSV file if we already have downloaded it for start in pd . date_range ( date_from , date_to , freq = 'MS' ): start_date = start . strftime ( \"%Y-%m- %d \" ) end_date = ( start + MonthEnd ( 1 )) . strftime ( \"%Y-%m- %d \" ) try : filename_races = f 'FT_AU_RACES_ { start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { start_date } .csv' filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' print ( f 'Loading data from { start_date } to { end_date } ' ) if os . path . isfile ( filepath_races ): # Load local CSV file month_race_details = pd . read_csv ( filepath_races ) month_dog_results = pd . read_csv ( filepath_dogs ) else : # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( start_date , end_date , au_tracks_filter ) month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) # Combine monthly data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) except : print ( f 'Could not load data from { start_date } to { end_date } ' ) race_details . tail () current_month_start_date = pd . Timestamp . now () . replace ( day = 1 ) . strftime ( \"%Y-%m- %d \" ) current_month_end_date = ( pd . Timestamp . now () . replace ( day = 1 ) + MonthEnd ( 1 )) current_month_end_date = ( current_month_end_date - pd . Timedelta ( '1 day' )) . strftime ( \"%Y-%m- %d \" ) print ( f 'Start date: { current_month_start_date } ' ) print ( f 'End Date: { current_month_end_date } ' ) # Download data for races that have concluded this current month up untill today # Start and end dates for current month current_month_start_date = pd . Timestamp . now () . replace ( day = 1 ) . strftime ( \"%Y-%m- %d \" ) current_month_end_date = ( pd . Timestamp . now () . replace ( day = 1 ) + MonthEnd ( 1 )) current_month_end_date = ( current_month_end_date - pd . Timedelta ( '1 day' )) . strftime ( \"%Y-%m- %d \" ) # Files names filename_races = f 'FT_AU_RACES_ { current_month_start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { current_month_start_date } .csv' # Where to store files locally filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( current_month_start_date , current_month_end_date , au_tracks_filter ) # Save the files locally and replace any out of date fields month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) dog_results # This is super important I have spent literally hours before I found out this was causing errors dog_results [ '@id' ] = pd . to_numeric ( dog_results [ '@id' ]) # Append the extra data to our data frames race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) # Download the data for todays races todays_date = pd . Timestamp . now () . strftime ( \"%Y-%m- %d \" ) todays_races , todays_dogs = client . getFullFormat ( dt = todays_date , tracks = au_tracks_filter ) # display is for ipython notebooks only # display(todays_races.head(1), todays_dogs.head(1)) # It seems that the todays_races dataframe doesn't have the date column, so let's add that on todays_races [ 'date' ] = pd . Timestamp . now () . strftime ( ' %d %b %y' ) todays_races . head ( 1 ) # It also seems that in todays_dogs dataframe Box is labeled as RaceBox instead, so let's rename it # We can also see that there are some specific dogs that have \"Res.\" as a suffix of their name, i.e. they are reserve dogs, # We will treat this later todays_dogs = todays_dogs . rename ( columns = { \"RaceBox\" : \"Box\" }) todays_dogs . tail ( 3 ) # Appending todays data to this months data month_dog_results = pd . concat ([ month_dog_results , todays_dogs ], join = 'outer' )[ month_dog_results . columns ] month_race_details = pd . concat ([ month_race_details , todays_races ], join = 'outer' )[ month_race_details . columns ] # Appending this months data to the rest of our historical data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) race_details ## Cleanse and normalise the data # Clean up the race dataset race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) # Clean up the dogs results dataset dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) # New line of code (rest of this code chunk is copied from bruno's code) dog_results [ 'FastTrack_DogId' ] = pd . to_numeric ( dog_results [ 'FastTrack_DogId' ]) # Combine dogs results with race attributes dog_results = dog_results . merge ( race_details , how = 'left' , on = 'FastTrack_RaceId' ) # Convert StartPrice to probability dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x is None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' )) if isinstance ( x , str ) else x ) dog_results [ 'StartPrice_probability' ] = ( 1 / dog_results [ 'StartPrice' ]) . fillna ( 0 ) dog_results [ 'StartPrice_probability' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x / x . sum ()) # Discard entries without results (scratched or did not finish) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results [ 'Box' ] = dog_results [ 'Box' ] . astype ( int ) # Clean up other attributes dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'SplitMargin' ] = dog_results [ 'SplitMargin' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) . fillna ( 0 ) dog_results [ 'Place' ] = pd . to_numeric ( dog_results [ 'Place' ] . apply ( lambda x : x . replace ( \"=\" , \"\" ) if isinstance ( x , str ) else 0 ), errors = 'coerce' ) . fillna ( 0 ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x == 1 else 0 ) # Normalise some of the raw values dog_results [ 'Prizemoney_norm' ] = np . log10 ( dog_results [ 'Prizemoney' ] + 1 ) / 12 dog_results [ 'Place_inv' ] = ( 1 / dog_results [ 'Place' ]) . fillna ( 0 ) dog_results [ 'Place_log' ] = np . log10 ( dog_results [ 'Place' ] + 1 ) . fillna ( 0 ) dog_results [ 'RunSpeed' ] = ( dog_results [ 'RunTime' ] / dog_results [ 'Distance' ]) . fillna ( 0 ) ## Generate features using raw data # Calculate median winner time per track/distance win_results = dog_results [ dog_results [ 'win' ] == 1 ] median_win_time = pd . DataFrame ( data = win_results [ win_results [ 'RunTime' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'RunTime' ] . median ()) . rename ( columns = { \"RunTime\" : \"RunTime_median\" }) . reset_index () median_win_split_time = pd . DataFrame ( data = win_results [ win_results [ 'SplitMargin' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'SplitMargin' ] . median ()) . rename ( columns = { \"SplitMargin\" : \"SplitMargin_median\" }) . reset_index () median_win_time . head () # Calculate track speed index median_win_time [ 'speed_index' ] = ( median_win_time [ 'RunTime_median' ] / median_win_time [ 'Distance' ]) median_win_time [ 'speed_index' ] = MinMaxScaler () . fit_transform ( median_win_time [[ 'speed_index' ]]) median_win_time . head () # Compare dogs finish time with median winner time dog_results = dog_results . merge ( median_win_time , on = [ 'Track' , 'Distance' ], how = 'left' ) dog_results = dog_results . merge ( median_win_split_time , on = [ 'Track' , 'Distance' ], how = 'left' ) # Normalise time comparison dog_results [ 'RunTime_norm' ] = ( dog_results [ 'RunTime_median' ] / dog_results [ 'RunTime' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'RunTime_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'RunTime_norm' ]]) dog_results [ 'SplitMargin_norm' ] = ( dog_results [ 'SplitMargin_median' ] / dog_results [ 'SplitMargin' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'SplitMargin_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'SplitMargin_norm' ]]) dog_results . head () # Calculate box winning percentage for each track/distance box_win_percent = pd . DataFrame ( data = dog_results . groupby ([ 'Track' , 'Distance' , 'Box' ])[ 'win' ] . mean ()) . rename ( columns = { \"win\" : \"box_win_percent\" }) . reset_index () # Add to dog results dataframe dog_results = dog_results . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) # Display example of barrier winning probabilities print ( box_win_percent . head ( 8 )) dog_results [ dog_results [ 'FastTrack_DogId' ] == 592253143 ] . tail ()[[ 'date_dt' , 'Place' , 'DogName' , 'RaceNum' , 'Track' , 'Distance' , 'win' , 'Prizemoney_norm' , 'Place_inv' , 'Place_log' ]] # Generate rolling window features dataset = dog_results . copy () dataset = dataset . set_index ([ 'FastTrack_DogId' , 'date_dt' ]) . sort_index () # Use rolling window of 28, 91 and 365 days rolling_windows = [ '28D' , '91D' , '365D' ] # Features to use for rolling windows calculation features = [ 'RunTime_norm' , 'SplitMargin_norm' , 'Place_inv' , 'Place_log' , 'Prizemoney_norm' ] # Aggregation functions to apply aggregates = [ 'min' , 'max' , 'mean' , 'median' , 'std' ] # Keep track of generated feature names feature_cols = [ 'speed_index' , 'box_win_percent' ] for rolling_window in rolling_windows : print ( f 'Processing rolling window { rolling_window } ' ) rolling_result = ( dataset . reset_index ( level = 0 ) . sort_index () . groupby ( 'FastTrack_DogId' )[ features ] . rolling ( rolling_window ) . agg ( aggregates ) . groupby ( level = 0 ) # Thanks to Brett for finding this! . shift ( 1 ) ) # My own dodgey code to work with reserve dogs temp = rolling_result . reset_index () temp = temp [ temp [ 'date_dt' ] == pd . Timestamp . now () . normalize ()] temp . groupby ([ 'FastTrack_DogId' , 'date_dt' ]) . first () rolling_result . loc [ pd . IndexSlice [:, pd . Timestamp . now () . normalize ()], :] = temp . groupby ([ 'FastTrack_DogId' , 'date_dt' ]) . first () # Generate list of rolling window feature names (eg: RunTime_norm_min_365D) agg_features_cols = [ f ' { f } _ { a } _ { rolling_window } ' for f , a in itertools . product ( features , aggregates )] # Add features to dataset dataset [ agg_features_cols ] = rolling_result # Keep track of generated feature names feature_cols . extend ( agg_features_cols ) # Replace missing values with 0 dataset . fillna ( 0 , inplace = True ) # display(dataset.head(8)) # display is only for ipython notebooks # Only keep data after 2018-12-01 model_df = dataset . reset_index () feature_cols = np . unique ( feature_cols ) . tolist () model_df = model_df [ model_df [ 'date_dt' ] >= '2018-12-01' ] # This line was originally part of Bruno's tutorial, but we don't run it in this script # model_df = model_df[['date_dt', 'FastTrack_RaceId', 'DogName', 'win', 'StartPrice_probability'] + feature_cols] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] # Generate predictions like normal # Range of dates that we want to simulate later '2022-03-01' to '2022-04-01' todays_data = model_df [( model_df [ 'date_dt' ] >= pd . Timestamp ( '2022-03-01' ) . strftime ( '%Y-%m- %d ' )) & ( model_df [ 'date_dt' ] < pd . Timestamp ( '2022-04-01' ) . strftime ( '%Y-%m- %d ' ))] dog_win_probabilities = brunos_model . predict_proba ( todays_data [ feature_cols ])[:, 1 ] todays_data [ 'prob_LogisticRegression' ] = dog_win_probabilities todays_data [ 'renormalise_prob' ] = todays_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x / x . sum ()) todays_data [ 'rating' ] = 1 / todays_data [ 'renormalise_prob' ] todays_data = todays_data . sort_values ( by = 'date_dt' ) todays_data def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" , \"meetings.races.number\" : \"RaceNum\" , \"meetings.name\" : \"Track\" , \"meetings.races.runners.name\" : \"DogName\" } ) # iggy_df = iggy_df[['market_id','selection_id','rating']] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) iggy_df [ 'date_dt' ] = date # Set market_id and selection_id as index for easy referencing # iggy_df = iggy_df.set_index(['market_id','selection_id']) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022-03-01' , end = '2022-04-01' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) iggy_df # format DogNames to merge todays_data [ 'DogName' ] = todays_data [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) iggy_df [ 'DogName' ] = iggy_df [ 'DogName' ] . str . upper () # Merge backtest = iggy_df [[ 'market_id' , 'selection_id' , 'DogName' , 'date_dt' ]] . merge ( todays_data [[ 'rating' , 'DogName' , 'date_dt' ]], how = 'inner' , on = [ 'DogName' , 'date_dt' ]) backtest # Save predictions for if we want to backtest/simulate it later backtest . to_csv ( 'backtest.csv' , index = False ) # Csv format # backtest.to_pickle('backtest.pkl') # pickle format (faster, but can't open in excel) todays_data [ todays_data [ 'FastTrack_RaceId' ] == '798906744' ] # Select todays data todays_data = model_df [ model_df [ 'date_dt' ] == pd . Timestamp . now () . strftime ( '%Y-%m- %d ' )] # Generate runner win predictions dog_win_probabilities = brunos_model . predict_proba ( todays_data [ feature_cols ])[:, 1 ] todays_data [ 'prob_LogisticRegression' ] = dog_win_probabilities # We no longer renomralise probability in this chunk of code, do it in Flumine instead # todays_data['renormalise_prob'] = todays_data.groupby('FastTrack_RaceId')['prob_LogisticRegression'].apply(lambda x: x / x.sum()) # todays_data['rating'] = 1/todays_data['renormalise_prob'] # todays_data = todays_data.sort_values(by = 'date_dt') todays_data # Prepare data for easy reference in flumine todays_data [ 'DogName_bf' ] = todays_data [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) todays_data . replace ({ 'Sandown (SAP)' : 'Sandown Park' }, regex = True , inplace = True ) todays_data = todays_data . set_index ([ 'DogName_bf' , 'Track' , 'RaceNum' ]) todays_data . head () # Credentials to login and logging in trading = betfairlightweight . APIClient ( 'username' , 'password' , app_key = 'appkey' ) client = clients . BetfairClient ( trading , interactive_login = True ) # Login framework = Flumine ( client = client ) # Code to login when using security certificates # trading = betfairlightweight.APIClient('username','password',app_key='appkey', certs=r'C:\\Users\\zhoui\\openssl_certs') # client = clients.BetfairClient(trading) # framework = Flumine(client=client) class FlatBetting ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Convert dataframe to a global variable global todays_data # At the 60 second mark: if market . seconds_to_start < 60 and market_book . inplay == False : # get the list of dog_names, name of the track/venue and race_number/RaceNum from Betfair Polling API dog_names = [] track = market . market_catalogue . event . venue race_number = market . market_catalogue . market_name . split ( ' ' , 1 )[ 0 ] # comes out as R1/R2/R3 .. etc race_number = re . sub ( \"[^0-9]\" , \"\" , race_number ) # only keep the numbers for runner_cata in market . market_catalogue . runners : dog_name = runner_cata . runner_name . split ( ' ' , 1 )[ 1 ] . upper () dog_names . append ( dog_name ) # Check if there are box changes, if there are then use Brett's code if market . market_catalogue . description . clarifications != None : # Brett's code to get Box changes: my_string = market . market_catalogue . description . clarifications . replace ( \"<br> Dog\" , \"<br>Dog\" ) pattern1 = r '(?<=<br>Dog ).+?(?= starts)' pattern2 = r \"(?<=\\bbox no. )(\\w+)\" runners_df = pd . DataFrame ( regexp_tokenize ( my_string , pattern1 ), columns = [ 'runner_name' ]) runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . astype ( str ) # Remove dog name from runner_number runners_df [ 'runner_number' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [:( x . find ( \" \" ) - 1 )] . upper ()) # Remove dog number from runner_name runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [( x . find ( \" \" ) + 1 ):] . upper ()) runners_df [ 'Box' ] = regexp_tokenize ( my_string , pattern2 ) # Replace any old Box info in our original dataframe with data available in runners_df runners_df = runners_df . set_index ( 'runner_name' ) todays_data . loc [( runners_df . index [ runners_df . index . isin ( dog_names )], track , race_number ), 'Box' ] = runners_df . loc [ runners_df . index . isin ( dog_names ), 'Box' ] . to_list () # Merge box_win_percentage back on: todays_data = todays_data . drop ( columns = 'box_win_percentage' , axis = 1 ) todays_data = todays_data . reset_index () . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) . set_index ([ 'DogName_bf' , 'Track' , 'RaceNum' ]) # Generate probabilities using Bruno's model todays_data . loc [( dog_names , track , race_number ), 'prob_LogisticRegression' ] = brunos_model . predict_proba ( todays_data . loc [( dog_names , track , race_number )][ feature_cols ])[:, 1 ] # renomalise probabilities probabilities = todays_data . loc [ dog_names , track , race_number ][ 'prob_LogisticRegression' ] todays_data . loc [( dog_names , track , race_number ), 'renormalised_prob' ] = probabilities / probabilities . sum () # convert probaiblities to ratings todays_data . loc [( dog_names , track , race_number ), 'rating' ] = 1 / todays_data . loc [ dog_names , track , race_number ][ 'renormalised_prob' ] # Use both the polling api (market.catalogue) and the streaming api at once: for runner_cata , runner in zip ( market . market_catalogue . runners , market_book . runners ): # Check the polling api and streaming api matches up (sometimes it doesn't) if runner_cata . selection_id == runner . selection_id : # Get the dog_name from polling api then reference our data for our model rating dog_name = runner_cata . runner_name . split ( ' ' , 1 )[ 1 ] . upper () # Rest is the same as How to Automate III model_price = todays_data . loc [ dog_name , track , race_number ][ 'rating' ] ### If you have an issue such as: # Unknown error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). # Then do model_price = todays_data.loc[dog_name,track,race_number]['rating'].item() # Log info before placing bets logging . info ( f 'dog_name: { dog_name } ' ) logging . info ( f 'model_price: { model_price } ' ) logging . info ( f 'market_id: { market_book . market_id } ' ) logging . info ( f 'selection_id: { runner . selection_id } ' ) # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) greyhounds_strategy = FlatBetting ( market_filter = streaming_market_filter ( event_type_ids = [ \"4339\" ], # Greyhounds markets country_codes = [ \"AU\" ], # Australian markets market_types = [ \"WIN\" ], # Win markets ), max_order_exposure = 50 , # Max exposure per order = 50 max_trade_count = 1 , # Max 1 trade per selection max_live_trade_count = 1 , # Max 1 unmatched trade per selection ) framework . add_strategy ( greyhounds_strategy ) # logger = logging.getLogger(__name__) \"\"\" Worker can be used as followed: framework.add_worker( BackgroundWorker( framework, terminate, func_kwargs={\"today_only\": True, \"seconds_closed\": 1200}, interval=60, start_delay=60, ) ) This will run every 60s and will terminate the framework if all markets starting 'today' have been closed for at least 1200s \"\"\" # Function that stops automation running at the end of the day def terminate ( context : dict , flumine , today_only : bool = True , seconds_closed : int = 600 ) -> None : \"\"\"terminate framework if no markets live today. \"\"\" markets = list ( flumine . markets . markets . values ()) markets_today = [ m for m in markets if m . market_start_datetime . date () == datetime . datetime . utcnow () . date () and ( m . elapsed_seconds_closed is None or ( m . elapsed_seconds_closed and m . elapsed_seconds_closed < seconds_closed ) ) ] if today_only : market_count = len ( markets_today ) else : market_count = len ( markets ) if market_count == 0 : # logger.info(\"No more markets available, terminating framework\") flumine . handler_queue . put ( TerminationEvent ( flumine )) # Add the stopped to our framework framework . add_worker ( BackgroundWorker ( framework , terminate , func_kwargs = { \"today_only\" : True , \"seconds_closed\" : 1200 }, interval = 60 , start_delay = 60 , ) ) logger = logging . getLogger ( __name__ ) FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] class LiveLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( LiveLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () # Changed file path and checks if the file orders_hta_4.csv already exists, if it doens't then create it def _setup ( self ): if os . path . exists ( \"orders_hta_4.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"orders_hta_4.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"orders_hta_4.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : 0 if not order . cleared_order else order . cleared_order . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) framework . add_logging_control ( LiveLoggingControl () ) framework . run ()","title":"Complete code"},{"location":"api/How_to_Automate_4/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"api/How_to_Automate_5/","text":"How to Automate 5 Before you start This tutorial follows on from our How to Automate series which stepped through how to create a trading bot for a few different strategies. Make sure you take a look through them first before This is the final part of the How to Automate series (for a while at least). In my previous posts we have created a few different strategies, but I haven't actually backtested or simulated any of them yet. How will do we even know if they have any edge? Today we test those strategies by running simulations and try to optimise performance. But how do we test strategies? One method is to follow the steps shown in this fantastic article: Backtesting wagering models with Betfair JSON stream data . But if you already have access to Betfair historic data or you have reccorded it yourself, you are not making full use of your data. This is because the above method will take all the amazing data thats has been collected and only extract a sliver of data from a few time points. Flumine is amazing because it can make full use of the entire dataset to simulate the market from when the market is first created to settlement. Instead of looking at the prices at 3 minutes before the race and assuming we get matched we can for example simulate placing a back bet hours before the race starts and replay exactly what happened in that market second by second to see if we would have gotten matched between the time we placed the bet and when the market settles. This is really cool because we might have a really awesome model that is close to being profitable but not quite and we want to optimise it. This will give us the most realistic back testing available and let us test if we are getting matched for the volume and price we want and if we have any edge at all. Something that is important to note is that although this is the most realistic backtest you can probably get, it is not 100% accurate. This is because we are simply replaying a market with our orders being added in, we cannot take into account how other market participants react. If we place a huge order e.g. $1000 or more we will likely trigger other peoples bots the market will likely move against us. But either way, we can still try some really cool things such as testing different time points to place bets without needing to re-extract data each time, change staking methodology or placing bets a few ticks away from the best available prices and hoping it gets matched. Set up Before we get started, although Jupyter Notebook/lab is a quants' favourite tool we need to use a different IDE such as VS Code for our simulation code (feel free to try it out, it didn't work for me and I read a note somewhere about it in the docs, but can't find it anymore). All code files are made available on github . I am going to use the March 2022 Greyhound Pro data and I've provided a sample of that data in the github repo which you can use to follow along, but if your an Australian and New Zealand customer make sure to shoot an email to data@betfair.com.au . Simulation mode in Flumine requires your data to be structured a certain way. So, if you have purchased data you will need it to be extracted formatted so that each market file is within a single file, instead or having files within files within files (default). You can do it manually, which will take an unimaginable amount of time, but I've written a simple script that will do it for you. But you just need to do few things before you run the script. - Take your data that has the .tar extension, mine was 2022_03_MarGreyhoundsPro.tar and extract it using winrar/7zip etc this will create a file named 2022_03_MarGreyhoundsPro - make sure 2022_03_MarGreyhoundsPro is stored in the same location as the data extractor script - create a new empty folder that you want the extracted data to be outputted to, I created output_2022_03_MarGreyhoundsPro - then run the script Extracts and formats the content of .tar files # Extracts all the bzip2 files (.bz2) # contained within the specified output_folder and any sub-folders # and writes them to a file with their market_id as their file name # This will take around 10 mins to run for one month of Pro Greyhound data import glob import bz2 import shutil # Folder containing bz2 files or any subfolders with bz2 files input_folder = '2022_04_AprGreyhoundsPro' # change to what you have named your folder e.g. 'sample_monthly_data' # Folder to write our extracted bz2 files to, this folder needs to already be created output_folder = 'output_2022_04_AprGreyhoundsPro' # change to what you have named your folder e.g. 'sample_monthly_data_output' # Returns a list of paths to bz2 files within the input folder and any sub folders files = glob . iglob ( f ' { input_folder } /**/**/**/**/**/*.bz2' , recursive = False ) # Extracts each bz2 file and write it to the output folder for path in files : market_id = path [ - 15 : - 4 ] print ( path , market_id ) with bz2 . BZ2File ( path ) as fr , open ( f ' { output_folder } / { market_id } ' , \"wb\" ) as fw : shutil . copyfileobj ( fr , fw ) Now we are all set up lets run our sim! How the sims work Flumine is pretty cool, by default it hooks up to the Betfair API and it will run our strategy on live markets. When we set it to simulation mode we can hook it up to the historic data instead. The historic data is basically photos of the exhange up to every 50ms, in simulation mode Flumine essentially quickly scans through each picture sequentially essentially replaying the market. Just like how you would add_strategy() to Flumine to add a strategy that runs live, you can do the same thing in simulation mode and it will place into the simulated markets it creates. The coolest thing is, it is super easy to change it to simulation mode: ```py title = \"Setting Flumine to simulation Mode\" Set Flumine to simulation mode client = clients.SimulatedClient() framework = FlumineSimulation(client=client) and instead of pointing it to markets you want to run your strategy on, you point it to your historic data files instead (as it is quite slow I would also suggest only replaying a subsection of the historic files, you can change that with the listner_kwargs), then just run it as you would any other strategy in Flumine: ```py title = \"Pointing the simulation to the historical files\" hl_lines=\"8 10\" # Set parameters for our strategy strategy = BackFavStrategy( # market_filter selects what portion of the historic data we simulate our strategy on # markets selects the list of betfair historic data files # market_types specifies the type of markets # listener_kwargs specifies the time period we simulate for each market market_filter={ \"markets\": data_files, 'market_types':['WIN'], \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80}, }, max_order_exposure=1000, max_selection_exposure=1000, max_live_trade_count=1, max_trade_count=1, ) # Run our strategy on the simulated market framework.add_strategy(strategy) framework.run() Running Sims: How to Automate II First off the bat is simulating the strategy we created in How to Automate II. Its actually pretty easy to simulate using Flumine especially if your strategy doesn't require outside data. In fact almost all our code we previously made can just be copied accross. We just need to set Flumine to simulation mode and point it to our data files instead of at the Betfair API, which is only a few lines of code and once you read it, its pretty self explanatory. One thing we must remember to do is to add the bet logging code we made in How to Automate II so we can analyse how our strategy went afterwards. I've copied both the changes you need to make and also the complete code, give that bad boy a spin, and it will create a csv file as a log of all bets placed. A months worth of data will take ages to run (like 8 hours on my slow laptop), but the sample data should only take around 10 mins (we will go into speeding up the sims later). Changes you need to make Complete Code, changes are at the bottom # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = BackFavStrategy ( # market_filter selects what portion of the historic data we simulate our strategy on # markets selects the list of betfair historic data files # market_types specifies the type of markets # listener_kwargs specifies the time period we simulate for each market market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Import libraries import glob import os import time import logging import csv from pythonjsonlogger import jsonlogger from flumine import FlumineSimulation , clients from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation class BackFavStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'BackFavStrategy'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Collect data on last price traded and the number of bets we have placed snapshot_last_price_traded = [] snapshot_runner_context = [] for runner in market_book . runners : snapshot_last_price_traded . append ([ runner . selection_id , runner . last_price_traded ]) # Get runner context for each runner runner_context = self . get_runner_context ( market . market_id , runner . selection_id , runner . handicap ) snapshot_runner_context . append ([ runner_context . selection_id , runner_context . executable_orders , runner_context . live_trade_count , runner_context . trade_count ]) # Convert last price traded data to dataframe snapshot_last_price_traded = pd . DataFrame ( snapshot_last_price_traded , columns = [ 'selection_id' , 'last_traded_price' ]) # Find the selection_id of the favourite snapshot_last_price_traded = snapshot_last_price_traded . sort_values ( by = [ 'last_traded_price' ]) fav_selection_id = snapshot_last_price_traded [ 'selection_id' ] . iloc [ 0 ] logging . info ( snapshot_last_price_traded ) # logging # Convert data on number of bets we have placed to a dataframe snapshot_runner_context = pd . DataFrame ( snapshot_runner_context , columns = [ 'selection_id' , 'executable_orders' , 'live_trade_count' , 'trade_count' ]) logging . info ( snapshot_runner_context ) # logging for runner in market_book . runners : if runner . status == \"ACTIVE\" and market . seconds_to_start < 60 and market_book . inplay == False and runner . selection_id == fav_selection_id and snapshot_runner_context . iloc [:, 1 :] . sum () . sum () == 0 : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . last_price_traded , size = 5 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_2.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_2.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_2.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_2.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = BackFavStrategy ( # market_filter selects what portion of the historic data we simulate our strategy on # markets selects the list of betfair historic data files # market_types specifies the type of markets # listener_kwargs specifies the time period we simulate for each market market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () Running Sims: How to Automate III Okay, so we got the first one running pretty easily, a little too easily (a few lines of code and no major issues or hacky work arounds), lets test out a strategy that requires external data. In How to Automate III we automated the betfair data scientists model, lets now simulate performance. I'm going to do just the greyhound model, 'Iggy', at the moment, but the code is basically the same for the thoroughbred model, 'Kash'. Because we didn't save any of our ratings in How to Automate III we will need to redownload it now. And instead or redownloading just one days worth of data lets test out a whole month at a time. Lets reuse the function we created in How to Automate IV that we used a hacky work around that downloads the ratings for a range of dates: Download a whole month of Iggy ratings and convert it to a DataFrame def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022/02/27' , end = '2022/03/05' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) print ( iggy_df ) Now that we have downloaded a whole month of Iggy ratings to simulate it is crazy easy to simulate. We do the same thing we did when simulating How to Automate II: copy and paste the original code, and set Flumine into simulation mode pointing it to the historic data instead of the Betfair API\" Changes made to the original How to Automate III code Complete Code def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022/02/27' , end = '2022/03/05' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) print ( iggy_df ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatIggyModel ( market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Import libraries import glob import os import time import logging import csv import pandas as pd from pythonjsonlogger import jsonlogger from flumine import FlumineSimulation , BaseStrategy , utils , clients from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.order.ordertype import OrderTypes from flumine.markets.market import Market from flumine.controls.loggingcontrols import LoggingControl from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022/02/27' , end = '2022/03/05' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) print ( iggy_df ) # Create strategy, this is the exact same strategy shown in How to Automate III class FlatIggyModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatIggyModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_3.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_3.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_3.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_3.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatIggyModel ( market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () Simulating How to Automate IV Because we coded How to Automate IV with simulating in mind (I didn't originally and had to recode it a few times), its easy for us to simulate the performance of our model. As we saved our model ratings to a csv, reading it in now actually makes the code simpler then what we created for placing live bets. This is because the issues we had working around reserve dogs and matching with the Betfair API has been taken care of (there are no reserve dogs to work around in the historic data). In fact thanks to my hacky work around in How to Automate IV the data is also in the same format as How to Automate III so we can basically use almost the exact same code we used to simulate How to Automate III. The only real differences from simulating How to Automate III and How to Automate IV is that we need to have the csv file of predictions already (available on github), read that in, and change any naming conventions that might be different. Read in model predictions and format dataframe for easy reference Complete code, almost the same as simulating How to Automate III # Read in predictions from hta_4 todays_data = pd . read_csv ( 'backtest.csv' , dtype = ({ \"market_id\" : str })) todays_data = todays_data . set_index ([ 'market_id' , 'selection_id' ]) # Import libraries import glob import os import time import logging import csv import pandas as pd import json from pythonjsonlogger import jsonlogger from flumine import FlumineSimulation , BaseStrategy , utils , clients from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.order.ordertype import OrderTypes from flumine.markets.market import Market from flumine.controls.loggingcontrols import LoggingControl from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook from dateutil import tz from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from betfairlightweight.resources import MarketCatalogue from flumine.markets.middleware import Middleware # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation # Read in predictions from hta_4 todays_data = pd . read_csv ( 'backtest.csv' , dtype = ({ \"market_id\" : str })) todays_data = todays_data . set_index ([ 'market_id' , 'selection_id' ]) ### New implementation class FlatBetting ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # At the 60 second mark: if market . seconds_to_start < 60 and market_book . inplay == False : # Can't simulate polling API # Only use streaming API: for runner in market_book . runners : model_price = todays_data . loc [ market . market_id ] . loc [ runner . selection_id ][ 'rating' ] # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_4.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_4.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_4.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_4.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatBetting ( market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () Gotta go fast Now that we have everything working, if you have tried any of the simulations you may notice its pretty slow. I definitely have especially for larger files such as on 1 months worth of data (probably took me around 8 hours of just running the code in the background). The good thing is we can speed it up, the bad thing is, its via multiprocessing which I have never touched before. But turns out its not too bad. You really only need to wrap your Flumine client into a function: def run_process ( markets ): \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data. Args: markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv \"\"\" # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatBetting ( market_filter = { \"markets\" : markets , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] and then split the files to run on muliple processors. You can copy the code, which is what I did, and it works without a hitch. # Multi processing if __name__ == \"__main__\" : all_markets = data_files # All the markets we want to simulate processes = os . cpu_count () # Returns the number of CPUs in the system. markets_per_process = 8 # 8 is optimal as it prevents data leakage. _process_jobs = [] with futures . ProcessPoolExecutor ( max_workers = processes ) as p : # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have. chunk = min ( markets_per_process , math . ceil ( len ( all_markets ) / processes ) ) # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs for m in ( utils . chunks ( all_markets , chunk )): _process_jobs . append ( p . submit ( run_process , markets = m , ) ) for job in futures . as_completed ( _process_jobs ): job . result () # wait for result But I wanted to understand how the code works, but it actually isn't too bad. Essentially it takes all the historical markets you have e.g. 1000, and splits it into 8 chunks. Then we run our strategy on all 8 chunks simultaniously. And this gives serious speed improvements. The complete code for all three simulations using multi processing are available at the end of this post. Future versions of Flumine are using the new BetfairData library to speed up simulations which once fully implemented should also give some serious speed benefits. Analysing and optimising our results Now that we have all our speedy simulation code lets look at our results and see if we found anything good. import numpy as np import pandas as pd import plotly.express as px # Read data results = pd . read_csv ( 'sim_hta_4.csv' , parse_dates = [ 'date_time_placed' ], dtype = { 'market_id' : str }) # calculate and display cumulative pnl results = results . sort_values ( by = [ 'date_time_placed' ]) results [ 'cum_profit' ] = results [ 'profit' ] . cumsum () px . line ( results , 'date_time_placed' , 'cum_profit' ) . show () post_sim_analysis_v1 Before commissions the model is profitable, which is awesome as I didn't think that would be the case. Bruno has mentioned to me that the model in the tutorial was quite \"basic\" and not profitable, but it seems we got super lucky with a few long shots getting up in March. Lets incorporate commissions into our results and see if it remains profitable: gross_profit = pd . DataFrame ( results . groupby ([ 'market_id' ])[ 'profit' ] . sum ()) # 7% commission rate on greyhounds, commissions calculated on profit at a market level calc_comms = lambda gross_profit : np . where ( gross_profit > 0 , gross_profit * ( 1 - 0.07 ), gross_profit ) gross_profit [ 'net_pnl' ] = calc_comms ( gross_profit [ 'profit' ]) gross_profit [ 'cum_npl' ] = gross_profit [ 'net_pnl' ] . cumsum () px . line ( gross_profit , gross_profit . index , 'cum_npl' ) . show () post_sim_analysis_v2 We are close, infact we were up a bit at the start but it seems after taking into account commissions we are no longer profitable and end the month down around $800. Lets try two different things to see if we can optimse our strategy: a different staking methodology and also a different time we start placing our bets. My theory is that because we are crossing the spread and taking whatever prices are available we are probably losing a bit of our edge there. If we bet when markets are more liquid then we will may lose less. But as markets become more liquid they also tend to become more efficient so lets it could work against us. Nonetheless lets test it out: Placing bets at 30 seconds instead of 60 seconds Results Before Commissions Results After Commissions def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # At the 60 second mark: if market . seconds_to_start < 30 and market_book . inplay == False : # Can't simulate polling API # Only use streaming API: for runner in market_book . runners : So the results seem pretty similar to before. After commisions we are down around $700 so we seem to be doing slighlty better. Lets try a different staking method instead, this time I have opted for a proportional staking strategy going for a fixed $10 profit on back bets and a fixed $10 liability on lay bets. There is an excellent post analysing different staking methods and I would encourage everyone to take a look at it. Lets see how our simulation went: Proportional Staking instead of Flat Staking Results Before Commissions Results After Commissions if market . seconds_to_start < 60 and market_book . inplay == False : # Can't simulate polling API # Only use streaming API: for runner in market_book . runners : model_price = todays_data . loc [ market . market_id ] . loc [ runner . selection_id ][ 'rating' ] # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = round ( 10 / ( runner . ex . available_to_back [ 0 ][ 'price' ] - 1 ), 2 )) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = round ( 10 / ( runner . ex . available_to_lay [ 0 ][ 'price' ] - 1 ), 2 )) ) market . place_order ( order ) I'm pretty surprised, I really did not expect to be profitable after commissions. We are slightly profitable at the end but we spent a significant amount of time in the negatives during the month. Without testing it out further with more historic data I going to put this down as variance for now. And I'll hand it over to you. Conclusion and next steps While we have tested our strategy and optimised it so far, I mearly tried one month of data and only three different variations of our strategy (most of which are unprofitable). Hopefully these posts help you think about what is possible when automating your strategy and how to optimise your strategy. There are plenty of other things to look at when optimising your strategy such as different staking methodologies or being more selective with your bets based on the track or state. The natural next step based on my above results would be to test out proportional staking at 30 seconds and to use a longer backtesting period. We need more data to draw a good conclusion about long term results, I have definitely found some strategies that fluke one month, but are long term losers using this method. Complete Code Multiprocess How to Automate II Multiprocess How to Automate III Multiprocess How to Automate IV Download from Github # Import libraries import glob import os import time import logging import csv import math from pythonjsonlogger import jsonlogger from concurrent import futures from flumine import FlumineSimulation , clients , utils from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import pandas as pd import numpy as np import logging # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation # Create a new strategy as a new class called BackFavStrategy, this in turn will allow us to create a new Python object later # BackFavStrategy is a child class inhereting from a predefined class in Flumine we imported above called BaseStrategy class BackFavStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : # We will want to change what is printed with we have multiple strategies print ( \"starting strategy 'BackFavStrategy'\" ) # Defines what happens when we first look at a market # This method will prevent looking at markets that are closed def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Find last traded price as a dataframe snapshot_last_price_traded = [] snapshot_runner_context = [] for runner in market_book . runners : snapshot_last_price_traded . append ([ runner . selection_id , runner . last_price_traded ]) # Get runner context for each runner runner_context = self . get_runner_context ( market . market_id , runner . selection_id , runner . handicap ) snapshot_runner_context . append ([ runner_context . selection_id , runner_context . executable_orders , runner_context . live_trade_count , runner_context . trade_count ]) snapshot_last_price_traded = pd . DataFrame ( snapshot_last_price_traded , columns = [ 'selection_id' , 'last_traded_price' ]) snapshot_last_price_traded = snapshot_last_price_traded . sort_values ( by = [ 'last_traded_price' ]) fav_selection_id = snapshot_last_price_traded [ 'selection_id' ] . iloc [ 0 ] snapshot_runner_context = pd . DataFrame ( snapshot_runner_context , columns = [ 'selection_id' , 'executable_orders' , 'live_trade_count' , 'trade_count' ]) for runner in market_book . runners : if runner . status == \"ACTIVE\" and market . seconds_to_start < 60 and market_book . inplay == False and runner . selection_id == fav_selection_id and snapshot_runner_context . iloc [:, 1 :] . sum () . sum () == 0 : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . last_price_traded , size = 5 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_2.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_2.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_2.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_2.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] def run_process ( markets ): \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data. Args: markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv \"\"\" # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = BackFavStrategy ( # market_filter selects what portion of the historic data we simulate our strategy on # markets selects the list of betfair historic data files # market_types specifies the type of markets # listener_kwargs specifies the time period we simulate for each market market_filter = { \"markets\" : markets , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Multi processing if __name__ == \"__main__\" : all_markets = data_files # All the markets we want to simulate processes = os . cpu_count () # Returns the number of CPUs in the system. markets_per_process = 8 # 8 is optimal as it prevents data leakage. _process_jobs = [] with futures . ProcessPoolExecutor ( max_workers = processes ) as p : # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have. chunk = min ( markets_per_process , math . ceil ( len ( all_markets ) / processes ) ) # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs for m in ( utils . chunks ( all_markets , chunk )): _process_jobs . append ( p . submit ( run_process , markets = m , ) ) for job in futures . as_completed ( _process_jobs ): job . result () # wait for result Download from Github # Import libraries import glob import os import time import logging import csv import math import pandas as pd from pythonjsonlogger import jsonlogger from concurrent import futures from flumine import FlumineSimulation , BaseStrategy , utils , clients from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.order.ordertype import OrderTypes from flumine.markets.market import Market from flumine.controls.loggingcontrols import LoggingControl from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) return ( iggy_df ) back_test_period = pd . date_range ( start = '2022/02/27' , end = '2022/03/05' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) print ( iggy_df ) # Create strategy, this is the exact same strategy shown in How to Automate III class FlatIggyModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatIggyModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_3.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_3.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_3.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_3.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] def run_process ( markets ): \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data. Args: markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv \"\"\" # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatIggyModel ( market_filter = { \"markets\" : markets , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Multi processing if __name__ == \"__main__\" : all_markets = data_files # All the markets we want to simulate processes = os . cpu_count () # Returns the number of CPUs in the system. markets_per_process = 8 # 8 is optimal as it prevents data leakage. _process_jobs = [] with futures . ProcessPoolExecutor ( max_workers = processes ) as p : # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have. chunk = min ( markets_per_process , math . ceil ( len ( all_markets ) / processes ) ) # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs for m in ( utils . chunks ( all_markets , chunk )): _process_jobs . append ( p . submit ( run_process , markets = m , ) ) for job in futures . as_completed ( _process_jobs ): job . result () # wait for result Download from Github # Import libraries import glob import os import time import logging import csv import pandas as pd import json import math from pythonjsonlogger import jsonlogger from flumine import FlumineSimulation , BaseStrategy , utils , clients from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.order.ordertype import OrderTypes from flumine.markets.market import Market from flumine.controls.loggingcontrols import LoggingControl from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook from pythonjsonlogger import jsonlogger from concurrent import futures # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation # Read in predictions from hta_4 todays_data = pd . read_csv ( 'backtest.csv' , dtype = ({ \"market_id\" : str })) todays_data = todays_data . set_index ([ 'market_id' , 'selection_id' ]) ### New implementation class FlatBetting ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # At the 60 second mark: if market . seconds_to_start < 60 and market_book . inplay == False : # Can't simulate polling API # Only use streaming API: for runner in market_book . runners : model_price = todays_data . loc [ market . market_id ] . loc [ runner . selection_id ][ 'rating' ] # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_4.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_4.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_4.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_4.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] def run_process ( markets ): \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data. Args: markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv \"\"\" # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatBetting ( market_filter = { \"markets\" : markets , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Multi processing if __name__ == \"__main__\" : all_markets = data_files # All the markets we want to simulate processes = os . cpu_count () # Returns the number of CPUs in the system. markets_per_process = 8 # 8 is optimal as it prevents data leakage. _process_jobs = [] with futures . ProcessPoolExecutor ( max_workers = processes ) as p : # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have. chunk = min ( markets_per_process , math . ceil ( len ( all_markets ) / processes ) ) # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs for m in ( utils . chunks ( all_markets , chunk )): _process_jobs . append ( p . submit ( run_process , markets = m , ) ) for job in futures . as_completed ( _process_jobs ): job . result () # wait for result Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"How to Automate 5"},{"location":"api/How_to_Automate_5/#how-to-automate-5","text":"Before you start This tutorial follows on from our How to Automate series which stepped through how to create a trading bot for a few different strategies. Make sure you take a look through them first before This is the final part of the How to Automate series (for a while at least). In my previous posts we have created a few different strategies, but I haven't actually backtested or simulated any of them yet. How will do we even know if they have any edge? Today we test those strategies by running simulations and try to optimise performance. But how do we test strategies? One method is to follow the steps shown in this fantastic article: Backtesting wagering models with Betfair JSON stream data . But if you already have access to Betfair historic data or you have reccorded it yourself, you are not making full use of your data. This is because the above method will take all the amazing data thats has been collected and only extract a sliver of data from a few time points. Flumine is amazing because it can make full use of the entire dataset to simulate the market from when the market is first created to settlement. Instead of looking at the prices at 3 minutes before the race and assuming we get matched we can for example simulate placing a back bet hours before the race starts and replay exactly what happened in that market second by second to see if we would have gotten matched between the time we placed the bet and when the market settles. This is really cool because we might have a really awesome model that is close to being profitable but not quite and we want to optimise it. This will give us the most realistic back testing available and let us test if we are getting matched for the volume and price we want and if we have any edge at all. Something that is important to note is that although this is the most realistic backtest you can probably get, it is not 100% accurate. This is because we are simply replaying a market with our orders being added in, we cannot take into account how other market participants react. If we place a huge order e.g. $1000 or more we will likely trigger other peoples bots the market will likely move against us. But either way, we can still try some really cool things such as testing different time points to place bets without needing to re-extract data each time, change staking methodology or placing bets a few ticks away from the best available prices and hoping it gets matched.","title":"How to Automate 5"},{"location":"api/How_to_Automate_5/#set-up","text":"Before we get started, although Jupyter Notebook/lab is a quants' favourite tool we need to use a different IDE such as VS Code for our simulation code (feel free to try it out, it didn't work for me and I read a note somewhere about it in the docs, but can't find it anymore). All code files are made available on github . I am going to use the March 2022 Greyhound Pro data and I've provided a sample of that data in the github repo which you can use to follow along, but if your an Australian and New Zealand customer make sure to shoot an email to data@betfair.com.au . Simulation mode in Flumine requires your data to be structured a certain way. So, if you have purchased data you will need it to be extracted formatted so that each market file is within a single file, instead or having files within files within files (default). You can do it manually, which will take an unimaginable amount of time, but I've written a simple script that will do it for you. But you just need to do few things before you run the script. - Take your data that has the .tar extension, mine was 2022_03_MarGreyhoundsPro.tar and extract it using winrar/7zip etc this will create a file named 2022_03_MarGreyhoundsPro - make sure 2022_03_MarGreyhoundsPro is stored in the same location as the data extractor script - create a new empty folder that you want the extracted data to be outputted to, I created output_2022_03_MarGreyhoundsPro - then run the script Extracts and formats the content of .tar files # Extracts all the bzip2 files (.bz2) # contained within the specified output_folder and any sub-folders # and writes them to a file with their market_id as their file name # This will take around 10 mins to run for one month of Pro Greyhound data import glob import bz2 import shutil # Folder containing bz2 files or any subfolders with bz2 files input_folder = '2022_04_AprGreyhoundsPro' # change to what you have named your folder e.g. 'sample_monthly_data' # Folder to write our extracted bz2 files to, this folder needs to already be created output_folder = 'output_2022_04_AprGreyhoundsPro' # change to what you have named your folder e.g. 'sample_monthly_data_output' # Returns a list of paths to bz2 files within the input folder and any sub folders files = glob . iglob ( f ' { input_folder } /**/**/**/**/**/*.bz2' , recursive = False ) # Extracts each bz2 file and write it to the output folder for path in files : market_id = path [ - 15 : - 4 ] print ( path , market_id ) with bz2 . BZ2File ( path ) as fr , open ( f ' { output_folder } / { market_id } ' , \"wb\" ) as fw : shutil . copyfileobj ( fr , fw ) Now we are all set up lets run our sim!","title":"Set up"},{"location":"api/How_to_Automate_5/#how-the-sims-work","text":"Flumine is pretty cool, by default it hooks up to the Betfair API and it will run our strategy on live markets. When we set it to simulation mode we can hook it up to the historic data instead. The historic data is basically photos of the exhange up to every 50ms, in simulation mode Flumine essentially quickly scans through each picture sequentially essentially replaying the market. Just like how you would add_strategy() to Flumine to add a strategy that runs live, you can do the same thing in simulation mode and it will place into the simulated markets it creates. The coolest thing is, it is super easy to change it to simulation mode: ```py title = \"Setting Flumine to simulation Mode\"","title":"How the sims work"},{"location":"api/How_to_Automate_5/#set-flumine-to-simulation-mode","text":"client = clients.SimulatedClient() framework = FlumineSimulation(client=client) and instead of pointing it to markets you want to run your strategy on, you point it to your historic data files instead (as it is quite slow I would also suggest only replaying a subsection of the historic files, you can change that with the listner_kwargs), then just run it as you would any other strategy in Flumine: ```py title = \"Pointing the simulation to the historical files\" hl_lines=\"8 10\" # Set parameters for our strategy strategy = BackFavStrategy( # market_filter selects what portion of the historic data we simulate our strategy on # markets selects the list of betfair historic data files # market_types specifies the type of markets # listener_kwargs specifies the time period we simulate for each market market_filter={ \"markets\": data_files, 'market_types':['WIN'], \"listener_kwargs\": {\"inplay\": False, \"seconds_to_start\": 80}, }, max_order_exposure=1000, max_selection_exposure=1000, max_live_trade_count=1, max_trade_count=1, ) # Run our strategy on the simulated market framework.add_strategy(strategy) framework.run()","title":"Set Flumine to simulation mode"},{"location":"api/How_to_Automate_5/#running-sims-how-to-automate-ii","text":"First off the bat is simulating the strategy we created in How to Automate II. Its actually pretty easy to simulate using Flumine especially if your strategy doesn't require outside data. In fact almost all our code we previously made can just be copied accross. We just need to set Flumine to simulation mode and point it to our data files instead of at the Betfair API, which is only a few lines of code and once you read it, its pretty self explanatory. One thing we must remember to do is to add the bet logging code we made in How to Automate II so we can analyse how our strategy went afterwards. I've copied both the changes you need to make and also the complete code, give that bad boy a spin, and it will create a csv file as a log of all bets placed. A months worth of data will take ages to run (like 8 hours on my slow laptop), but the sample data should only take around 10 mins (we will go into speeding up the sims later). Changes you need to make Complete Code, changes are at the bottom # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = BackFavStrategy ( # market_filter selects what portion of the historic data we simulate our strategy on # markets selects the list of betfair historic data files # market_types specifies the type of markets # listener_kwargs specifies the time period we simulate for each market market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Import libraries import glob import os import time import logging import csv from pythonjsonlogger import jsonlogger from flumine import FlumineSimulation , clients from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation class BackFavStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : print ( \"starting strategy 'BackFavStrategy'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Collect data on last price traded and the number of bets we have placed snapshot_last_price_traded = [] snapshot_runner_context = [] for runner in market_book . runners : snapshot_last_price_traded . append ([ runner . selection_id , runner . last_price_traded ]) # Get runner context for each runner runner_context = self . get_runner_context ( market . market_id , runner . selection_id , runner . handicap ) snapshot_runner_context . append ([ runner_context . selection_id , runner_context . executable_orders , runner_context . live_trade_count , runner_context . trade_count ]) # Convert last price traded data to dataframe snapshot_last_price_traded = pd . DataFrame ( snapshot_last_price_traded , columns = [ 'selection_id' , 'last_traded_price' ]) # Find the selection_id of the favourite snapshot_last_price_traded = snapshot_last_price_traded . sort_values ( by = [ 'last_traded_price' ]) fav_selection_id = snapshot_last_price_traded [ 'selection_id' ] . iloc [ 0 ] logging . info ( snapshot_last_price_traded ) # logging # Convert data on number of bets we have placed to a dataframe snapshot_runner_context = pd . DataFrame ( snapshot_runner_context , columns = [ 'selection_id' , 'executable_orders' , 'live_trade_count' , 'trade_count' ]) logging . info ( snapshot_runner_context ) # logging for runner in market_book . runners : if runner . status == \"ACTIVE\" and market . seconds_to_start < 60 and market_book . inplay == False and runner . selection_id == fav_selection_id and snapshot_runner_context . iloc [:, 1 :] . sum () . sum () == 0 : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . last_price_traded , size = 5 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_2.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_2.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_2.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_2.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = BackFavStrategy ( # market_filter selects what portion of the historic data we simulate our strategy on # markets selects the list of betfair historic data files # market_types specifies the type of markets # listener_kwargs specifies the time period we simulate for each market market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run ()","title":"Running Sims: How to Automate II"},{"location":"api/How_to_Automate_5/#running-sims-how-to-automate-iii","text":"Okay, so we got the first one running pretty easily, a little too easily (a few lines of code and no major issues or hacky work arounds), lets test out a strategy that requires external data. In How to Automate III we automated the betfair data scientists model, lets now simulate performance. I'm going to do just the greyhound model, 'Iggy', at the moment, but the code is basically the same for the thoroughbred model, 'Kash'. Because we didn't save any of our ratings in How to Automate III we will need to redownload it now. And instead or redownloading just one days worth of data lets test out a whole month at a time. Lets reuse the function we created in How to Automate IV that we used a hacky work around that downloads the ratings for a range of dates: Download a whole month of Iggy ratings and convert it to a DataFrame def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022/02/27' , end = '2022/03/05' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) print ( iggy_df ) Now that we have downloaded a whole month of Iggy ratings to simulate it is crazy easy to simulate. We do the same thing we did when simulating How to Automate II: copy and paste the original code, and set Flumine into simulation mode pointing it to the historic data instead of the Betfair API\" Changes made to the original How to Automate III code Complete Code def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022/02/27' , end = '2022/03/05' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) print ( iggy_df ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatIggyModel ( market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Import libraries import glob import os import time import logging import csv import pandas as pd from pythonjsonlogger import jsonlogger from flumine import FlumineSimulation , BaseStrategy , utils , clients from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.order.ordertype import OrderTypes from flumine.markets.market import Market from flumine.controls.loggingcontrols import LoggingControl from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) return ( iggy_df ) # Download historical ratings over a time period and convert into a big DataFrame. back_test_period = pd . date_range ( start = '2022/02/27' , end = '2022/03/05' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) print ( iggy_df ) # Create strategy, this is the exact same strategy shown in How to Automate III class FlatIggyModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatIggyModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_3.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_3.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_3.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_3.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatIggyModel ( market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run ()","title":"Running Sims: How to Automate III"},{"location":"api/How_to_Automate_5/#simulating-how-to-automate-iv","text":"Because we coded How to Automate IV with simulating in mind (I didn't originally and had to recode it a few times), its easy for us to simulate the performance of our model. As we saved our model ratings to a csv, reading it in now actually makes the code simpler then what we created for placing live bets. This is because the issues we had working around reserve dogs and matching with the Betfair API has been taken care of (there are no reserve dogs to work around in the historic data). In fact thanks to my hacky work around in How to Automate IV the data is also in the same format as How to Automate III so we can basically use almost the exact same code we used to simulate How to Automate III. The only real differences from simulating How to Automate III and How to Automate IV is that we need to have the csv file of predictions already (available on github), read that in, and change any naming conventions that might be different. Read in model predictions and format dataframe for easy reference Complete code, almost the same as simulating How to Automate III # Read in predictions from hta_4 todays_data = pd . read_csv ( 'backtest.csv' , dtype = ({ \"market_id\" : str })) todays_data = todays_data . set_index ([ 'market_id' , 'selection_id' ]) # Import libraries import glob import os import time import logging import csv import pandas as pd import json from pythonjsonlogger import jsonlogger from flumine import FlumineSimulation , BaseStrategy , utils , clients from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.order.ordertype import OrderTypes from flumine.markets.market import Market from flumine.controls.loggingcontrols import LoggingControl from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook from dateutil import tz from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from betfairlightweight.resources import MarketCatalogue from flumine.markets.middleware import Middleware # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation # Read in predictions from hta_4 todays_data = pd . read_csv ( 'backtest.csv' , dtype = ({ \"market_id\" : str })) todays_data = todays_data . set_index ([ 'market_id' , 'selection_id' ]) ### New implementation class FlatBetting ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # At the 60 second mark: if market . seconds_to_start < 60 and market_book . inplay == False : # Can't simulate polling API # Only use streaming API: for runner in market_book . runners : model_price = todays_data . loc [ market . market_id ] . loc [ runner . selection_id ][ 'rating' ] # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_4.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_4.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_4.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_4.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatBetting ( market_filter = { \"markets\" : data_files , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run ()","title":"Simulating How to Automate IV"},{"location":"api/How_to_Automate_5/#gotta-go-fast","text":"Now that we have everything working, if you have tried any of the simulations you may notice its pretty slow. I definitely have especially for larger files such as on 1 months worth of data (probably took me around 8 hours of just running the code in the background). The good thing is we can speed it up, the bad thing is, its via multiprocessing which I have never touched before. But turns out its not too bad. You really only need to wrap your Flumine client into a function: def run_process ( markets ): \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data. Args: markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv \"\"\" # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatBetting ( market_filter = { \"markets\" : markets , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] and then split the files to run on muliple processors. You can copy the code, which is what I did, and it works without a hitch. # Multi processing if __name__ == \"__main__\" : all_markets = data_files # All the markets we want to simulate processes = os . cpu_count () # Returns the number of CPUs in the system. markets_per_process = 8 # 8 is optimal as it prevents data leakage. _process_jobs = [] with futures . ProcessPoolExecutor ( max_workers = processes ) as p : # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have. chunk = min ( markets_per_process , math . ceil ( len ( all_markets ) / processes ) ) # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs for m in ( utils . chunks ( all_markets , chunk )): _process_jobs . append ( p . submit ( run_process , markets = m , ) ) for job in futures . as_completed ( _process_jobs ): job . result () # wait for result But I wanted to understand how the code works, but it actually isn't too bad. Essentially it takes all the historical markets you have e.g. 1000, and splits it into 8 chunks. Then we run our strategy on all 8 chunks simultaniously. And this gives serious speed improvements. The complete code for all three simulations using multi processing are available at the end of this post. Future versions of Flumine are using the new BetfairData library to speed up simulations which once fully implemented should also give some serious speed benefits.","title":"Gotta go fast"},{"location":"api/How_to_Automate_5/#analysing-and-optimising-our-results","text":"Now that we have all our speedy simulation code lets look at our results and see if we found anything good. import numpy as np import pandas as pd import plotly.express as px # Read data results = pd . read_csv ( 'sim_hta_4.csv' , parse_dates = [ 'date_time_placed' ], dtype = { 'market_id' : str }) # calculate and display cumulative pnl results = results . sort_values ( by = [ 'date_time_placed' ]) results [ 'cum_profit' ] = results [ 'profit' ] . cumsum () px . line ( results , 'date_time_placed' , 'cum_profit' ) . show () post_sim_analysis_v1 Before commissions the model is profitable, which is awesome as I didn't think that would be the case. Bruno has mentioned to me that the model in the tutorial was quite \"basic\" and not profitable, but it seems we got super lucky with a few long shots getting up in March. Lets incorporate commissions into our results and see if it remains profitable: gross_profit = pd . DataFrame ( results . groupby ([ 'market_id' ])[ 'profit' ] . sum ()) # 7% commission rate on greyhounds, commissions calculated on profit at a market level calc_comms = lambda gross_profit : np . where ( gross_profit > 0 , gross_profit * ( 1 - 0.07 ), gross_profit ) gross_profit [ 'net_pnl' ] = calc_comms ( gross_profit [ 'profit' ]) gross_profit [ 'cum_npl' ] = gross_profit [ 'net_pnl' ] . cumsum () px . line ( gross_profit , gross_profit . index , 'cum_npl' ) . show () post_sim_analysis_v2 We are close, infact we were up a bit at the start but it seems after taking into account commissions we are no longer profitable and end the month down around $800. Lets try two different things to see if we can optimse our strategy: a different staking methodology and also a different time we start placing our bets. My theory is that because we are crossing the spread and taking whatever prices are available we are probably losing a bit of our edge there. If we bet when markets are more liquid then we will may lose less. But as markets become more liquid they also tend to become more efficient so lets it could work against us. Nonetheless lets test it out: Placing bets at 30 seconds instead of 60 seconds Results Before Commissions Results After Commissions def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # At the 60 second mark: if market . seconds_to_start < 30 and market_book . inplay == False : # Can't simulate polling API # Only use streaming API: for runner in market_book . runners : So the results seem pretty similar to before. After commisions we are down around $700 so we seem to be doing slighlty better. Lets try a different staking method instead, this time I have opted for a proportional staking strategy going for a fixed $10 profit on back bets and a fixed $10 liability on lay bets. There is an excellent post analysing different staking methods and I would encourage everyone to take a look at it. Lets see how our simulation went: Proportional Staking instead of Flat Staking Results Before Commissions Results After Commissions if market . seconds_to_start < 60 and market_book . inplay == False : # Can't simulate polling API # Only use streaming API: for runner in market_book . runners : model_price = todays_data . loc [ market . market_id ] . loc [ runner . selection_id ][ 'rating' ] # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = round ( 10 / ( runner . ex . available_to_back [ 0 ][ 'price' ] - 1 ), 2 )) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = round ( 10 / ( runner . ex . available_to_lay [ 0 ][ 'price' ] - 1 ), 2 )) ) market . place_order ( order ) I'm pretty surprised, I really did not expect to be profitable after commissions. We are slightly profitable at the end but we spent a significant amount of time in the negatives during the month. Without testing it out further with more historic data I going to put this down as variance for now. And I'll hand it over to you.","title":"Analysing and optimising our results"},{"location":"api/How_to_Automate_5/#conclusion-and-next-steps","text":"While we have tested our strategy and optimised it so far, I mearly tried one month of data and only three different variations of our strategy (most of which are unprofitable). Hopefully these posts help you think about what is possible when automating your strategy and how to optimise your strategy. There are plenty of other things to look at when optimising your strategy such as different staking methodologies or being more selective with your bets based on the track or state. The natural next step based on my above results would be to test out proportional staking at 30 seconds and to use a longer backtesting period. We need more data to draw a good conclusion about long term results, I have definitely found some strategies that fluke one month, but are long term losers using this method.","title":"Conclusion and next steps"},{"location":"api/How_to_Automate_5/#complete-code","text":"Multiprocess How to Automate II Multiprocess How to Automate III Multiprocess How to Automate IV Download from Github # Import libraries import glob import os import time import logging import csv import math from pythonjsonlogger import jsonlogger from concurrent import futures from flumine import FlumineSimulation , clients , utils from flumine.controls.loggingcontrols import LoggingControl from flumine.order.ordertype import OrderTypes from flumine import BaseStrategy from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.markets.market import Market from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook import pandas as pd import numpy as np import logging # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation # Create a new strategy as a new class called BackFavStrategy, this in turn will allow us to create a new Python object later # BackFavStrategy is a child class inhereting from a predefined class in Flumine we imported above called BaseStrategy class BackFavStrategy ( BaseStrategy ): # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy def start ( self ) -> None : # We will want to change what is printed with we have multiple strategies print ( \"starting strategy 'BackFavStrategy'\" ) # Defines what happens when we first look at a market # This method will prevent looking at markets that are closed def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : # process_market_book only executed if this returns True if market_book . status != \"CLOSED\" : return True # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # Find last traded price as a dataframe snapshot_last_price_traded = [] snapshot_runner_context = [] for runner in market_book . runners : snapshot_last_price_traded . append ([ runner . selection_id , runner . last_price_traded ]) # Get runner context for each runner runner_context = self . get_runner_context ( market . market_id , runner . selection_id , runner . handicap ) snapshot_runner_context . append ([ runner_context . selection_id , runner_context . executable_orders , runner_context . live_trade_count , runner_context . trade_count ]) snapshot_last_price_traded = pd . DataFrame ( snapshot_last_price_traded , columns = [ 'selection_id' , 'last_traded_price' ]) snapshot_last_price_traded = snapshot_last_price_traded . sort_values ( by = [ 'last_traded_price' ]) fav_selection_id = snapshot_last_price_traded [ 'selection_id' ] . iloc [ 0 ] snapshot_runner_context = pd . DataFrame ( snapshot_runner_context , columns = [ 'selection_id' , 'executable_orders' , 'live_trade_count' , 'trade_count' ]) for runner in market_book . runners : if runner . status == \"ACTIVE\" and market . seconds_to_start < 60 and market_book . inplay == False and runner . selection_id == fav_selection_id and snapshot_runner_context . iloc [:, 1 :] . sum () . sum () == 0 : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . last_price_traded , size = 5 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_2.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_2.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_2.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_2.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] def run_process ( markets ): \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data. Args: markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv \"\"\" # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = BackFavStrategy ( # market_filter selects what portion of the historic data we simulate our strategy on # markets selects the list of betfair historic data files # market_types specifies the type of markets # listener_kwargs specifies the time period we simulate for each market market_filter = { \"markets\" : markets , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Multi processing if __name__ == \"__main__\" : all_markets = data_files # All the markets we want to simulate processes = os . cpu_count () # Returns the number of CPUs in the system. markets_per_process = 8 # 8 is optimal as it prevents data leakage. _process_jobs = [] with futures . ProcessPoolExecutor ( max_workers = processes ) as p : # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have. chunk = min ( markets_per_process , math . ceil ( len ( all_markets ) / processes ) ) # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs for m in ( utils . chunks ( all_markets , chunk )): _process_jobs . append ( p . submit ( run_process , markets = m , ) ) for job in futures . as_completed ( _process_jobs ): job . result () # wait for result Download from Github # Import libraries import glob import os import time import logging import csv import math import pandas as pd from pythonjsonlogger import jsonlogger from concurrent import futures from flumine import FlumineSimulation , BaseStrategy , utils , clients from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.order.ordertype import OrderTypes from flumine.markets.market import Market from flumine.controls.loggingcontrols import LoggingControl from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation def download_iggy_ratings ( date ): \"\"\"Downloads the Betfair Iggy model ratings for a given date and formats it into a nice DataFrame. Args: date (datetime): the date we want to download the ratings for \"\"\" iggy_url_1 = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/iggy-joey/datasets?date=' iggy_url_2 = date . strftime ( \"%Y-%m- %d \" ) iggy_url_3 = '&presenter=RatingsPresenter&csv=true' iggy_url = iggy_url_1 + iggy_url_2 + iggy_url_3 # Download todays greyhounds ratings iggy_df = pd . read_csv ( iggy_url ) # Data clearning iggy_df = iggy_df . rename ( columns = { \"meetings.races.bfExchangeMarketId\" : \"market_id\" , \"meetings.races.runners.bfExchangeSelectionId\" : \"selection_id\" , \"meetings.races.runners.ratedPrice\" : \"rating\" }) iggy_df = iggy_df [[ 'market_id' , 'selection_id' , 'rating' ]] iggy_df [ 'market_id' ] = iggy_df [ 'market_id' ] . astype ( str ) # Set market_id and selection_id as index for easy referencing iggy_df = iggy_df . set_index ([ 'market_id' , 'selection_id' ]) return ( iggy_df ) back_test_period = pd . date_range ( start = '2022/02/27' , end = '2022/03/05' ) frames = [ download_iggy_ratings ( day ) for day in back_test_period ] iggy_df = pd . concat ( frames ) print ( iggy_df ) # Create strategy, this is the exact same strategy shown in How to Automate III class FlatIggyModel ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatIggyModel'\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : if market . seconds_to_start < 60 and market_book . inplay == False : for runner in market_book . runners : if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < iggy_df . loc [ market_book . market_id ] . loc [ runner . selection_id ] . item (): trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_3.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_3.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_3.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_3.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] def run_process ( markets ): \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data. Args: markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv \"\"\" # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatIggyModel ( market_filter = { \"markets\" : markets , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Multi processing if __name__ == \"__main__\" : all_markets = data_files # All the markets we want to simulate processes = os . cpu_count () # Returns the number of CPUs in the system. markets_per_process = 8 # 8 is optimal as it prevents data leakage. _process_jobs = [] with futures . ProcessPoolExecutor ( max_workers = processes ) as p : # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have. chunk = min ( markets_per_process , math . ceil ( len ( all_markets ) / processes ) ) # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs for m in ( utils . chunks ( all_markets , chunk )): _process_jobs . append ( p . submit ( run_process , markets = m , ) ) for job in futures . as_completed ( _process_jobs ): job . result () # wait for result Download from Github # Import libraries import glob import os import time import logging import csv import pandas as pd import json import math from pythonjsonlogger import jsonlogger from flumine import FlumineSimulation , BaseStrategy , utils , clients from flumine.order.trade import Trade from flumine.order.order import LimitOrder , OrderStatus from flumine.order.ordertype import OrderTypes from flumine.markets.market import Market from flumine.controls.loggingcontrols import LoggingControl from betfairlightweight.filters import streaming_market_filter from betfairlightweight.resources import MarketBook from pythonjsonlogger import jsonlogger from concurrent import futures # Logging logger = logging . getLogger () custom_format = \" %(asctime) % (levelname) %(message)\" log_handler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( custom_format ) formatter . converter = time . gmtime log_handler . setFormatter ( formatter ) logger . addHandler ( log_handler ) logger . setLevel ( logging . INFO ) # Set to logging.CRITICAL to speed up simulation # Read in predictions from hta_4 todays_data = pd . read_csv ( 'backtest.csv' , dtype = ({ \"market_id\" : str })) todays_data = todays_data . set_index ([ 'market_id' , 'selection_id' ]) ### New implementation class FlatBetting ( BaseStrategy ): def start ( self ) -> None : print ( \"starting strategy 'FlatBetting' using the model we created the Greyhound modelling in Python Tutorial\" ) def check_market_book ( self , market : Market , market_book : MarketBook ) -> bool : if market_book . status != \"CLOSED\" : return True def process_market_book ( self , market : Market , market_book : MarketBook ) -> None : # At the 60 second mark: if market . seconds_to_start < 60 and market_book . inplay == False : # Can't simulate polling API # Only use streaming API: for runner in market_book . runners : model_price = todays_data . loc [ market . market_id ] . loc [ runner . selection_id ][ 'rating' ] # If best available to back price is > rated price then flat $5 back if runner . status == \"ACTIVE\" and runner . ex . available_to_back [ 0 ][ 'price' ] > model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"BACK\" , order_type = LimitOrder ( price = runner . ex . available_to_back [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # If best available to lay price is < rated price then flat $5 lay if runner . status == \"ACTIVE\" and runner . ex . available_to_lay [ 0 ][ 'price' ] < model_price : trade = Trade ( market_id = market_book . market_id , selection_id = runner . selection_id , handicap = runner . handicap , strategy = self , ) order = trade . create_order ( side = \"LAY\" , order_type = LimitOrder ( price = runner . ex . available_to_lay [ 0 ][ 'price' ], size = 5.00 ) ) market . place_order ( order ) # Fields we want to log in our simulations FIELDNAMES = [ \"bet_id\" , \"strategy_name\" , \"market_id\" , \"selection_id\" , \"trade_id\" , \"date_time_placed\" , \"price\" , \"price_matched\" , \"size\" , \"size_matched\" , \"profit\" , \"side\" , \"elapsed_seconds_executable\" , \"order_status\" , \"market_note\" , \"trade_notes\" , \"order_notes\" , ] # Log results from simulation into csv file named sim_hta_4.csv # If the csv file doesn't exist then it is created, otherwise we append results to the csv file class BacktestLoggingControl ( LoggingControl ): NAME = \"BACKTEST_LOGGING_CONTROL\" def __init__ ( self , * args , ** kwargs ): super ( BacktestLoggingControl , self ) . __init__ ( * args , ** kwargs ) self . _setup () def _setup ( self ): if os . path . exists ( \"sim_hta_4.csv\" ): logging . info ( \"Results file exists\" ) else : with open ( \"sim_hta_4.csv\" , \"w\" ) as m : csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writeheader () def _process_cleared_orders_meta ( self , event ): orders = event . event with open ( \"sim_hta_4.csv\" , \"a\" ) as m : for order in orders : if order . order_type . ORDER_TYPE == OrderTypes . LIMIT : size = order . order_type . size else : size = order . order_type . liability if order . order_type . ORDER_TYPE == OrderTypes . MARKET_ON_CLOSE : price = None else : price = order . order_type . price try : order_data = { \"bet_id\" : order . bet_id , \"strategy_name\" : order . trade . strategy , \"market_id\" : order . market_id , \"selection_id\" : order . selection_id , \"trade_id\" : order . trade . id , \"date_time_placed\" : order . responses . date_time_placed , \"price\" : price , \"price_matched\" : order . average_price_matched , \"size\" : size , \"size_matched\" : order . size_matched , \"profit\" : order . simulated . profit , \"side\" : order . side , \"elapsed_seconds_executable\" : order . elapsed_seconds_executable , \"order_status\" : order . status . value , \"market_note\" : order . trade . market_notes , \"trade_notes\" : order . trade . notes_str , \"order_notes\" : order . notes_str , } csv_writer = csv . DictWriter ( m , delimiter = \",\" , fieldnames = FIELDNAMES ) csv_writer . writerow ( order_data ) except Exception as e : logger . error ( \"_process_cleared_orders_meta: %s \" % e , extra = { \"order\" : order , \"error\" : e }, ) logger . info ( \"Orders updated\" , extra = { \"order_count\" : len ( orders )}) def _process_cleared_markets ( self , event ): cleared_markets = event . event for cleared_market in cleared_markets . orders : logger . info ( \"Cleared market\" , extra = { \"market_id\" : cleared_market . market_id , \"bet_count\" : cleared_market . bet_count , \"profit\" : cleared_market . profit , \"commission\" : cleared_market . commission , }, ) # Searches for all betfair data files within the folder sample_monthly_data_output data_folder = 'sample_monthly_data_output' data_files = os . listdir ( data_folder ,) data_files = [ f ' { data_folder } / { path } ' for path in data_files ] def run_process ( markets ): \"\"\"Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data. Args: markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv \"\"\" # Set Flumine to simulation mode client = clients . SimulatedClient () framework = FlumineSimulation ( client = client ) # Set parameters for our strategy strategy = FlatBetting ( market_filter = { \"markets\" : markets , 'market_types' :[ 'WIN' ], \"listener_kwargs\" : { \"inplay\" : False , \"seconds_to_start\" : 80 }, }, max_order_exposure = 1000 , max_selection_exposure = 1000 , max_live_trade_count = 1 , max_trade_count = 1 , ) # Run our strategy on the simulated market framework . add_strategy ( strategy ) framework . add_logging_control ( BacktestLoggingControl () ) framework . run () # Multi processing if __name__ == \"__main__\" : all_markets = data_files # All the markets we want to simulate processes = os . cpu_count () # Returns the number of CPUs in the system. markets_per_process = 8 # 8 is optimal as it prevents data leakage. _process_jobs = [] with futures . ProcessPoolExecutor ( max_workers = processes ) as p : # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have. chunk = min ( markets_per_process , math . ceil ( len ( all_markets ) / processes ) ) # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs for m in ( utils . chunks ( all_markets , chunk )): _process_jobs . append ( p . submit ( run_process , markets = m , ) ) for job in futures . as_completed ( _process_jobs ): job . result () # wait for result","title":"Complete Code"},{"location":"api/How_to_Automate_5/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"api/apiPythontutorial/","text":"Betfair API tutorial in Python This tutorial will walk you through the process of connecting to Betfair's API, grabbing data and placing a bet in Python. It will utilise the betfairlightweight Python library. Requirements This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here . This tutorial will also assume that you have a basic understanding of what an API is. For a summary in layman's terms, read this article . Quick Links Here are some other useful links for accessing our API: How to create an API app key Developer Docs - the official dev docs for Betfair's API Sports API Visualiser - Useful for exploring what the API has to offer Account API Visualiser Examples using betfairlightweight There's a more complete list of resources here Getting Started Setting Up Your Certificates To use the API securely, Betfair recommends generating certificates. The betfairlightweight package requires this to login non-interactively. For detailed instructions on how to generate certificates on a windows machine, follow the instructions outlined here . For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here . You should then create a folder for your certs, perhaps named 'certs' and grab the path location. Installing betfairlightweight We also need to install betfairlightweight . To do this, simply use pip install betfairlightweight in the cmd prompt/terminal. If this doesn't work, you will have to Google your error. If you're just starting out with Python, you may have to add Python to your environment variables. Sending Requests to the API Log into the API Client Now we're finally ready to log in and use the API. First, we create an APIClient object and then log in. To log in, we'll need to specify where we put our certs. In this example, I'll put them in a folder named 'certs', on my desktop. You'll also need to change the username , password and app_key variables to your own. In [206]: # Import libraries import betfairlightweight from betfairlightweight import filters import pandas as pd import numpy as np import os import datetime import json # Change this certs path to wherever you're storing your certificates with open ( 'credentials.json' ) as f : cred = jsonload ( f ) my_username = cred [ 'username' ] my_password = cred [ 'password' ] my_app_key = cred [ 'app_key' ] trading = betfairlightweight . APIClient ( username = my_username , password = my_password , app_key = my_app_key , certs = certs_path ) trading . login () Out[206]: < LoginResource > Get Event IDs Betfair's API has a number of operations. For example, if you want to list the market book for a market, you would use the listMarketBook operation. These endpoints are shown in the Sports API Visualiser and in the docs. They are also listed below: Sports API listEventTypes listCompetitions listTimeRanges listEvents listMarketTypes listCountries listVenues listMarketCatalogue listMarketBook listRunnerBook placeOrders cancelOrders updateOrders replaceOrders listCurrentOrders listClearedOrders listMarketProfitAndLoss The Account Operations API operations/endpoints can be found here . First we need to grab the 'Event Type Id'. Each sport has a different ID. Below we will find the ids for all sports by requesting the event_type_ids without a filter. In [43]: # Grab all event type ids. This will return a list which we will iterate over to print out the id and the name of the sport event_types = trading . betting . list_event_types () sport_ids = pd . DataFrame ({ 'Sport' : [ event_type_object . event_type . name for event_type_object in event_types ], 'ID' : [ event_type_object . event_type . id for event_type_object in event_types ] }) . set_index ( 'Sport' ) . sort_index () sport_ids Out[43]: Sport ID American Football 6423 Athletics 3988 Australian Rules 61420 Baseball 7511 Basketball 7522 Boxing 6 Chess 136332 Cricket 4 Cycling 11 Darts 3503 Esports 27454571 Financial Bets 6231 Gaelic Games 2152880 Golf 3 Greyhound Racing 4339 Handball 468328 Horse Racing 7 Ice Hockey 7524 Mixed Martial Arts 26420387 Motor Sport 8 Netball 606611 Politics 2378961 Rugby League 1477 Rugby Union 5 Snooker 6422 Soccer 1 Special Bets 10 Tennis 2 Volleyball 998917 If we just wanted to get the event id for horse racing, we could use the filter function from betfairlightweight as shown in the examples and below. In [50]: # Filter for just horse racing horse_racing_filter = betfairlightweight . filters . market_filter ( text_query = 'Horse Racing' ) # This returns a list horse_racing_event_type = trading . betting . list_event_types ( filter = horse_racing_filter ) # Get the first element of the list horse_racing_event_type = horse_racing_event_type [ 0 ] horse_racing_event_type_id = horse_racing_event_type . event_type . id print ( f \"The event type id for horse racing is { horse_racing_event_type_id } \" ) # The event type id for horse racing is 7 Get Competition IDs Sometimes you may want to get markets based on the competition. An example may be the Brownlow medal, or the EPL. Let's have a look at all the soccer competitions over the next week and filter to only get the EPL Competition ID. In [90]: # Get a datetime object in a week and convert to string datetime_in_a_week = ( datetime . datetime . utcnow () + datetime . timedelta ( weeks = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) # Create a competition filter competition_filter = betfairlightweight . filters . market_filter ( event_type_ids = [ 1 ], # Soccer's event type id is 1 market_start_time = { 'to' : datetime_in_a_week }) # Get a list of competitions for soccer competitions = trading . betting . list_competitions ( filter = competition_filter ) # Iterate over the competitions and create a dataframe of competitions and competition ids soccer_competitions = pd . DataFrame ({ 'Competition' : [ competition_object . competition . name for competition_object in competitions ], 'ID' : [ competition_object . competition . id for competition_object in competitions ] }) In [94]: # Get the English Premier League Competition ID soccer_competitions [ soccer_competitions . Competition . str . contains ( 'English Premier' )] Out[94]: Competition ID 116 English Premier League 10932509 Get Upcoming Events Say you want to get all the upcoming events for Thoroughbreads for the next 24 hours. We will use the listEvents operation for this. First, as before, we define a market filter, and then using the betting method from our trading object which we defined earlier. In [207]: # Define a market filter thoroughbreds_event_filter = betfairlightweight . filters . market_filter ( event_type_ids = [ horse_racing_event_type_id ], market_countries = [ 'AU' ], market_start_time = { 'to' : ( datetime . datetime . utcnow () + datetime . timedelta ( days = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) } ) # Print the filter thoroughbreds_event_filter Out[207]: { 'eventTypeIds' : [ '7' ], 'marketCountries' : [ 'AU' ], 'marketStartTime' : { 'to' : '2018-10-26T22:25:00Z' }} In [208]: # Get a list of all thoroughbred events as objects aus_thoroughbred_events = trading . betting . list_events ( filter = thoroughbreds_event_filter ) # Create a DataFrame with all the events by iterating over each event object aus_thoroughbred_events_today = pd . DataFrame ({ 'Event Name' : [ event_object . event . name for event_object in aus_thoroughbred_events ], 'Event ID' : [ event_object . event . id for event_object in aus_thoroughbred_events ], 'Event Venue' : [ event_object . event . venue for event_object in aus_thoroughbred_events ], 'Country Code' : [ event_object . event . country_code for event_object in aus_thoroughbred_events ], 'Time Zone' : [ event_object . event . time_zone for event_object in aus_thoroughbred_events ], 'Open Date' : [ event_object . event . open_date for event_object in aus_thoroughbred_events ], 'Market Count' : [ event_object . market_count for event_object in aus_thoroughbred_events ] }) aus_thoroughbred_events_today Out[208]: Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 MVal (AUS) 26th Oct 28971066 Moonee Valley AU Australia/Sydney 2018-10-26 07:30:00 24 1 Newc (AUS) 26th Oct 28974559 Newcastle AU Australia/Sydney 2018-10-26 07:07:00 20 2 Bath (AUS) 26th Oct 28974547 Bathurst AU Australia/Sydney 2018-10-26 02:43:00 16 3 Cant (AUS) 26th Oct 28974545 Canterbury AU Australia/Sydney 2018-10-26 07:15:00 16 4 Scne (AUS) 26th Oct 28973942 Scone AU Australia/Sydney 2018-10-26 02:25:00 16 5 Gawl (AUS) 26th Oct 28974550 Gawler AU Australia/Adelaide 2018-10-26 04:00:00 16 6 Gatt (AUS) 26th Oct 28974549 Gatton AU Australia/Queensland 2018-10-26 01:55:00 16 7 GlPk (AUS) 26th Oct 28974562 Gloucester Park AU Australia/Perth 2018-10-26 09:10:00 20 8 Hoba (AUS) 26th Oct 28974563 Hobart AU Australia/Sydney 2018-10-26 05:23:00 18 9 Echu (AUS) 26th Oct 28974016 Echuca AU Australia/Sydney 2018-10-26 01:30:00 18 10 Melt (AUS) 26th Oct 28974560 Melton AU Australia/Sydney 2018-10-26 07:18:00 18 11 MVal (AUS) 26th Oct 28921730 None AU Australia/Sydney 2018-10-26 11:00:00 1 12 Redc (AUS) 26th Oct 28974561 Redcliffe AU Australia/Queensland 2018-10-26 02:17:00 16 13 SCst (AUS) 26th Oct 28974149 Sunshine Coast AU Australia/Queensland 2018-10-26 06:42:00 20 Get Market Types Say we want to know what market types a certain event is offering. To do this, we use the listMarketTypes operation. Let's take the Moonee Valley event from above (ID: 28971066). As this is a horse race we would expect that it would have Win and Place markets. In [209]: # Define a market filter market_types_filter = betfairlightweight . filters . market_filter ( event_ids = [ '28971066' ]) # Request market types market_types = trading . betting . list_market_types ( filter = market_types_filter ) # Create a DataFrame of market types market_types_mooney_valley = pd . DataFrame ({ 'Market Type' : [ market_type_object . market_type for market_type_object in market_types ], }) market_types_mooney_valley Out[209]: Market Type 0 OTHER_PLACE 1 PLACE 2 WIN Get Market Catalogues If we want to know the various market names that there are for a particular event, as well as how much has been matched on each market, we want to request data from the listMarketCatalogue operation. We can provide a number of filters, including the Competition ID, the Event ID, the Venue etc. to the filter. We must also specify the maximum number of results, and if we want additional data like the event data or runner data, we can also request that. For a more comprehensive understanding of the options for filters and what we can request, please have a look at the Sports API Visualiser . The options listed under market filter should be put into a filter, whilst the others should be arguments to the relevant operation function in betfairlightweight . For example, if we want all the markets for Moonee Valley, we should use the following filters and arguments. In [210]: market_catalogue_filter = betfairlightweight . filters . market_filter ( event_ids = [ '28971066' ]) market_catalogues = trading . betting . list_market_catalogue ( filter = market_catalogue_filter , max_results = '100' , sort = 'FIRST_TO_START' ) # Create a DataFrame for each market catalogue market_types_mooney_valley = pd . DataFrame ({ 'Market Name' : [ market_cat_object . market_name for market_cat_object in market_catalogues ], 'Market ID' : [ market_cat_object . market_id for market_cat_object in market_catalogues ], 'Total Matched' : [ market_cat_object . total_matched for market_cat_object in market_catalogues ], }) market_types_mooney_valley Out[210]: Market Name Market ID Total Matched 0 4 TBP 1.150090094 0.000000 1 To Be Placed 1.150090092 0.000000 2 R1 1000m 3yo 1.150090091 2250.188360 3 4 TBP 1.150090101 0.000000 4 To Be Placed 1.150090099 141.775816 5 R2 2040m Hcap 1.150090098 1093.481760 6 To Be Placed 1.150090106 0.000000 7 R3 1500m Hcap 1.150090105 1499.642480 8 4 TBP 1.150090108 0.000000 9 To Be Placed 1.150090113 19.855136 10 R4 2040m Hcap 1.150090112 588.190288 11 4 TBP 1.150090115 0.000000 12 4 TBP 1.150090122 0.000000 13 R5 955m Hcap 1.150090119 545.762616 14 To Be Placed 1.150090120 91.920584 15 4 TBP 1.150090129 48.623344 16 To Be Placed 1.150090127 65.616152 17 R6 1200m Hcap 1.150090126 506.342200 18 R7 1200m Grp1 1.150038686 34480.834976 19 4 TBP 1.150038689 701.052968 20 To Be Placed 1.150038687 1504.823656 21 R8 1500m Hcap 1.150090140 232.971760 22 4 TBP 1.150090143 0.000000 23 To Be Placed 1.150090141 73.768352 Get Market Books If we then want to get the prices available/last traded for a market, we should use the listMarketBook operation. Let's Look at the market book for Moonee Valley R7. We will need to define a function which processes the runner books and collates the data into a DataFrame. In [212]: def process_runner_books ( runner_books ): ''' This function processes the runner books and returns a DataFrame with the best back/lay prices + vol for each runner :param runner_books: :return: ''' best_back_prices = [ runner_book . ex . available_to_back [ 0 ] . price if runner_book . ex . available_to_back . price else 1.01 for runner_book in runner_books ] best_back_sizes = [ runner_book . ex . available_to_back [ 0 ] . size if runner_book . ex . available_to_back . size else 1.01 for runner_book in runner_books ] best_lay_prices = [ runner_book . ex . available_to_lay [ 0 ] . price if runner_book . ex . available_to_lay . price else 1000.0 for runner_book in runner_books ] best_lay_sizes = [ runner_book . ex . available_to_lay [ 0 ] . size if runner_book . ex . available_to_lay . size else 1.01 for runner_book in runner_books ] selection_ids = [ runner_book . selection_id for runner_book in runner_books ] last_prices_traded = [ runner_book . last_price_traded for runner_book in runner_books ] total_matched = [ runner_book . total_matched for runner_book in runner_books ] statuses = [ runner_book . status for runner_book in runner_books ] scratching_datetimes = [ runner_book . removal_date for runner_book in runner_books ] adjustment_factors = [ runner_book . adjustment_factor for runner_book in runner_books ] df = pd . DataFrame ({ 'Selection ID' : selection_ids , 'Best Back Price' : best_back_prices , 'Best Back Size' : best_back_sizes , 'Best Lay Price' : best_lay_prices , 'Best Lay Size' : best_lay_sizes , 'Last Price Traded' : last_prices_traded , 'Total Matched' : total_matched , 'Status' : statuses , 'Removal Date' : scratching_datetimes , 'Adjustment Factor' : adjustment_factors }) return df In [213]: # Create a price filter. Get all traded and offer data price_filter = ` betfairlightweight ` . filters . price_projection ( price_data = [ 'EX_BEST_OFFERS' ] ) # Request market books market_books = trading . betting . list_market_book ( market_ids = [ '1.150038686' ], price_projection = price_filter ) # Grab the first market book from the returned list as we only requested one market market_book = market_books [ 0 ] runners_df = process_runner_books ( market_book . runners ) runners_df Out[213]: Selection ID Best Back Price Best Back Size Best Lay Price Best Lay Size Last Price Traded Total Matched Status Removal Date Adjustment Factor 0 16905731 12.0 65.54 13.0 33.09 12.0 1226.67 ACTIVE None 8.333 1 15815968 6.6 96.64 7.0 9.00 6.6 5858.61 ACTIVE None 14.286 2 9384677 14.0 114.71 15.0 76.71 14.0 964.80 ACTIVE None 6.667 3 8198751 17.5 14.67 19.0 33.02 17.5 940.56 ACTIVE None 5.556 4 9507057 38.0 53.13 100.0 40.22 46.0 224.72 ACTIVE None 3.125 5 21283266 15.0 121.46 19.5 5.56 19.5 1102.37 ACTIVE None 7.692 6 21283267 80.0 37.58 760.0 9.70 760.0 125.30 ACTIVE None 1.087 7 21063807 6.4 1503.62 7.2 50.00 6.6 8011.44 ACTIVE None 13.333 8 21283268 48.0 54.57 60.0 51.93 50.0 150.22 ACTIVE None 2.381 9 21283269 8.8 235.77 9.4 30.40 8.8 1729.96 ACTIVE None 11.111 10 4883975 46.0 33.42 55.0 5.00 46.0 208.45 ACTIVE None 2.381 11 202351 25.0 20.00 30.0 6.00 24.0 658.09 ACTIVE None 2.632 12 21283270 19.5 69.33 22.0 20.00 19.5 825.59 ACTIVE None 4.545 13 21283271 5.3 96.14 5.7 5.03 5.3 12654.32 ACTIVE None 16.871 Orderbook Workflow Now that we have the market book in an easy to read DataFrame, we can go ahead and start placing orders based on the market book. Although it is a simple (and probably not profitable) strategy, in the next few sections we will be backing the favourite and adjusting our orders. Placing Orders To place an order we use the placeOrders operation. A handy component of placeOrders is that you can send your strategy along with the runner that you want to back, so it is extremely easy to analyse how your strategy performed later. Let's place a 5 dollar back bet on the favourite at $7 call this strategy 'back_the_fav' . Note that if you are placing a limit order you must specify a price which is allowed by Betfair. For example, the price 6.3 isn't allowed, whereas 6.4 is, as prices go up by 20c increments at that price range. You can read about tick points here . In [232]: # Get the favourite's price and selection id fav_selection_id = runners_df . loc [ runners_df [ 'Best Back Price' ] . idxmin (), 'Selection ID' ] fav_price = runners_df . loc [ runners_df [ 'Best Back Price' ] . idxmin (), 'Best Back Price' ] In [276]: # Define a limit order filter limit_order_filter = betfairlightweight . filters . limit_order ( size = 5 , price = 7 , persistence_type = 'LAPSE' ) # Define an instructions filter instructions_filter = betfairlightweight . filters . place_instruction ( selection_id = str ( fav_selection_id ), order_type = \"LIMIT\" , side = \"BACK\" , limit_order = limit_order_filter ) instructions_filter Out[276]: { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7 , 'size' : 5 }, 'orderType' : 'LIMIT' , 'selectionId' : '21283271' , 'side' : 'BACK' } In [277]: # Place the order order = trading . betting . place_orders ( market_id = '1.150038686' , # The market id we obtained from before customer_strategy_ref = 'back_the_fav' , instructions = [ instructions_filter ] # This must be a list ) Now that we've placed the other, we can check if the order placing was a success and if any has been matched. In [306]: order . __dict__ Out[306]: { '_data' : { 'instructionReports' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'instruction' : { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7.0 , 'size' : 5.0 }, 'orderType' : 'LIMIT' , 'selectionId' : 21283271 , 'side' : 'BACK' }, 'orderStatus' : 'EXECUTABLE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'sizeMatched' : 0.0 , 'status' : 'SUCCESS' }], 'marketId' : '1.150038686' , 'status' : 'SUCCESS' }, '_datetime_created' : datetime . datetime ( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), '_datetime_updated' : datetime . datetime ( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), 'customer_ref' : None , 'elapsed_time' : 1.484069 , 'error_code' : None , 'market_id' : '1.150038686' , 'place_instruction_reports' : [ < betfairlightweight . resources . bettingresources . PlaceOrderInstructionReports at 0x23e0f7952e8 > ], 'status' : 'SUCCESS' } As we can see, the status is 'SUCCESS' , whilst the sizeMatched is 0. Let's now look at our current orders. Get Current Orders To get our current orders, we need to use the listCurrentOrders operation. We can then use either the bet id, the market id, or the bet strategy to filter our orders. In [311]: trading . betting . list_current_orders ( customer_strategy_refs = [ 'back_the_fav' ]) . __dict__ Out [ 311 ]: { '_data' : { 'currentOrders' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'bspLiability' : 0.0 , 'customerStrategyRef' : 'back_the_fav' , 'handicap' : 0.0 , 'marketId' : '1.150038686' , 'orderType' : 'LIMIT' , 'persistenceType' : 'LAPSE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'priceSize' : { 'price' : 7.0 , 'size' : 5.0 }, 'regulatorCode' : 'MALTA LOTTERIES AND GAMBLING AUTHORITY' , 'selectionId' : 21283271 , 'side' : 'BACK' , 'sizeCancelled' : 0.0 , 'sizeLapsed' : 0.0 , 'sizeMatched' : 0.0 , 'sizeRemaining' : 5.0 , 'sizeVoided' : 0.0 , 'status' : 'EXECUTABLE' }], 'moreAvailable' : False }, '_datetime_created' : datetime . datetime ( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), '_datetime_updated' : datetime . datetime ( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), 'elapsed_time' : 1.327456 , 'more_available' : False , 'orders' : [ < betfairlightweight . resources . bettingresources . CurrentOrder at 0x23e0e7acd30 > ], 'publish_time' : None , 'streaming_unique_id' : None , 'streaming_update' : None } As we can see, we have one order which is unmatched for our strategy 'back_the_fav' Cancelling Orders Let's now cancel this bet. To do this, we will use the cancelOrders operation. If you pass in a market ID it will cancel all orders for that specific market ID, like you can do on the website. In [312]: cancelled_order = trading . betting . cancel_orders ( market_id = '1.150038686' ) In [328]: # Create a DataFrame to view the instruction report pd . Series ( cancelled_order . cancel_instruction_reports [ 0 ] . __dict__ ) . to_frame () . T Out[328]: status size_cancelled cancelled_date instruction error_code 0 SUCCESS 5 2018-10-26 06:01:26 betfairlightweight.resources.bettingresources... None Get Past Orders and Results If we want to go back and look at past orders we have made, there are two main operations for this: listClearedOrders - this operation takes a range of data down to the individual selection ID level, and returns a summary of those specific orders listMarketProfitAndLoss - this operation is more specific, and only takes Market IDs to return the Profit/Loss for that market Alternatively, we can use the getAccountStatement operation from the Account Operations API. Let's now use both Sports API operations based on our previous orders and then compare it to the getAccountStatement operation. Get Cleared Orders In [346]: # listClearedOrders cleared_orders = trading . betting . list_cleared_orders ( bet_status = \"SETTLED\" , market_ids = [ \"1.150038686\" ]) In [371]: # Create a DataFrame from the orders pd . DataFrame ( cleared_orders . _data [ 'clearedOrders' ]) Out[371]: betCount betId betOutcome eventId eventTypeId handicap lastMatchedDate marketId orderType persistenceType placedDate priceMatched priceReduced priceRequested profit selectionId settledDate side sizeSettled 0 1 142383373022 LOST 28971066 7 0.0 2018-10-26T10:31:53.000Z 1.150038686 MARKET_ON_CLOSE LAPSE 2018-10-26T00:12:03.000Z 5.74 False 5.74 -5.0 21283271 2018-10-26T10:34:39.000Z BACK 5.0 1 1 142383570640 WON 28971066 7 0.0 2018-10-26T00:16:32.000Z 1.150038686 LIMIT LAPSE 2018-10-26T00:16:31.000Z 5.40 False 5.50 5.0 21283271 2018-10-26T10:34:39.000Z LAY 5.0 Note that we can also filter for certain dates, bet ids, event ids, selection ids etc. We can also group by the event type, the event, the market, the runner, the side, the bet and the strategy, which is extremely useful if you're looking for a quick summary of how your strategy is performing. Get Market Profit and Loss Now let's find the Profit and Loss for the market. To do this we will use the listMarketProfitAndLoss operation. Note that this function only works with market IDs, and once the website clears the market, the operation will no longer work. However the market is generally up for about a minute after the race, so if your strategy is automated, you can check once if your bet is settled and if it is, hit the getMarketProfitAndLoss endpoint. Because of this, we will check a different market ID to the example above. In [406]: # Get the profit/loss - this returns a list pl = trading . betting . list_market_profit_and_loss ( market_ids = [ \"1.150318913\" ], include_bsp_bets = 'true' , include_settled_bets = 'true' ) In [410]: # Create a profit/loss DataFrame pl_df = pd . DataFrame ( pl [ 0 ] . _data [ 'profitAndLosses' ]) . assign ( marketId = pl [ 0 ] . market_id ) pl_df Out[410]: ifWin selectionId marketId 0 -5.0 10065177 1.150318913 1 14.0 17029506 1.150318913 2 -5.0 5390339 1.150318913 3 -5.0 13771011 1.150318913 4 -5.0 138209 1.150318913 5 -5.0 10503541 1.150318913 6 -5.0 12165809 1.150318913 Get Account Statement Another method is to use the getAccountStatement , which provides an overview of all your bets over a certain time period. You can then filter this for specific dates if you wish. In [428]: # Define a date filter - get all bets for the past 4 days four_days_ago = ( datetime . datetime . utcnow () - datetime . timedelta ( days = 4 )) . strftime ( \"%Y-%m- %d T%TZ\" ) acct_statement_date_filter = betfairlightweight . filters . time_range ( from_ = four_days_ago ) # Request account statement account_statement = trading . account . get_account_statement ( item_date_range = acct_statement_date_filter ) In [450]: # Create df of recent transactions recent_transactions = pd . DataFrame ( account_statement . _data [ 'accountStatement' ]) recent_transactions Out[450]: amount balance itemClass itemClassData itemDate legacyData refId 0 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":3.8,\"bet... 2018-10-28T23:14:28.000Z {'avgPrice': 3.8, 'betSize': 5.0, 'betType': '... 142845441633 1 5.0 261.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.4,\"bet... 2018-10-26T10:34:39.000Z {'avgPrice': 5.4, 'betSize': 5.0, 'betType': '... 142383570640 2 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.74,\"be... 2018-10-26T10:34:39.000Z {'avgPrice': 5.74, 'betSize': 5.0, 'betType': ... 142383373022 In [468]: # Create df of itemClassData - iterate over the account statement list and convert to json so that the DataFrame function # can read it correctly class_data = [ json . loads ( account_statement . account_statement [ i ] . item_class_data [ 'unknownStatementItem' ]) for i in range ( len ( account_statement . account_statement ))] In [471]: class_df = pd . DataFrame ( class_data ) class_df Out [471]: avgPrice betCategoryType betSize betType commissionRate eventId eventTypeId fullMarketName grossBetAmount marketName marketType placedDate selectionId selectionName startDate transactionId transactionType winLose 0 3.80 M 5.0 B None 150318913 7 USA / TPara (US) 28th Oct/ 16:06 R8 1m Allw Claim 0.0 R8 1m Allw Claim O 2018-10-28T23:02:28.000Z 17029506 Gato Guapo 2018-10-28T23:06:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST 1 5.40 E 5.0 L None 150038686 7 AUS / MVal (AUS) 26th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:16:31.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_CREDIT 2 5.74 M 5.0 B None 150038686 7 AUS / MVal (AUS) 26th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:12:03.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST As we can see, this DataFrame provides a much more comprehensive view of each of our bets. However, it lacks the ability to filter by strategy like the listClearedOrders operation in the Sports API.","title":"API tutorial in Python"},{"location":"api/apiPythontutorial/#betfair-api-tutorial-in-python","text":"This tutorial will walk you through the process of connecting to Betfair's API, grabbing data and placing a bet in Python. It will utilise the betfairlightweight Python library.","title":"Betfair API tutorial in Python"},{"location":"api/apiPythontutorial/#requirements","text":"This tutorial will assume that you have an API app key. If you don't, please follow the steps outlined here . This tutorial will also assume that you have a basic understanding of what an API is. For a summary in layman's terms, read this article .","title":"Requirements"},{"location":"api/apiPythontutorial/#quick-links","text":"Here are some other useful links for accessing our API: How to create an API app key Developer Docs - the official dev docs for Betfair's API Sports API Visualiser - Useful for exploring what the API has to offer Account API Visualiser Examples using betfairlightweight There's a more complete list of resources here","title":"Quick Links"},{"location":"api/apiPythontutorial/#getting-started","text":"","title":"Getting Started"},{"location":"api/apiPythontutorial/#setting-up-your-certificates","text":"To use the API securely, Betfair recommends generating certificates. The betfairlightweight package requires this to login non-interactively. For detailed instructions on how to generate certificates on a windows machine, follow the instructions outlined here . For alternate instructions for Windows, or for Mac/Linux machines, follow the instructions outlined here . You should then create a folder for your certs, perhaps named 'certs' and grab the path location.","title":"Setting Up Your Certificates"},{"location":"api/apiPythontutorial/#installing-betfairlightweight","text":"We also need to install betfairlightweight . To do this, simply use pip install betfairlightweight in the cmd prompt/terminal. If this doesn't work, you will have to Google your error. If you're just starting out with Python, you may have to add Python to your environment variables.","title":"Installing betfairlightweight"},{"location":"api/apiPythontutorial/#sending-requests-to-the-api","text":"","title":"Sending Requests to the API"},{"location":"api/apiPythontutorial/#log-into-the-api-client","text":"Now we're finally ready to log in and use the API. First, we create an APIClient object and then log in. To log in, we'll need to specify where we put our certs. In this example, I'll put them in a folder named 'certs', on my desktop. You'll also need to change the username , password and app_key variables to your own. In [206]: # Import libraries import betfairlightweight from betfairlightweight import filters import pandas as pd import numpy as np import os import datetime import json # Change this certs path to wherever you're storing your certificates with open ( 'credentials.json' ) as f : cred = jsonload ( f ) my_username = cred [ 'username' ] my_password = cred [ 'password' ] my_app_key = cred [ 'app_key' ] trading = betfairlightweight . APIClient ( username = my_username , password = my_password , app_key = my_app_key , certs = certs_path ) trading . login () Out[206]: < LoginResource >","title":"Log into the API Client"},{"location":"api/apiPythontutorial/#get-event-ids","text":"Betfair's API has a number of operations. For example, if you want to list the market book for a market, you would use the listMarketBook operation. These endpoints are shown in the Sports API Visualiser and in the docs. They are also listed below:","title":"Get Event IDs"},{"location":"api/apiPythontutorial/#sports-api","text":"listEventTypes listCompetitions listTimeRanges listEvents listMarketTypes listCountries listVenues listMarketCatalogue listMarketBook listRunnerBook placeOrders cancelOrders updateOrders replaceOrders listCurrentOrders listClearedOrders listMarketProfitAndLoss The Account Operations API operations/endpoints can be found here . First we need to grab the 'Event Type Id'. Each sport has a different ID. Below we will find the ids for all sports by requesting the event_type_ids without a filter. In [43]: # Grab all event type ids. This will return a list which we will iterate over to print out the id and the name of the sport event_types = trading . betting . list_event_types () sport_ids = pd . DataFrame ({ 'Sport' : [ event_type_object . event_type . name for event_type_object in event_types ], 'ID' : [ event_type_object . event_type . id for event_type_object in event_types ] }) . set_index ( 'Sport' ) . sort_index () sport_ids Out[43]: Sport ID American Football 6423 Athletics 3988 Australian Rules 61420 Baseball 7511 Basketball 7522 Boxing 6 Chess 136332 Cricket 4 Cycling 11 Darts 3503 Esports 27454571 Financial Bets 6231 Gaelic Games 2152880 Golf 3 Greyhound Racing 4339 Handball 468328 Horse Racing 7 Ice Hockey 7524 Mixed Martial Arts 26420387 Motor Sport 8 Netball 606611 Politics 2378961 Rugby League 1477 Rugby Union 5 Snooker 6422 Soccer 1 Special Bets 10 Tennis 2 Volleyball 998917 If we just wanted to get the event id for horse racing, we could use the filter function from betfairlightweight as shown in the examples and below. In [50]: # Filter for just horse racing horse_racing_filter = betfairlightweight . filters . market_filter ( text_query = 'Horse Racing' ) # This returns a list horse_racing_event_type = trading . betting . list_event_types ( filter = horse_racing_filter ) # Get the first element of the list horse_racing_event_type = horse_racing_event_type [ 0 ] horse_racing_event_type_id = horse_racing_event_type . event_type . id print ( f \"The event type id for horse racing is { horse_racing_event_type_id } \" ) # The event type id for horse racing is 7","title":"Sports API"},{"location":"api/apiPythontutorial/#get-competition-ids","text":"Sometimes you may want to get markets based on the competition. An example may be the Brownlow medal, or the EPL. Let's have a look at all the soccer competitions over the next week and filter to only get the EPL Competition ID. In [90]: # Get a datetime object in a week and convert to string datetime_in_a_week = ( datetime . datetime . utcnow () + datetime . timedelta ( weeks = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) # Create a competition filter competition_filter = betfairlightweight . filters . market_filter ( event_type_ids = [ 1 ], # Soccer's event type id is 1 market_start_time = { 'to' : datetime_in_a_week }) # Get a list of competitions for soccer competitions = trading . betting . list_competitions ( filter = competition_filter ) # Iterate over the competitions and create a dataframe of competitions and competition ids soccer_competitions = pd . DataFrame ({ 'Competition' : [ competition_object . competition . name for competition_object in competitions ], 'ID' : [ competition_object . competition . id for competition_object in competitions ] }) In [94]: # Get the English Premier League Competition ID soccer_competitions [ soccer_competitions . Competition . str . contains ( 'English Premier' )] Out[94]: Competition ID 116 English Premier League 10932509","title":"Get Competition IDs"},{"location":"api/apiPythontutorial/#get-upcoming-events","text":"Say you want to get all the upcoming events for Thoroughbreads for the next 24 hours. We will use the listEvents operation for this. First, as before, we define a market filter, and then using the betting method from our trading object which we defined earlier. In [207]: # Define a market filter thoroughbreds_event_filter = betfairlightweight . filters . market_filter ( event_type_ids = [ horse_racing_event_type_id ], market_countries = [ 'AU' ], market_start_time = { 'to' : ( datetime . datetime . utcnow () + datetime . timedelta ( days = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) } ) # Print the filter thoroughbreds_event_filter Out[207]: { 'eventTypeIds' : [ '7' ], 'marketCountries' : [ 'AU' ], 'marketStartTime' : { 'to' : '2018-10-26T22:25:00Z' }} In [208]: # Get a list of all thoroughbred events as objects aus_thoroughbred_events = trading . betting . list_events ( filter = thoroughbreds_event_filter ) # Create a DataFrame with all the events by iterating over each event object aus_thoroughbred_events_today = pd . DataFrame ({ 'Event Name' : [ event_object . event . name for event_object in aus_thoroughbred_events ], 'Event ID' : [ event_object . event . id for event_object in aus_thoroughbred_events ], 'Event Venue' : [ event_object . event . venue for event_object in aus_thoroughbred_events ], 'Country Code' : [ event_object . event . country_code for event_object in aus_thoroughbred_events ], 'Time Zone' : [ event_object . event . time_zone for event_object in aus_thoroughbred_events ], 'Open Date' : [ event_object . event . open_date for event_object in aus_thoroughbred_events ], 'Market Count' : [ event_object . market_count for event_object in aus_thoroughbred_events ] }) aus_thoroughbred_events_today Out[208]: Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 MVal (AUS) 26th Oct 28971066 Moonee Valley AU Australia/Sydney 2018-10-26 07:30:00 24 1 Newc (AUS) 26th Oct 28974559 Newcastle AU Australia/Sydney 2018-10-26 07:07:00 20 2 Bath (AUS) 26th Oct 28974547 Bathurst AU Australia/Sydney 2018-10-26 02:43:00 16 3 Cant (AUS) 26th Oct 28974545 Canterbury AU Australia/Sydney 2018-10-26 07:15:00 16 4 Scne (AUS) 26th Oct 28973942 Scone AU Australia/Sydney 2018-10-26 02:25:00 16 5 Gawl (AUS) 26th Oct 28974550 Gawler AU Australia/Adelaide 2018-10-26 04:00:00 16 6 Gatt (AUS) 26th Oct 28974549 Gatton AU Australia/Queensland 2018-10-26 01:55:00 16 7 GlPk (AUS) 26th Oct 28974562 Gloucester Park AU Australia/Perth 2018-10-26 09:10:00 20 8 Hoba (AUS) 26th Oct 28974563 Hobart AU Australia/Sydney 2018-10-26 05:23:00 18 9 Echu (AUS) 26th Oct 28974016 Echuca AU Australia/Sydney 2018-10-26 01:30:00 18 10 Melt (AUS) 26th Oct 28974560 Melton AU Australia/Sydney 2018-10-26 07:18:00 18 11 MVal (AUS) 26th Oct 28921730 None AU Australia/Sydney 2018-10-26 11:00:00 1 12 Redc (AUS) 26th Oct 28974561 Redcliffe AU Australia/Queensland 2018-10-26 02:17:00 16 13 SCst (AUS) 26th Oct 28974149 Sunshine Coast AU Australia/Queensland 2018-10-26 06:42:00 20","title":"Get Upcoming Events"},{"location":"api/apiPythontutorial/#get-market-types","text":"Say we want to know what market types a certain event is offering. To do this, we use the listMarketTypes operation. Let's take the Moonee Valley event from above (ID: 28971066). As this is a horse race we would expect that it would have Win and Place markets. In [209]: # Define a market filter market_types_filter = betfairlightweight . filters . market_filter ( event_ids = [ '28971066' ]) # Request market types market_types = trading . betting . list_market_types ( filter = market_types_filter ) # Create a DataFrame of market types market_types_mooney_valley = pd . DataFrame ({ 'Market Type' : [ market_type_object . market_type for market_type_object in market_types ], }) market_types_mooney_valley Out[209]: Market Type 0 OTHER_PLACE 1 PLACE 2 WIN","title":"Get Market Types"},{"location":"api/apiPythontutorial/#get-market-catalogues","text":"If we want to know the various market names that there are for a particular event, as well as how much has been matched on each market, we want to request data from the listMarketCatalogue operation. We can provide a number of filters, including the Competition ID, the Event ID, the Venue etc. to the filter. We must also specify the maximum number of results, and if we want additional data like the event data or runner data, we can also request that. For a more comprehensive understanding of the options for filters and what we can request, please have a look at the Sports API Visualiser . The options listed under market filter should be put into a filter, whilst the others should be arguments to the relevant operation function in betfairlightweight . For example, if we want all the markets for Moonee Valley, we should use the following filters and arguments. In [210]: market_catalogue_filter = betfairlightweight . filters . market_filter ( event_ids = [ '28971066' ]) market_catalogues = trading . betting . list_market_catalogue ( filter = market_catalogue_filter , max_results = '100' , sort = 'FIRST_TO_START' ) # Create a DataFrame for each market catalogue market_types_mooney_valley = pd . DataFrame ({ 'Market Name' : [ market_cat_object . market_name for market_cat_object in market_catalogues ], 'Market ID' : [ market_cat_object . market_id for market_cat_object in market_catalogues ], 'Total Matched' : [ market_cat_object . total_matched for market_cat_object in market_catalogues ], }) market_types_mooney_valley Out[210]: Market Name Market ID Total Matched 0 4 TBP 1.150090094 0.000000 1 To Be Placed 1.150090092 0.000000 2 R1 1000m 3yo 1.150090091 2250.188360 3 4 TBP 1.150090101 0.000000 4 To Be Placed 1.150090099 141.775816 5 R2 2040m Hcap 1.150090098 1093.481760 6 To Be Placed 1.150090106 0.000000 7 R3 1500m Hcap 1.150090105 1499.642480 8 4 TBP 1.150090108 0.000000 9 To Be Placed 1.150090113 19.855136 10 R4 2040m Hcap 1.150090112 588.190288 11 4 TBP 1.150090115 0.000000 12 4 TBP 1.150090122 0.000000 13 R5 955m Hcap 1.150090119 545.762616 14 To Be Placed 1.150090120 91.920584 15 4 TBP 1.150090129 48.623344 16 To Be Placed 1.150090127 65.616152 17 R6 1200m Hcap 1.150090126 506.342200 18 R7 1200m Grp1 1.150038686 34480.834976 19 4 TBP 1.150038689 701.052968 20 To Be Placed 1.150038687 1504.823656 21 R8 1500m Hcap 1.150090140 232.971760 22 4 TBP 1.150090143 0.000000 23 To Be Placed 1.150090141 73.768352","title":"Get Market Catalogues"},{"location":"api/apiPythontutorial/#get-market-books","text":"If we then want to get the prices available/last traded for a market, we should use the listMarketBook operation. Let's Look at the market book for Moonee Valley R7. We will need to define a function which processes the runner books and collates the data into a DataFrame. In [212]: def process_runner_books ( runner_books ): ''' This function processes the runner books and returns a DataFrame with the best back/lay prices + vol for each runner :param runner_books: :return: ''' best_back_prices = [ runner_book . ex . available_to_back [ 0 ] . price if runner_book . ex . available_to_back . price else 1.01 for runner_book in runner_books ] best_back_sizes = [ runner_book . ex . available_to_back [ 0 ] . size if runner_book . ex . available_to_back . size else 1.01 for runner_book in runner_books ] best_lay_prices = [ runner_book . ex . available_to_lay [ 0 ] . price if runner_book . ex . available_to_lay . price else 1000.0 for runner_book in runner_books ] best_lay_sizes = [ runner_book . ex . available_to_lay [ 0 ] . size if runner_book . ex . available_to_lay . size else 1.01 for runner_book in runner_books ] selection_ids = [ runner_book . selection_id for runner_book in runner_books ] last_prices_traded = [ runner_book . last_price_traded for runner_book in runner_books ] total_matched = [ runner_book . total_matched for runner_book in runner_books ] statuses = [ runner_book . status for runner_book in runner_books ] scratching_datetimes = [ runner_book . removal_date for runner_book in runner_books ] adjustment_factors = [ runner_book . adjustment_factor for runner_book in runner_books ] df = pd . DataFrame ({ 'Selection ID' : selection_ids , 'Best Back Price' : best_back_prices , 'Best Back Size' : best_back_sizes , 'Best Lay Price' : best_lay_prices , 'Best Lay Size' : best_lay_sizes , 'Last Price Traded' : last_prices_traded , 'Total Matched' : total_matched , 'Status' : statuses , 'Removal Date' : scratching_datetimes , 'Adjustment Factor' : adjustment_factors }) return df In [213]: # Create a price filter. Get all traded and offer data price_filter = ` betfairlightweight ` . filters . price_projection ( price_data = [ 'EX_BEST_OFFERS' ] ) # Request market books market_books = trading . betting . list_market_book ( market_ids = [ '1.150038686' ], price_projection = price_filter ) # Grab the first market book from the returned list as we only requested one market market_book = market_books [ 0 ] runners_df = process_runner_books ( market_book . runners ) runners_df Out[213]: Selection ID Best Back Price Best Back Size Best Lay Price Best Lay Size Last Price Traded Total Matched Status Removal Date Adjustment Factor 0 16905731 12.0 65.54 13.0 33.09 12.0 1226.67 ACTIVE None 8.333 1 15815968 6.6 96.64 7.0 9.00 6.6 5858.61 ACTIVE None 14.286 2 9384677 14.0 114.71 15.0 76.71 14.0 964.80 ACTIVE None 6.667 3 8198751 17.5 14.67 19.0 33.02 17.5 940.56 ACTIVE None 5.556 4 9507057 38.0 53.13 100.0 40.22 46.0 224.72 ACTIVE None 3.125 5 21283266 15.0 121.46 19.5 5.56 19.5 1102.37 ACTIVE None 7.692 6 21283267 80.0 37.58 760.0 9.70 760.0 125.30 ACTIVE None 1.087 7 21063807 6.4 1503.62 7.2 50.00 6.6 8011.44 ACTIVE None 13.333 8 21283268 48.0 54.57 60.0 51.93 50.0 150.22 ACTIVE None 2.381 9 21283269 8.8 235.77 9.4 30.40 8.8 1729.96 ACTIVE None 11.111 10 4883975 46.0 33.42 55.0 5.00 46.0 208.45 ACTIVE None 2.381 11 202351 25.0 20.00 30.0 6.00 24.0 658.09 ACTIVE None 2.632 12 21283270 19.5 69.33 22.0 20.00 19.5 825.59 ACTIVE None 4.545 13 21283271 5.3 96.14 5.7 5.03 5.3 12654.32 ACTIVE None 16.871","title":"Get Market Books"},{"location":"api/apiPythontutorial/#orderbook-workflow","text":"Now that we have the market book in an easy to read DataFrame, we can go ahead and start placing orders based on the market book. Although it is a simple (and probably not profitable) strategy, in the next few sections we will be backing the favourite and adjusting our orders.","title":"Orderbook Workflow"},{"location":"api/apiPythontutorial/#placing-orders","text":"To place an order we use the placeOrders operation. A handy component of placeOrders is that you can send your strategy along with the runner that you want to back, so it is extremely easy to analyse how your strategy performed later. Let's place a 5 dollar back bet on the favourite at $7 call this strategy 'back_the_fav' . Note that if you are placing a limit order you must specify a price which is allowed by Betfair. For example, the price 6.3 isn't allowed, whereas 6.4 is, as prices go up by 20c increments at that price range. You can read about tick points here . In [232]: # Get the favourite's price and selection id fav_selection_id = runners_df . loc [ runners_df [ 'Best Back Price' ] . idxmin (), 'Selection ID' ] fav_price = runners_df . loc [ runners_df [ 'Best Back Price' ] . idxmin (), 'Best Back Price' ] In [276]: # Define a limit order filter limit_order_filter = betfairlightweight . filters . limit_order ( size = 5 , price = 7 , persistence_type = 'LAPSE' ) # Define an instructions filter instructions_filter = betfairlightweight . filters . place_instruction ( selection_id = str ( fav_selection_id ), order_type = \"LIMIT\" , side = \"BACK\" , limit_order = limit_order_filter ) instructions_filter Out[276]: { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7 , 'size' : 5 }, 'orderType' : 'LIMIT' , 'selectionId' : '21283271' , 'side' : 'BACK' } In [277]: # Place the order order = trading . betting . place_orders ( market_id = '1.150038686' , # The market id we obtained from before customer_strategy_ref = 'back_the_fav' , instructions = [ instructions_filter ] # This must be a list ) Now that we've placed the other, we can check if the order placing was a success and if any has been matched. In [306]: order . __dict__ Out[306]: { '_data' : { 'instructionReports' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'instruction' : { 'limitOrder' : { 'persistenceType' : 'LAPSE' , 'price' : 7.0 , 'size' : 5.0 }, 'orderType' : 'LIMIT' , 'selectionId' : 21283271 , 'side' : 'BACK' }, 'orderStatus' : 'EXECUTABLE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'sizeMatched' : 0.0 , 'status' : 'SUCCESS' }], 'marketId' : '1.150038686' , 'status' : 'SUCCESS' }, '_datetime_created' : datetime . datetime ( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), '_datetime_updated' : datetime . datetime ( 2018 , 10 , 26 , 0 , 46 , 46 , 455349 ), 'customer_ref' : None , 'elapsed_time' : 1.484069 , 'error_code' : None , 'market_id' : '1.150038686' , 'place_instruction_reports' : [ < betfairlightweight . resources . bettingresources . PlaceOrderInstructionReports at 0x23e0f7952e8 > ], 'status' : 'SUCCESS' } As we can see, the status is 'SUCCESS' , whilst the sizeMatched is 0. Let's now look at our current orders.","title":"Placing Orders"},{"location":"api/apiPythontutorial/#get-current-orders","text":"To get our current orders, we need to use the listCurrentOrders operation. We can then use either the bet id, the market id, or the bet strategy to filter our orders. In [311]: trading . betting . list_current_orders ( customer_strategy_refs = [ 'back_the_fav' ]) . __dict__ Out [ 311 ]: { '_data' : { 'currentOrders' : [{ 'averagePriceMatched' : 0.0 , 'betId' : '142384852665' , 'bspLiability' : 0.0 , 'customerStrategyRef' : 'back_the_fav' , 'handicap' : 0.0 , 'marketId' : '1.150038686' , 'orderType' : 'LIMIT' , 'persistenceType' : 'LAPSE' , 'placedDate' : '2018-10-26T00:46:46.000Z' , 'priceSize' : { 'price' : 7.0 , 'size' : 5.0 }, 'regulatorCode' : 'MALTA LOTTERIES AND GAMBLING AUTHORITY' , 'selectionId' : 21283271 , 'side' : 'BACK' , 'sizeCancelled' : 0.0 , 'sizeLapsed' : 0.0 , 'sizeMatched' : 0.0 , 'sizeRemaining' : 5.0 , 'sizeVoided' : 0.0 , 'status' : 'EXECUTABLE' }], 'moreAvailable' : False }, '_datetime_created' : datetime . datetime ( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), '_datetime_updated' : datetime . datetime ( 2018 , 10 , 26 , 2 , 14 , 56 , 84036 ), 'elapsed_time' : 1.327456 , 'more_available' : False , 'orders' : [ < betfairlightweight . resources . bettingresources . CurrentOrder at 0x23e0e7acd30 > ], 'publish_time' : None , 'streaming_unique_id' : None , 'streaming_update' : None } As we can see, we have one order which is unmatched for our strategy 'back_the_fav'","title":"Get Current Orders"},{"location":"api/apiPythontutorial/#cancelling-orders","text":"Let's now cancel this bet. To do this, we will use the cancelOrders operation. If you pass in a market ID it will cancel all orders for that specific market ID, like you can do on the website. In [312]: cancelled_order = trading . betting . cancel_orders ( market_id = '1.150038686' ) In [328]: # Create a DataFrame to view the instruction report pd . Series ( cancelled_order . cancel_instruction_reports [ 0 ] . __dict__ ) . to_frame () . T Out[328]: status size_cancelled cancelled_date instruction error_code 0 SUCCESS 5 2018-10-26 06:01:26 betfairlightweight.resources.bettingresources... None","title":"Cancelling Orders"},{"location":"api/apiPythontutorial/#get-past-orders-and-results","text":"If we want to go back and look at past orders we have made, there are two main operations for this: listClearedOrders - this operation takes a range of data down to the individual selection ID level, and returns a summary of those specific orders listMarketProfitAndLoss - this operation is more specific, and only takes Market IDs to return the Profit/Loss for that market Alternatively, we can use the getAccountStatement operation from the Account Operations API. Let's now use both Sports API operations based on our previous orders and then compare it to the getAccountStatement operation.","title":"Get Past Orders and Results"},{"location":"api/apiPythontutorial/#get-cleared-orders","text":"In [346]: # listClearedOrders cleared_orders = trading . betting . list_cleared_orders ( bet_status = \"SETTLED\" , market_ids = [ \"1.150038686\" ]) In [371]: # Create a DataFrame from the orders pd . DataFrame ( cleared_orders . _data [ 'clearedOrders' ]) Out[371]: betCount betId betOutcome eventId eventTypeId handicap lastMatchedDate marketId orderType persistenceType placedDate priceMatched priceReduced priceRequested profit selectionId settledDate side sizeSettled 0 1 142383373022 LOST 28971066 7 0.0 2018-10-26T10:31:53.000Z 1.150038686 MARKET_ON_CLOSE LAPSE 2018-10-26T00:12:03.000Z 5.74 False 5.74 -5.0 21283271 2018-10-26T10:34:39.000Z BACK 5.0 1 1 142383570640 WON 28971066 7 0.0 2018-10-26T00:16:32.000Z 1.150038686 LIMIT LAPSE 2018-10-26T00:16:31.000Z 5.40 False 5.50 5.0 21283271 2018-10-26T10:34:39.000Z LAY 5.0 Note that we can also filter for certain dates, bet ids, event ids, selection ids etc. We can also group by the event type, the event, the market, the runner, the side, the bet and the strategy, which is extremely useful if you're looking for a quick summary of how your strategy is performing.","title":"Get Cleared Orders"},{"location":"api/apiPythontutorial/#get-market-profit-and-loss","text":"Now let's find the Profit and Loss for the market. To do this we will use the listMarketProfitAndLoss operation. Note that this function only works with market IDs, and once the website clears the market, the operation will no longer work. However the market is generally up for about a minute after the race, so if your strategy is automated, you can check once if your bet is settled and if it is, hit the getMarketProfitAndLoss endpoint. Because of this, we will check a different market ID to the example above. In [406]: # Get the profit/loss - this returns a list pl = trading . betting . list_market_profit_and_loss ( market_ids = [ \"1.150318913\" ], include_bsp_bets = 'true' , include_settled_bets = 'true' ) In [410]: # Create a profit/loss DataFrame pl_df = pd . DataFrame ( pl [ 0 ] . _data [ 'profitAndLosses' ]) . assign ( marketId = pl [ 0 ] . market_id ) pl_df Out[410]: ifWin selectionId marketId 0 -5.0 10065177 1.150318913 1 14.0 17029506 1.150318913 2 -5.0 5390339 1.150318913 3 -5.0 13771011 1.150318913 4 -5.0 138209 1.150318913 5 -5.0 10503541 1.150318913 6 -5.0 12165809 1.150318913","title":"Get Market Profit and Loss"},{"location":"api/apiPythontutorial/#get-account-statement","text":"Another method is to use the getAccountStatement , which provides an overview of all your bets over a certain time period. You can then filter this for specific dates if you wish. In [428]: # Define a date filter - get all bets for the past 4 days four_days_ago = ( datetime . datetime . utcnow () - datetime . timedelta ( days = 4 )) . strftime ( \"%Y-%m- %d T%TZ\" ) acct_statement_date_filter = betfairlightweight . filters . time_range ( from_ = four_days_ago ) # Request account statement account_statement = trading . account . get_account_statement ( item_date_range = acct_statement_date_filter ) In [450]: # Create df of recent transactions recent_transactions = pd . DataFrame ( account_statement . _data [ 'accountStatement' ]) recent_transactions Out[450]: amount balance itemClass itemClassData itemDate legacyData refId 0 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":3.8,\"bet... 2018-10-28T23:14:28.000Z {'avgPrice': 3.8, 'betSize': 5.0, 'betType': '... 142845441633 1 5.0 261.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.4,\"bet... 2018-10-26T10:34:39.000Z {'avgPrice': 5.4, 'betSize': 5.0, 'betType': '... 142383570640 2 -5.0 256.74 UNKNOWN {'unknownStatementItem': '{\"avgPrice\":5.74,\"be... 2018-10-26T10:34:39.000Z {'avgPrice': 5.74, 'betSize': 5.0, 'betType': ... 142383373022 In [468]: # Create df of itemClassData - iterate over the account statement list and convert to json so that the DataFrame function # can read it correctly class_data = [ json . loads ( account_statement . account_statement [ i ] . item_class_data [ 'unknownStatementItem' ]) for i in range ( len ( account_statement . account_statement ))] In [471]: class_df = pd . DataFrame ( class_data ) class_df Out [471]: avgPrice betCategoryType betSize betType commissionRate eventId eventTypeId fullMarketName grossBetAmount marketName marketType placedDate selectionId selectionName startDate transactionId transactionType winLose 0 3.80 M 5.0 B None 150318913 7 USA / TPara (US) 28th Oct/ 16:06 R8 1m Allw Claim 0.0 R8 1m Allw Claim O 2018-10-28T23:02:28.000Z 17029506 Gato Guapo 2018-10-28T23:06:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST 1 5.40 E 5.0 L None 150038686 7 AUS / MVal (AUS) 26th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:16:31.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_CREDIT 2 5.74 M 5.0 B None 150038686 7 AUS / MVal (AUS) 26th Oct/ 21:30 R7 1200m Grp1 0.0 R7 1200m Grp1 O 2018-10-26T00:12:03.000Z 21283271 14. Sunlight 2018-10-26T10:30:00.000Z 0 ACCOUNT_DEBIT RESULT_LOST As we can see, this DataFrame provides a much more comprehensive view of each of our bets. However, it lacks the ability to filter by strategy like the listClearedOrders operation in the Sports API.","title":"Get Account Statement"},{"location":"api/apiResources/","text":"API Resources There are a wide range of resources available to help make it easier to interact with the Betfair API, including those created by Betfair as well as the wider community. Here are some of the resources we'd recommend taking a look at if you're building a program to interact with either the polling and/or Stream APIs. The basics Creating & activating your app key Dev Docs Developer Program knowledge base Developer Forum where you can share your experiences and find out what's worked for other clients API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support. Github Our Datascientists' repos for using R and Python to access the API There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange, including an Awesome List and Knowledge Share with helper functions and guidance on best practice shared by the ANZ automation community. The UK\u2019s Github repo including libraries for other languages Visualisers Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Other resources Towards Data Science provide a decent basic walk through of how to log in to the polling API and pull market data, with some interesting commentary along the way. Betfair Quants Slack Group betfair quants is really active community-owned Slack group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation.","title":"API resources"},{"location":"api/apiResources/#api-resources","text":"There are a wide range of resources available to help make it easier to interact with the Betfair API, including those created by Betfair as well as the wider community. Here are some of the resources we'd recommend taking a look at if you're building a program to interact with either the polling and/or Stream APIs.","title":"API Resources"},{"location":"api/apiResources/#the-basics","text":"Creating & activating your app key Dev Docs Developer Program knowledge base Developer Forum where you can share your experiences and find out what's worked for other clients API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support.","title":"The basics"},{"location":"api/apiResources/#github","text":"Our Datascientists' repos for using R and Python to access the API There's an ANZ Betfair Down Under community GitHub repo where you can find sample code, libraries, tutorials and other resources for automating and modelling on the Exchange, including an Awesome List and Knowledge Share with helper functions and guidance on best practice shared by the ANZ automation community. The UK\u2019s Github repo including libraries for other languages","title":"Github"},{"location":"api/apiResources/#visualisers","text":"Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries","title":"Visualisers"},{"location":"api/apiResources/#other-resources","text":"Towards Data Science provide a decent basic walk through of how to log in to the polling API and pull market data, with some interesting commentary along the way. Betfair Quants Slack Group betfair quants is really active community-owned Slack group for people interested in modelling and automation on the Exchange. Please reach out if you'd like an invitation.","title":"Other resources"},{"location":"api/apiRtutorial/","text":"Betfair API tutorials in R Betfair's API can be easily traversed in R. It allows you to retrieve market information, create/cancel bets and manage your account. Here's a collection of easy to follow API tutorials in R: Accessing the API using R Get Worldcup Odds AFL Odds PulleR Tutorial Accessing the API using R Set up R What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Required Packages Two R packages are required: library ( tidyverse ) library ( abettor ) The abettor package can be downloaded here . For an in-depth understanding of the package, have a read of the documentation. Instructions are also provided in the sample code. Login to Betfair To login to Betfair, replace the following dummy username, password and app key with your own. abettor :: loginBF ( username = \"betfair_username\" , password = \"betfair_password\" , applicationKey = \"betfair_app_key\" ) If you don't have a live app key for the API yet take a look at this page . Finding Event IDs In order to find data for specific markets, you will first need to know the event ID. This is easily achieved with the abettor package . To find the event IDs of events in the next 60 days: abettor :: listEventTypes ( toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" ))) This will return a DataFrame of the following structure: eventType.id eventType.name marketCount 1 Soccer 1193 2 Tennis 2184 7522 Basketball 1 4 Cricket 37 7 Horse Racing 509 61420 Australian Rules 31 4339 Greyhound Racing 527 Finding Competition IDs Once you have the event ID, the next logical step is to find the competition IDs for the event you want to get data for. For example, if you want to find the competition IDs for Australian Rules, you would use the following abettor :: listCompetitions ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) This will return the following structured DataFrame: competition.id competition.name marketCount competitionRegion 11516633 Brownlow Medal 2018 3 AUS 11897406 AFL 78 AUS Finding Specific Markets The next logical step is to find the market that you are interested in. Furthering our example above, if you want the Match Odds for all Australian Rules games over the next 60 days, simply use the Competition ID from above in the following. abettor :: listMarketCatalogue ( eventTypeIds = 61420 , marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = 11897406 , toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) This returns a large DataFrame object with each market, participants and associated odds. Get World Cup Odds Tutorial This tutorial walks you through the process of retrieving exchange odds for all the matches from the 2018 FIFA World Cup 2018. This can be modified for other sports and uses. You can run this script in R. ################################################### ### FIFA World Cup Datathon ### Betfair API Tutorial ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches from the upcoming FIFA World Cup 2018 ################################################### ################################################### ### Setup ################################################### ## Loading required packages library ( tidyverse ) ## package for general data manipulation - https://www.tidyverse.org/ library ( abettor ) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Enter your Betfair API Credentials below betfair_username <- \"\" betfair_password <- \"\" betfair_app_key <- \"\" ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF ( username = betfair_username , password = betfair_password , applicationKey = betfair_app_key ) ################################################### ## Retrieving all soccer competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_soccer_markets <- abettor :: listCompetitions ( eventTypeIds = 1 , ## Soccer is eventTypeId 1, toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Retrieving the competition id ## for the 2018 World Cup ################################################### world_cup_competition_id <- all_soccer_markets %>% dplyr :: pull ( competition ) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter ( name == \"2018 FIFA World Cup\" ) %>% ## Filtering for the competition we need dplyr :: pull ( id ) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the World Cup ################################################### all_world_cup_markets <- abettor :: listMarketCatalogue ( eventTypeIds = 1 , ## Soccer is eventTypeId 1 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = world_cup_competition_id , ## Restrict our search to World Cup matches only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step (World Cup Matches) ################################################### ## Creating a vector/array of all market ids all_world_cup_markets_market_ids <- all_world_cup_markets %>% pull ( marketId ) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function ( market_id ) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook ( marketIds = market_id , ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull ( runners ) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select ( lastPriceTraded ) %>% ## Extracting team and last matched odds mutate ( market_id = market_id ) %>% ## Padding market id to the data to make it unique to this match bind_cols ( data.frame ( outcome = c ( \"o_1\" , \"o_2\" , \"o_3\" ))) %>% ## Creating outcome order to maintain consistency spread ( outcome , lastPriceTraded ) %>% ## Reshaping data to make it 1 row per match rename ( team_1_odds = o_1 , team_2_odds = o_2 , draw_odds = o_3 ) %>% ##Renaming columns such that all matches can be combined into one data frame select ( market_id , team_1_odds , draw_odds , team_2_odds ) ## Ordering columns in the right order return ( odds ) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame world_cup_market_odds <- map_dfr ( .x = all_world_cup_markets_market_ids , ##Iterate over market ids .f = fetch_odds ## through function fetch_odds ) %>% bind_cols ( all_world_cup_markets %>% ## Merge with event names to identify which match odds it is pull ( event ) %>% select ( name )) %>% mutate ( team_1 = gsub ( \" v .*\" , \"\" , name ), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name )) %>% ## Extracing team 2 from match name select ( team_1 , team_2 , team_1_odds , draw_odds , team_2_odds ) ## Extracting columns that we need ## Writing output to csv file write_csv ( world_cup_market_odds , \"world_cup_market_odds.csv\" ) AFL Odds PulleR Tutorial This tutorial walks you through the process of retrieving exchange odds for the the next round of Australian Rules. You can run this script in R. ################################################### ### AFL Model ### Betfair API Odds GrabbR ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches for the upcoming round of AFL games ################################################### ## Loading required packages library ( tidyverse ) ## package for general data manipulation - https://www.tidyverse.org/ library ( abettor ) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF ( username = 'your_username' , password = 'your_password' , applicationKey = \"your_betfair_app_key\" ) ################################################### ## Retrieving all AFL competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_afl_markets <- abettor :: listCompetitions ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) ################################################### ## Retrieving the competition id ## for the regular AFL season ################################################### afl_competition_id <- all_afl_markets %>% dplyr :: pull ( competition ) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter ( name == \"AFL\" ) %>% ## Filtering for the competition we need dplyr :: pull ( id ) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the AFL ################################################### all_afl_markets <- abettor :: listMarketCatalogue ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = afl_competition_id , ## Restrict our search to AFL Matches Only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step ################################################### ## Creating a vector/array of all market ids all_afl_markets_market_ids <- all_afl_markets %>% pull ( marketId ) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function ( market_id ) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook ( marketIds = market_id , ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull ( runners ) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select ( lastPriceTraded ) %>% ## Extracting team and last matched odds mutate ( market_id = market_id ) %>% ## Padding market id to the data to make it unique to this match bind_cols ( data.frame ( outcome = c ( \"o_1\" , \"o_2\" ))) %>% ## Creating outcome order to maintain consistency spread ( outcome , lastPriceTraded ) %>% ## Reshaping data to make it 1 row per match rename ( team_1_odds = o_1 , team_2_odds = o_2 ) %>% ##Renaming columns such that all matches can be combined into one data frame select ( market_id , team_1_odds , team_2_odds ) ## Ordering columns in the right order return ( odds ) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame afl_market_odds <- map_dfr ( .x = all_afl_markets_market_ids , ##Iterate over market ids .f = fetch_odds ## through function fetch_odds ) %>% bind_cols ( all_afl_markets %>% ## Merge with event names to identify which match odds it is pull ( event ) %>% select ( name )) %>% mutate ( team_1 = gsub ( \" v .*\" , \"\" , name ), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name )) %>% ## Extracing team 2 from match name select ( team_1 , team_2 , team_1_odds , team_2_odds ) ## Extracting columns that we need ## Writing output to csv file write_csv ( afl_market_odds , \"weekly_afl_odds.csv\" )","title":"API tutorials in R"},{"location":"api/apiRtutorial/#betfair-api-tutorials-in-r","text":"Betfair's API can be easily traversed in R. It allows you to retrieve market information, create/cancel bets and manage your account. Here's a collection of easy to follow API tutorials in R: Accessing the API using R Get Worldcup Odds AFL Odds PulleR Tutorial","title":"Betfair API tutorials in R"},{"location":"api/apiRtutorial/#accessing-the-api-using-r","text":"","title":"Accessing the API using R"},{"location":"api/apiRtutorial/#set-up-r","text":"What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R","title":"Set up R"},{"location":"api/apiRtutorial/#required-packages","text":"Two R packages are required: library ( tidyverse ) library ( abettor ) The abettor package can be downloaded here . For an in-depth understanding of the package, have a read of the documentation. Instructions are also provided in the sample code.","title":"Required Packages"},{"location":"api/apiRtutorial/#login-to-betfair","text":"To login to Betfair, replace the following dummy username, password and app key with your own. abettor :: loginBF ( username = \"betfair_username\" , password = \"betfair_password\" , applicationKey = \"betfair_app_key\" ) If you don't have a live app key for the API yet take a look at this page .","title":"Login to Betfair"},{"location":"api/apiRtutorial/#finding-event-ids","text":"In order to find data for specific markets, you will first need to know the event ID. This is easily achieved with the abettor package . To find the event IDs of events in the next 60 days: abettor :: listEventTypes ( toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" ))) This will return a DataFrame of the following structure: eventType.id eventType.name marketCount 1 Soccer 1193 2 Tennis 2184 7522 Basketball 1 4 Cricket 37 7 Horse Racing 509 61420 Australian Rules 31 4339 Greyhound Racing 527","title":"Finding Event IDs"},{"location":"api/apiRtutorial/#finding-competition-ids","text":"Once you have the event ID, the next logical step is to find the competition IDs for the event you want to get data for. For example, if you want to find the competition IDs for Australian Rules, you would use the following abettor :: listCompetitions ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) This will return the following structured DataFrame: competition.id competition.name marketCount competitionRegion 11516633 Brownlow Medal 2018 3 AUS 11897406 AFL 78 AUS","title":"Finding Competition IDs"},{"location":"api/apiRtutorial/#finding-specific-markets","text":"The next logical step is to find the market that you are interested in. Furthering our example above, if you want the Match Odds for all Australian Rules games over the next 60 days, simply use the Competition ID from above in the following. abettor :: listMarketCatalogue ( eventTypeIds = 61420 , marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = 11897406 , toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) This returns a large DataFrame object with each market, participants and associated odds.","title":"Finding Specific Markets"},{"location":"api/apiRtutorial/#get-world-cup-odds-tutorial","text":"This tutorial walks you through the process of retrieving exchange odds for all the matches from the 2018 FIFA World Cup 2018. This can be modified for other sports and uses. You can run this script in R. ################################################### ### FIFA World Cup Datathon ### Betfair API Tutorial ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches from the upcoming FIFA World Cup 2018 ################################################### ################################################### ### Setup ################################################### ## Loading required packages library ( tidyverse ) ## package for general data manipulation - https://www.tidyverse.org/ library ( abettor ) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Enter your Betfair API Credentials below betfair_username <- \"\" betfair_password <- \"\" betfair_app_key <- \"\" ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF ( username = betfair_username , password = betfair_password , applicationKey = betfair_app_key ) ################################################### ## Retrieving all soccer competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_soccer_markets <- abettor :: listCompetitions ( eventTypeIds = 1 , ## Soccer is eventTypeId 1, toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Retrieving the competition id ## for the 2018 World Cup ################################################### world_cup_competition_id <- all_soccer_markets %>% dplyr :: pull ( competition ) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter ( name == \"2018 FIFA World Cup\" ) %>% ## Filtering for the competition we need dplyr :: pull ( id ) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the World Cup ################################################### all_world_cup_markets <- abettor :: listMarketCatalogue ( eventTypeIds = 1 , ## Soccer is eventTypeId 1 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = world_cup_competition_id , ## Restrict our search to World Cup matches only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step (World Cup Matches) ################################################### ## Creating a vector/array of all market ids all_world_cup_markets_market_ids <- all_world_cup_markets %>% pull ( marketId ) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function ( market_id ) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook ( marketIds = market_id , ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull ( runners ) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select ( lastPriceTraded ) %>% ## Extracting team and last matched odds mutate ( market_id = market_id ) %>% ## Padding market id to the data to make it unique to this match bind_cols ( data.frame ( outcome = c ( \"o_1\" , \"o_2\" , \"o_3\" ))) %>% ## Creating outcome order to maintain consistency spread ( outcome , lastPriceTraded ) %>% ## Reshaping data to make it 1 row per match rename ( team_1_odds = o_1 , team_2_odds = o_2 , draw_odds = o_3 ) %>% ##Renaming columns such that all matches can be combined into one data frame select ( market_id , team_1_odds , draw_odds , team_2_odds ) ## Ordering columns in the right order return ( odds ) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame world_cup_market_odds <- map_dfr ( .x = all_world_cup_markets_market_ids , ##Iterate over market ids .f = fetch_odds ## through function fetch_odds ) %>% bind_cols ( all_world_cup_markets %>% ## Merge with event names to identify which match odds it is pull ( event ) %>% select ( name )) %>% mutate ( team_1 = gsub ( \" v .*\" , \"\" , name ), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name )) %>% ## Extracing team 2 from match name select ( team_1 , team_2 , team_1_odds , draw_odds , team_2_odds ) ## Extracting columns that we need ## Writing output to csv file write_csv ( world_cup_market_odds , \"world_cup_market_odds.csv\" )","title":"Get World Cup Odds Tutorial"},{"location":"api/apiRtutorial/#afl-odds-puller-tutorial","text":"This tutorial walks you through the process of retrieving exchange odds for the the next round of Australian Rules. You can run this script in R. ################################################### ### AFL Model ### Betfair API Odds GrabbR ### ### This script allows you to access the Betfair ### API and retrive exchange odds for all the ### matches for the upcoming round of AFL games ################################################### ## Loading required packages library ( tidyverse ) ## package for general data manipulation - https://www.tidyverse.org/ library ( abettor ) ## wrapper package for the Betfair API - https://github.com/phillc73/abettor ## Login to Betfair - should return \"SUCCESS:\" on successful login betfair_login <- abettor :: loginBF ( username = 'your_username' , password = 'your_password' , applicationKey = \"your_betfair_app_key\" ) ################################################### ## Retrieving all AFL competitions for ## which markets are currently alive ## on the Betfair Exchange ################################################### all_afl_markets <- abettor :: listCompetitions ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420, toDate = ( format ( Sys.time () + 86400 * 180 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 180 days ) ################################################### ## Retrieving the competition id ## for the regular AFL season ################################################### afl_competition_id <- all_afl_markets %>% dplyr :: pull ( competition ) %>% ## Extracting the variable competition which is a nested data frame dplyr :: filter ( name == \"AFL\" ) %>% ## Filtering for the competition we need dplyr :: pull ( id ) ## Extracting the id for the competition we need ################################################### ## Obtaining all markets that are currently ## alive on the Betfair Exchange that belong to ## Competition ID that is mapped to the AFL ################################################### all_afl_markets <- abettor :: listMarketCatalogue ( eventTypeIds = 61420 , ## AFL is eventTypeId 61420 marketTypeCodes = \"MATCH_ODDS\" , ## Restrict our search to Match Odds only, not other markets for the same match competitionIds = afl_competition_id , ## Restrict our search to AFL Matches Only toDate = ( format ( Sys.time () + 86400 * 60 , \"%Y-%m-%dT%TZ\" )) ## Look ahead until the next 60 days ) ################################################### ## Obtaining the current odds on the Betfair ## Exchange for all the markets that were ## obtained in the previous step ################################################### ## Creating a vector/array of all market ids all_afl_markets_market_ids <- all_afl_markets %>% pull ( marketId ) ## This function takes in a single market id and returns ## the current live odds on the Betfair Exchange for that market fetch_odds <- function ( market_id ) { ## Retrieving market odds for a single market odds <- abettor :: listMarketBook ( marketIds = market_id , ##Runs listMarketBook for given market_id priceData = \"EX_BEST_OFFERS\" ##Fetching the top 3 odds, EX_ALL_OFFERS fetches the entire depth of prices ) %>% pull ( runners ) %>% ## Extracting the runners field which has details of odds as.data.frame () %>% ## Converting to data frame from list select ( lastPriceTraded ) %>% ## Extracting team and last matched odds mutate ( market_id = market_id ) %>% ## Padding market id to the data to make it unique to this match bind_cols ( data.frame ( outcome = c ( \"o_1\" , \"o_2\" ))) %>% ## Creating outcome order to maintain consistency spread ( outcome , lastPriceTraded ) %>% ## Reshaping data to make it 1 row per match rename ( team_1_odds = o_1 , team_2_odds = o_2 ) %>% ##Renaming columns such that all matches can be combined into one data frame select ( market_id , team_1_odds , team_2_odds ) ## Ordering columns in the right order return ( odds ) } ## The code below maps (or loops) each market id in the vector we created ## above through the fetch_odds function and retrives the market odds ## into a single data frame afl_market_odds <- map_dfr ( .x = all_afl_markets_market_ids , ##Iterate over market ids .f = fetch_odds ## through function fetch_odds ) %>% bind_cols ( all_afl_markets %>% ## Merge with event names to identify which match odds it is pull ( event ) %>% select ( name )) %>% mutate ( team_1 = gsub ( \" v .*\" , \"\" , name ), ## Extracting team 1 from match name team_2 = gsub ( \".* v \" , \"\" , name )) %>% ## Extracing team 2 from match name select ( team_1 , team_2 , team_1_odds , team_2_odds ) ## Extracting columns that we need ## Writing output to csv file write_csv ( afl_market_odds , \"weekly_afl_odds.csv\" )","title":"AFL Odds PulleR Tutorial"},{"location":"api/apiappkey/","text":"How to access the Betfair API Betfair has it\u2019s own Exchange API . You can use it to programmatically retrieve live markets, automate successful trading strategies or create your own customised trading interface. Professional punters use it for these functions and many more. This guide helps Australian and New Zealand customers with obtaining their Betfair API Key. If you\u2019re outside of these two regions please go to the UK's Developer Program website . There are four steps involved in getting access to our API Obtain an SSOID token Register your application Obtain your app key Activate your app key API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support. Find your SSOID token The simplest way to setup your browser with the SSOID is to follow this link and log in - this will allow for the SSOID to be automatically populated in the next step. After logging in, you\u2019ll be sent to the main Betfair website. Note: it may not show that you\u2019re logged in on the site. You can ignore that. Proceed to step two. Register your application Navigate to the API-NG accounts visualiser . If you\u2019ve followed step 1 correctly, your SSOID token should be automatically populated in the visualiser. Next click on createDeveloperAppKeys in the left hand navigation. Type in an application name (this is your app key name, so make sure this is unique), then click \u2018Execute\u2019 down the bottom of the page. If you receive an error message saying that your app key couldn\u2019t be created, it\u2019s most likely because you already have one. Use the getDeveloperAppKeys method in the left hand menu to check whether there\u2019s already an app key associated with your account. Find your app key After your key is created, you should see in the right hand panel your application: You\u2019ll notice that two application keys have been created; Version \u2013 1.0-Delay: is a delayed app key for development purposes Version \u2013 1.0: is the live pricing app key; on yours it should have a status \u2018No\u2019 in Active. Grab the application key listed for the live price one - for the example above, that is \u2018MkcBqyZrD53V6A..\u2019 Activate your app key This process will generate two app keys: A developer key which is designed for development purposes. This has a variable delay of between 1 and 180 seconds, doesn\u2019t show matched volume and doesn\u2019t need to be activated prior to use. A live app key is intended for transacting on the Exchange and should only be used when you\u2019re ready to start placing bets or can no longer test your strategy effectively using the developer key. Please note that if the live key is used to pull data from the Exchange without corresponding bets being placed a delay may be automatically applied to the live key. If you\u2019re ready to start testing your strategy or placing bets, please contact api@betfair.com.au and we will be happy to assist with activating the live key and implementing your strategy.","title":"How to access the Betfair API"},{"location":"api/apiappkey/#how-to-access-the-betfair-api","text":"Betfair has it\u2019s own Exchange API . You can use it to programmatically retrieve live markets, automate successful trading strategies or create your own customised trading interface. Professional punters use it for these functions and many more. This guide helps Australian and New Zealand customers with obtaining their Betfair API Key. If you\u2019re outside of these two regions please go to the UK's Developer Program website . There are four steps involved in getting access to our API Obtain an SSOID token Register your application Obtain your app key Activate your app key API access Customers are able to access our API to embed it into their programs and automate their strategies Please reach out if you're an Australian or New Zealand based customer and are keen for support.","title":"How to access the Betfair API"},{"location":"api/apiappkey/#find-your-ssoid-token","text":"The simplest way to setup your browser with the SSOID is to follow this link and log in - this will allow for the SSOID to be automatically populated in the next step. After logging in, you\u2019ll be sent to the main Betfair website. Note: it may not show that you\u2019re logged in on the site. You can ignore that. Proceed to step two.","title":"Find your SSOID token"},{"location":"api/apiappkey/#register-your-application","text":"Navigate to the API-NG accounts visualiser . If you\u2019ve followed step 1 correctly, your SSOID token should be automatically populated in the visualiser. Next click on createDeveloperAppKeys in the left hand navigation. Type in an application name (this is your app key name, so make sure this is unique), then click \u2018Execute\u2019 down the bottom of the page. If you receive an error message saying that your app key couldn\u2019t be created, it\u2019s most likely because you already have one. Use the getDeveloperAppKeys method in the left hand menu to check whether there\u2019s already an app key associated with your account.","title":"Register your application"},{"location":"api/apiappkey/#find-your-app-key","text":"After your key is created, you should see in the right hand panel your application: You\u2019ll notice that two application keys have been created; Version \u2013 1.0-Delay: is a delayed app key for development purposes Version \u2013 1.0: is the live pricing app key; on yours it should have a status \u2018No\u2019 in Active. Grab the application key listed for the live price one - for the example above, that is \u2018MkcBqyZrD53V6A..\u2019","title":"Find your app key"},{"location":"api/apiappkey/#activate-your-app-key","text":"This process will generate two app keys: A developer key which is designed for development purposes. This has a variable delay of between 1 and 180 seconds, doesn\u2019t show matched volume and doesn\u2019t need to be activated prior to use. A live app key is intended for transacting on the Exchange and should only be used when you\u2019re ready to start placing bets or can no longer test your strategy effectively using the developer key. Please note that if the live key is used to pull data from the Exchange without corresponding bets being placed a delay may be automatically applied to the live key. If you\u2019re ready to start testing your strategy or placing bets, please contact api@betfair.com.au and we will be happy to assist with activating the live key and implementing your strategy.","title":"Activate your app key"},{"location":"autoTools/","text":"Betfair is one of the only betting platforms in the world that demands winning clients. Unlike bookies, we don\u2019t ban you when you succeed. We need you, and we want you to be able to keep improving your strategies so you win more. We're here to help you in your automation journey, and this site is dedicated to sharing the tools and resources you need to succeed in this journey. Accessing our API As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help Historic Data We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data Using third party tools for automation Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Overview Ratings automation Market favourite automation Tipping automation Automating multiple simultaneous markets Gruss Ratings automation Market favourite automation Automating multiple simultaneous markets Cymatic Trader Ratings automation BF Bot Manager Double or Bust Data modelling An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL modelling series AFL modelling series Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies Inspiration & information The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Some extra info There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy. Need extra help? If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au. We have a dedicated in-house resource that is here to automate your betting strategies.","title":"Index"},{"location":"autoTools/#accessing-our-api","text":"As you may already know, Betfair has its own API to allow you to integrate your program into the Exchange. Many of our most successful clients bet exclusively through this by placing automated bets using custom software. There are lots of resources available to support you in accessing the API effectively: Creating & activating your app key Developer Program knowledge base Dev Docs Developer Forum where you can share your experiences and find out what's worked for other clients Exchange Sports API visualiser for testing market-related queries Exchange Account API visualiser for testing account-related queries Our Datascientists' repos for using R and Python to access the API The UK\u2019s Github repo including libraries for other languages API access Customers are able to access our API to embed it into their programs and automate their strategies If you're a programmer there are lots of resources around to help","title":"Accessing our API"},{"location":"autoTools/#historic-data","text":"We know that automated strategies are only as good as your data. There\u2019s a huge variety of historic pricing data available for almost any race or sport \u2013 you can take a look at our explanation of the different data sources if you\u2019re not quite sure where to start. We\u2019ve also shared some tips on learning to create predictive models using this data , which link in with the models shared in the modelling section . Betfair data sources Accessing the official Historic Data site Historic Data FAQs & sample data Historic Data Specifications API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API The Stream API dev docs are the best source of information for interpreting the data from the Historic Data site Historic BSP csv files Historic Betfair data There is a lot of historical price data available for all makrets offered on the Exchange, ranging from aggregate, market-level csv files to complete JSON recreations of API Stream data","title":"Historic Data"},{"location":"autoTools/#using-third-party-tools-for-automation","text":"Whilst the following tools are not custom built for your approach, they do allow you to automate your betting strategies. You just set up specific betting conditions and let the third party application do the work for you. Bet Angel and Gruss Betting Assistant are the most popular third party tools. We\u2019re putting together a collection of articles on how to use some of these third party tools to automate basic strategies, to give you a starting point that you can then work from. Bet Angel Overview Ratings automation Market favourite automation Tipping automation Automating multiple simultaneous markets Gruss Ratings automation Market favourite automation Automating multiple simultaneous markets Cymatic Trader Ratings automation BF Bot Manager Double or Bust","title":"Using third party tools for automation"},{"location":"autoTools/#data-modelling","text":"An intro to building a predictive model Open source predictive models built by our in-house Data Scientists Modelling the Aus Open EPL modelling series AFL modelling series Brownlow modelling tutorial Predictive modelling Many of our most successful customers use predictive models as the basis for their betting strategies","title":"Data modelling"},{"location":"autoTools/#inspiration-information","text":"The Banker: A Quant's AFL Betting Strategy The Mathematician 'Back and Lay' is a subreddit dedicated to discussing trading techniques Our Twitter community is really active Staking Plans and Strategies Staking and Money Management Some extra info There are a lot of people who use data, models and automation to make a living out of professional betting. Here are some of their stories, and some extra tools to help you develop your own strategy.","title":"Inspiration &amp; information"},{"location":"autoTools/#need-extra-help","text":"If you\u2019re looking for bespoke advice or have extra questions, please contact us at bdp@betfair.com.au. We have a dedicated in-house resource that is here to automate your betting strategies.","title":"Need extra help?"},{"location":"autoTools/EloModelIntro/","text":"Bet Angel Pro: Ratings automation Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the auotmation. We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Assigning multiple markets to your Excel worksheets in Bet Angel so you dont miss a race: Betfair automating simultaneous markets tutorial - Set up Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Downloading & formatting ratings Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. - Writing your rules As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet RunnerName refers to the entire column H in the 'RATINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds2<UserOverround, TimeTillJump2<UserTimeTillJump, ISBLANK(InPlay2)), BACKLAY, \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds3<UserOverround, TimeTillJump3<UserTimeTillJump, ISBLANK(InPlay3)), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",G9) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake)) - Connecting to Bet Angel Video walk through We've put together a litte video walk through to help make this process easier. - Selecting markets We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. - Linking the spreadsheet Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time. - Editing the spreadsheet The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Bet Angel Pro: Ratings automation"},{"location":"autoTools/EloModelIntro/#bet-angel-pro-ratings-automation","text":"","title":"Bet Angel Pro: Ratings automation"},{"location":"autoTools/EloModelIntro/#automating-a-thoroughbred-ratings-strategy-across-multiple-markets-using-bet-angel-pro","text":"Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro"},{"location":"autoTools/EloModelIntro/#-the-plan","text":"Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the auotmation. We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Assigning multiple markets to your Excel worksheets in Bet Angel so you dont miss a race: Betfair automating simultaneous markets tutorial","title":"- The plan"},{"location":"autoTools/EloModelIntro/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"autoTools/EloModelIntro/#-downloading-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.","title":"- Downloading &amp; formatting ratings"},{"location":"autoTools/EloModelIntro/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/EloModelIntro/#-trigger-to-place-bet","text":"In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"autoTools/EloModelIntro/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet RunnerName refers to the entire column H in the 'RATINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds2<UserOverround, TimeTillJump2<UserTimeTillJump, ISBLANK(InPlay2)), BACKLAY, \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds3<UserOverround, TimeTillJump3<UserTimeTillJump, ISBLANK(InPlay3)), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/EloModelIntro/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))), AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(G9>(INDEX(Ratings,MATCH(B9,RunnerName,0))))),AND(BACKLAY=\"LAY\",(G9<(INDEX(Ratings,MATCH(B9,RunnerName,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",G9) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake))","title":"- Preparing the spreadsheet"},{"location":"autoTools/EloModelIntro/#-connecting-to-bet-angel","text":"","title":"- Connecting to Bet Angel"},{"location":"autoTools/EloModelIntro/#video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"Video walk through"},{"location":"autoTools/EloModelIntro/#-selecting-markets","text":"We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature.","title":"- Selecting markets"},{"location":"autoTools/EloModelIntro/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"autoTools/EloModelIntro/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/EloModelIntro/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"autoTools/EloModelIntro/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"autoTools/EloModelIntro/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time.","title":"- Turning off bet confirmation"},{"location":"autoTools/EloModelIntro/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"autoTools/EloModelIntro/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts.","title":"Areas for improvement"},{"location":"autoTools/EloModelIntro/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/EloModelIntro/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/GeeksToybasicmarketviewstakingandoneclickbetting/","text":"Geeks Toy: Setup basic market view and one click betting Setup basic market view Select market type and location Expand Track/Meet/Game and select Market This will open the market in a new window Setting Default Stake Click Staking and Tools Set the desired default stake. Laying for default liability Click the coloured button to the right of the stake and select \u201cLiability\u201d Set the desired default Liability. Setup one click betting Make sure you are happy to not confirm bets before turning this off! Open a Market and right click the top then go to: Advanced Betting \u2013 Available Stake Click \u2013 One Click And Advanced Betting \u2013 Price Click \u2013 One Click This will allow you to instantly place a bet with one click with your default Stake/Liability by clicking on a runner in the market. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Setup basic market view and one click betting"},{"location":"autoTools/GeeksToybasicmarketviewstakingandoneclickbetting/#geeks-toy-setup-basic-market-view-and-one-click-betting","text":"","title":"Geeks Toy: Setup basic market view and one click betting"},{"location":"autoTools/GeeksToybasicmarketviewstakingandoneclickbetting/#setup-basic-market-view","text":"Select market type and location Expand Track/Meet/Game and select Market This will open the market in a new window Setting Default Stake Click Staking and Tools Set the desired default stake. Laying for default liability Click the coloured button to the right of the stake and select \u201cLiability\u201d Set the desired default Liability.","title":"Setup basic market view"},{"location":"autoTools/GeeksToybasicmarketviewstakingandoneclickbetting/#setup-one-click-betting","text":"Make sure you are happy to not confirm bets before turning this off! Open a Market and right click the top then go to: Advanced Betting \u2013 Available Stake Click \u2013 One Click And Advanced Betting \u2013 Price Click \u2013 One Click This will allow you to instantly place a bet with one click with your default Stake/Liability by clicking on a runner in the market.","title":"Setup one click betting"},{"location":"autoTools/GeeksToybasicmarketviewstakingandoneclickbetting/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/GeeksToyinstallationandsetup/","text":"Geeks Toy: Installation and setup Geeks Toy is a 3rd party program that uses the Betfair API. Essentially, it is a dedicated platform that can utilise the Exchange in ways the website and mobile app cannot. It is built to be able to enhance the experience of the Exchange for the punters and potentially increase the amount the punters benefit from the Exchange. This guide aims to help get a better look into Geeks Toy and how to set it up and take a further look into the features that it offers. Installation So, Geeks Toy sounds like something to check out, but how is it installed on my device? It is worth noting that Geeks Toy is only available on a computer or laptop and is not available for mobile devices. With that being said, let us look into how to install Geeks Toy. Open your browser of choice. Either type \u201cGeeks Toy\u201d into your browser or in your URL bar, type in www.geekstoy.com View of the Search engine results and the correct link to click on. Welcome to Geeks Toy! Feel free to look around and explore the website and when ready, click on the download button. The main screen of Geeks Toy. Download button highlighted. Next, click the \u201cDownload for Betfair\u201d button, this will start a download for the program installer (commonly called the install wizard). Download page and install wizard download highlighted. Follow the prompts on your install wizard and once complete, open Geeks Toy. Sign in using your Betfair login details to access Geeks Toy. One thing to be aware of, you can set the program to operate under a training mode, so you can get a better understanding of Geeks Toy before betting from your actual Betfair account. You can change it from this initial login screen under \u201c Operation Mode.\u201d Once logged in, you have successfully installed and are now able to use Geeks Toy. Basic set up and settings Congratulations, now you have access to one of Betfair\u2019s most popular 3rd party apps. But where do you go from here? Next, we will look at the basic set up and settings that Geeks toy has to offer so that you may be able to maximise your experience using Geeks Toy. Market Navigator The first screen you will see once logged in is the \u201cMarket Navigator\u201d screen. Market Navigator Screen This is the main screen you will see when logging into Geeks Toy. From here You will be able to access all the markets that are available via the Betfair Exchange. By navigating through the menus, you will be able to find any horse race, any soccer or AFL match that you would normally find through the Betfair website. From this screen, you will also be able to select which market you would like to be involved in the event. You can also access a whole host of other options from this screen by right clicking. Market Navigator Screen with more options by right clicking. Customizable widgets and interface A list of potential widgets ready to customise. A unique and attractive feature of Geeks Toy is its fully customisable interface in that you can pick and choose what you would like to see, make it as big or small as you like. This allows for an experience and for punting that suits your needs. Profiles can be saved and opened after creation Once you have gotten your ideal view or setup, you may realise that this is ideal for horse racing, but not so much for a soccer game. You can then choose to save your horse racing \u201cprofile\u201d so that you may create another view that is better for soccer, which when created you can save as your soccer \u201cprofile\u201d. The profile in this case is the way you have manipulated the widgets to get the specific experience you are wanting to achieve. The handiness of this feature is it allows quick and easy access between different profiles, and more importantly, not having to construct the profile each time. If you are having troubles figuring out a profile to begin with, click here to watch pro Geeks Toy trader, Caan Berry, do a tutorial on setting up a profile. A view of where to save and load profiles menu. Ladder view of the markets One of the other features that Geeks Toy offers is the \u201cLadder view\u201d of the markets. Essentially, the ladder view is beneficial to punters as it can show not only the best back and lay prices, as well as the next 2 best prices on both sides, but it goes into greater depth in that it is able to go back further and still show how much money is available. This then helps give a better idea of how the selection has traded thus far and where the money is currently, so that you may be able to get those better odds, all refreshing with Geeks Toy's impressive 200 millisecond refresh rate. Market Selection Now that you have set up some profiles and played with the settings, now it is time to select the market you wish to bet into. On the Market Navigator screen, select which Sport or Race type (Horse Racing or Greyhounds) Once you have selected the Sport or Race type you would like, now it is time to choose a league or racecourse. Horse Racing -> Great Britain Races -> Newbury Races, 13th May. Lastly, is to select the Match or Race that you are wanting to bet into. Also, when doing this making sure you are in the correct market within that Match or Race, for example, Winners Market or Place Market for races and Match Odds or Handicap Market for sports. Newbury Races, 13th May -> Select race number. Now that you have selected your desired market, you are able to place the bet you want and start trading on the Exchange. The Race Market Grid View \u2013 Alternative to the Ladder View. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Installation and setup"},{"location":"autoTools/GeeksToyinstallationandsetup/#geeks-toy-installation-and-setup","text":"Geeks Toy is a 3rd party program that uses the Betfair API. Essentially, it is a dedicated platform that can utilise the Exchange in ways the website and mobile app cannot. It is built to be able to enhance the experience of the Exchange for the punters and potentially increase the amount the punters benefit from the Exchange. This guide aims to help get a better look into Geeks Toy and how to set it up and take a further look into the features that it offers.","title":"Geeks Toy: Installation and setup"},{"location":"autoTools/GeeksToyinstallationandsetup/#installation","text":"So, Geeks Toy sounds like something to check out, but how is it installed on my device? It is worth noting that Geeks Toy is only available on a computer or laptop and is not available for mobile devices. With that being said, let us look into how to install Geeks Toy. Open your browser of choice. Either type \u201cGeeks Toy\u201d into your browser or in your URL bar, type in www.geekstoy.com View of the Search engine results and the correct link to click on. Welcome to Geeks Toy! Feel free to look around and explore the website and when ready, click on the download button. The main screen of Geeks Toy. Download button highlighted. Next, click the \u201cDownload for Betfair\u201d button, this will start a download for the program installer (commonly called the install wizard). Download page and install wizard download highlighted. Follow the prompts on your install wizard and once complete, open Geeks Toy. Sign in using your Betfair login details to access Geeks Toy. One thing to be aware of, you can set the program to operate under a training mode, so you can get a better understanding of Geeks Toy before betting from your actual Betfair account. You can change it from this initial login screen under \u201c Operation Mode.\u201d Once logged in, you have successfully installed and are now able to use Geeks Toy.","title":"Installation"},{"location":"autoTools/GeeksToyinstallationandsetup/#basic-set-up-and-settings","text":"Congratulations, now you have access to one of Betfair\u2019s most popular 3rd party apps. But where do you go from here? Next, we will look at the basic set up and settings that Geeks toy has to offer so that you may be able to maximise your experience using Geeks Toy.","title":"Basic set up and settings"},{"location":"autoTools/GeeksToyinstallationandsetup/#market-navigator","text":"The first screen you will see once logged in is the \u201cMarket Navigator\u201d screen. Market Navigator Screen This is the main screen you will see when logging into Geeks Toy. From here You will be able to access all the markets that are available via the Betfair Exchange. By navigating through the menus, you will be able to find any horse race, any soccer or AFL match that you would normally find through the Betfair website. From this screen, you will also be able to select which market you would like to be involved in the event. You can also access a whole host of other options from this screen by right clicking. Market Navigator Screen with more options by right clicking.","title":"Market Navigator"},{"location":"autoTools/GeeksToyinstallationandsetup/#customizable-widgets-and-interface","text":"A list of potential widgets ready to customise. A unique and attractive feature of Geeks Toy is its fully customisable interface in that you can pick and choose what you would like to see, make it as big or small as you like. This allows for an experience and for punting that suits your needs.","title":"Customizable widgets and interface"},{"location":"autoTools/GeeksToyinstallationandsetup/#profiles-can-be-saved-and-opened-after-creation","text":"Once you have gotten your ideal view or setup, you may realise that this is ideal for horse racing, but not so much for a soccer game. You can then choose to save your horse racing \u201cprofile\u201d so that you may create another view that is better for soccer, which when created you can save as your soccer \u201cprofile\u201d. The profile in this case is the way you have manipulated the widgets to get the specific experience you are wanting to achieve. The handiness of this feature is it allows quick and easy access between different profiles, and more importantly, not having to construct the profile each time. If you are having troubles figuring out a profile to begin with, click here to watch pro Geeks Toy trader, Caan Berry, do a tutorial on setting up a profile. A view of where to save and load profiles menu.","title":"Profiles can be saved and opened after creation"},{"location":"autoTools/GeeksToyinstallationandsetup/#ladder-view-of-the-markets","text":"One of the other features that Geeks Toy offers is the \u201cLadder view\u201d of the markets. Essentially, the ladder view is beneficial to punters as it can show not only the best back and lay prices, as well as the next 2 best prices on both sides, but it goes into greater depth in that it is able to go back further and still show how much money is available. This then helps give a better idea of how the selection has traded thus far and where the money is currently, so that you may be able to get those better odds, all refreshing with Geeks Toy's impressive 200 millisecond refresh rate.","title":"Ladder\u00a0view of the markets"},{"location":"autoTools/GeeksToyinstallationandsetup/#market-selection","text":"Now that you have set up some profiles and played with the settings, now it is time to select the market you wish to bet into. On the Market Navigator screen, select which Sport or Race type (Horse Racing or Greyhounds) Once you have selected the Sport or Race type you would like, now it is time to choose a league or racecourse. Horse Racing -> Great Britain Races -> Newbury Races, 13th May. Lastly, is to select the Match or Race that you are wanting to bet into. Also, when doing this making sure you are in the correct market within that Match or Race, for example, Winners Market or Place Market for races and Match Odds or Handicap Market for sports. Newbury Races, 13th May -> Select race number. Now that you have selected your desired market, you are able to place the bet you want and start trading on the Exchange. The Race Market Grid View \u2013 Alternative to the Ladder View.","title":"Market Selection"},{"location":"autoTools/GeeksToyinstallationandsetup/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/GrussSettingupbasicmarketview/","text":"Gruss Betting Assistant: Setting up basic market view and one click betting Setting up basic market view These steps provided below are a simple way to navigate into the market\u2019s you wish to place bets or trade in. When Gruss is opened you will see the market tree on the left hand side of the program. Step 1. Select what market you would like to bet in. Step 2. What event/track you would like to bet in. Step 3. Select the market you wish to place a bet in. Quick pick list Quick pick list has been filtered from the main markets to allow quicker access into your markets without having to manually search for them. There are numerous ways to add markets to your quick picks. Click and drag the market to the quick pick list. Click into the market you want to enter and once it is highlighted press Q. Once you are in a market, you can highlight the market selection at the top of Gruss, select add to quick pick list. Then select this market and it will add it to your quick picks. When you have added markets to quick pick they will appear like this: Once the markets are over or you no longer need to place bets or trade you can delete the markets out of your quick picks. Click on the market and press the key \u2018delete\u2019 when it is highlighted. Right click on your mouse once the market is highlighted, this will bring up numerous options to clear your Quick pick list. Delete market will remove the market you wish to remove from the Quick pick list. The overall market view is similar to the Exchange, however with the added Gruss features that enable faster betting and easier market overview for live betting. Selection name \u2013 name of the teams or selections you wish to bet or trade on. Rev (reverse) \u2013 this can be selected to make the lay on the left side of the market and the back on the right side of the market. Dutch \u2013 this can be selected to back multiple selections to get a potential profit on both or multiple selections in your selected market. Price history \u2013 This is a graph that shows the movement of the back or lay prices throughout the match. WOM (Weight of Money) \u2013 This is a calculation of the three amounts available to back and lay divided by each other. If it is pink it may indicate that the price is likely to go out so you can lay and back later at a higher price. If its blue, then you may be able to lay it at a lower price. Blue arrows \u2013 these can be used to move your odds down or up to place your bets at higher or lower odds. Once you change the odds using the arrows it will stay fixed at those odds and will not update. The best price will continue to update as normal. Percentage of market - the book % for the back odds and likewise above the best lay odds. You can right click on these percentages to sort the grid either by the back odds or lay odds in ascending or\u202fdescending\u202forder. Back stake \u2013 This is how much you are willing to back on the selection. Lay Stake \u2013 This is how you are willing to lay on the selection. Profit \u2013 This will show you your potential profit if that selection were to win depending on if you have backed or laid the selection. Level Profit \u2013 This can also be referred to as a green book or greening up. You can hover the mouse over the level profit column selection to see if there is a opportunity to lock in profit or minimise your loses. The column will then display the potential that can be taken against each selection in the market. One click betting Make sure you are happy to not confirm bets before turning this off! By unticking the \u2018verify bets\u2019 box it will allow you to bet into the market without having to verify your bets, allowing the one click betting feature to be enabled. This can be used to place bets into the market fast and efficiently especially if there are small amounts of liquidity in the market. To stop the one click betting feature all you have to do is click the \u2018verify bets\u2019 box which will now bring up a verify bets page. This feature allows you to verify and ensure you are placing the correct bets into the market. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Setup basic market view and one click betting"},{"location":"autoTools/GrussSettingupbasicmarketview/#gruss-betting-assistant-setting-up-basic-market-view-and-one-click-betting","text":"","title":"Gruss Betting Assistant: Setting up basic market view and one click betting"},{"location":"autoTools/GrussSettingupbasicmarketview/#setting-up-basic-market-view","text":"These steps provided below are a simple way to navigate into the market\u2019s you wish to place bets or trade in. When Gruss is opened you will see the market tree on the left hand side of the program. Step 1. Select what market you would like to bet in. Step 2. What event/track you would like to bet in. Step 3. Select the market you wish to place a bet in.","title":"Setting up basic market view"},{"location":"autoTools/GrussSettingupbasicmarketview/#quick-pick-list","text":"Quick pick list has been filtered from the main markets to allow quicker access into your markets without having to manually search for them. There are numerous ways to add markets to your quick picks. Click and drag the market to the quick pick list. Click into the market you want to enter and once it is highlighted press Q. Once you are in a market, you can highlight the market selection at the top of Gruss, select add to quick pick list. Then select this market and it will add it to your quick picks. When you have added markets to quick pick they will appear like this: Once the markets are over or you no longer need to place bets or trade you can delete the markets out of your quick picks. Click on the market and press the key \u2018delete\u2019 when it is highlighted. Right click on your mouse once the market is highlighted, this will bring up numerous options to clear your Quick pick list. Delete market will remove the market you wish to remove from the Quick pick list. The overall market view is similar to the Exchange, however with the added Gruss features that enable faster betting and easier market overview for live betting. Selection name \u2013 name of the teams or selections you wish to bet or trade on. Rev (reverse) \u2013 this can be selected to make the lay on the left side of the market and the back on the right side of the market. Dutch \u2013 this can be selected to back multiple selections to get a potential profit on both or multiple selections in your selected market. Price history \u2013 This is a graph that shows the movement of the back or lay prices throughout the match. WOM (Weight of Money) \u2013 This is a calculation of the three amounts available to back and lay divided by each other. If it is pink it may indicate that the price is likely to go out so you can lay and back later at a higher price. If its blue, then you may be able to lay it at a lower price. Blue arrows \u2013 these can be used to move your odds down or up to place your bets at higher or lower odds. Once you change the odds using the arrows it will stay fixed at those odds and will not update. The best price will continue to update as normal. Percentage of market - the book % for the back odds and likewise above the best lay odds. You can right click on these percentages to sort the grid either by the back odds or lay odds in ascending or\u202fdescending\u202forder. Back stake \u2013 This is how much you are willing to back on the selection. Lay Stake \u2013 This is how you are willing to lay on the selection. Profit \u2013 This will show you your potential profit if that selection were to win depending on if you have backed or laid the selection. Level Profit \u2013 This can also be referred to as a green book or greening up. You can hover the mouse over the level profit column selection to see if there is a opportunity to lock in profit or minimise your loses. The column will then display the potential that can be taken against each selection in the market.","title":"Quick pick list"},{"location":"autoTools/GrussSettingupbasicmarketview/#one-click-betting","text":"Make sure you are happy to not confirm bets before turning this off! By unticking the \u2018verify bets\u2019 box it will allow you to bet into the market without having to verify your bets, allowing the one click betting feature to be enabled. This can be used to place bets into the market fast and efficiently especially if there are small amounts of liquidity in the market. To stop the one click betting feature all you have to do is click the \u2018verify bets\u2019 box which will now bring up a verify bets page. This feature allows you to verify and ensure you are placing the correct bets into the market.","title":"One click betting"},{"location":"autoTools/GrussSettingupbasicmarketview/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/Kelly/","text":"Kelly Criterion staking Automating with Kelly staking method In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Bet Angel Pro and Gruss have a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and Gruss - we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan We'll be building on the Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Bet Angel Ratings tutorial Rules: here's the spreadsheet for Bet Angel we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Rules: here's the spreadsheet for Gruss Understanding how the Kelly Criterion staking strategy works Tools: Bet Angel Pro and Gruss Betting Assistant - Recapping the strategy covered in the Ratings automation tutorial We'll be using the same trigger strategy that's outlined in the Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N) for Bet Angel users or 'MARKET' worksheet (column S) for Gruss users. We've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here . - Set up Make sure you've downloaded and installed Bet Angel Pro or Gruss, and signed in. For Bet Angel users, Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. Bet Angel Gruss - Writing your rules We're using a customised version of the Ratings tutorial template for Bet Angel Professional and Gruss Betting Assistant to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell C6 of the 'BET ANGEL' worksheet or cell I2 of the 'MARKET' worksheet for Gruss users UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake StakeType refers to cell X1 of the \"SETTINGS' worksheet Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet. Stepping through each step: Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column =IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType)) And you're set! Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Kelly Criterion staking"},{"location":"autoTools/Kelly/#kelly-criterion-staking","text":"","title":"Kelly Criterion staking"},{"location":"autoTools/Kelly/#automating-with-kelly-staking-method","text":"In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Bet Angel Pro and Gruss have a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and Gruss - we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating with Kelly staking method"},{"location":"autoTools/Kelly/#-the-plan","text":"We'll be building on the Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Bet Angel Ratings tutorial Rules: here's the spreadsheet for Bet Angel we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Rules: here's the spreadsheet for Gruss Understanding how the Kelly Criterion staking strategy works Tools: Bet Angel Pro and Gruss Betting Assistant","title":"- The plan"},{"location":"autoTools/Kelly/#-recapping-the-strategy-covered-in-the-ratings-automation-tutorial","text":"We'll be using the same trigger strategy that's outlined in the Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N) for Bet Angel users or 'MARKET' worksheet (column S) for Gruss users. We've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here .","title":"- Recapping the strategy covered in the Ratings automation tutorial"},{"location":"autoTools/Kelly/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro or Gruss, and signed in. For Bet Angel users, Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. Bet Angel Gruss","title":"- Set up"},{"location":"autoTools/Kelly/#-writing-your-rules","text":"We're using a customised version of the Ratings tutorial template for Bet Angel Professional and Gruss Betting Assistant to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/Kelly/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell C6 of the 'BET ANGEL' worksheet or cell I2 of the 'MARKET' worksheet for Gruss users UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake StakeType refers to cell X1 of the \"SETTINGS' worksheet Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/Kelly/#stepping-through-each-step","text":"Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column =IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"Stepping through each step:"},{"location":"autoTools/Kelly/#-preparing-the-spreadsheet","text":"You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))","title":"- Preparing the spreadsheet"},{"location":"autoTools/Kelly/#and-youre-set","text":"Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off.","title":"And you're set!"},{"location":"autoTools/Kelly/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this.","title":"Areas for improvement"},{"location":"autoTools/Kelly/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/Kelly/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/betAngelKellyStake/","text":"Bet Angel Pro: Kelly Criterion staking Automating with Kelly staking method and Bet Angel Pro In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan We'll be building on the Bet Angel Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Bet Angel Ratings tutorial Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Understanding how the Kelly Criterion staking strategy works Tool: Bet Angel Pro - Recapping the strategy covered in the Bet Angel ratings automation tutorial We'll be using the same trigger strategy that's outlined in the Bet Angel Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . The trigger has been simplified in this tutorial and we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N). We've also added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Bet Angel ratings tutorial, we highly recommend that you do so as to understand how the concept of the bet placement trigger works. The tutorial can be found here . - Set up Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Writing your rules We're using a customised version of the Bet Angel Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell C6 of the 'BET ANGEL' worksheet StakeType refers to cell I5 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet. Stepping through each step: Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column =IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType)) And you're set! Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Kelly criterion staking"},{"location":"autoTools/betAngelKellyStake/#bet-angel-pro-kelly-criterion-staking","text":"","title":"Bet Angel Pro: Kelly Criterion staking"},{"location":"autoTools/betAngelKellyStake/#automating-with-kelly-staking-method-and-bet-angel-pro","text":"In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating with Kelly staking method and Bet Angel Pro"},{"location":"autoTools/betAngelKellyStake/#-the-plan","text":"We'll be building on the Bet Angel Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Bet Angel Ratings tutorial Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Understanding how the Kelly Criterion staking strategy works Tool: Bet Angel Pro","title":"- The plan"},{"location":"autoTools/betAngelKellyStake/#-recapping-the-strategy-covered-in-the-bet-angel-ratings-automation-tutorial","text":"We'll be using the same trigger strategy that's outlined in the Bet Angel Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . The trigger has been simplified in this tutorial and we'll need to make small tweaks to the stake column of the 'BET ANGEL' worksheet (column N). We've also added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Bet Angel ratings tutorial, we highly recommend that you do so as to understand how the concept of the bet placement trigger works. The tutorial can be found here .","title":"- Recapping the strategy covered in the Bet Angel ratings automation tutorial"},{"location":"autoTools/betAngelKellyStake/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"autoTools/betAngelKellyStake/#-writing-your-rules","text":"We're using a customised version of the Bet Angel Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/betAngelKellyStake/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell C6 of the 'BET ANGEL' worksheet StakeType refers to cell I5 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/betAngelKellyStake/#stepping-through-each-step","text":"Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('BET ANGEL' worksheet) and return the best available back odds from the G column =IFERROR(INDEX('Bet Angel'!$E$9:$J$68,MATCH(H2,'Bet Angel'!$B$9:$B$68,0),3),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Bet Angel populates in cell C6 of the 'BET ANGEL' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"Stepping through each step:"},{"location":"autoTools/betAngelKellyStake/#-preparing-the-spreadsheet","text":"You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))","title":"- Preparing the spreadsheet"},{"location":"autoTools/betAngelKellyStake/#and-youre-set","text":"Once you've set your Kelly strategy set up with your strategy, it should only take a number of seconds to load your markets for the day. Just make sure you have all of the Bet Angel settings correctly selected before you leave your to run, as some of them reset by default when you turn Bet Angel off.","title":"And you're set!"},{"location":"autoTools/betAngelKellyStake/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - We missed some races because of this.","title":"Areas for improvement"},{"location":"autoTools/betAngelKellyStake/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/betAngelKellyStake/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/betAngelMarketFavouriteAutomation/","text":"Bet Angel Pro: Market favourite automation Automating a market favourite strategy using Bet Angel Pro Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them. Building on our previous articles , we're using the spreadsheet functionality available in Bet Angel Pro to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. - The plan Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Bet Angel Pro - Set up Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Writing your rules As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet In short, we want to back runners when: the selection's available to back price (Column G) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this the scheduled event start time is less and greater than what we specify Back market percentage is less than a certain value that we choose the event isn't in play - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell C4 in the 'OPTIONS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets UserOverround refers to cell C3 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market MinTime refers to cell C2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell E2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market TakeSP refers to cell C5 in the 'OPTIONS' worksheet which allows you to change a single value more easily. This is our trigger on Excel formula: ``` excel tab=\"Multi line\" =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column G) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") TimeTillJump1 < MaxTime and > MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C2 and greater than cell E2 in the 'OPTIONS' worksheet (named 'MinTime' and 'MaxTime' respectively), as we need to both place the bet and then convert it to a BSP bet before the off (more on this later). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 (named 'TimeTillJump1')in the 'SETTINGS' worksheet, where we've already done the calculations for you. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") Overrounds1 < UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C3 of the 'OPTIONS' worksheet =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to G1 in the 'BET ANGEL' worksheet, if this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") Result: if the statement above is true, the formula returns 'BACK', at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, ISBLANK(InPlay2)), \"BACK\", \"\") Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, ISBLANK(InPlay2)), \"BACK\", \"\") Convert bets to Betfair Starting Price: Bet Angel Pro doesn't offer the option to place straight BSP bets, so we've got around that here by placing the bets initially at odds of 1000 (which won't get matched for short favourites), and then at certain amount of seconds from the scheduled start using what Bet Angel calls a 'Global Command' to convert all unmatched bets to BSP. The exact number of seconds can be easily changed by updating cell C5 from the 'OPTIONS' worksheet. This formula goes in cell L6 of the 'BET ANGEL' worksheet, and once it's triggered the bets will automatically convert. Remember, just like the main trigger, each worksheet's global command will need to be updated to reference its respective TimeTillJump calculation. -- 'BET ANGEL' worksheet global command formula =IF(TimeTillJump1 < TakeSP, \"TAKE_SP_ALL\", \"\") -- 'BET ANGEL 2' worksheet global command formula =IF(TimeTillJump2 < TakeSP, \"TAKE_SP_ALL\", \"\") -- 'BET ANGEL 3' worksheet global command formula =IF(TimeTillJump3 < TakeSP, \"TAKE_SP_ALL\", \"\") Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",\"1000\") Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat staking here, so will just place $10 on each runner. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",\"10\") Global Command: this is what triggers the open bets to convert to BSP, and only goes in one cell, L6. As soon as the countdown timer reaches less than 60 seconds this will fire. As mentioned above, remember to update the formula for each worksheet. =IF(TimeTillJump1 < TakeSP, \"TAKE_SP_ALL\", \"\") - You know the drill The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Bet Angel Pro. - Video walk through We've put together a litte video walk through to help make this process easier. - Selecting markets We used the markets menu in the Guardian tool to navigate to the tracks we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). - Linking the spreadsheet Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as my check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status and global status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. - Editing the spreadsheet The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. If the market changes significantly in those last few minutes and a third selection shortens in past the two we've placed bets on you could end up with bets on more than the intended two runner. This is something you could check for in your bet rule if you wanted to ensure you were only backing a set number of runners. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Market fav auto"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#bet-angel-pro-market-favourite-automation","text":"","title":"Bet Angel Pro: Market favourite automation"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#automating-a-market-favourite-strategy-using-bet-angel-pro","text":"Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them. Building on our previous articles , we're using the spreadsheet functionality available in Bet Angel Pro to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions.","title":"Automating a market favourite strategy using Bet Angel Pro"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-the-plan","text":"Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Bet Angel Pro","title":"- The plan"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: the selection's available to back price (Column G) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this the scheduled event start time is less and greater than what we specify Back market percentage is less than a certain value that we choose the event isn't in play","title":"- Trigger to place bet"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell C4 in the 'OPTIONS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets UserOverround refers to cell C3 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market MinTime refers to cell C2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell E2 in the 'OPTIONS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market TakeSP refers to cell C5 in the 'OPTIONS' worksheet which allows you to change a single value more easily. This is our trigger on Excel formula: ``` excel tab=\"Multi line\" =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column G) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") TimeTillJump1 < MaxTime and > MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C2 and greater than cell E2 in the 'OPTIONS' worksheet (named 'MinTime' and 'MaxTime' respectively), as we need to both place the bet and then convert it to a BSP bet before the off (more on this later). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E4 (named 'TimeTillJump1')in the 'SETTINGS' worksheet, where we've already done the calculations for you. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") Overrounds1 < UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C3 of the 'OPTIONS' worksheet =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to G1 in the 'BET ANGEL' worksheet, if this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") Result: if the statement above is true, the formula returns 'BACK', at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, ISBLANK(InPlay2)), \"BACK\", \"\") Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, ISBLANK(InPlay2)), \"BACK\", \"\") Convert bets to Betfair Starting Price: Bet Angel Pro doesn't offer the option to place straight BSP bets, so we've got around that here by placing the bets initially at odds of 1000 (which won't get matched for short favourites), and then at certain amount of seconds from the scheduled start using what Bet Angel calls a 'Global Command' to convert all unmatched bets to BSP. The exact number of seconds can be easily changed by updating cell C5 from the 'OPTIONS' worksheet. This formula goes in cell L6 of the 'BET ANGEL' worksheet, and once it's triggered the bets will automatically convert. Remember, just like the main trigger, each worksheet's global command will need to be updated to reference its respective TimeTillJump calculation. -- 'BET ANGEL' worksheet global command formula =IF(TimeTillJump1 < TakeSP, \"TAKE_SP_ALL\", \"\") -- 'BET ANGEL 2' worksheet global command formula =IF(TimeTillJump2 < TakeSP, \"TAKE_SP_ALL\", \"\") -- 'BET ANGEL 3' worksheet global command formula =IF(TimeTillJump3 < TakeSP, \"TAKE_SP_ALL\", \"\") Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste these three formulas into the relevant cell on each green row - We copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( (COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, ISBLANK(InPlay1)), \"BACK\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67)-RANK(G9,($G$9,$G$11,$G$13,$G$15,$G$17,$G$19,$G$21,$G$23,$G$25,$G$27,$G$29,$G$31,$G$33,$G$35,$G$37,$G$39,$G$41,$G$43,$G$45,$G$47,$G$49,$G$51,$G$53,$G$55,$G$57,$G$59,$G$61,$G$63,$G$65,$G$67))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,ISBLANK(InPlay1)),\"BACK\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",\"1000\") Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat staking here, so will just place $10 on each runner. This goes in column N (N9 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",\"10\") Global Command: this is what triggers the open bets to convert to BSP, and only goes in one cell, L6. As soon as the countdown timer reaches less than 60 seconds this will fire. As mentioned above, remember to update the formula for each worksheet. =IF(TimeTillJump1 < TakeSP, \"TAKE_SP_ALL\", \"\")","title":"- Preparing the spreadsheet"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-you-know-the-drill","text":"The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Bet Angel Pro.","title":"- You know the drill"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"- Video walk through"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-selecting-markets","text":"We used the markets menu in the Guardian tool to navigate to the tracks we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. If you wanted to include all horse or greyhound races for a day you could use the 'quick picks' tab to do this more efficiently. Once you've chosen the races you're interested in click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up).","title":"- Selecting markets"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in Guardian, then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as my check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use the same sheet for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status and global status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings.","title":"- Turning off bet confirmation"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. If the market changes significantly in those last few minutes and a third selection shortens in past the two we've placed bets on you could end up with bets on more than the intended two runner. This is something you could check for in your bet rule if you wanted to ensure you were only backing a set number of runners.","title":"Areas for improvement"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/betAngelMarketFavouriteAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/betAngelRatingsAutomation/","text":"Bet Angel Pro: Ratings automation Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the automation. We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Assigning multiple markets to your Excel worksheets in Bet Angel so you don't miss a race: Betfair automating simultaneous markets tutorial - Set up Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Downloading & formatting ratings Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. - Writing your rules As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet SelectionID refers to the entire column G in the 'RATINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. Instead of matching the runner name from the market to that which is defined in our ratings, we're using their selection ID instead. As the Selection ID is unique to each runner, it will help us eliminate any potential errors or issues. Because the ratings from the Betfair data scientists includes Selection ID, there's no additional work or mucking around to get this implemented. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",G9) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake)) - Connecting to Bet Angel Video walk through We've put together a litte video walk through to help make this process easier. - Selecting markets We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. - Linking the spreadsheet Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Bet Angel features Here are some Bet Angel features that you'll need to consider. - Multiple bets/clearing status cells The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs. - Turning off bet confirmation Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time. - Editing the spreadsheet The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Ratings auto"},{"location":"autoTools/betAngelRatingsAutomation/#bet-angel-pro-ratings-automation","text":"","title":"Bet Angel Pro: Ratings automation"},{"location":"autoTools/betAngelRatingsAutomation/#automating-a-thoroughbred-ratings-strategy-across-multiple-markets-using-bet-angel-pro","text":"Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to some of our other Betfair automation tutorials, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Bet Angel. We've also added some functionality to the spreadsheet to allow you to automate three simultaneous markets in the event that a market is delayed and race times for multiple markets overlap. Bet Angel Pro has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Bet Angel and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a thoroughbred ratings strategy across multiple markets using Bet Angel Pro"},{"location":"autoTools/betAngelRatingsAutomation/#-the-plan","text":"Bet Angel Pro's 'Guardian' feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here, incorporating the thoroughbred racing ratings into the automation. We'll step through how we went about getting Bet Angel Pro to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Bet Angel Pro Assigning multiple markets to your Excel worksheets in Bet Angel so you don't miss a race: Betfair automating simultaneous markets tutorial","title":"- The plan"},{"location":"autoTools/betAngelRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"autoTools/betAngelRatingsAutomation/#-downloading-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Bet Angel template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.","title":"- Downloading &amp; formatting ratings"},{"location":"autoTools/betAngelRatingsAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/betAngelRatingsAutomation/#-trigger-to-place-bet","text":"In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"autoTools/betAngelRatingsAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet SelectionID refers to the entire column G in the 'RATINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BACKLAY refers to cell H5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell I5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. Instead of matching the runner name from the market to that which is defined in our ratings, we're using their selection ID instead. As the Selection ID is unique to each runner, it will help us eliminate any potential errors or issues. Because the ratings from the Betfair data scientists includes Selection ID, there's no additional work or mucking around to get this implemented. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell F3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false And Or function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/betAngelRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))), AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))), Overrounds1<UserOverround, TimeTillJump1<UserTimeTillJump, ISBLANK(InPlay1)), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (G9 > (INDEX(Ratings,MATCH(A9,SelectionID,0))))),AND(BACKLAY = \"LAY\", (G9 < (INDEX(Ratings,MATCH(A9,SelectionID,0)))))),Overrounds1<UserOverround,TimeTillJump1<UserTimeTillJump,ISBLANK(InPlay1)),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell G9 for the first runner). This goes in column M (M9 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IF(B9=\"\",\"\",G9) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(B9=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(G9-1),stake*(H9/(H9-1))-stake))","title":"- Preparing the spreadsheet"},{"location":"autoTools/betAngelRatingsAutomation/#-connecting-to-bet-angel","text":"","title":"- Connecting to Bet Angel"},{"location":"autoTools/betAngelRatingsAutomation/#video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"Video walk through"},{"location":"autoTools/betAngelRatingsAutomation/#-selecting-markets","text":"We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature.","title":"- Selecting markets"},{"location":"autoTools/betAngelRatingsAutomation/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"autoTools/betAngelRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/betAngelRatingsAutomation/#bet-angel-features","text":"Here are some Bet Angel features that you'll need to consider.","title":"Bet Angel features"},{"location":"autoTools/betAngelRatingsAutomation/#-multiple-betsclearing-status-cells","text":"The Bet Angel spreadsheet won't let a bet go on if there is a value in column 0 for the runner, the 'status' column, to avoid accidentally placing multiple bets unintentionally. As soon as a bet triggers, Bet Angel automatically changes this cell to 'PLACING', then to 'PLACED' when the bet is confirmed as having been received by Betfair. In this strategy we only want to place one bet per runner, but if you wanted to place multiple bets on a runner you'd need to have a play with the macros to clear the 'status' cells more regularly, and instead reference the number of bets placed/matched in columns T:AE. Careful here though, as the values in these columns sometimes take a little time to update, and we've had more bets go on than we intended when using these cells as our check, as bet trigger re-evaluated before columns T:AE had updated. As we want to use each worksheet over and over again for multiple races, and the 'status' cells don't clear automatically, we've created a macro in the Excel sheet that auto-clears the status cells whenever a new race loads. It also clears the cells if they say 'FAILED', as we found that if there were internet network issues or similar it would fail once then not try to place the bet again. This was based on some logic we found in a forum discussion on Bet Angel . If you're feeling adventurous you can have a play with the macros and edit them to suit your specific needs.","title":"- Multiple bets/clearing status cells"},{"location":"autoTools/betAngelRatingsAutomation/#-turning-off-bet-confirmation","text":"Unless you want to manually confirm each individual bet you're placing (which you definitely might want to do until you feel comfortable that the program and strategy are behaving as you expect) you'll need to go into the 'Settings' tab on the main Bet Angel Pro program, click 'Edit settings', go to the 'Behaviour' tab, and remove the tick at the top next to 'Confirm Bets?' - you can then save these settings. Bet Angel allows you to save different settings configurations as profiles. Depending what you are wanting to use Bet Angel for each time you open it up, you can select the appropriate setting profile to suit your needs without having to go through and change them every time.","title":"- Turning off bet confirmation"},{"location":"autoTools/betAngelRatingsAutomation/#-editing-the-spreadsheet","text":"The spreadsheet really doesn't like it when you try and edit it 'live', so make sure you untick 'connect' on the Excel tab in Guardian before you make any changes, save the sheet, then tick 'connect' again once you've finished your edits.","title":"- Editing the spreadsheet"},{"location":"autoTools/betAngelRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts.","title":"Areas for improvement"},{"location":"autoTools/betAngelRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/betAngelRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/betAngelSimultaneousMarkets/","text":"Bet Angel Pro: Automating simultaneous markets Dont miss out on a market with simultaneous automation If you have a concern of missing markets due to delays or unforeseen circumstances, Bet Angel is able to work off multiple worksheets for different meetings, all from the same workbook. For example, if we are wanting to automate markets at 1.45pm at Swan Hill, 1.55pm at Townsville and 2.02pm at Menangle, but the Swan Hill market is delayed, we don't want to miss out on the Townsville or Menangle markets. Click Here to download the spreadsheet that we have edited which will allow you to bet on multiple markets To set this up, we need to make sure that there are enough instances of the \u2018Bet Angel\u2019 worksheet to cover markets in the event of a delay or on track issue. In this context, three worksheets should be enough to cover the days markets if some of them are delayed for whatever reason. We've created a special Excel file with macros that will allow up to three markets to be linked at the same time. These macros automatically clear the contents of each sheet when they detect a market change and clear any errors that may occur. If you want to have more worksheets to be linked to different markets, you will need to update the macros accordingly so that they all work independently from one another. Simultaneous markets spreadsheet Please note that the above edited excel workbook does not include any automated strategies. You will need to add this in yourself or take a look at our Multiple market ratings tutorial Once this is done, save the file and close out of it completely (Bet Angel will open it back up for us automatically when we\u2019re ready to start our automation). In Bet Angel, follow the usual process of clicking the \u2018Guardian\u2019 Icon, select your markets and the usual \u2018Browse for file\u2019 button to locate your Excel file. The only thing you will need to do differently for Bet Angel is to simply allocate a specific worksheet to a particular market that you are betting on. As each market concludes, the assigned worksheet will then reset and then re-assign itself to the next market in the guaradian list. When you\u2019re ready for automation to take over, click the \u2018Connect\u2019 check box and then it will do its thing. And you're set! Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Simultaneous markets"},{"location":"autoTools/betAngelSimultaneousMarkets/#bet-angel-pro-automating-simultaneous-markets","text":"","title":"Bet Angel Pro: Automating simultaneous markets"},{"location":"autoTools/betAngelSimultaneousMarkets/#dont-miss-out-on-a-market-with-simultaneous-automation","text":"If you have a concern of missing markets due to delays or unforeseen circumstances, Bet Angel is able to work off multiple worksheets for different meetings, all from the same workbook. For example, if we are wanting to automate markets at 1.45pm at Swan Hill, 1.55pm at Townsville and 2.02pm at Menangle, but the Swan Hill market is delayed, we don't want to miss out on the Townsville or Menangle markets. Click Here to download the spreadsheet that we have edited which will allow you to bet on multiple markets To set this up, we need to make sure that there are enough instances of the \u2018Bet Angel\u2019 worksheet to cover markets in the event of a delay or on track issue. In this context, three worksheets should be enough to cover the days markets if some of them are delayed for whatever reason. We've created a special Excel file with macros that will allow up to three markets to be linked at the same time. These macros automatically clear the contents of each sheet when they detect a market change and clear any errors that may occur. If you want to have more worksheets to be linked to different markets, you will need to update the macros accordingly so that they all work independently from one another. Simultaneous markets spreadsheet Please note that the above edited excel workbook does not include any automated strategies. You will need to add this in yourself or take a look at our Multiple market ratings tutorial Once this is done, save the file and close out of it completely (Bet Angel will open it back up for us automatically when we\u2019re ready to start our automation). In Bet Angel, follow the usual process of clicking the \u2018Guardian\u2019 Icon, select your markets and the usual \u2018Browse for file\u2019 button to locate your Excel file. The only thing you will need to do differently for Bet Angel is to simply allocate a specific worksheet to a particular market that you are betting on. As each market concludes, the assigned worksheet will then reset and then re-assign itself to the next market in the guaradian list. When you\u2019re ready for automation to take over, click the \u2018Connect\u2019 check box and then it will do its thing.","title":"Dont miss out on a market with simultaneous automation"},{"location":"autoTools/betAngelSimultaneousMarkets/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Bet Angel Pro it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/betAngelSimultaneousMarkets/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au","title":"What next?"},{"location":"autoTools/betAngelSimultaneousMarkets/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/betAngelTippingAutomation/","text":"Bet Angel Pro: Tipping automation Automating a (non-ratings based) tipping strategy using Bet Angel Pro We all love getting some good racing tips, but who has time to sit and place bets all day? Wouldn't it be easier if you could take those tips and get a program to automatically place the bets on your behalf? This is what we're going to explore here - we'll be using Bet Angel Pro to place bets automatically based on a set of tips. This is our first-time using Bet Angel for this approach and we are very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. - The plan We have a set of tips that we've taken from our DataScientists' Racing Prediction Model, but this approach should work for any set of tips you may have. Our goal is to create an automated process which will let us choose our tips for the day, then walk away and the program do the leg work. Here we'll step through how we went about getting Bet Angel Pro to place bets on the favourite runner identified by Betfair's Data Scientists . There are no ratings associated with these tips, so we're happy to take Betfair's Starting Price instead of a price for these bets. If you want to follow along and try this approach yourself you'll need to download Bet Angel Pro and sign up for either a subscription or at least a test period. They have a 14 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Tips: Betfair Data Scientists' Racing Prediction Model Rules: here's the spreadsheet , we used to automate our tips but you may need to tweak it a bit to suit your own tips. Tool: Bet Angel Pro - Set up First up we need to make sure we've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up. - Writing your rules As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our tips and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet In short, we want to back or lay runners when: The runners name has been specified in our tipping list Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial RunnerName refers to the entire column A in the 'TIP' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H2 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BackStake refers to cell H3 in the 'SETTINGS' worksheet and like the name suggests, will be the stake for any back bets that are triggered LayStake refers to cell H4 in the 'SETTINGS' worksheet and will be the stake for any lay bets that are triggered BetType is the entire B column in the 'TIP' worksheet. Depending on your tip for each runner, you can choose whether you want a back or lay bet to be triggered for that runner This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) ``` excel tab=\"Single line\" =IF(AND(COUNTIF(RunnerName,B9)>0,TimeTillJump1<UserTimeTillJump,Overrounds1<UserOverround,ISBLANK(InPlay1)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\") Stepping through each step: Checking whether the runner is to have a bet placed: Here the trigger is checking the list of runners in the 'TIPS' worksheet so it can decide whether we want a bet to be placed or not. If the name does appear in our list, then it returns a TRUE flag and continues with the next trigger condition. If the name is not in the list, then no bet will be placed. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell H2 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E9 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Result: if the statement above is true, check whether BACK or LAY has been selected in column B of the 'TIPS' worksheet for that runner. Whatever has been specified, trigger that bet type. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump2<UserTimeTillJump, Overrounds2<UserOverround, ISBLANK(InPlay2)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump3<UserTimeTillJump, Overrounds3<UserOverround, ISBLANK(InPlay3)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false [COUNTIF function:] Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump3<UserTimeTillJump, Overrounds3<UserOverround, ISBLANK(InPlay3)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) ``` excel tab=\"Single line\" =IF(AND(COUNTIF(RunnerName,B9)>0,TimeTillJump3<UserTimeTillJump,Overrounds3<UserOverround,ISBLANK(InPlay3)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\") Odds: Because we will be taking the BSP, we want to ensure that the initial bets that we place are not matched so that the \"TAKE_SP_ALL\" command can trigger for the global command. To do this, it checks the bet type for that particular runner. If it is Backing, then place odds at 1000 and if its going to be a lay bet, then set odds at 1.01 Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR which we're also using, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"LAY\",1.01,1000)),\"\") Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are using seperate variables for a back bet and lay bet. These variables can be easily changed from the 'SETTINGS' tab. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"BACK\", BackStake,LayStake)),\"\") - Connecting to Bet Angel Video walk through We've put together a litte video walk through to help make this process easier. - Selecting markets We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature. - Linking the spreadsheet Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins. And you're set! Once you've set your rules up and got comfortable using Bet Angel Pro it should only take number of seconds to load the markets up and choose your selections for the day. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to us at automation@betfair.com.au Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Tipping auto"},{"location":"autoTools/betAngelTippingAutomation/#bet-angel-pro-tipping-automation","text":"","title":"Bet Angel Pro: Tipping automation"},{"location":"autoTools/betAngelTippingAutomation/#automating-a-non-ratings-based-tipping-strategy-using-bet-angel-pro","text":"We all love getting some good racing tips, but who has time to sit and place bets all day? Wouldn't it be easier if you could take those tips and get a program to automatically place the bets on your behalf? This is what we're going to explore here - we'll be using Bet Angel Pro to place bets automatically based on a set of tips. This is our first-time using Bet Angel for this approach and we are very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions.","title":"Automating a (non-ratings based) tipping strategy using Bet Angel Pro"},{"location":"autoTools/betAngelTippingAutomation/#-the-plan","text":"We have a set of tips that we've taken from our DataScientists' Racing Prediction Model, but this approach should work for any set of tips you may have. Our goal is to create an automated process which will let us choose our tips for the day, then walk away and the program do the leg work. Here we'll step through how we went about getting Bet Angel Pro to place bets on the favourite runner identified by Betfair's Data Scientists . There are no ratings associated with these tips, so we're happy to take Betfair's Starting Price instead of a price for these bets. If you want to follow along and try this approach yourself you'll need to download Bet Angel Pro and sign up for either a subscription or at least a test period. They have a 14 day free trial that's valuable for establishing whether this tool will do what you want it to for your specific strategy. Resources Tips: Betfair Data Scientists' Racing Prediction Model Rules: here's the spreadsheet , we used to automate our tips but you may need to tweak it a bit to suit your own tips. Tool: Bet Angel Pro","title":"- The plan"},{"location":"autoTools/betAngelTippingAutomation/#-set-up","text":"First up we need to make sure we've downloaded and installed Bet Angel Pro, and signed in. Once you open the program up click on the 'G' Guardian icon and open the Guardian functionality up.","title":"- Set up"},{"location":"autoTools/betAngelTippingAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using an customised Bet Angel template Excel sheet to implement our strategy, so it can make betting decisions based on our tips and automate on multiple markets. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"-  Writing your rules"},{"location":"autoTools/betAngelTippingAutomation/#-trigger-to-place-bet","text":"In short, we want to back or lay runners when: The runners name has been specified in our tipping list Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"autoTools/betAngelTippingAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial RunnerName refers to the entire column A in the 'TIP' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell AF8 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets. UserOverround refers to cell H5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell E9, E13 and E17 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market. UserTimeTillJump refers to cell H2 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell G1 in the 'BET ANGEL', 'BET ANGEL 2' and 'BET ANGEL 3' worksheets respectively. Bet Angel will populate a status in these worksheet cells such as \"In Play\" or \"Suspended\" for each market BackStake refers to cell H3 in the 'SETTINGS' worksheet and like the name suggests, will be the stake for any back bets that are triggered LayStake refers to cell H4 in the 'SETTINGS' worksheet and will be the stake for any lay bets that are triggered BetType is the entire B column in the 'TIP' worksheet. Depending on your tip for each runner, you can choose whether you want a back or lay bet to be triggered for that runner This is our trigger for the 'BET ANGEL' worksheet: ``` excel tab=\"Multi line\" =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) ``` excel tab=\"Single line\" =IF(AND(COUNTIF(RunnerName,B9)>0,TimeTillJump1<UserTimeTillJump,Overrounds1<UserOverround,ISBLANK(InPlay1)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\") Stepping through each step: Checking whether the runner is to have a bet placed: Here the trigger is checking the list of runners in the 'TIPS' worksheet so it can decide whether we want a bet to be placed or not. If the name does appear in our list, then it returns a TRUE flag and continues with the next trigger condition. If the name is not in the list, then no bet will be placed. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell H2 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E9 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Calculating the time until the jump for multiple markets at the same time One thing to be aware of here is that because we're wanting to follow up to three markets in our excel workbook, we need to have three instances of the time conversion formula - One for each possible market that we may want to link into our Excel file. These formulas are located in the 'SETTINGS' worksheet on columns C, D and E. In the 'BET ANGEL' worksheet, the formulas will be written TimeTillJump1<UserTimeTillJump, while in the 'BET ANGEL 2' and 'BET ANGEL 3' worksheets it will be written TimeTillJump2<UserTimeTillJump, and TimeTillJump3<UserTimeTillJump, respectively. This will mean that every 'BET ANGEL' worksheet will display and track the correct time till jump for their own applicable market. Back market percentage (Overrounds1) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'SETTINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is blank it means it's not displaying the 'in-play' flag, so it's safe to place bets. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Result: if the statement above is true, check whether BACK or LAY has been selected in column B of the 'TIPS' worksheet for that runner. Whatever has been specified, trigger that bet type. =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump1<UserTimeTillJump, Overrounds1<UserOverround, ISBLANK(InPlay1)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) updating the trigger for 'BET ANGEL 2' and 'Bet ANGEL 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'BET ANGEL 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump2<UserTimeTillJump, Overrounds2<UserOverround, ISBLANK(InPlay2)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Trigger for 'BET ANGEL 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump3<UserTimeTillJump, Overrounds3<UserOverround, ISBLANK(InPlay3)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false [COUNTIF function:] Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/betAngelTippingAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cell on each green row - we copied ours into all of the coloured cells in the sheet, just in case the fields are bigger in future events such as the Melbourne Cup. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column L (L9 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( COUNTIF(RunnerName,B9)>0, TimeTillJump3<UserTimeTillJump, Overrounds3<UserOverround, ISBLANK(InPlay3)), INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2), \"\" ) ``` excel tab=\"Single line\" =IF(AND(COUNTIF(RunnerName,B9)>0,TimeTillJump3<UserTimeTillJump,Overrounds3<UserOverround,ISBLANK(InPlay3)),INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2),\"\") Odds: Because we will be taking the BSP, we want to ensure that the initial bets that we place are not matched so that the \"TAKE_SP_ALL\" command can trigger for the global command. To do this, it checks the bet type for that particular runner. If it is Backing, then place odds at 1000 and if its going to be a lay bet, then set odds at 1.01 Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column B. A similar effect to IFERROR which we're also using, if Bet Angel hasn't populated cell B9 with a runner name, then dont populate this cell at all. =IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"LAY\",1.01,1000)),\"\") Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are using seperate variables for a back bet and lay bet. These variables can be easily changed from the 'SETTINGS' tab. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IFERROR(IF(B9=\"\",\"\",IF(INDEX(TIP!A:B,MATCH(B9,RunnerName,0),2)=\"BACK\", BackStake,LayStake)),\"\")","title":"- Preparing the spreadsheet"},{"location":"autoTools/betAngelTippingAutomation/#-connecting-to-bet-angel","text":"","title":"- Connecting to Bet Angel"},{"location":"autoTools/betAngelTippingAutomation/#video-walk-through","text":"We've put together a litte video walk through to help make this process easier.","title":"Video walk through"},{"location":"autoTools/betAngelTippingAutomation/#-selecting-markets","text":"We used the markets menu in the 'Guardian' tool to navigate to Australian tracks that we have ratings for, then multi-selected all the win markets by holding down the control key and clicking on the different markets. Once you've chosen the races you're interested in, click the 'add' button and you'll see them appear in the main body of the screen. Make sure you sort the races by start time , so Bet Angel will automatically move through them in the right order and allocate the next race to the spreadsheet once the previous one ends. You do this by clicking on the 'start time' column heading until the races are in time order (when the arrow is pointing up). Below is an example of doing this on Australian markets. The Excel spreadsheet used in this tutorial is created in a way that allows it to link multiple markets at the same time. Take a look at the Betfair automating simultaneous markets tutorial on the hub which will step you through the process so you can take advantage of this feature.","title":"- Selecting markets"},{"location":"autoTools/betAngelTippingAutomation/#-linking-the-spreadsheet","text":"Open the 'Excel' tab in 'Guardian', then use the browse functionality to choose the spreadsheet you've been working on. From there, click on 'open workbook', then make sure you have 'connect', 'auto-bind Bet Angel sheets and 'auto-clear Bet Angel bindings' all selected. You also need to make sure that the first race has the 'Bet Angel' tab selected in the 'Excel sheet' column - from there it will then automatically update this for each race as one finishes and the next one begins.","title":"- Linking the spreadsheet"},{"location":"autoTools/betAngelTippingAutomation/#and-youre-set","text":"Once you've set your rules up and got comfortable using Bet Angel Pro it should only take number of seconds to load the markets up and choose your selections for the day. Note: You will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/betAngelTippingAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to us at automation@betfair.com.au","title":"What next?"},{"location":"autoTools/betAngelTippingAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/betAngeladvanced/","text":"Bet Angel: An Advanced guide Automation Bet Angel offers two forms of automation through their Guardian interface: Triggered betting which operates by linking pre-defined commands together to form an automated strategy Use Excel to monitor multiple markets and execute bets or trades based upon triggers set within the Excel spreadsheet within one or more markets. The triggered betting feature is a good way to implement simple automations quickly while using Excel gives you a greater level of control and sophistication as you have the ability to use formulas and even create your own macros using Visual Basic for Applications (VBA). Triggered Betting To begin forming your automation in the triggered betting feature within Bet Angel, you will need to open the Guardian feature by clicking on the icon on the main Bet Angel toolbar. The Guardian window will open where you have a number of tabs to select from. First, we want to have a list of markets that we want to automate. From the \u2018Market Selection\u2019 tab select each market (use Ctrl to select multiple) you want Click the \u2018Add Bet Angel markets\u2019 button. Keep in mind that the order that the markets are in the list, will be the order that your automation will go through. Tip: Click on the \u2018Start time\u2019 column to order your list based on race start time. Guardian will work through your list in order. Within the \u2018Markets\u2019 tab you will see the \u2018Refresh interval\u2019 dropdown box. This instructs Bet Angel how often to refresh the markets in the list. We recommend setting this value to be the lowest possible by: Navigate back to the Main Bet Angel home screen Click the \u2018Settings\u2019 tab Select \u2018Edit Settings\u2019 Click the \u2018Communications\u2019 tab Check \u2018Use Exchange Streaming\u2019 Click \u2018Save\u2019 then \u2018Close\u2019 Navigate to the Guardian screen and select \u201820ms\u2019 from the \u2018Refresh interval\u2019 dropdown box. For triggered betting automations: Click on the \u2018Automation\u2019 tab You can create an automation file from scratch and apply it to your selected markets or import other people's Bet Angel automation files. To get started: Highlight one of the markets in your list and select \u2018Create a new rules file for the selected market\u2019. The Automation rules editor window will appear where you can start defining all the settings needed for your automation. Rule settings are divided into 5 different tabs: \u2018General\u2019 \u2013 where the type of action is defined and when that action will be triggered in relation to the market \u2018Parameters\u2019 \u2013 Here you\u2019ll specify the finer details of your bets such as the odds, stake and global settings \u2018Conditions\u2019 \u2013 create the conditions that have to be met before the rule triggers such as the markets weight of money or volume \u2018Signals\u2019 \u2013 for more complex automations, you may choose to set your rule to give off a certain signal when it triggers which in turn will cause other rules to trigger when they\u2019re set to listen for that signal \u2018Stored Values\u2019 \u2013 which allows you to record things like a Back or Lay bet, last traded price or volume which then can be used as conditions for other rules. There\u2019s a lot of options here, so we recommend taking a look at online resources such as the Bet Angel user guide and Youtube tutorials to get a more in depth understanding of what each option does and how it may apply to your specific strategy. Once you have finished creating your automation, remember to click the save icon and give it a name. Now you can close the Automation Rules editor window and select your automation from the drop down box called \u2018Rules file name\u2019. Once your automation rule file is selected, in the \u2018Rules File Usage\u2019 you can choose from: \u2018Apply rules to all markets\u2019, \u2018Apply rules to selected markets \u2018 \u2018Remove rules from selected markets\u2019. When your automation file is linked to a market it will appear in the \u2018Automation Rules\u2019 column and will begin automating your strategy for you. Custom columns - Advanced Please see our Intermediate guide on custom columns here before starting this advanced tutorial. Custom columns can be very useful to tailor your betting experience when placing manual bets. They can also be used to trigger betting automations. To link an automation to a custom column, click the star icon on the \u2018One-click\u2019 betting screen which will bring up your custom column settings window. Select \u2018Start an Automation Servant\u2019 from the \u2018Action\u2019 drop down box (highlighted below) Then from the \u2018Rules File\u2019 drop down box select the name of your automation file. Add steps here (same as intermediate tute) to add the column After you\u2019ve saved the settings for your custom column, whenever the custom column is clicked on, it will trigger your automation. Applications for this can be varied such as backing a specific runner while laying the rest of the field and much more. Excel Automation Using an excel spreadsheet is a great tool to implement complex strategies through Bet Angel. The main way you will go about setting up an automation is to control when a back, lay or take SP command is printed into specific cells which Bet Angel continuously checks. If Bet Angel detects that a certain cell contains \u201cBACK\u201d, it will place a back bet. The same applies with lay bets as well. Formulas and macros can be used to control when these messages are sent to Bet Angel which can be tied into various market factors or conditions. To begin setting up an Excel automation: click on the icon to open Guardian and then click the Excel tab. From here, you can choose a specific Excel template that you have set up on your computer or import an automation that has been downloaded online using: \u2018Browse for file\u2019 button and then clicking \u2018Open workbook\u2019. By default, Bet Angel won't automatically start using the Excel sheet to start placing bets until you check the boxes next to: \u2018Connect\u2019, \u2018Auto-bind Bet Angel sheets\u2019 and \u2018Auto-clear bindings\u2019 options. Then select your file name from the \u2018Excel Sheet\u2019 column next to the markets you want to connect Once this is done, Bet Angel will start populating your spreadsheet with market data and start listening for the betting commands. If you try to edit the Excel sheet while it\u2019s connected with Bet Angel it will most likely cause an error. If you want to make changes to your Excel workbook, simply untick the \u2018Connect\u2019 option in Guardian, make your changes and re select connect. As previously mentioned, Bet Angel will populate data into specific cells and will listen for betting commands from other specific cells. You will need to be mindful of this If you wish to start adding functionality to your excel workbook to make sure that you\u2019re not placing it in a cell/s that Bet Angel will use. Bet Angel will simply override whatever you have entered with its own data. For more tutorials on using the Bet Angel Excel function, take a look through our Automation Hub where we have created a number of tutorials for different strategies. Resources Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel User Guide Triggered Betting Custom Columns Excel Automation Youtube Triggered Betting Custom Column (Advanced) Automation Hub Ratings Automation tutorial Market favourite automation Tipping automation tutorial Simultaneous markets tutorial Kelly criterion staking tutorial Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Advanced guide"},{"location":"autoTools/betAngeladvanced/#bet-angel-an-advanced-guide","text":"","title":"Bet Angel: An Advanced guide"},{"location":"autoTools/betAngeladvanced/#automation","text":"Bet Angel offers two forms of automation through their Guardian interface: Triggered betting which operates by linking pre-defined commands together to form an automated strategy Use Excel to monitor multiple markets and execute bets or trades based upon triggers set within the Excel spreadsheet within one or more markets. The triggered betting feature is a good way to implement simple automations quickly while using Excel gives you a greater level of control and sophistication as you have the ability to use formulas and even create your own macros using Visual Basic for Applications (VBA).","title":"Automation"},{"location":"autoTools/betAngeladvanced/#triggered-betting","text":"To begin forming your automation in the triggered betting feature within Bet Angel, you will need to open the Guardian feature by clicking on the icon on the main Bet Angel toolbar. The Guardian window will open where you have a number of tabs to select from. First, we want to have a list of markets that we want to automate. From the \u2018Market Selection\u2019 tab select each market (use Ctrl to select multiple) you want Click the \u2018Add Bet Angel markets\u2019 button. Keep in mind that the order that the markets are in the list, will be the order that your automation will go through. Tip: Click on the \u2018Start time\u2019 column to order your list based on race start time. Guardian will work through your list in order. Within the \u2018Markets\u2019 tab you will see the \u2018Refresh interval\u2019 dropdown box. This instructs Bet Angel how often to refresh the markets in the list. We recommend setting this value to be the lowest possible by: Navigate back to the Main Bet Angel home screen Click the \u2018Settings\u2019 tab Select \u2018Edit Settings\u2019 Click the \u2018Communications\u2019 tab Check \u2018Use Exchange Streaming\u2019 Click \u2018Save\u2019 then \u2018Close\u2019 Navigate to the Guardian screen and select \u201820ms\u2019 from the \u2018Refresh interval\u2019 dropdown box. For triggered betting automations: Click on the \u2018Automation\u2019 tab You can create an automation file from scratch and apply it to your selected markets or import other people's Bet Angel automation files. To get started: Highlight one of the markets in your list and select \u2018Create a new rules file for the selected market\u2019. The Automation rules editor window will appear where you can start defining all the settings needed for your automation. Rule settings are divided into 5 different tabs: \u2018General\u2019 \u2013 where the type of action is defined and when that action will be triggered in relation to the market \u2018Parameters\u2019 \u2013 Here you\u2019ll specify the finer details of your bets such as the odds, stake and global settings \u2018Conditions\u2019 \u2013 create the conditions that have to be met before the rule triggers such as the markets weight of money or volume \u2018Signals\u2019 \u2013 for more complex automations, you may choose to set your rule to give off a certain signal when it triggers which in turn will cause other rules to trigger when they\u2019re set to listen for that signal \u2018Stored Values\u2019 \u2013 which allows you to record things like a Back or Lay bet, last traded price or volume which then can be used as conditions for other rules. There\u2019s a lot of options here, so we recommend taking a look at online resources such as the Bet Angel user guide and Youtube tutorials to get a more in depth understanding of what each option does and how it may apply to your specific strategy. Once you have finished creating your automation, remember to click the save icon and give it a name. Now you can close the Automation Rules editor window and select your automation from the drop down box called \u2018Rules file name\u2019. Once your automation rule file is selected, in the \u2018Rules File Usage\u2019 you can choose from: \u2018Apply rules to all markets\u2019, \u2018Apply rules to selected markets \u2018 \u2018Remove rules from selected markets\u2019. When your automation file is linked to a market it will appear in the \u2018Automation Rules\u2019 column and will begin automating your strategy for you.","title":"Triggered Betting"},{"location":"autoTools/betAngeladvanced/#custom-columns-advanced","text":"Please see our Intermediate guide on custom columns here before starting this advanced tutorial. Custom columns can be very useful to tailor your betting experience when placing manual bets. They can also be used to trigger betting automations. To link an automation to a custom column, click the star icon on the \u2018One-click\u2019 betting screen which will bring up your custom column settings window. Select \u2018Start an Automation Servant\u2019 from the \u2018Action\u2019 drop down box (highlighted below) Then from the \u2018Rules File\u2019 drop down box select the name of your automation file. Add steps here (same as intermediate tute) to add the column After you\u2019ve saved the settings for your custom column, whenever the custom column is clicked on, it will trigger your automation. Applications for this can be varied such as backing a specific runner while laying the rest of the field and much more.","title":"Custom columns - Advanced"},{"location":"autoTools/betAngeladvanced/#excel-automation","text":"Using an excel spreadsheet is a great tool to implement complex strategies through Bet Angel. The main way you will go about setting up an automation is to control when a back, lay or take SP command is printed into specific cells which Bet Angel continuously checks. If Bet Angel detects that a certain cell contains \u201cBACK\u201d, it will place a back bet. The same applies with lay bets as well. Formulas and macros can be used to control when these messages are sent to Bet Angel which can be tied into various market factors or conditions. To begin setting up an Excel automation: click on the icon to open Guardian and then click the Excel tab. From here, you can choose a specific Excel template that you have set up on your computer or import an automation that has been downloaded online using: \u2018Browse for file\u2019 button and then clicking \u2018Open workbook\u2019. By default, Bet Angel won't automatically start using the Excel sheet to start placing bets until you check the boxes next to: \u2018Connect\u2019, \u2018Auto-bind Bet Angel sheets\u2019 and \u2018Auto-clear bindings\u2019 options. Then select your file name from the \u2018Excel Sheet\u2019 column next to the markets you want to connect Once this is done, Bet Angel will start populating your spreadsheet with market data and start listening for the betting commands. If you try to edit the Excel sheet while it\u2019s connected with Bet Angel it will most likely cause an error. If you want to make changes to your Excel workbook, simply untick the \u2018Connect\u2019 option in Guardian, make your changes and re select connect. As previously mentioned, Bet Angel will populate data into specific cells and will listen for betting commands from other specific cells. You will need to be mindful of this If you wish to start adding functionality to your excel workbook to make sure that you\u2019re not placing it in a cell/s that Bet Angel will use. Bet Angel will simply override whatever you have entered with its own data. For more tutorials on using the Bet Angel Excel function, take a look through our Automation Hub where we have created a number of tutorials for different strategies.","title":"Excel Automation"},{"location":"autoTools/betAngeladvanced/#resources","text":"Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel User Guide Triggered Betting Custom Columns Excel Automation Youtube Triggered Betting Custom Column (Advanced) Automation Hub Ratings Automation tutorial Market favourite automation Tipping automation tutorial Simultaneous markets tutorial Kelly criterion staking tutorial","title":"Resources"},{"location":"autoTools/betAngeladvanced/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/betAngelbeginners/","text":"Bet Angel: A beginner\u2019s guide Installing Bet Angel Bet Angel offers three products for use with Betfair. We\u2019ll be using Bet Angel Professional in these tutorials as it is the Ultimate Trading Toolkit containing all the advanced tools and features every trader needs. To start using and learning Bet Angel Professional, first you need to download and install the program from the Bet Angel website. By downloading it from the Bet Angel website, you can ensure that you\u2019re installing the most up to date version of the software which can be obtained from here. Make sure that you save the downloaded .exe file somewhere that you can easily access. Once it has finished downloading, double click the file (which will be called BAP_1_54_1.exe or something similar) to begin the setup process. For more details on downloading and installing Bet Angel see their handy user guide here. Follow the prompts throughout the installation and once completed, open Bet Angel Professional. Registering Bet Angel Step 1 After opening Bet Angel Pro you will see a login dialogue box. Click on the \u2018Register using a serial number\u2026\u2019 button (highlighted below). Step 2 \u2013 Free Trial. (Skip to Step 4 if you already have a Bet Angel license). Bet Angel offer a free 14-day trial for first time users. Select the option \u201cI wish to register for a FREE trial\u2026\u201d as highlighted below. Then enter your \u201cBetfair User Name\u201d and \u201cPassword\u201d details, and click \u2018Register Account\u2019. Step 3 A new window will appear where you will be prompted to provide your first and last name along with your email address. From here you\u2019ll need to verify your email by clicking a link that will be sent to you in an automated email by Bet Angel. If you don\u2019t see an automatic email from Bet Angel, make sure you check your junk / spam folder. Step 4 \u2013 Purchased serial number (Skip to Step 5 if you have registered for a free trial) Select \u201cI have a serial number......\u201d. You will now see a registration box in which you must enter the serial number sent to you along with the Betfair account username and password (for account verification). Your account password will NOT be stored or sent to us. Once you have entered your details click on the \u201cRegister Account\u201d button. If you have not already done so a window will appear asking you to first associate a\u202fverified e-mail\u202faddress with your Betfair Client ID. This is a one-time process and will make activating future subscriptions much quicker and easier. Otherwise immediate access will be granted, and a confirmation message will appear on your screen. Step 5 Once you have submitted your register account request, log into Bet Angel by entering your \u201cBetfair User Name\u201d and \u201cPassword\u201d in the boxes highlighted below. You can select either \u201cLive mode\u201d or \u201cPractice mode\u201d at the log in screen. We recommend starting in Practice mode as this will allow you to experiment with Bet Angel without placing any bets to the Betfair exchange which is great while learning the ropes. Note that it can take up to 15 minutes before you have full access to Bet Angel, so be patient while the system sets you up in the background. Ensure that you tick the \u201cI have read and fully understand the risk notice\u201d and the \u201cI accept the Betfair API Terms and conditions\u201d boxes and then click the \u201cLog in\u201d button. Basic Setup Once we\u2019re logged into Bet Angel, the first thing that we\u2019re going to want to do is to configure Bet Angel so that it behaves in a way that is best suited for what we\u2019re going to use it for. Each user will have their own configuration preferences and we strongly recommend getting acquainted with the various options available to take full advantage of your Bet Angel experience. To understand all of the options available in Bet Angel, we recommend taking a look at the Bet Angel user guide. To get started, simply click the \u2018Settings\u2019 tab from the top toolbar and then \u2018Edit settings\u2019 from the drop-down box. The Settings window will then appear and is organized into 10 sections; Display: to alter the colour preferences for market views, font size and weight of money Staking: where you can set default stakes and liabilities which will be applied to every market Behavior: Enable / disable confirm bet warnings as well as other program specific warnings Green Up: Specific options that affect unmatched bets when closing / greening a trade Communications: Control the data that Bet Angel is pulling from the Betfair exchange API Ladder: Change the colour scheme, columns, charts and more for the ladder functionality Automation: Modify some basic automation controls for bookmaking, ditching, Back and Lay Charts: Customise how charts are displayed in Bet Angel such as colour Sound Alerts: change what sounds will be heard from Bet Angel such as market jump warnings Excel: Control additional information which is populated in a connected excel sheet such as saddle cloth numbers, stall number projected / actual SP and more. The most common settings to edit straight away are: - Turning off \u201cConfirm Bets\u201d - As it states, unticking this option will mean you don\u2019t need to confirm bets. This is one of the advantages of using a 3rd party tool \u2013 please be aware that this will make betting faster, so gamble responsibly . To use the ladder functionality within Bet Angel, go to the Ladder tab and then click \u2018Show the Ladder Settings editor\u2019 which will bring up another window with a lot more options to play with. A lot of users like to enable the \u201cShow last traded volume\u201d option by Clicking on the \u201cGeneral\u201d tab and then check \u201cShow last traded volume\u201d then Clicking on the \u201cColumns\u201d tab and under \u2018Last traded price chart column\u2019 (near top of window) check the \u2018Show chart column\u2019 box. Whatever settings you decide to use, you can save them as a settings profile and use different profiles for different types of betting. Just click \u2018save\u2019 from the Settings window and give your settings profile a name. You can easily switch between different settings profiles that you\u2019ve created by clicking the \u2018Settings\u2019 tab and then choosing the profile name from the dropdown box next to \u2018Load settings\u2019 To help you through getting setup in Bet Angel, take a look at the Bet Angel: Recommended Settings From Betfair on the Betfair Hub created specifically to help you get up and running. Market Selection To open a market so we can start placing bets, first we need to select the market that we want to look at. Click the \u2018File\u2019 tab from the main navigation bar and click \u2018Select Market\u2019. This will open the \u201cMarket Selection\u201d window where you can browse all the different markets available on the Betfair exchange. Click the arrows to the left of the menu items to expand and see the specific markets available. Once you\u2019ve found a market that you\u2019re interested in, click on it once then click the \u2018switch to market\u2019 button. Another handy feature is to create a quick pick list for specific markets that you\u2019re interested in. For example, if you\u2019re only interested in win horse racing markets within Australia, you can click the \u201cSettings\u201d tab in the \u201cMarket Selection\u201d window and modify the options to be applicable to what you\u2019re interested in. For example here we have selected Australian Horse Race Win markets by: Checking the \u201cAUS\u201d box under \u201cHorse Races\u201d Checking the \u201cWin Markets\u201d under \u201cHorse Races\u201d Checking the box next to \u201cDisplay event start time first\u201d Then only those markets will appear in the \u2018Quick Picks\u2019 tab. Once you have clicked the \u2018Switch to market\u2019 button for your chosen market, you can easily cycle through to the next scheduled market by pressing Ctrl+N (or Ctrl+P for the previous market) without the need to return back to the market selection window. Default stakes and lay to a maximum liability If you\u2019re a person who wants to place bets quickly in order to secure the best price, then you\u2019ll also want to be able to set a default stake for your bets. In a market, you\u2019ll see two columns named \u2018Back stake\u2019 and \u2018Lay stake\u2019. You can easily change the stake value for each runner to whatever you like by clicking on the number and entering your own value manually. You can also set a default stake that will apply to all runners within a market. Simply change the value at the top of the column (highlighted in yellow) and ensure the \u2018All\u2019 check box is selected. This can also be done for the lay column in the exact same way. But what if we wanted to set a maximum liability for our lay bets as opposed to a straight stake? This is where the staking method option comes into play. Simply change the \u201cStaking Method\u201d drop down box (top of screen) to \u2018By Liability (Lay only)\u2019 and any values which are entered in the lay stake column becomes your maximum liability. Your lay stake is calculated behind the scenes to ensure that you don\u2019t accidentally empty your Betfair account should your lay bet lose. One click betting Previously, we went over the available settings in Bet Angel to switch off an option called \u2018Confirm bets?\u2019 under the \u2018Behavior\u2019 tab of the settings window. This option is usually enabled by default and will cause a pop-up window to appear asking for you to confirm whether you want a bet to be placed on the exchange. By switching this setting off, you can place bets onto the exchange using a single click meaning that you can react much faster to changes in a market. This will also affect other views within Bet Angel such as the ladder feature within Bet Angel. Refresh Settings Bet Angel gives you the ability to control how often data is refreshed from the Betfair exchange through a number of options in settings. We recommend making sure that the refresh rate for Bet Angel is set to as low as possible to ensure that you are seeing the most up to date data at any one time. Click the \u2018Settings\u2019 tab and then Select \u2018Edit Settings\u2019 and then Click the \u2018Communications\u2019 tab. Check \u2018Use Exchange Streaming\u2019 Click \u2018Save\u2019 then \u2018Close\u2019. On the Bet Angel menu bar, change the \u2018Refresh every\u2019 option to 20ms (note 20ms will only be available if you have checked \u2018Use Exchange Streaming\u2019 in the previous step). Fill or Kill Fill or kill is a useful tool to instruct Bet Angel to cancel (kill) a bet if it\u2019s not matched within a period of time you have specified. To set up fill or kill, simply follow the following steps: Ensure that the \u2018Use Global Settings when placing a bet\u2019 is enabled (see highlighted button below) Tick \u2018Place fill or Kill bets\u2019 in the header menu In the \u2018seconds delay\u2019 box specify the number of seconds you would like Bet Angel to wait until it cancels the bet should it not be matched. Bet Angel recommends to ensure that a minimum value of 0.5 (half a second) is always applied to the fill or kill. Resources Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel User Guide Installation Settings Market Selection One Click betting Refresh settings Fill or Kill Youtube Using the settings feature One click trading screen Getting Bet Angel to update ten times faster - Refresh settings Betfair Hub Bet Angel overview Recommended settings from Betfair What next? Now that you've got the beginner level sorted, take the next step and have a look at our Intermediate guide to Bet Angel . Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Beginner's guide"},{"location":"autoTools/betAngelbeginners/#bet-angel-a-beginners-guide","text":"","title":"Bet Angel: A beginner\u2019s guide"},{"location":"autoTools/betAngelbeginners/#installing-bet-angel","text":"Bet Angel offers three products for use with Betfair. We\u2019ll be using Bet Angel Professional in these tutorials as it is the Ultimate Trading Toolkit containing all the advanced tools and features every trader needs. To start using and learning Bet Angel Professional, first you need to download and install the program from the Bet Angel website. By downloading it from the Bet Angel website, you can ensure that you\u2019re installing the most up to date version of the software which can be obtained from here. Make sure that you save the downloaded .exe file somewhere that you can easily access. Once it has finished downloading, double click the file (which will be called BAP_1_54_1.exe or something similar) to begin the setup process. For more details on downloading and installing Bet Angel see their handy user guide here. Follow the prompts throughout the installation and once completed, open Bet Angel Professional.","title":"Installing Bet Angel"},{"location":"autoTools/betAngelbeginners/#registering-bet-angel","text":"Step 1 After opening Bet Angel Pro you will see a login dialogue box. Click on the \u2018Register using a serial number\u2026\u2019 button (highlighted below). Step 2 \u2013 Free Trial. (Skip to Step 4 if you already have a Bet Angel license). Bet Angel offer a free 14-day trial for first time users. Select the option \u201cI wish to register for a FREE trial\u2026\u201d as highlighted below. Then enter your \u201cBetfair User Name\u201d and \u201cPassword\u201d details, and click \u2018Register Account\u2019. Step 3 A new window will appear where you will be prompted to provide your first and last name along with your email address. From here you\u2019ll need to verify your email by clicking a link that will be sent to you in an automated email by Bet Angel. If you don\u2019t see an automatic email from Bet Angel, make sure you check your junk / spam folder. Step 4 \u2013 Purchased serial number (Skip to Step 5 if you have registered for a free trial) Select \u201cI have a serial number......\u201d. You will now see a registration box in which you must enter the serial number sent to you along with the Betfair account username and password (for account verification). Your account password will NOT be stored or sent to us. Once you have entered your details click on the \u201cRegister Account\u201d button. If you have not already done so a window will appear asking you to first associate a\u202fverified e-mail\u202faddress with your Betfair Client ID. This is a one-time process and will make activating future subscriptions much quicker and easier. Otherwise immediate access will be granted, and a confirmation message will appear on your screen. Step 5 Once you have submitted your register account request, log into Bet Angel by entering your \u201cBetfair User Name\u201d and \u201cPassword\u201d in the boxes highlighted below. You can select either \u201cLive mode\u201d or \u201cPractice mode\u201d at the log in screen. We recommend starting in Practice mode as this will allow you to experiment with Bet Angel without placing any bets to the Betfair exchange which is great while learning the ropes. Note that it can take up to 15 minutes before you have full access to Bet Angel, so be patient while the system sets you up in the background. Ensure that you tick the \u201cI have read and fully understand the risk notice\u201d and the \u201cI accept the Betfair API Terms and conditions\u201d boxes and then click the \u201cLog in\u201d button.","title":"Registering Bet Angel"},{"location":"autoTools/betAngelbeginners/#basic-setup","text":"Once we\u2019re logged into Bet Angel, the first thing that we\u2019re going to want to do is to configure Bet Angel so that it behaves in a way that is best suited for what we\u2019re going to use it for. Each user will have their own configuration preferences and we strongly recommend getting acquainted with the various options available to take full advantage of your Bet Angel experience. To understand all of the options available in Bet Angel, we recommend taking a look at the Bet Angel user guide. To get started, simply click the \u2018Settings\u2019 tab from the top toolbar and then \u2018Edit settings\u2019 from the drop-down box. The Settings window will then appear and is organized into 10 sections; Display: to alter the colour preferences for market views, font size and weight of money Staking: where you can set default stakes and liabilities which will be applied to every market Behavior: Enable / disable confirm bet warnings as well as other program specific warnings Green Up: Specific options that affect unmatched bets when closing / greening a trade Communications: Control the data that Bet Angel is pulling from the Betfair exchange API Ladder: Change the colour scheme, columns, charts and more for the ladder functionality Automation: Modify some basic automation controls for bookmaking, ditching, Back and Lay Charts: Customise how charts are displayed in Bet Angel such as colour Sound Alerts: change what sounds will be heard from Bet Angel such as market jump warnings Excel: Control additional information which is populated in a connected excel sheet such as saddle cloth numbers, stall number projected / actual SP and more. The most common settings to edit straight away are: - Turning off \u201cConfirm Bets\u201d - As it states, unticking this option will mean you don\u2019t need to confirm bets. This is one of the advantages of using a 3rd party tool \u2013 please be aware that this will make betting faster, so gamble responsibly . To use the ladder functionality within Bet Angel, go to the Ladder tab and then click \u2018Show the Ladder Settings editor\u2019 which will bring up another window with a lot more options to play with. A lot of users like to enable the \u201cShow last traded volume\u201d option by Clicking on the \u201cGeneral\u201d tab and then check \u201cShow last traded volume\u201d then Clicking on the \u201cColumns\u201d tab and under \u2018Last traded price chart column\u2019 (near top of window) check the \u2018Show chart column\u2019 box. Whatever settings you decide to use, you can save them as a settings profile and use different profiles for different types of betting. Just click \u2018save\u2019 from the Settings window and give your settings profile a name. You can easily switch between different settings profiles that you\u2019ve created by clicking the \u2018Settings\u2019 tab and then choosing the profile name from the dropdown box next to \u2018Load settings\u2019 To help you through getting setup in Bet Angel, take a look at the Bet Angel: Recommended Settings From Betfair on the Betfair Hub created specifically to help you get up and running.","title":"Basic Setup"},{"location":"autoTools/betAngelbeginners/#market-selection","text":"To open a market so we can start placing bets, first we need to select the market that we want to look at. Click the \u2018File\u2019 tab from the main navigation bar and click \u2018Select Market\u2019. This will open the \u201cMarket Selection\u201d window where you can browse all the different markets available on the Betfair exchange. Click the arrows to the left of the menu items to expand and see the specific markets available. Once you\u2019ve found a market that you\u2019re interested in, click on it once then click the \u2018switch to market\u2019 button. Another handy feature is to create a quick pick list for specific markets that you\u2019re interested in. For example, if you\u2019re only interested in win horse racing markets within Australia, you can click the \u201cSettings\u201d tab in the \u201cMarket Selection\u201d window and modify the options to be applicable to what you\u2019re interested in. For example here we have selected Australian Horse Race Win markets by: Checking the \u201cAUS\u201d box under \u201cHorse Races\u201d Checking the \u201cWin Markets\u201d under \u201cHorse Races\u201d Checking the box next to \u201cDisplay event start time first\u201d Then only those markets will appear in the \u2018Quick Picks\u2019 tab. Once you have clicked the \u2018Switch to market\u2019 button for your chosen market, you can easily cycle through to the next scheduled market by pressing Ctrl+N (or Ctrl+P for the previous market) without the need to return back to the market selection window.","title":"Market Selection"},{"location":"autoTools/betAngelbeginners/#default-stakes-and-lay-to-a-maximum-liability","text":"If you\u2019re a person who wants to place bets quickly in order to secure the best price, then you\u2019ll also want to be able to set a default stake for your bets. In a market, you\u2019ll see two columns named \u2018Back stake\u2019 and \u2018Lay stake\u2019. You can easily change the stake value for each runner to whatever you like by clicking on the number and entering your own value manually. You can also set a default stake that will apply to all runners within a market. Simply change the value at the top of the column (highlighted in yellow) and ensure the \u2018All\u2019 check box is selected. This can also be done for the lay column in the exact same way. But what if we wanted to set a maximum liability for our lay bets as opposed to a straight stake? This is where the staking method option comes into play. Simply change the \u201cStaking Method\u201d drop down box (top of screen) to \u2018By Liability (Lay only)\u2019 and any values which are entered in the lay stake column becomes your maximum liability. Your lay stake is calculated behind the scenes to ensure that you don\u2019t accidentally empty your Betfair account should your lay bet lose.","title":"Default stakes and lay to a maximum liability"},{"location":"autoTools/betAngelbeginners/#one-click-betting","text":"Previously, we went over the available settings in Bet Angel to switch off an option called \u2018Confirm bets?\u2019 under the \u2018Behavior\u2019 tab of the settings window. This option is usually enabled by default and will cause a pop-up window to appear asking for you to confirm whether you want a bet to be placed on the exchange. By switching this setting off, you can place bets onto the exchange using a single click meaning that you can react much faster to changes in a market. This will also affect other views within Bet Angel such as the ladder feature within Bet Angel.","title":"One click betting"},{"location":"autoTools/betAngelbeginners/#refresh-settings","text":"Bet Angel gives you the ability to control how often data is refreshed from the Betfair exchange through a number of options in settings. We recommend making sure that the refresh rate for Bet Angel is set to as low as possible to ensure that you are seeing the most up to date data at any one time. Click the \u2018Settings\u2019 tab and then Select \u2018Edit Settings\u2019 and then Click the \u2018Communications\u2019 tab. Check \u2018Use Exchange Streaming\u2019 Click \u2018Save\u2019 then \u2018Close\u2019. On the Bet Angel menu bar, change the \u2018Refresh every\u2019 option to 20ms (note 20ms will only be available if you have checked \u2018Use Exchange Streaming\u2019 in the previous step).","title":"Refresh Settings"},{"location":"autoTools/betAngelbeginners/#fill-or-kill","text":"Fill or kill is a useful tool to instruct Bet Angel to cancel (kill) a bet if it\u2019s not matched within a period of time you have specified. To set up fill or kill, simply follow the following steps: Ensure that the \u2018Use Global Settings when placing a bet\u2019 is enabled (see highlighted button below) Tick \u2018Place fill or Kill bets\u2019 in the header menu In the \u2018seconds delay\u2019 box specify the number of seconds you would like Bet Angel to wait until it cancels the bet should it not be matched. Bet Angel recommends to ensure that a minimum value of 0.5 (half a second) is always applied to the fill or kill.","title":"Fill or Kill"},{"location":"autoTools/betAngelbeginners/#resources","text":"Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel User Guide Installation Settings Market Selection One Click betting Refresh settings Fill or Kill Youtube Using the settings feature One click trading screen Getting Bet Angel to update ten times faster - Refresh settings Betfair Hub Bet Angel overview Recommended settings from Betfair","title":"Resources"},{"location":"autoTools/betAngelbeginners/#what-next","text":"Now that you've got the beginner level sorted, take the next step and have a look at our Intermediate guide to Bet Angel .","title":"What next?"},{"location":"autoTools/betAngelbeginners/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/betAngelintermediate/","text":"Bet Angel: An Intermediate guide The Ladder interface The Bet Angel Ladder interface allows you to gain a deeper view of how bets are being placed on a small number of runners as opposed to the high-level overview of a market that we get from the \u2018One-Click\u2019 view. This also allows you to get an idea of any potential trends and help you make a more informed decision about your own bets. By default, the ladder interface will show you the first three runners in a market. A handy button to keep in mind is the \u2018123\u2019 button which will automatically adjust which runners you see and order them in ascending price order with the favourite placed in \u2018Ladder 1\u2019 position. You can also easily specify the runners you would like to look at by simply selecting them from the drop-down box above each ladder. Just like the \u2018One-click screen\u2019, you can click within the blue (back) or pink (lay) columns to place a bet at those specific odds, allowing you to not only get a more detailed view of what the market is doing but quickly place bets of your own. In-Play trader view The in-play trader view is an alternative way to look and place bets on markets which are in-play. It gives an overview of the highest and lowest price points traded for runners and allows you to easily place bets onto the market. To open the in-play trader view: click the in-play trader icon near the top right of the main Bet Angel screen. Once you\u2019ve clicked the button, a new window will open up showing every runner in the race. As you can see, there are a number of dots and lines which give us specific information relating to where bets are being placed for each runner. The blue dot indicates the last back price, the pink shows the last lay price and the yellow indicates the last traded price. The grey bar gives us the highest and lowest traded range of that particular runner. You\u2019ll also notice that there are a number of options that you can use to customize your experience with the tool depending on what information is important to you. At the top left of the window from left to right, you can hide the blue, pink and yellow dots to simplify your view change the window to reflect the win percentage scale or tick scale sort your runner list depending of a range of factors change the height of your runner rows adjust the labels for the dots to reflect different information You can also use the in-play trader view to place bets just like in the one-click screen or ladder view. You can customise what betting action to take when the left mouse button is clicked (place a back bet, a lay bet or even set that the left mouse button will back while the right button lays). By default, the odds that bets are placed at and the runner you bet on in this view will be determined by the location of your mouse cursor. It pays to be mindful of this when using this view, especially if you have confirm bets turned off as an accidental click on the in-play trader view may cause you to place a bet that you didn\u2019t want. This being said, the price that bets are placed at can be easily changed to a different factor such as the best available back price instead of the mouse cursor location. Custom Columns Custom columns are an extremely helpful way to speed up how you bet in Bet Angel markets. They allow you customize not only the information that you see in the one-click betting screen, but also lets you place specific bets faster into the market. This is particularly useful when placing bets in-play and speed is a considerable factor for your strategy. To get started with your own custom columns click the yellow star icon which will bring up the custom column's editor. Once the editor window pops up: Click \u2018New\u2019 Then enter a Profile Name for your custom column and click \u2018OK\u2019 Choose from the configure options (see below) Then click \u2018Save Column\u2019 You will have a range of options to configure your columns: Custom Column Profiles \u2013 Create multiple profiles for different betting strategies that you may implement. Each profile can have different columns that do different things and these profiles allow you to easily switch between different modes depending on what you\u2019re wanting to do in Bet Angel. General: Title \u2013 Give your column a name to make it easily identifiable (this will be the name of the column in the column chooser list and title in the column header on the Bet Angel screen) ToolTip \u2013 a custom info box that appears when the mouse is hovered over your custom column Action \u2013 Specify what happens when you click on the column, whether it places a back or lay bet, cancels a bet or triggers an automation to run Button colour \u2013 customise the colour of your column to make it easily distinguishable Display \u2013 Choose whether each box within your custom column is populated with the runners odds or text Odds \u2013 If you decide to have odds populated in your custom column, you can choose from: fixed odds, ticks offset from the best back price ticks offset from the best lay price a percentage offset from best lay price Stake \u2013 This allows you to program a pre-determined stake that can be triggered when you click in the column, essentially giving you your own bet placement shortcut buttons. Once you have set up your custom column the way that you would like it to operate, the next step is to make it visible on your screen so you can use it. Select your custom column name from the drop down box next to the yellow star icon Click on icon Scroll through the list and choose your custom column Your custom column should now be added to the right of the existing columns (see image below) If you haven\u2019t created a profile for your custom columns, click the settings button (see below), then choose columns where pop up window will appear. Find your custom column from the list and tick the box next to it. Offset Bets Offset bets are extremely easy to achieve in Bet Angel thanks to a simple to use tool at the top of the One-click screen. Here you can choose to use a number of different offset bet actions and specify the number of ticks or percentage that you want your offset bet to be placed at. In the above example, if I placed a back bet on the runner named \u201c1. Andrew Swagger\u201d, the back bet will be placed at $55 and then Bet Angel will automatically place a lay bet on the same runner 2 ticks lower. Dutching The dutching screen within Bet Angel is often overlooked by Bet Angel users but is extremely handy, especially if your preference is lower risk (and lower profit/loss). The dutching screen will only place back bets into a market (while the bookmaking screen will only place lay bets-see next section). Bet Angel makes dutching a lot easier by allowing you to select a number of runners within a market that you think that the runner may be amongst and place bets with a specific stake, achieve a target profit or set a minimum stake for Bet Angel to use. In the above example, we\u2019ve selected 5 runners, out of a market of 10, who we think could possibly win the race. Our aim in this scenario is to come away with a profit of $20 if one of our selected runners win. Bet Angel will do all the calculations for you to work out the odds and stake for each runner, but you do have the ability to override a specific runners target profit \u2013 especially handy if you're not completely confident that a particular runner will win or not but you hold more or less confidence than the rest of your selections. For example, for 8. Nobodys Puppet, which the best available back price is 10.5, we may think that there is still a possibility for it to win, so we can override the target profit for that runner to be $0 which will mean that if it wins, we won't make any profit but we also won't lose our stake. You\u2019ll notice that there are 3 check box columns: Back, Lay and Manual. These refer to the odds that you\u2019ll be using. If the back-check box is selected, then you\u2019ll be using the best available back odds and the same concept applies if you have the lay check box selected. Manual will allow you to set your own odds for bets to be placed onto the market. Bet Angel will also display the total stake for all of the bets to be placed based on your settings as well as the potential profit (indicated in green) should one of your selections win and the potential loss should they lose (indicated in red). For a more in depth run-through of the Dutching functionality, check out the Bet Angel YouTube video here . Bookmaking The bookmaking screen operates almost identically to the Dutching screen except that it will only place lay bets into the market. The aim of the game in this screen to create a book that ends up being over 100% so that we can secure a profit. To see an example of the bookmaking screen in action, take a look at the Bet Angel video on Youtube here . Charting One of the main benefits to using programs such as Bet Angel is the functionality that allows you to gain a more in depth insight into how a market is going, see trends and help you make smarter calls when it comes to placing your own bets. Charts within Bet Angel takes the information from the Betfair exchange and illustrates it in a way that allows you to interpret complex data in a more concise visual format. Advanced charting is a feature that is easily accessed from the One-click, Ladder and manual bet screens and can be accessed by clicking the graph icon next to a runners name. From here, a chart window will appear with various graphs and options to change the type of graph and the data being displayed. For more detail and information regarding Bet Angels advanced charts, check out their video tutorial here . Resources Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel user guide Ladder Mode In-Play trader view Custom columns Offset bets Dutching Bookmaking Youtube Ladder Mode In-Play trader view Custom Columns Dutching Bookmaking What next? Now that you've got the intermediate level sorted, take the next step and have a look at our Advanced guide to Bet Angel . Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Intermediate guide"},{"location":"autoTools/betAngelintermediate/#bet-angel-an-intermediate-guide","text":"","title":"Bet Angel: An Intermediate guide"},{"location":"autoTools/betAngelintermediate/#the-ladder-interface","text":"The Bet Angel Ladder interface allows you to gain a deeper view of how bets are being placed on a small number of runners as opposed to the high-level overview of a market that we get from the \u2018One-Click\u2019 view. This also allows you to get an idea of any potential trends and help you make a more informed decision about your own bets. By default, the ladder interface will show you the first three runners in a market. A handy button to keep in mind is the \u2018123\u2019 button which will automatically adjust which runners you see and order them in ascending price order with the favourite placed in \u2018Ladder 1\u2019 position. You can also easily specify the runners you would like to look at by simply selecting them from the drop-down box above each ladder. Just like the \u2018One-click screen\u2019, you can click within the blue (back) or pink (lay) columns to place a bet at those specific odds, allowing you to not only get a more detailed view of what the market is doing but quickly place bets of your own.","title":"The Ladder interface"},{"location":"autoTools/betAngelintermediate/#in-play-trader-view","text":"The in-play trader view is an alternative way to look and place bets on markets which are in-play. It gives an overview of the highest and lowest price points traded for runners and allows you to easily place bets onto the market. To open the in-play trader view: click the in-play trader icon near the top right of the main Bet Angel screen. Once you\u2019ve clicked the button, a new window will open up showing every runner in the race. As you can see, there are a number of dots and lines which give us specific information relating to where bets are being placed for each runner. The blue dot indicates the last back price, the pink shows the last lay price and the yellow indicates the last traded price. The grey bar gives us the highest and lowest traded range of that particular runner. You\u2019ll also notice that there are a number of options that you can use to customize your experience with the tool depending on what information is important to you. At the top left of the window from left to right, you can hide the blue, pink and yellow dots to simplify your view change the window to reflect the win percentage scale or tick scale sort your runner list depending of a range of factors change the height of your runner rows adjust the labels for the dots to reflect different information You can also use the in-play trader view to place bets just like in the one-click screen or ladder view. You can customise what betting action to take when the left mouse button is clicked (place a back bet, a lay bet or even set that the left mouse button will back while the right button lays). By default, the odds that bets are placed at and the runner you bet on in this view will be determined by the location of your mouse cursor. It pays to be mindful of this when using this view, especially if you have confirm bets turned off as an accidental click on the in-play trader view may cause you to place a bet that you didn\u2019t want. This being said, the price that bets are placed at can be easily changed to a different factor such as the best available back price instead of the mouse cursor location.","title":"In-Play trader view"},{"location":"autoTools/betAngelintermediate/#custom-columns","text":"Custom columns are an extremely helpful way to speed up how you bet in Bet Angel markets. They allow you customize not only the information that you see in the one-click betting screen, but also lets you place specific bets faster into the market. This is particularly useful when placing bets in-play and speed is a considerable factor for your strategy. To get started with your own custom columns click the yellow star icon which will bring up the custom column's editor. Once the editor window pops up: Click \u2018New\u2019 Then enter a Profile Name for your custom column and click \u2018OK\u2019 Choose from the configure options (see below) Then click \u2018Save Column\u2019 You will have a range of options to configure your columns: Custom Column Profiles \u2013 Create multiple profiles for different betting strategies that you may implement. Each profile can have different columns that do different things and these profiles allow you to easily switch between different modes depending on what you\u2019re wanting to do in Bet Angel. General: Title \u2013 Give your column a name to make it easily identifiable (this will be the name of the column in the column chooser list and title in the column header on the Bet Angel screen) ToolTip \u2013 a custom info box that appears when the mouse is hovered over your custom column Action \u2013 Specify what happens when you click on the column, whether it places a back or lay bet, cancels a bet or triggers an automation to run Button colour \u2013 customise the colour of your column to make it easily distinguishable Display \u2013 Choose whether each box within your custom column is populated with the runners odds or text Odds \u2013 If you decide to have odds populated in your custom column, you can choose from: fixed odds, ticks offset from the best back price ticks offset from the best lay price a percentage offset from best lay price Stake \u2013 This allows you to program a pre-determined stake that can be triggered when you click in the column, essentially giving you your own bet placement shortcut buttons. Once you have set up your custom column the way that you would like it to operate, the next step is to make it visible on your screen so you can use it. Select your custom column name from the drop down box next to the yellow star icon Click on icon Scroll through the list and choose your custom column Your custom column should now be added to the right of the existing columns (see image below) If you haven\u2019t created a profile for your custom columns, click the settings button (see below), then choose columns where pop up window will appear. Find your custom column from the list and tick the box next to it.","title":"Custom Columns"},{"location":"autoTools/betAngelintermediate/#offset-bets","text":"Offset bets are extremely easy to achieve in Bet Angel thanks to a simple to use tool at the top of the One-click screen. Here you can choose to use a number of different offset bet actions and specify the number of ticks or percentage that you want your offset bet to be placed at. In the above example, if I placed a back bet on the runner named \u201c1. Andrew Swagger\u201d, the back bet will be placed at $55 and then Bet Angel will automatically place a lay bet on the same runner 2 ticks lower.","title":"Offset Bets"},{"location":"autoTools/betAngelintermediate/#dutching","text":"The dutching screen within Bet Angel is often overlooked by Bet Angel users but is extremely handy, especially if your preference is lower risk (and lower profit/loss). The dutching screen will only place back bets into a market (while the bookmaking screen will only place lay bets-see next section). Bet Angel makes dutching a lot easier by allowing you to select a number of runners within a market that you think that the runner may be amongst and place bets with a specific stake, achieve a target profit or set a minimum stake for Bet Angel to use. In the above example, we\u2019ve selected 5 runners, out of a market of 10, who we think could possibly win the race. Our aim in this scenario is to come away with a profit of $20 if one of our selected runners win. Bet Angel will do all the calculations for you to work out the odds and stake for each runner, but you do have the ability to override a specific runners target profit \u2013 especially handy if you're not completely confident that a particular runner will win or not but you hold more or less confidence than the rest of your selections. For example, for 8. Nobodys Puppet, which the best available back price is 10.5, we may think that there is still a possibility for it to win, so we can override the target profit for that runner to be $0 which will mean that if it wins, we won't make any profit but we also won't lose our stake. You\u2019ll notice that there are 3 check box columns: Back, Lay and Manual. These refer to the odds that you\u2019ll be using. If the back-check box is selected, then you\u2019ll be using the best available back odds and the same concept applies if you have the lay check box selected. Manual will allow you to set your own odds for bets to be placed onto the market. Bet Angel will also display the total stake for all of the bets to be placed based on your settings as well as the potential profit (indicated in green) should one of your selections win and the potential loss should they lose (indicated in red). For a more in depth run-through of the Dutching functionality, check out the Bet Angel YouTube video here .","title":"Dutching"},{"location":"autoTools/betAngelintermediate/#bookmaking","text":"The bookmaking screen operates almost identically to the Dutching screen except that it will only place lay bets into the market. The aim of the game in this screen to create a book that ends up being over 100% so that we can secure a profit. To see an example of the bookmaking screen in action, take a look at the Bet Angel video on Youtube here .","title":"Bookmaking"},{"location":"autoTools/betAngelintermediate/#charting","text":"One of the main benefits to using programs such as Bet Angel is the functionality that allows you to gain a more in depth insight into how a market is going, see trends and help you make smarter calls when it comes to placing your own bets. Charts within Bet Angel takes the information from the Betfair exchange and illustrates it in a way that allows you to interpret complex data in a more concise visual format. Advanced charting is a feature that is easily accessed from the One-click, Ladder and manual bet screens and can be accessed by clicking the graph icon next to a runners name. From here, a chart window will appear with various graphs and options to change the type of graph and the data being displayed. For more detail and information regarding Bet Angels advanced charts, check out their video tutorial here .","title":"Charting"},{"location":"autoTools/betAngelintermediate/#resources","text":"Betfair's interview with Bet Angel's creator, Peter Webb Bet Angel user guide Ladder Mode In-Play trader view Custom columns Offset bets Dutching Bookmaking Youtube Ladder Mode In-Play trader view Custom Columns Dutching Bookmaking","title":"Resources"},{"location":"autoTools/betAngelintermediate/#what-next","text":"Now that you've got the intermediate level sorted, take the next step and have a look at our Advanced guide to Bet Angel .","title":"What next?"},{"location":"autoTools/betAngelintermediate/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/cymaticTraderRatingsAutomation/","text":"Cymatic Trader: Ratings automation Automating a thoroughbred ratings strategy using Cymatic Trader Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings tutorials for Bet Angel and Gruss, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Cymatic Trader. Cymatic Trader has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Cymatic Trader and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan We'll step through how we went about getting Cymatic Trader to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Cymatic Trader - Set up Make sure you've downloaded and installed Cymatic Trader, and signed in. Once you open the program, you will see an Excel icon which is where we will link our spreadsheet to Cymatic Trader - Downloading & formatting ratings Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Cymatic Trader template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. - Writing your rules As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using a customised Cymatic Trader template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet RunnerName refers to the entire column H in the 'RATINGS' worksheet Overrounds refers to cell BJ7 in the 'CYMATIC'worksheet, where the overrounds for the current market are calculated. UserOverround refers to cell G4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell D4 in the 'SETTINGS' worksheet UserTimeTillJump refers to cell G3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell E4 in the 'CYMATIC' worksheet. Cymatic Trader will populate a 'FALSE' flag leading up to the jump BACKLAY refers to cell G5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our Excel formula trigger: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell G5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Back market percentage (Overrounds) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell G3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell D4 of the 'SETTINGS' worksheet (named 'TimeTillJump'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is populated with the 'FALSE' flag, it's safe to place bets. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste the trigger formula into the relevant cells on each row in the 'Command' (BA) column. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column BA (BA8 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell I8 for the first runner). This goes in column BB (BB8 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are cells that are not populated with runners. A similar effect to IFERROR, if Cymatic Trader hasn't populated cell A8 with a runner name, then dont populate this cell at all. =IF(A8=0,\"\",I8) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A8=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(I8-1), stake*(J8/(J8-1))-stake)) - Selecting markets We used the Navigator menu in Cymatic Trader to navigate to the tracks we had ratings for. If you wanted to include all horse or greyhound races for a day you could use the 'autopilot' tool to do this more efficiently. Once you've chosen the races you're interested in tick the 'autopilot' button and Gruss will automatically cycle through each market for you. - Linking the spreadsheet Click the Excel icon in the main tool bar and then 'connect Excel' from the drop down menu. From here, you will be able to point Cymatic Trader in the direction of where your Excel sheet is located on your computer. Make sure 'Enable Trigger Commands' is selected and 'Clear status cells when selecting different market\" if you are automating a series of markets. And you're set! Once you've set your spreadsheet set up and you're comfortable using Cymatic Trader it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Ratings auto"},{"location":"autoTools/cymaticTraderRatingsAutomation/#cymatic-trader-ratings-automation","text":"","title":"Cymatic Trader: Ratings automation"},{"location":"autoTools/cymaticTraderRatingsAutomation/#automating-a-thoroughbred-ratings-strategy-using-cymatic-trader","text":"Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings tutorials for Bet Angel and Gruss, but here we'll be using the ratings for thoroughbreds, created by the data science team at Betfair and incorporate them into our automation in Cymatic Trader. Cymatic Trader has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part of Cymatic Trader and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a thoroughbred ratings strategy using Cymatic Trader"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-the-plan","text":"We'll step through how we went about getting Cymatic Trader to place bets using the Betfair's Data Scientists' thoroughbred ratings model . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program to run and be able to walk away. You'll also be able to use this approach to automate using your own ratings. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Rules: here's the spreadsheet We set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Cymatic Trader","title":"- The plan"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Cymatic Trader, and signed in. Once you open the program, you will see an Excel icon which is where we will link our spreadsheet to Cymatic Trader","title":"- Set up"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-downloading-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' thoroughbred ratings model for horse racing but alternatively you can follow the same process using the Betfair's Data Scientists' Greyhound Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Cymatic Trader template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.","title":"- Downloading &amp; formatting ratings"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take, and writing rules that suit. We're using a customised Cymatic Trader template Excel sheet to implement our strategy, so it can make betting decisions based on our ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-trigger-to-place-bet","text":"In short, we want to back or lay runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column I in the 'RATINGS' worksheet RunnerName refers to the entire column H in the 'RATINGS' worksheet Overrounds refers to cell BJ7 in the 'CYMATIC'worksheet, where the overrounds for the current market are calculated. UserOverround refers to cell G4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell D4 in the 'SETTINGS' worksheet UserTimeTillJump refers to cell G3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell E4 in the 'CYMATIC' worksheet. Cymatic Trader will populate a 'FALSE' flag leading up to the jump BACKLAY refers to cell G5 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners This is our Excel formula trigger: ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell G5). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Back market percentage (Overrounds) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell G3 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell D4 of the 'SETTINGS' worksheet (named 'TimeTillJump'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If this cell is populated with the 'FALSE' flag, it's safe to place bets. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste the trigger formula into the relevant cells on each row in the 'Command' (BA) column. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column BA (BA8 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))), AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))), Overrounds<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"FALSE\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY = \"BACK\", (H8 > (INDEX(Ratings,MATCH(A8,RunnerName,0))))),AND(BACKLAY = \"LAY\", (H8 < (INDEX(Ratings,MATCH(A8,RunnerName,0)))))),Overrounds<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"FALSE\"),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell I8 for the first runner). This goes in column BB (BB8 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this article. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are cells that are not populated with runners. A similar effect to IFERROR, if Cymatic Trader hasn't populated cell A8 with a runner name, then dont populate this cell at all. =IF(A8=0,\"\",I8) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A8=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(I8-1), stake*(J8/(J8-1))-stake))","title":"- Preparing the spreadsheet"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-selecting-markets","text":"We used the Navigator menu in Cymatic Trader to navigate to the tracks we had ratings for. If you wanted to include all horse or greyhound races for a day you could use the 'autopilot' tool to do this more efficiently. Once you've chosen the races you're interested in tick the 'autopilot' button and Gruss will automatically cycle through each market for you.","title":"- Selecting markets"},{"location":"autoTools/cymaticTraderRatingsAutomation/#-linking-the-spreadsheet","text":"Click the Excel icon in the main tool bar and then 'connect Excel' from the drop down menu. From here, you will be able to point Cymatic Trader in the direction of where your Excel sheet is located on your computer. Make sure 'Enable Trigger Commands' is selected and 'Clear status cells when selecting different market\" if you are automating a series of markets.","title":"- Linking the spreadsheet"},{"location":"autoTools/cymaticTraderRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Cymatic Trader it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/cymaticTraderRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts.","title":"Areas for improvement"},{"location":"autoTools/cymaticTraderRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/cymaticTraderRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/geeksToyRefreshSettings/","text":"Geeks Toy: Optimising refresh settings Refresh settings Geeks Toy can refresh up to 4 times faster than the Exchange! You do also can change some settings to make the full market depth refresh constantly or just have selections closest to the match point refresh constantly, while the rest of the market displays at a slower rate. When you load Geeks Toy: Right click and go to \u201cShow/Hide\u201d Then click on \u201cAPI Settings Manager\u201d You then have six different options, allowing you to change various refresh rates inside the software. (It is worth noting that the Exchange updates every 1000ms, so setting refresh rates to 250ms makes them refresh 4 times faster than the Exchange) Bets - the information relating to your bets for a market, both matched and unmatched Prices - the information relating to the amounts waiting to back and lay. This varies depending on the setting used on the Ladder for Price Display. If Standard or Hybrid is selected it refers to the front three back and lay prices. If Complete is selected it refers to all of the prices, this requires a lot more information to be downloaded each call which is not suitable for slower internet connections and those on limited download allowances Complete Prices in Hybrid mode - the amounts waiting to back and lay outside of the front three prices, i.e. full market depth, when Hybrid Price Display is selected on the Ladder. For people with slow internet connections and those on limited download allowances it should not be set low i.e. less than 1000ms Traded Volume - the information relating to traded volumes External Bets - how often to poll for bets made external to the application. The default is 10 seconds. This saves on bandwith and number of calls if you only use this application for betting. Account Funds - the information relating to your balance. If a change is made to a bet, i.e. a bet is submitted, altered or matched the balance will be updated automatically You can change visibility of these settings within the ladder format. Changing market ladder settings Once you are in a market ladder do the following: Right click the market header Select Visual Options Select Price Display You will see three options: Standard is where you will only see the first 3 sets of odds and they will be updated as per your API settings. The rest of the market will not be visible. Hybrid is where the entire market is visible but only the first 3 prices will be updated as per your API settings. Complete is where the entire market is updated as per the API settings. Selection profit or hedged (market) profit As Geeks Toy is predominantly used for trading markets, a great setting to use is the function to show you your Profit/Loss on a certain selection within the market, or your overall Profit/Loss on the entire market (also known as your hedged profit). Once you are in a market ladder do the following: Right click the market header Select Visual Options Select Profit/Loss You will see two options: Selection Profit is the option you will can select if you would like to see Profit/Loss on each selection in the market individually. Hedged Profit is the option you can select for the software to show you your overall profit/loss across the entire market cumulatively. Resources Betfair Geeks Toy Overview Geeks Toy Youtube Channel Caan Berry Pro Trader Youtube Channel Betfair Interview With Cann Berry Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Optimising refresh settings"},{"location":"autoTools/geeksToyRefreshSettings/#geeks-toy-optimising-refresh-settings","text":"","title":"Geeks Toy: Optimising refresh settings"},{"location":"autoTools/geeksToyRefreshSettings/#refresh-settings","text":"Geeks Toy can refresh up to 4 times faster than the Exchange! You do also can change some settings to make the full market depth refresh constantly or just have selections closest to the match point refresh constantly, while the rest of the market displays at a slower rate. When you load Geeks Toy: Right click and go to \u201cShow/Hide\u201d Then click on \u201cAPI Settings Manager\u201d You then have six different options, allowing you to change various refresh rates inside the software. (It is worth noting that the Exchange updates every 1000ms, so setting refresh rates to 250ms makes them refresh 4 times faster than the Exchange) Bets - the information relating to your bets for a market, both matched and unmatched Prices - the information relating to the amounts waiting to back and lay. This varies depending on the setting used on the Ladder for Price Display. If Standard or Hybrid is selected it refers to the front three back and lay prices. If Complete is selected it refers to all of the prices, this requires a lot more information to be downloaded each call which is not suitable for slower internet connections and those on limited download allowances Complete Prices in Hybrid mode - the amounts waiting to back and lay outside of the front three prices, i.e. full market depth, when Hybrid Price Display is selected on the Ladder. For people with slow internet connections and those on limited download allowances it should not be set low i.e. less than 1000ms Traded Volume - the information relating to traded volumes External Bets - how often to poll for bets made external to the application. The default is 10 seconds. This saves on bandwith and number of calls if you only use this application for betting. Account Funds - the information relating to your balance. If a change is made to a bet, i.e. a bet is submitted, altered or matched the balance will be updated automatically You can change visibility of these settings within the ladder format.","title":"Refresh settings"},{"location":"autoTools/geeksToyRefreshSettings/#changing-market-ladder-settings","text":"Once you are in a market ladder do the following: Right click the market header Select Visual Options Select Price Display You will see three options: Standard is where you will only see the first 3 sets of odds and they will be updated as per your API settings. The rest of the market will not be visible. Hybrid is where the entire market is visible but only the first 3 prices will be updated as per your API settings. Complete is where the entire market is updated as per the API settings.","title":"Changing market ladder settings"},{"location":"autoTools/geeksToyRefreshSettings/#selection-profit-or-hedged-market-profit","text":"As Geeks Toy is predominantly used for trading markets, a great setting to use is the function to show you your Profit/Loss on a certain selection within the market, or your overall Profit/Loss on the entire market (also known as your hedged profit). Once you are in a market ladder do the following: Right click the market header Select Visual Options Select Profit/Loss You will see two options: Selection Profit is the option you will can select if you would like to see Profit/Loss on each selection in the market individually. Hedged Profit is the option you can select for the software to show you your overall profit/loss across the entire market cumulatively.","title":"Selection profit or hedged (market) profit"},{"location":"autoTools/geeksToyRefreshSettings/#resources","text":"Betfair Geeks Toy Overview Geeks Toy Youtube Channel Caan Berry Pro Trader Youtube Channel Betfair Interview With Cann Berry","title":"Resources"},{"location":"autoTools/geeksToyRefreshSettings/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/grussKellyStake/","text":"Gruss Betting Assistant: Kelly Criterion staking Automating with Kelly staking method and Gruss Betting Assistant In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan We'll be building on the Gruss Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Gruss Ratings tutorial Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Understanding how the Kelly Criterion staking strategy works Tool: Gruss Betting Assistant - Recapping the strategy covered in the Gruss ratings automation tutorial We'll be using the same trigger strategy that's outlined in the Gruss Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'MARKET' worksheet (column S) and we've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Gruss ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here . - Set up Make sure you've downloaded and installed Gruss, and signed in. - Writing your rules We're using a customised version of the Gruss Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell I2 of the 'MARKET' worksheet UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake StakeType refers to cell X1 of the \"SETTINGS' worksheet Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet. Stepping through each step: Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('MARKET' worksheet) and return the best available back odds from the G column =IFERROR(INDEX(Market!$B$5:$M$50,MATCH(H2,Market!$A$5:$A$50,0),5),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Gruss populates in cell I2 of the 'MARKET' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType)) - You know the drill The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss. - Selecting markets Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps. - Linking the spreadsheet This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Show balance which is required for this staking strategy to work Then click OK and the sheet with be linked with the program. And you're set! Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Kelly criterion staking"},{"location":"autoTools/grussKellyStake/#gruss-betting-assistant-kelly-criterion-staking","text":"","title":"Gruss Betting Assistant: Kelly Criterion staking"},{"location":"autoTools/grussKellyStake/#automating-with-kelly-staking-method-and-gruss-betting-assistant","text":"In other tutorials on the Betfair Automation Hub, we've gone through how to automate betting strategies based on ratings, market favourites and tips. For this tutorial, we're going to implement a staking strategy which can be used in conjunction with most other betting strategies. Determining how much you stake on a wager is a crucial consideration for successful punters. The Kelly Criterion is a staking method well known across wagering and investment professionals which should be known and considered by all Betfair punters. Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to not only automate thoroughbred ratings from the Hub, but to also add the Kelly staking method. There are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating with Kelly staking method and Gruss Betting Assistant"},{"location":"autoTools/grussKellyStake/#-the-plan","text":"We'll be building on the Gruss Ratings tutorial which utilizes the the Betfair's Data Scientists' thoroughbred ratings model . For this tutorial, we'll be assuming that you have already gone through the ratings tutorial, but if you havn't, you can check it out here , as the concepts and underlying trigger based strategy here do build on what we covered previously. Staking strategies such as Kelly Criterion can be adventagous for automation when used in conjunction with a successful selection strategy. Rather than sending your bot to place static stake values for every bet, methods such as this let you place bet stakes which take into consideration your ratings and betting bank. If you're not familiar with the Kelly Criterion staking strategy, we recommend having a quick read of the Kelly staking page on the Betfair Hub . There are plenty more resources on the internet relating to the strategy which may provide a more in depth understanding. Resources Ratings: Betfair's Data Scientists' thoroughbred ratings model Before you start: check out the Gruss Ratings tutorial Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy Understanding how the Kelly Criterion staking strategy works Tool: Gruss Betting Assistant","title":"- The plan"},{"location":"autoTools/grussKellyStake/#-recapping-the-strategy-covered-in-the-gruss-ratings-automation-tutorial","text":"We'll be using the same trigger strategy that's outlined in the Gruss Ratings tutorial which uses the thoroughbred ratings shared by our Data Scientists on the Hub . Whilst the trigger will remain unchanged, we'll need to make small tweaks to the stake column of the 'MARKET' worksheet (column S) and we've added an additional option to the 'SETTINGS' worksheet which will allow you to choose either a half Kelly or full Kelly stake. If you havn't yet read our Gruss ratings tutorial, we highly recommend that you do so as to understand how the bet placement trigger works. The tutorial can be found here .","title":"- Recapping the strategy covered in the Gruss ratings automation tutorial"},{"location":"autoTools/grussKellyStake/#-set-up","text":"Make sure you've downloaded and installed Gruss, and signed in.","title":"- Set up"},{"location":"autoTools/grussKellyStake/#-writing-your-rules","text":"We're using a customised version of the Gruss Ratings tutorial template to implement our staking strategy, so it can not only make betting decisions based on our ratings, but also calculate the stakes based on the Kelly Criterion staking strategy. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/grussKellyStake/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Account Balance refers to cell I2 of the 'MARKET' worksheet UserStake refers to cell D4 of the \"SETTINGS' worksheet where you can change between half Kelly or Full Kelly stake StakeType refers to cell X1 of the \"SETTINGS' worksheet Full_Kelly refers to the entire Q Column of the 'KELLY' worksheet HALF_Kelly refers to the entire R Column of the 'KELLY' worksheet Calculating the Kelly stake As explained in the Kelly Criterion staking strategy Betfair page , the formula to claculate the Kelly stake is: (BP-Q)/B Where B is the odds you are getting -1 (because we're using decimal odds), P is the likelihood of the bet winning and Q is the probability of losing (or 1 \u2013 P). To show the steps of the calculation and to ensure that it's doing what we're expecting it to, we've created a 8 column table in the 'KELLY' worksheet.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/grussKellyStake/#stepping-through-each-step","text":"Column K - best available Back odds Check the runner's name in our ratings and match it with the runners listed in the market ('MARKET' worksheet) and return the best available back odds from the G column =IFERROR(INDEX(Market!$B$5:$M$50,MATCH(H2,Market!$A$5:$A$50,0),5),\"\") Column L - Exchange odds -1 Take the value returned in column K and minus 1 (because we're using decimal odds) =IFERROR(K2-1,\"\") Column M - Probability of win % 1 divided by the rated price from column I which converts the decimal odds to a percentage probability =IFERROR((1/I2),\"\") Column N - Probability of loss % 1 divided by the probability of a win from column M =IFERROR(1-M2,\"\") Column O - % of bankroll to use - Full Kelly Take the best available back odds (minus 1) from the L column, times it by the probability to win in the M column, then minus the probability to lose from the N column. Finally, divide that by the best available back odds (minus 1) =IFERROR(((L2*M2)-N2)/L2,\"\") Column P - % of bankroll to use - Half Kelly Take the calculation from column O and simply divide it by 2 =IFERROR(O2/2,\"\") Column Q - Amount to bet - Full Kelly If the account balance times the percentage of the bankroll to use for half kelly stake is greater than 0, retrieve the account balance which Gruss populates in cell I2 of the 'MARKET' worksheet and times it by the percentage of the bankroll (Column O) to use for the Full Kelly. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*O2,\"0\"),\"\") Column R - Amount to bet - half Kelly If the account balance times the percentage of the bankroll to use for half Kelly stake is greater than 0, then calculate the account balance times the percentage of the bankroll. If it's not greater than 0, then simply print 0. =IFERROR(IF(AccountBalance*P2>0,AccountBalance*P2,\"0\"),\"\") Result: Once the calculations are complete, we're left with two stake values that we will be able to use with our trigger. Column Q for a full Kelly stake and column R for a half Kelly stake. In the below image examples, we have a market for Geelong with the half and full Kelly. Excel functions IF statement: IF(if this is true, do this, else do this) IFERROR: If there is an error that occurs in the cell, display nothing AND statement: AND(this is true, and so is this, and so is this) - returns true or false Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"Stepping through each step:"},{"location":"autoTools/grussKellyStake/#-preparing-the-spreadsheet","text":"You need to copy/paste these eight formulas into the relevant column cells which is highlighted as blue - We copied ours into 1500 rows in the sheet, just in case you have a large number of ratings. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Stake: Here we're telling excel to take a look into B column, if a runner name is present, then match that name to either the value which has been calculated for the full or half Kelly, depending on what has been selected by the drop down box in the 'SETTINGS' worksheet. =IF(B9=\"\",\"\",INDEX(KELLY!Q:R,MATCH(B9,RunnerName,0),StakeType))","title":"- Preparing the spreadsheet"},{"location":"autoTools/grussKellyStake/#-you-know-the-drill","text":"The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss.","title":"- You know the drill"},{"location":"autoTools/grussKellyStake/#-selecting-markets","text":"Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.","title":"- Selecting markets"},{"location":"autoTools/grussKellyStake/#-linking-the-spreadsheet","text":"This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Show balance which is required for this staking strategy to work Then click OK and the sheet with be linked with the program.","title":"- Linking the spreadsheet"},{"location":"autoTools/grussKellyStake/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/grussKellyStake/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this.","title":"Areas for improvement"},{"location":"autoTools/grussKellyStake/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/grussKellyStake/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/grussMarketFavouriteAutomation/","text":"Gruss Betting Assistant: Market favourite automation Automating a market favourite strategy using Gruss Betting Assistant Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them. Building on our previous articles , we're using the spreadsheet functionality available in Gruss Betting Assistant to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions. - The plan Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Gruss Betting Assistant - Set up Make sure you've downloaded and installed Gruss, and signed in. - Writing your rules As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules. - Trigger to place bet In short, we want to back runners when: the selection's available to back price (Column F) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this the scheduled event start time is less and greater than what we specify Back market percentage is less than a certain value that we choose the event isn't in play - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell C5 in the 'SETTINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell Y4 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets UserOverround refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell C9, C10 and C11 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market MinTime refers to cell C3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell E3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell E2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Not in Play\" for each market MarketStatus1, MarketStatus2, MarketStatus3 refers to cell F2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Suspended\" for each market This is our trigger on Excel formula: ``` excel tab=\"Multi line\" =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") ``` excel tab=\"Single line\" =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1<>\"Suspended\"),\"BACK-SP\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column F) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") TimeTillJump1 < MaxTime and > MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C3 and greater than cell E3 in the 'SETTINGS' worksheet (named 'MinTime' and 'MaxTime' respectively). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. We'll keep it simple by referencing the value in cell C9 (named 'TimeTillJump1') in the 'SETTINGS' worksheet, where we've already done the calculations for you. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") Overrounds1 < UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C4 of the 'SETTINGS' worksheet =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to E2 in the 'MARKET' worksheet, if this cell displays \"Not In Play\" as a value, it's safe to place bets. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") MarketStatus1: checks whether the event is suspended, by checking if Gruss has populated cell F2 in the \"MARKET\" worksheet with \"Suspended\". If the cell has any other value, it's safe to place bets. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") Result: if the statement above is true, the formula returns 'BACK-SP', at which point the bet will trigger, taking BSP. If any of the previous conditions are not met, then no bet will be placed and the cell will remin blank. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") updating the trigger for 'MARKET 2' and 'MARKET 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'MARKET 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, InPlay2=\"Not In Play\", MarketStatus2<>\"Suspended\"), \"BACK-SP\", \"\") Trigger for 'MARKET 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump3 < MaxTime, TimeTillJump3 > MinTime, Overrounds3<UserOverrounds, InPlay3=\"Not In Play\", MarketStatus3<>\"Suspended\"), \"BACK-SP\", \"\") Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste this formula into the relevant cells for each of the runners. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1<>\"Suspended\"),\"BACK-SP\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column A. A similar effect to IFERROR, if Gruss hasn't populated cell A5 with a runner name, then dont populate this cell at all. =IF(A5=\"\",\"\",\"1000\") Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat stake here, so will just place $10 on each runner. This goes in column S (S5 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A5=\"\",\"\",stake) - You know the drill The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss. - Selecting markets Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps. - Linking the spreadsheet This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program. And you're set! Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Market fav auto"},{"location":"autoTools/grussMarketFavouriteAutomation/#gruss-betting-assistant-market-favourite-automation","text":"","title":"Gruss Betting Assistant: Market favourite automation"},{"location":"autoTools/grussMarketFavouriteAutomation/#automating-a-market-favourite-strategy-using-gruss-betting-assistant","text":"Here we explore how to implement an automated strategy to place Betfair Starting Price (BSP) bets on the top two runners in the market. This lets you choose your selections based on market sentiment close to the jump, and not worry about current market price by using BSP to place your bets. You could equally use effectively the same approach if you wanted to lay the favourite(s) instead of backing them. Building on our previous articles , we're using the spreadsheet functionality available in Gruss Betting Assistant to implement this strategy. If you haven't already we'd recommend going back and having a read of this article , as the concepts here do build on what we covered previously. As we've said before, there are so many different ways to use this part of Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us at automation@betfair.com.au with your feedback and opinions.","title":"Automating a market favourite strategy using Gruss Betting Assistant"},{"location":"autoTools/grussMarketFavouriteAutomation/#-the-plan","text":"Given that we're simply choosing our selections based on the market we don't need any ratings for this strategy. The plan is to look at the market a couple of minutes before the scheduled jump and place BSP bets based on its formation. Our approach here, and how we've set up the accompanying spreadsheet, backs the top two runners in the market two minutes out from the scheduled start time using the Betfair Starting Price. Resources Rules: here's the spreadsheet we set up with our macros and rules included, but you'll obviously need to tweak it to suit your strategy and approach Tool: Gruss Betting Assistant","title":"- The plan"},{"location":"autoTools/grussMarketFavouriteAutomation/#-set-up","text":"Make sure you've downloaded and installed Gruss, and signed in.","title":"- Set up"},{"location":"autoTools/grussMarketFavouriteAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on the favourites being shown in the market. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/grussMarketFavouriteAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: the selection's available to back price (Column F) is either the lowest or second lowest in the market - the top two market favourites with the ability to easily change this the scheduled event start time is less and greater than what we specify Back market percentage is less than a certain value that we choose the event isn't in play","title":"- Trigger to place bet"},{"location":"autoTools/grussMarketFavouriteAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Fav refers to cell C5 in the 'SETTINGS' worksheet Overrounds1, Overrounds2 and Overrounds3 refers to cell Y4 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets repectively, where the overrounds are calculated. Each worksheet needs to contain their own formula calculations as they will each be working off different markets UserOverround refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump1, TimeTillJump2 and TimeTillJump3 refers to cell C9, C10 and C11 in the 'SETTINGS' worksheet respectively. Just like the overrounds, each worksheet needs their own TimeTillJump calculation - one for each market MinTime refers to cell C3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners MaxTime refers to cell E3 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay1, InPlay2, InPlay3 refers to cell E2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Not in Play\" for each market MarketStatus1, MarketStatus2, MarketStatus3 refers to cell F2 in the 'MARKET', 'MARKET 2' and 'MARKET 3' worksheets respectively. Gruss will populate a status in these worksheet cells such as \"Suspended\" for each market This is our trigger on Excel formula: ``` excel tab=\"Multi line\" =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") ``` excel tab=\"Single line\" =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1<>\"Suspended\"),\"BACK-SP\",\"\") Stepping through each step: Finding the top two selections in the market: check each runner to see if they're one of the two market favourites - We're doing this by going through the best available to back (column F) price for each runner, ranking them in order (which sorts them from highest to lowest - which is the opposite of what we want) then subtracting that rank number from the total number of selections available to inverse the order. Finally, we plus one to the resulting rank - if we didn't do this then you'd have a rank order that started at 0, not 1, and we thought that would just confuse matters! Once it's established what each selection's rank is, we then check if that rank is less than three, and if it is we know that the runner in question is one of the top two in the market, based on the current available to back prices. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") TimeTillJump1 < MaxTime and > MinTime: check whether the seconds left on the countdown are smaller than what is defined in cell C3 and greater than cell E3 in the 'SETTINGS' worksheet (named 'MinTime' and 'MaxTime' respectively). This one's a bit complicated, as the time is actually returned as a percentage of a 24 hour day, which you need to convert into positive or negative seconds. We'll keep it simple by referencing the value in cell C9 (named 'TimeTillJump1') in the 'SETTINGS' worksheet, where we've already done the calculations for you. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") Overrounds1 < UserOverrounds: checking whether the market overrounds are less than the specific value that is specified in cell C4 of the 'SETTINGS' worksheet =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") InPlay1: checking whether the event has gone in play, as this is purely a pre-play strategy, though you could certainly take a similar approach to in-play markets. InPlay refers to E2 in the 'MARKET' worksheet, if this cell displays \"Not In Play\" as a value, it's safe to place bets. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") MarketStatus1: checks whether the event is suspended, by checking if Gruss has populated cell F2 in the \"MARKET\" worksheet with \"Suspended\". If the cell has any other value, it's safe to place bets. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") Result: if the statement above is true, the formula returns 'BACK-SP', at which point the bet will trigger, taking BSP. If any of the previous conditions are not met, then no bet will be placed and the cell will remin blank. =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") updating the trigger for 'MARKET 2' and 'MARKET 3' worksheets You will need to ensure that the reference names for Overrounds, TimeTillJump and InPlay are changed so that they are referencing the cells that are applicable for those specific worksheets. Forgetting to do this can lead to the automation working off information from the wrong market. Trigger for 'MARKET 2' worksheet: Note that Overrounds has been changed to Overrounds2, TimeTillJump1 to TimeTillJump2 and InPlay1 to InPlay2 =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump2 < MaxTime, TimeTillJump2 > MinTime, Overrounds2<UserOverrounds, InPlay2=\"Not In Play\", MarketStatus2<>\"Suspended\"), \"BACK-SP\", \"\") Trigger for 'MARKET 3' worksheet: Note that Overrounds has been changed to Overrounds3, TimeTillJump1 to TimeTillJump3 and InPlay1 to InPlay3 =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump3 < MaxTime, TimeTillJump3 > MinTime, Overrounds3<UserOverrounds, InPlay3=\"Not In Play\", MarketStatus3<>\"Suspended\"), \"BACK-SP\", \"\") Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false COUNT function: returns number of cells in the range you pass in tha contain a number RANK function: returns the rank of a number in a list of numbers, with the smallest number returning the highest rank. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/grussMarketFavouriteAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste this formula into the relevant cells for each of the runners. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner). ``` excel tab=\"Multi line\" =IF( AND( (COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50) -RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1, TimeTillJump1 < MaxTime, TimeTillJump1 > MinTime, Overrounds1<UserOverrounds, InPlay1=\"Not In Play\", MarketStatus1<>\"Suspended\"), \"BACK-SP\", \"\") ``` excel tab=\"Single line\" =IF(AND((COUNT($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50)-RANK(F5,($F$5,$F$6,$F$7,$F$8,$F$9,$F$10,$F$11,$F$12,$F$13,$F$14,$F$15,$F$16,$F$17,$F$18,$F$19,$F$20,$F$21,$F$22,$F$23,$F$24,$F$25,$F$26,$F$27,$F$28,$F$29,$F$30,$F$31,$F$32,$F$33,$F$34,$F$35,$F$36,$F$37,$F$38,$F$39,$F$40,$F$41,$F$42,$F$43,$F$44,$F$45,$F$46,$F$47,$F$48,$F$49,$F$50))+1) < Fav+1,TimeTillJump1 < MaxTime,TimeTillJump1 > MinTime,Overrounds1<UserOverrounds,InPlay1=\"Not In Play\",MarketStatus1<>\"Suspended\"),\"BACK-SP\",\"\") Odds: as we said we're putting the bet up initially at odds of 1000, so this is a simple one. Note: The IF statement in both the odds and stake cells is purely to keep our document clean of clutter when there are no runners in column A. A similar effect to IFERROR, if Gruss hasn't populated cell A5 with a runner name, then dont populate this cell at all. =IF(A5=\"\",\"\",\"1000\") Stake: it's completely up to you what staking approach you want to take. We're keeping it simple and using flat stake here, so will just place $10 on each runner. This goes in column S (S5 for the first runner). We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A5=\"\",\"\",stake)","title":"- Preparing the spreadsheet"},{"location":"autoTools/grussMarketFavouriteAutomation/#-you-know-the-drill","text":"The process is effectively the same from here on as for our previously automated strategy, but we've included it here just in case you want a refresher or are new to Gruss.","title":"- You know the drill"},{"location":"autoTools/grussMarketFavouriteAutomation/#-selecting-markets","text":"Gruss makes it really easy to select markets in bulk. You could go through an add each market individually, but it's much easier to just use the quick pick functionality to add all Australian racing win markets. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.","title":"- Selecting markets"},{"location":"autoTools/grussMarketFavouriteAutomation/#-linking-the-spreadsheet","text":"This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program.","title":"- Linking the spreadsheet"},{"location":"autoTools/grussMarketFavouriteAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/grussMarketFavouriteAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this.","title":"Areas for improvement"},{"location":"autoTools/grussMarketFavouriteAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/grussMarketFavouriteAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/grussRatingsAutomation/","text":"Gruss Betting Assistant: Ratings automation Automating a greyhound ratings strategy using Gruss Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings automation tutorial for Bet Angel, but here we'll be using the ratings for greyhounds, created by the data science team at Betfair and incorporate them into our automation in Gruss. Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions. - The plan We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian and Queensland greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Gruss Betting Assistant's Excel triggered betting feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. Here we'll step through how we went about getting Gruss to place bets using these ratings . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Obviously, you can use your own ratings and change the rules according to what your strategy is. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet We set up with our rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Gruss Betting Assistant - Set up Make sure you've downloaded and installed Gruss Betting Assistant , and signed in. - Downloading & formatting ratings Here we're using the Betfair's Data Scientists' greyhound ratings model for greyhound racing but alternatively you can follow the same process using the Betfair's Data Scientists' thoroughbred Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Gruss template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated. - Writing your rules As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. There are lots of posts on the Gruss Forum on the topic if you want to explore it more yourself. This is how we used Excel to implement our set of rules. - Trigger to place bet In short, we want to back runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play - Using cell references to simplify formulas Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column J in the 'RATINGS' worksheet SelectionID refers to the entire column G in the 'RATINGS' worksheet Overround refers to cell Y in the 'MARKET'worksheet, where the overrounds for the current market are calculated. UserOverround refers to cell C5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell C9 in the 'SETTINGS' worksheet UserTimeTillJump refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell E2 in the 'MARKET' worksheet. Gruss will populate a 'Not In Play' status leading up to the jump BACKLAY refers to cell C6 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners MARKETSTATUS refers to F2 in the 'MARKET' worksheet This is our trigger for the 'MARKET' worksheet ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus<>\"Suspended\"),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell D6). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Back market percentage (Overround) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell C4 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If cell E2 in the 'MARKET' worksheet has a 'Not In Play' flag, it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Market Status checking whether the event has been suspended - if there is any flag other than 'Suspended' in cell F2 of the 'MARKET' worksheet, it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'. - Preparing the spreadsheet You need to copy/paste these three formulas into the relevant cell on each runner - we did a few extra rows than the number of runners in the markets we were looking at, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner) in the 'MARKET' worksheet. ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus<>\"Suspended\"),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell F5 for the first runner). This goes in column R (R5 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. =IF(F5=0,\"\",F5) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A5=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(F5-1),stake*(H5/(H5-1))-stake) - Selecting markets Gruss makes it really easy to select markets in bulk. You could go through an add each market you have in your ratings individually, but it's much easier to just use the quick Pick functionality to add all Australian racing win markets. This is safe, because bets will only fire when they link up with a runner in your 'RATINGS' sheet. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps. - Linking the spreadsheet This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program. And you're set! Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. Areas for improvement There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Ratings auto"},{"location":"autoTools/grussRatingsAutomation/#gruss-betting-assistant-ratings-automation","text":"","title":"Gruss Betting Assistant: Ratings automation"},{"location":"autoTools/grussRatingsAutomation/#automating-a-greyhound-ratings-strategy-using-gruss","text":"Using ratings from reputable sources can be a great way to increase your wagering IQ. In this tutorial, we'll be following a similar process to the ratings automation tutorial for Bet Angel, but here we'll be using the ratings for greyhounds, created by the data science team at Betfair and incorporate them into our automation in Gruss. Gruss Betting Assistant has a spreadsheet functionality that lets you place bets using your own variables and information from the live market, which is what we've used here to automate these ratings. There are so many different ways to use this part Gruss and we're very open to any thoughts about more effective ways of implementing this sort of strategy. You're welcome to reach out to us on automation@betfair.com.au with your feedback and opinions.","title":"Automating a greyhound ratings strategy using Gruss"},{"location":"autoTools/grussRatingsAutomation/#-the-plan","text":"We're using the Greyhound Ratings Model put together by some of our Data Scientists. This model creates ratings for Victorian and Queensland greyhound races daily and is freely available on the Hub. It's pretty good at predicting winners, so we're going to place back bets on the dogs with shorter ratings where the market price is better than the model's rating. Gruss Betting Assistant's Excel triggered betting feature has the capacity to let you create spreadsheets with pretty complicated rules that can be applied to multiple markets, which is what we've used for the automation here. Here we'll step through how we went about getting Gruss to place bets using these ratings . Once it's set up the goal is to be able to upload a new set of ratings, choose your races, set the program running and be able to walk away. Obviously, you can use your own ratings and change the rules according to what your strategy is. Resources Ratings: Betfair Data Scientists' Greyhound Ratings Model Rules: here's the spreadsheet We set up with our rules included, but you'll obviously need to tweak it to suit your strategy and the format of your ratings Tool: Gruss Betting Assistant","title":"- The plan"},{"location":"autoTools/grussRatingsAutomation/#-set-up","text":"Make sure you've downloaded and installed Gruss Betting Assistant , and signed in.","title":"- Set up"},{"location":"autoTools/grussRatingsAutomation/#-downloading-formatting-ratings","text":"Here we're using the Betfair's Data Scientists' greyhound ratings model for greyhound racing but alternatively you can follow the same process using the Betfair's Data Scientists' thoroughbred Ratings Model which is also available on the hub. When there are ratings made available, you will have the options to download them as a CSV or JSON file. For this tutorial, we'll go ahead and download the ratings as a CSV file. Once we've downloaded the ratings, we'll go ahead and open up the files in Excel and copy the contents (Excluding the column headers) from cell A2, across all applicable columns which in this example is column N. Make sure to copy all rows that has data in them. Copy the ratings data over to the customised Gruss template Excel sheet 'RATINGS' worksheet, being sure that cell A2 is selected when pasting the data. In the Excel template that we've provided, we've coloured the cells green where the data should be populated.","title":"- Downloading &amp; formatting ratings"},{"location":"autoTools/grussRatingsAutomation/#-writing-your-rules","text":"As with any automated strategy, one of the most important steps is deciding what logical approach you want to take and writing rules that suit. We're using a customised version of the default Gruss template Excel sheet to implement our strategy, so it can make betting decisions based on my ratings. Excel is an excellent tool, but it can take an investment of time to be able to use it effectively. There are lots of posts on the Gruss Forum on the topic if you want to explore it more yourself. This is how we used Excel to implement our set of rules.","title":"- Writing your rules"},{"location":"autoTools/grussRatingsAutomation/#-trigger-to-place-bet","text":"In short, we want to back runners when: The available to back price is greater than the rating for that runner, then we will back the runner The available to back price is less than the rating for that runner, then we will lay the runner Back market percentage is less than a certain value that we choose The scheduled event start time is less than a certain number of seconds that we choose The event isn't in play","title":"- Trigger to place bet"},{"location":"autoTools/grussRatingsAutomation/#-using-cell-references-to-simplify-formulas","text":"Throughout this tutorial, we'll be referencing certain cells with custom names that will make it easier to understand and follow the formulas as we progress. This is an especially effective method to keep on top of more complex strategies that require long formaulas to implement. Cell names used in this tutorial Ratings refers to the entire Column J in the 'RATINGS' worksheet SelectionID refers to the entire column G in the 'RATINGS' worksheet Overround refers to cell Y in the 'MARKET'worksheet, where the overrounds for the current market are calculated. UserOverround refers to cell C5 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners TimeTillJump refers to cell C9 in the 'SETTINGS' worksheet UserTimeTillJump refers to cell C4 in the 'SETTINGS' worksheet which allows you to change a single value that will automatically update the formulas for all runners InPlay refers to cell E2 in the 'MARKET' worksheet. Gruss will populate a 'Not In Play' status leading up to the jump BACKLAY refers to cell C6 in the 'SETTINGS' worksheet which allows you to easily switch between Back and Lay bet typers via a drop-down box and will automatically update the formulas for all runners MARKETSTATUS refers to F2 in the 'MARKET' worksheet This is our trigger for the 'MARKET' worksheet ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus<>\"Suspended\"),BACKLAY,\"\") Stepping through each step: Checking market odds based on back or lay bet type: Here we're checking which bet type we've chosen from the dropdown box in the 'SETTINGS' worksheet (cell D6). If a BACK bet has been selected, the best available back bet must greater than our ratings that have been defined for that particular runner in the 'RATINGS' worksheet. On the flip side, if a LAY bet has been selected, then the best available back bet must be less than our ratings. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Back market percentage (Overround) is less than what we define (UserOverround): Here we're making a calculation for each runner (100 / best back price) and then calculating the sum of all of the runners together to give us the back market percentage. As the closer the BMP is to 100%, the fairer the market is, we use this to ensure that we only place bets when the market is less than what we define in the 'RATINGS' worksheet. Additional information relating to over-rounds can be found on the Hub. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Time until the jump is less than what we define: Check whether the seconds left on the countdown timer are less than what we define in cell C4 in the 'SETTINGS' worksheet. This one's a bit complicated, as the time is actually returned as a percentage of a 24-hour day, which you need to convert into positive or negative seconds. You can read about the formula here or just keep it simple by referencing the value in cell E2 of the 'SETTINGS' worksheet (named 'TimeTillJump1'), where we've already done the calculations for you. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Not in play: checking whether the event has gone in play - as odds change so much in the run we only want to use this strategy pre-play. If cell E2 in the 'MARKET' worksheet has a 'Not In Play' flag, it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Market Status checking whether the event has been suspended - if there is any flag other than 'Suspended' in cell F2 of the 'MARKET' worksheet, it's safe to place bets. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Result: if the statement above is true, the formula returns either a \"BACK\" or \"LAY\" depending on what has been selected from the 'SETTINGS' worksheet, at which point the bet will trigger, otherwise the cell will remain blank and no bet will be placed. =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) Excel functions IF function: IF(if this is true, do this, else do this) AND function: AND(this is true, and so is this, and so is this) - returns true or false AND OR function: checks that the statement meets more than one condition. If this OR that, then do the following. Absolute references: if you're copy/pasting formulas it's important that you make links absolute when you don't want the cell being referenced to change relative to the new cell the formula is being pasted into. You do this by putting a $ in front of the parts of the reference you don't want to 'move'.","title":"- Using cell references to simplify formulas"},{"location":"autoTools/grussRatingsAutomation/#-preparing-the-spreadsheet","text":"You need to copy/paste these three formulas into the relevant cell on each runner - we did a few extra rows than the number of runners in the markets we were looking at, just in case the fields are bigger in future events. Excel is clever enough to automatically update the relative links in the formulas, so you should be able to copy/paste the same formula into each cell as long as you've got your relative and absolute references straight . Trigger bet rule: this is the bet trigger Excel formula we created earlier, and it needs to go in column Q (Q5 for the first runner) in the 'MARKET' worksheet. ``` excel tab=\"Multi line\" =IF( AND( OR( AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))), AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))), Overround<UserOverround, TimeTillJump<UserTimeTillJump, InPlay=\"Not In Play\", MarketStatus<>\"Suspended\"), BACKLAY, \"\" ) ``` excel tab=\"Single line\" =IF(AND(OR(AND(BACKLAY=\"BACK\",(F5>(INDEX(Ratings,MATCH(Y5,SelectionID,0))))),AND(BACKLAY=\"LAY\",(F5<(INDEX(Ratings,MATCH(Y5,SelectionID,0)))))),Overround<UserOverround,TimeTillJump<UserTimeTillJump,InPlay=\"Not In Play\",MarketStatus<>\"Suspended\"),BACKLAY,\"\") Odds: initially we were using the runner's rating as the price, but we got a bet placement error for some of the selections - eventually we realised that the odds the bet's being placed at need to be valid Betfair 'ticks' . For simplicity's sake we're now just using the currently available back odds (cell F5 for the first runner). This goes in column R (R5 for the first runner). Another option would be to create a look up table that rounded your rating to the nearest 'tick' price - if you do this, please do send us through your formula and we'll add it to this tutorial. =IF(F5=0,\"\",F5) Stake: It's completely up to you what staking approach you want to take. We've kept it simple and are just using a 'to win / to lose' strategy. Each bet aims to win whatever value has been entered in the 'SETTINGS' worksheet on that runner at the current odds if the bet type has been set to BACK. If the bet type has been changed to LAY, then the stake becomes the liability - again, easily changed in the 'SETTINGS' worksheet. We've got some good resources on the Hub that look at different staking approaches - these might be useful in helping you decide which strategy you want to use. =IF(A5=\"\",\"\",IF(BACKLAY=\"BACK\", stake/(F5-1),stake*(H5/(H5-1))-stake)","title":"- Preparing the spreadsheet"},{"location":"autoTools/grussRatingsAutomation/#-selecting-markets","text":"Gruss makes it really easy to select markets in bulk. You could go through an add each market you have in your ratings individually, but it's much easier to just use the quick Pick functionality to add all Australian racing win markets. This is safe, because bets will only fire when they link up with a runner in your 'RATINGS' sheet. You also need to make sure you set it up so that the program will automatically move on to the next market, when the previous one jumps.","title":"- Selecting markets"},{"location":"autoTools/grussRatingsAutomation/#-linking-the-spreadsheet","text":"This is a little tricky the first time, but easy once you know how. Make sure you have the Excel sheet saved to your local computer - when we tried using a file we had saved in OneDrive it simply didn't work. Open the Excel sheet, then click on Excel/Log current prices. It will autofill the workbook and sheet names. You'll then need to make sure you tick: Enable triggered betting Clear Bet refs on auto select market Quick pick reload triggers select first market Then click OK and the sheet with be linked with the program.","title":"- Linking the spreadsheet"},{"location":"autoTools/grussRatingsAutomation/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss Betting Assistant it should only take a number of seconds to load your markets and ratings up and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/grussRatingsAutomation/#areas-for-improvement","text":"There are parts of this approach that we're still trying to get to work to our liking, and we'll update this article as we find better solutions. If you have any suggestions for improvements please reach out to automation@betfair.com.au - we'd love to hear your thoughts. For example, the spreadsheet only binds with one market at a time, so if one market gets delayed and runs overtime the program won't be able to move on to the next market - we missed some races because of this.","title":"Areas for improvement"},{"location":"autoTools/grussRatingsAutomation/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au","title":"What next?"},{"location":"autoTools/grussRatingsAutomation/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/grusslSimultaneousMarkets/","text":"Gruss: Automating simultaneous markets Don't miss out on a market with simultaneous automation If you have a concern of missing markets due to delays or unforeseen circumstances at a market, Gruss is able to work off multiple worksheets for different meetings, all from the same workbook. For example, we can have worksheet one from our spreadsheet work through the markets taking place in Flemington, while worksheet 2 will work through markets at Sandown, worksheet 3 works through markets at Bendigo etc. To Set this up, we need to do need to duplicate the main \u2018Market\u2019 worksheet and have enough instances of the sheet for each meeting. There are four meetings that I want to cover, so I have duplicated the worksheet four times and renamed them to help me keep track. After these changes have been made, restart Gruss so that the changes go into effect. Once Gruss is back open, as before, click \u2018Market\u2019, \u2018Add to Quick Pick List\u2019, \u2018Horse Racing\u2019 (Or any event you are automating for), the country that you would like to bet and then \u2018All Win\u2019. This should generate a bunch of tabs in Gruss, each tab containing all the markets for a specific meeting. Simply close the ones you don\u2019t want. Once you\u2019re ready, click the \u2018Excel\u2019 menu in Gruss, and \u2018Log Multiple sheets Quick link\u2019. Select your Excel Workbook, assign the worksheets, select \u2018Enable Triggered Betting\u2019, \u201cCLEAR trigger clears matched odds\u2019, \u2018Clear bet refs on auto select markets\u2019, \u2018Clear bet refs on manual select market\u2019 and finally \u2018Auto Select First Market\u2019. Your screen should look something like this: When you\u2019re ready for automation to take over, click \u2018Start logging\u2019 at the bottom of the window. And you're set! Once you've set your spreadsheet set up and you're comfortable using Gruss it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run. What next? We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros. Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Simultaneous markets"},{"location":"autoTools/grusslSimultaneousMarkets/#gruss-automating-simultaneous-markets","text":"","title":"Gruss: Automating simultaneous markets"},{"location":"autoTools/grusslSimultaneousMarkets/#dont-miss-out-on-a-market-with-simultaneous-automation","text":"If you have a concern of missing markets due to delays or unforeseen circumstances at a market, Gruss is able to work off multiple worksheets for different meetings, all from the same workbook. For example, we can have worksheet one from our spreadsheet work through the markets taking place in Flemington, while worksheet 2 will work through markets at Sandown, worksheet 3 works through markets at Bendigo etc. To Set this up, we need to do need to duplicate the main \u2018Market\u2019 worksheet and have enough instances of the sheet for each meeting. There are four meetings that I want to cover, so I have duplicated the worksheet four times and renamed them to help me keep track. After these changes have been made, restart Gruss so that the changes go into effect. Once Gruss is back open, as before, click \u2018Market\u2019, \u2018Add to Quick Pick List\u2019, \u2018Horse Racing\u2019 (Or any event you are automating for), the country that you would like to bet and then \u2018All Win\u2019. This should generate a bunch of tabs in Gruss, each tab containing all the markets for a specific meeting. Simply close the ones you don\u2019t want. Once you\u2019re ready, click the \u2018Excel\u2019 menu in Gruss, and \u2018Log Multiple sheets Quick link\u2019. Select your Excel Workbook, assign the worksheets, select \u2018Enable Triggered Betting\u2019, \u201cCLEAR trigger clears matched odds\u2019, \u2018Clear bet refs on auto select markets\u2019, \u2018Clear bet refs on manual select market\u2019 and finally \u2018Auto Select First Market\u2019. Your screen should look something like this: When you\u2019re ready for automation to take over, click \u2018Start logging\u2019 at the bottom of the window.","title":"Don't miss out on a market with simultaneous automation"},{"location":"autoTools/grusslSimultaneousMarkets/#and-youre-set","text":"Once you've set your spreadsheet set up and you're comfortable using Gruss it should only take a number of seconds to load your markets and set your strategy running for the day. Just make sure you have all of the app settings correctly selected before you leave the bot to run, as some of them reset by default when you turn the program off. Note: you will need to leave your computer up and running for the duration of the chosen markets, as the program needs the computer to be 'awake' to be able to run.","title":"And you're set!"},{"location":"autoTools/grusslSimultaneousMarkets/#what-next","text":"We're working through some of the popular automation tools and creating articles like this one to help you learn how to use them to implement different styles of strategies. If you have any thoughts or feedback on this article or other programs you'd like to see us explore please reach out to automation@betfair.com.au - this article has already been updated with extra learnings including variable percentages and new macros.","title":"What next?"},{"location":"autoTools/grusslSimultaneousMarkets/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/overview/","text":"Existing tools There are many applications that have been built by external developers using Betfair's Vendor offering that can be used in conjunction with Betfair which can help wagering from more one-click bets to automating your strategies. If you haven\u2019t before used a betting tool, below is a overview of some of the most popular tools available. If you have any suggestions for new tutorials/improvements please reach out to automation@betfair.com.au - We'd love to hear your thoughts and feedback. Popular tools Bet Angel Pro // Gruss // Market Feeder // Cymatic Trader Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Tools Overview"},{"location":"autoTools/overview/#existing-tools","text":"There are many applications that have been built by external developers using Betfair's Vendor offering that can be used in conjunction with Betfair which can help wagering from more one-click bets to automating your strategies. If you haven\u2019t before used a betting tool, below is a overview of some of the most popular tools available. If you have any suggestions for new tutorials/improvements please reach out to automation@betfair.com.au - We'd love to hear your thoughts and feedback.","title":"Existing tools"},{"location":"autoTools/overview/#popular-tools","text":"Bet Angel Pro // Gruss // Market Feeder // Cymatic Trader","title":"Popular tools"},{"location":"autoTools/overview/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/BfBotManager/BfBotManager/","text":"Bf Bot Manager is software designed to help you automate your betting and trading at Betfair betting exchange. Bf Bot Manager allows you to run unlimited number of strategies/bots at the same time. You can have one strategy betting on favourites, second one trading on horse races, third one betting on tennis matches and several strategies betting on football events or tipster tips. You can set software to automatically download or import your or tipster betting tips and bet on them just few seconds before event start time. Your settings can be exported to file allowing you to easily share strategy with your friends or to create backup. Software also supports manual bet placement and has ladder and grid controls for one click betting. Hedge (green/red up) functionality can be set to execute automatically or on single click. Bf Bot Manager supports simulation mode, many staking plans, loss recovery and much more. It even has football statistics for major leagues allowing you to automatically bet only on teams with specific previous results. You can also order custom functionality that we can develop for you and adjust settings to suit your own requirements. BF BOT MANAGER KEY FEATURES Custom Bots Dutching Automation Greening Tools Simulation Mode One-click Betting To find out more and to sign up for a FREE 5 Day trial, head to the BF Bot Manager Website","title":"BfBotManager"},{"location":"autoTools/CymaticTrader/CymaticTrader/","text":"Taking trading to another level, this fully featured, super fast trading software has many unique features developed over a number of years. Intuitive throughout, it was designed for professional traders but is equally welcomed by novices. It was the first program to actually reveal your estimated position in the exchange order book queue (PIQ), helping you gain an advantage when trading manually or with the built in robot. PIQ is now an essential feature for many traders. You can open multiple markets simultaneously and perform one-click trading using customizable ladders or grids. Greening, tick-offset, stop-loss and a range of other features are included, as you\u2019d expect in a great low-latency trading app. The Excel integration capability enables you to view real-time prices in Excel, trigger orders from Excel and even create your own fully automated trading robot! The integrated advanced charting is capable of displaying candle-stick, bar or line charts, in various time frames, plus a huge range of technical analysis indicators. Zooming, panning and drawing magnetic trend lines are all made easy. Another unique feature is the \u2018API Monitor\u2019, it can instantly reveal errors or bottlenecks caused by the internet or the exchange \u2013 thus helping you avoid trading during bad periods that other traders may often be oblivious to. It also calculates real time statistics such as the latency of various types of calls to the Betfair API for your current trading session. Warning messages and statistical reports can even be automatically emailed to you by the software. The software is well suited to single or multiple monitor scenarios. Downloading/filtering of betting history and account statements is also included. GRUSS KEY FEATURES Rapid keyboard betting short cuts Advanced charting Football Odds predictor Ladder interface Excel integration Practice mode To find out more and to sign up for a FREE 14 Day trial, head to the Cymatic Trader Website .","title":"Overview"},{"location":"autoTools/Elo/Elo/","text":"- What is Elo? The Elo rating system was orginally created by Hungarian born physics professor Arpad Elo who was a chess master and competed within the United States Chess Federation (USCF) and developed the system as an alternative to other rating systems that were considered to be inaccurate. Rather than rating a players performance by their overall wins and losses like previous systems, the Elo rating system works by assigning each player or team an Elo rating. When a team or player beats another, the winning side gains a portion of the losing sides points. This difference between Elo ratings between competitors is then used to create a probability for a particular outcome. The Elo rating system has become so popular since its inception, it is widely used today behind the scenes for a range of applications such as dating websites to determince compatibility between matches and even video game tournaments to ensure players of a similar skill level are matched together. - How does Elo work? For the purpose of this explanation, we'll be using Elo in the context of rating AFL teams. Each team begins with an Elo rating of 1500 and will gain or lose a portion of their Elo from / to the opposite team depending if they win or lose a game. For example, if team A has 1500 Elo and they beat team B - who also started with a 1500 Elo, then a certain portion of Elo (usually 30) is deducted from the loser and awarded to the winner. In this scenario, Team A will have an Elo rating of 1530 and Team B will be left with 1470. What makes Elo great for betting in sports is that it can be converted to a probability which in turn can be converted into odds used to place bets. - The Algorithm Each team will have their own Elo rating which will be determined based on their historical performance. If a team has never played a game, then they will begin with an Elo rating of 1500. The following formula is used to calculate the probability of a win for each team: A crucial consideration of any Elo rating system is the implementation of what is known as the K-factor. This determines the \"sensitivity\" how much of an impact a win or a loss has on a teams Elo rating. A good K-factor is generally around 32 but this can be adjusted to suit. Once the match has been concluded, each teams Elo is then updated to reflect any changes to their Elo rating, taking into account the full sequence of wins and loses up to the most recent match played. For this, the below formula is used: Where elo_i (t+1) is the updated Elo, elo_i (t) was their Elo before the match, K is the K-factor mentioned before (which we determined to be 32), outcome is an indicator of the match outcome (1 if it was won by team A, 0 if a loss), and Pwin was the pre-match probability of winning for team A, as given by the previous formula. If two teams play their first match, both with Elo of 1500, Pwin is equal to 0.5 so, for a K-factor of 32, the winning team would gain 16 points and the losing team would lose 16 points. In their next match, the teams start with Elo ratings of 1,516 and 1,484 respectively, and by updating their rating for all matches played, their current Elo rating can be calculated. - Taking Elo to the next level If you're interested in learning more about Elo and implementing a dynamic K-factor instead of a static one for greater accuracy, check out the article we've created in the Betfair Hub which applies Elo in the context of modelling tennis . Doing so can bring greater accuracy, especially when applied to the AFL season as the K-factor can be set to change depending on the importance of a particular match such as finals and early / late season games. Resources Learn more about the origins of Elo by visiting the Elo Wikipedia page Take a look at the Elo Tennis model article on the Betfair Hub Here's another good resource to help gain a better understanding of how Elo works and is applied: The Math behind Elo Disclaimer Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Elo"},{"location":"autoTools/Elo/Elo/#-what-is-elo","text":"The Elo rating system was orginally created by Hungarian born physics professor Arpad Elo who was a chess master and competed within the United States Chess Federation (USCF) and developed the system as an alternative to other rating systems that were considered to be inaccurate. Rather than rating a players performance by their overall wins and losses like previous systems, the Elo rating system works by assigning each player or team an Elo rating. When a team or player beats another, the winning side gains a portion of the losing sides points. This difference between Elo ratings between competitors is then used to create a probability for a particular outcome. The Elo rating system has become so popular since its inception, it is widely used today behind the scenes for a range of applications such as dating websites to determince compatibility between matches and even video game tournaments to ensure players of a similar skill level are matched together.","title":"- What is Elo?"},{"location":"autoTools/Elo/Elo/#-how-does-elo-work","text":"For the purpose of this explanation, we'll be using Elo in the context of rating AFL teams. Each team begins with an Elo rating of 1500 and will gain or lose a portion of their Elo from / to the opposite team depending if they win or lose a game. For example, if team A has 1500 Elo and they beat team B - who also started with a 1500 Elo, then a certain portion of Elo (usually 30) is deducted from the loser and awarded to the winner. In this scenario, Team A will have an Elo rating of 1530 and Team B will be left with 1470. What makes Elo great for betting in sports is that it can be converted to a probability which in turn can be converted into odds used to place bets.","title":"- How does Elo work?"},{"location":"autoTools/Elo/Elo/#-the-algorithm","text":"Each team will have their own Elo rating which will be determined based on their historical performance. If a team has never played a game, then they will begin with an Elo rating of 1500. The following formula is used to calculate the probability of a win for each team: A crucial consideration of any Elo rating system is the implementation of what is known as the K-factor. This determines the \"sensitivity\" how much of an impact a win or a loss has on a teams Elo rating. A good K-factor is generally around 32 but this can be adjusted to suit. Once the match has been concluded, each teams Elo is then updated to reflect any changes to their Elo rating, taking into account the full sequence of wins and loses up to the most recent match played. For this, the below formula is used: Where elo_i (t+1) is the updated Elo, elo_i (t) was their Elo before the match, K is the K-factor mentioned before (which we determined to be 32), outcome is an indicator of the match outcome (1 if it was won by team A, 0 if a loss), and Pwin was the pre-match probability of winning for team A, as given by the previous formula. If two teams play their first match, both with Elo of 1500, Pwin is equal to 0.5 so, for a K-factor of 32, the winning team would gain 16 points and the losing team would lose 16 points. In their next match, the teams start with Elo ratings of 1,516 and 1,484 respectively, and by updating their rating for all matches played, their current Elo rating can be calculated.","title":"- The Algorithm"},{"location":"autoTools/Elo/Elo/#-taking-elo-to-the-next-level","text":"If you're interested in learning more about Elo and implementing a dynamic K-factor instead of a static one for greater accuracy, check out the article we've created in the Betfair Hub which applies Elo in the context of modelling tennis . Doing so can bring greater accuracy, especially when applied to the AFL season as the K-factor can be set to change depending on the importance of a particular match such as finals and early / late season games. Resources Learn more about the origins of Elo by visiting the Elo Wikipedia page Take a look at the Elo Tennis model article on the Betfair Hub Here's another good resource to help gain a better understanding of how Elo works and is applied: The Math behind Elo","title":"- Taking Elo to the next level"},{"location":"autoTools/Elo/Elo/#disclaimer","text":"Note that whilst automated strategies are fun and rewarding to create, we can't promise that your betting strategy will be profitable, and we make no representations in relation to the information on this page. If you're implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"autoTools/GeeksToy/GeeksToy/","text":"Overview Have this cookie :)","title":"Overview"},{"location":"autoTools/GeeksToy/GeeksToy/#overview","text":"Have this cookie :)","title":"Overview"},{"location":"autoTools/Gruss/Gruss/","text":"Gruss Software offers one-click betting in the standard or ladder type interface and also the ability to link into Excel using pre-defined triggers to place bets. Betting Assistant deploys a wealth of functionality such as green-up, fill or kill, stop loss, dutch betting, coupon market view and more. Gruss Software is a constantly evolving solution where the developers listen to the users and their needs. Users can communicate directly with the developers, make suggestions and exchange ideas on the Betting Assistant Forum. GRUSS KEY FEATURES One click betting Dutching tool Ladder interface One click betting Advanced market navigation Excel integration To find out more and to sign up for a FREE 30 Day trial, head to the Gruss Software Website .","title":"Overview"},{"location":"autoTools/MarketFeeder/MarketFeeder/","text":"Auto-Green-Up, Auto-Dutch and implement triggered betting with MarketFeeder Pro. As well as the standard manual betting tools you know and love, the auto-trading functions allow for stress-free betting with the triggered betting feature. A trigger is an effective way to maximise your time and safe-guard you from any errors. A short set of instructions can be set to perform one of 50 actions, including backing, laying, cancelling bets, slowing or speeding up market refresh, greening up and spreading the loss, alerting, emailing and indeed being your second hand on Betfair. MARKETFEEDER PRO KEY FEATURES Monitoring Simultanious Markets 0.3 Second Refresh Rate Automated Market Search tool Triggered Betting Excel Spreadsheet Interaction Automatic Greenup Automatic Dutching Ladder Interface for Scalping To find out more and to sign up for a FREE 1 month trial, head to the MarketFeeder Pro Website","title":"MarketFeeder"},{"location":"autoTools/betAngel/betAngel/","text":"Bet Angel is one of the longest established API Products available. Bet Angel has three separate products available for Betfair customers. Bet Angel Basic is free and useful for placing occasional bets but has limited trading and professional tools. Bet Angel Trader contains the essential trading tools and Bet Angel Professional is the ultimate trading tool kit providing all the advanced tools and features required. Bet Angel Betting Applications offer a complete suite of tools all of which can be used with live or via a fully featured, risk free, practice mode. In a fully customisable interface, you can Back & Lay across multiple screens on a range of sports. The Guardian Advanced Automation feature allows you to determine your automation rules with ease. Enabling complex automated betting based on timing, price triggers, comparative odds conditions and more. Automatically Cash-Out when your desired profit level is achieved and assure yourself a level profit (greening) no matter the result. For those who wish to write their own automated betting or trading strategies, Bet Angel can link to Microsoft Excel offering automation opportunities that are limitless. BET ANGEL KEY FEATURES Practice Mode One Click Betting Ladder Interface Advanced Charting Dutching and Bookmaking tools Trigger based automation Excel Spreadhseet integration Cash-Out To find out more and to sign up for a FREE 14 Day trial, head to the Bet Angel Website","title":"Overview"},{"location":"historicData/analysingAndPredictingBSP/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Wisdom of the crowd? Analysing & understanding BSP This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the Market Movements tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet This is presented as a Jupyter notebook as this format is interactive and lets you run snippets of code from wihtin the notebook. To use this functionality you'll need to download a copy of the ipynb file locally and open it in a text editor (i.e. VS code). If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo . 0.0 Setup 0.1 Importing libraries Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language. Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files. You'll also need betfairlightweight which you can install with something like pip install betfairlightweight . import pandas as pd import numpy as np import requests import os import re import csv import plotly.express as px import plotly.graph_objects as go import math import logging import yaml import csv import tarfile import zipfile import bz2 import glob import ast from datetime import date , timedelta from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) 0.2 Context The Betfair Starting Price (BSP) is a starting price product offered by Betfair (on large enough markets, almost all racing and some sports) that gives customers a chance to back or lay any selection at a \"fair\" price. Without getting too complex too quickly, the BSP allows you lock in a bet at any time after the market is opened and for as much stake as you can afford. The BSP is a good option for many different segments of customers: Recreational punters that don't have a particular strategy for trying to get the best odds can lock in a price that is (in the aggregate) a lot better than what they'd get at a corporate book or they'd get by taking limit bets early in a market's trading Automated customers that don't want the hassle of managing live market trading can implement automated strategies a lot easier whilst also protecting them from edge cases like race reschedules Is perfect for simply backtesting fundamental models as it's a resilient and robust single price There\u2019s a lot of background reading available on BSP if you\u2019re keen to understand it better. Here are some resources that might be interesting if you want to learn more: Overview FAQ summary Detailed explanation Worked examples of the reconciliation process How the projected odds work Despite it being a good option for a lot of customers it's also a fairly contraversial topic for some other types of customers. Some people firmly believe that the BSP on big markets reflects the \"true chance\" of a selection so betting it is a fools errand that will simply lose you commission over the long run. You might have heard a version of this story before: given the large pool sizes, the 0% overround, the settlement at the exact moment the market is suspended the BSP perfectly synthesises all available public information and demand and arrives at a true fair odds. Some will attempt to prove this to you by showing you a predicted chance vs observed win rate scatterplot which shows a perfect correlation between chance implied by the BSP and a horses true chance. Whilst I don't disagree that the BSP is a very strong estimate of a selections chance it's pretty obviously not perfect. Furthermore, it presents some other tricky challenges to use in practical situations. It's not knowable perfectly before it's the exact moment of market suspension so many model or strategy builders make the mistake of unknowingly leaking it into their preplay model development or their theoretical staking calculations. Where the final number will land is actually another source of uncertainty in your processes which presents anothing forecasting / predictive modelling application as I'll explore later in this piece. I'll take you through how I'd measure the accuracy of the BSP, show you how it's traded on the exchange, and take you through a host of methods of estimating the BSP and build a custom machine learning approach that's better than each of them. 0.3 The Algorithm The actual logic of how betfair arrives at the final BSP number is quite complex and for a few reasons you won't be able to perfectly replicate it at home. However, the general gist of the BSP reconciliation algorithm that is executed just as the market suspended goes something like: The algorithm combines 4 distinct groups of open bets for a given selection: Non price limited BSP orders on both the back and lay side ( market_on_close orders) Price limited orders on both the back and lay side ( limit_on_close orders) All non filled open lay orders All non filled open back orders It then combines them all together, passes a sophisticated balancing algorithm over the top of them and arrives at a single fair price for the BSP that balances the demand on either side of the ledger 0.4 This Example For this exercise we'll again take advantage of the betfair historical stream json files. The slice of betfair markets I'll be analysing is all thoroughbred races over July 2020 - June 2021. As an aside the projected BSP number you see on the betfair website isn't collected inside betfair's own internal database of orders, so any custom data request you may be able to get as a VIP won't include this number. So if you were planning to include it in any kind of projection or bet placement logic operation you were making the only way to anlayse it historically is to mine these data files. Another good reason to learn the skills to do so! 1.0 Data Like the previous tutorial we won't be able to collapse the stream data down into a single row per runner because I'm interested in anlaysing how the projected BSP moves late in betfair markets. I'm also interested in plotting the efficiency of certain odds values at certain distinct time points leading up the the races so I need multiple records per runner. Like in the previous tutorial I'll split out the selection metadata, BSP and win flag values as a seperate data file to reduce the size of the datafiles extracted for this analysis. For the preplay prices dataset I'll: Start extraction at 2 mins before the scheduled off Extract prices every 10 seconds thereafter until the market is suspended I'll also extract the final market state the instant before the market is suspended 1.1 Sourcing Data First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Ask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access, download them to your computer and store them together in a folder. 1.2 Utility functions First like always we'll need some general utility functions that you may have seen before: # General Utility Functions # _________________________________ def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def pull_ladder ( availableLadder , n = 5 ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : n ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"p\" ] = price out [ \"v\" ] = volume return ( out ) def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) 1.3 Selection Metadata Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes. def final_market_book ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): return ( None ) for market_book in market_books : last_market_book = market_book return ( last_market_book ) def parse_final_selection_meta ( dir , out_file ): with open ( out_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,venue,market_time,selection_name,win,bsp \\n \" ) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) last_market_book = final_market_book ( stream ) if last_market_book is None : continue # Extract Info ++++++++++++++++++++++++++++++++++ runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in last_market_book . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'win' : np . where ( r . status == \"WINNER\" , 1 , 0 ), 'sp' : r . sp . actual_sp } for r in last_market_book . runners ] # Return Info ++++++++++++++++++++++++++++++++++ for runnerMeta in runnerMeta : if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( last_market_book . market_id ), runnerMeta [ 'selection_id' ], last_market_book . market_definition . venue , last_market_book . market_definition . market_time , runnerMeta [ 'selection_name' ], runnerMeta [ 'win' ], runnerMeta [ 'sp' ] ) ) selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) print ( \"__ Parsing Selection Metadata ___ \" ) # parse_final_selection_meta(stream_files, selection_meta) __ Parsing Selection Metadata ___ 1.4 Preplay Prices and Projections In this set of preplay prices I'm interested in many of the same fields as we've extracted in previous tutorials as well as fields relating to the current state of the BSP. These objects sit under the sp slot within the returned runner object. The fields we'll extract are: The so called \"near price\" The near price is the projected SP value you can see on the website It includes both bets already placed into the SP pools as well as open limit orders to estimate what the final BSP value will be The so called \"far price\" This is the same as the near price except it excludes limit orders on the exchange This makes it fairly redundant value and we'll see how poor of an estimator it is a bit later The volume currently bet into the BSP back pool The liability currently laid into the BSP lay pool We'll also extract the top 5 rungs of the available to back and available to lay ladders as well as the traded volume of limit bets. It's worth noting that I am discarding some key information about the BSP pools that I could have extracted if I wanted to. The current SP bets are laid out in a way that I could split out limit_on_close as well as market_on_close sp bets but I've rolled everything together in SP stake on the back side and sp liability on the lay side. This is just to reduce complexity of this article but including it would increase the predictive power of the BSP model in the final step. def loop_preplay_prices ( s , o ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () marketID = None tradeVols = None time = None last_book_recorded = False prev_book = None for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): break for market_book in market_books : # Time Step Management ++++++++++++++++++++++++++++++++++ if marketID is None : # No market initialised marketID = market_book . market_id time = market_book . publish_time elif market_book . inplay and last_book_recorded : break else : seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () if seconds_to_start > 120 : # Too early before off to start logging prices prev_book = market_book continue else : # Update data at different time steps depending on seconds to off wait = 10 # New Market if market_book . market_id != marketID : last_book_recorded = False marketID = market_book . market_id time = market_book . publish_time continue # (wait) seconds elapsed since last write elif ( market_book . publish_time - time ) . total_seconds () > wait : time = market_book . publish_time # if current marketbook is inplay want to record the previous market book as it's the last preplay marketbook elif market_book . inplay : last_book_recorded = True market_book = prev_book # fewer than (wait) seconds elapsed continue to next loop else : prev_book = market_book continue # Execute Data Logging ++++++++++++++++++++++++++++++++++ for runner in market_book . runners : try : atb_ladder = pull_ladder ( runner . ex . available_to_back , n = 5 ) atl_ladder = pull_ladder ( runner . ex . available_to_lay , n = 5 ) except : atb_ladder = {} atl_ladder = {} limitTradedVol = sum ([ rung . size for rung in runner . ex . traded_volume ]) o . writerow ( ( market_book . market_id , runner . selection_id , market_book . publish_time , int ( limitTradedVol ), # SP Fields runner . sp . near_price , runner . sp . far_price , int ( sum ([ ps . size for ps in runner . sp . back_stake_taken ])), int ( sum ([ ps . size for ps in runner . sp . lay_liability_taken ])), # Limit bets available str ( atb_ladder ) . replace ( ' ' , '' ), str ( atl_ladder ) . replace ( ' ' , '' ) ) ) prev_book = market_book def parse_preplay_prices ( dir , out_file ): with open ( out_file , \"w+\" ) as output : writer = csv . writer ( output , delimiter = ',' , lineterminator = ' \\r\\n ' , quoting = csv . QUOTE_ALL ) writer . writerow (( \"market_id\" , \"selection_id\" , \"time\" , \"traded_volume\" , \"near_price\" , \"far_price\" , \"bsp_back_pool_stake\" , \"bsp_lay_pool_liability\" , \"atb_ladder\" , 'atl_ladder' )) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) loop_preplay_prices ( stream , writer ) price = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) print ( \"__ Parsing Selection Prices ___ \" ) # parse_final_selection_meta(stream_files, price) __ Parsing Selection Prices ___ 2.0 Analysis First step let's boot up the datasets we extracted in the previous steps and take a look at what we've managed to extract from the raw stream files. 2.1 Load & Inspect First we have the highlevel selection metadata as we have already seen in other tutorials selection = pd . read_csv ( \"[PATH TO YOUR SELECTION METADATA FILE]\" , dtype = { 'market_id' : object , 'selection_id' : object }, parse_dates = [ 'market_time' ]) selection . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue market_time selection_name win bsp 0 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.20 1 1.179845158 16374800 Cowra 2021-03-01 04:15:00 3. Careering Away 1 3.60 2 1.179845158 19740699 Cowra 2021-03-01 04:15:00 4. Bells N Bows 0 6.62 Now let's load the prices file. We'll apply some extra logic to parse the ladder columns into dictionaries and also remove the first odds record per group as it's the first record as the market was instantiated. prices = pd . read_csv ( \"[PATH TO YOUR PRICES FILE]\" , quoting = csv . QUOTE_ALL , dtype = { 'market_id' : 'string' , 'selection_id' : 'string' , 'atb_ladder' : 'string' , 'atl_ladder' : 'string' }, parse_dates = [ 'time' ] ) # Parse ladder columns prices [ 'atb_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atb_ladder' ]] prices [ 'atl_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atl_ladder' ]] # Drop the first row within each group prices = prices . drop ( prices . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( 0 ) . index ) prices . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder 21 1.179845158 23493550 2021-03-01 04:13:00.058 2465 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [14.63, 13... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [22.86, 12... 22 1.179845158 16374800 2021-03-01 04:13:00.058 5046 3.35 1.70 449 300 {'p': [3.65, 3.6, 3.55, 3.5, 3.45], 'v': [0.45... {'p': [3.75, 3.8, 3.9, 4.1, 4.3], 'v': [5.14, ... 23 1.179845158 19740699 2021-03-01 04:13:00.058 1978 6.39 3.25 154 251 {'p': [6, 5.9, 5.8, 5.7, 5.6], 'v': [4.71, 89.... {'p': [6.4, 6.6, 6.8, 7, 7.2], 'v': [30.24, 2.... f 'The shape of the prices data file is { prices . shape [ 0 ] } rows and { prices . shape [ 1 ] } columns' 'The shape of the prices data file is 3937805 rows and 10 columns' # Let's have a look at the prices datafile for a distinct market and selection prices . query ( 'market_id == \"1.183995724\" and selection_id == \"22832649\"' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder 2762714 1.183995724 22832649 2021-06-01 01:38:00.062 1894 8.60 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [31.23, 37... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [63.98, 14... 2762724 1.183995724 22832649 2021-06-01 01:38:10.158 2082 8.60 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [25.54, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [37.63, 68... 2762734 1.183995724 22832649 2021-06-01 01:38:20.159 2094 8.00 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [94.52, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [37.63, 56... 2762744 1.183995724 22832649 2021-06-01 01:38:30.182 2229 8.00 7.69 27 190 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [62.88, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [22.99, 56... 2762754 1.183995724 22832649 2021-06-01 01:38:40.221 2240 8.00 7.69 136 205 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [13.92, 12... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.28, 41... 2762764 1.183995724 22832649 2021-06-01 01:38:50.923 2294 8.00 7.69 136 205 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [129.4, 53... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [67.76, 41... 2762774 1.183995724 22832649 2021-06-01 01:39:00.955 2297 8.00 7.69 137 214 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [26.93, 13... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.28, 41... 2762784 1.183995724 22832649 2021-06-01 01:39:10.962 2417 8.00 7.69 137 229 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [29.2, 146... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.26, 23... 2762794 1.183995724 22832649 2021-06-01 01:39:20.966 2677 8.18 2.68 212 243 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [19.2, 89.... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [69.9, 16.... 2762804 1.183995724 22832649 2021-06-01 01:39:30.971 2795 8.18 2.68 212 243 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [79.45, 82... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [59.89, 65... 2762814 1.183995724 22832649 2021-06-01 01:39:41.018 3039 8.18 2.68 245 324 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [37.01, 12... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [73.69, 37... 2762824 1.183995724 22832649 2021-06-01 01:39:51.119 3290 8.18 2.68 324 459 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [63.46, 94... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [19.59, 18... 2762834 1.183995724 22832649 2021-06-01 01:40:01.123 3488 8.18 2.68 324 748 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [32.81, 97... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [0.96, 15.... 2762844 1.183995724 22832649 2021-06-01 01:40:11.136 3831 8.18 2.68 447 2285 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [46.15, 99... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [20.11, 47... 2762854 1.183995724 22832649 2021-06-01 01:40:18.201 3950 8.04 7.60 327 2469 {'p': [6.8, 6.4, 5.9, 5.1, 4.4], 'v': [4.01, 3... {'p': [10, 11, 11.5, 15, 15.5], 'v': [6.05, 0.... We can see some expected behaviour as we zoom in on a particular selection The traded volume increases on this selection as we get closer to the jump The projected BSP (the near_price column) stays constant for a number of increments as its update is cached for 60 seconds at a time The sizes in the BSP pools also increases as we get closer to the jump The prices offered and traded closer to the jump are closer to the BSP than those at the start of the 2 minute period 2.2 Transform and Assemble We have our 2 core datasets, but we'd prefer to work with one now. We'd also like to add some key columns that will be reused throughout our analysis so we'll add those now too. 2.2.1 Mid points The first semi-interesting thing we'll do in this analysis is add selection mid-points to our dataset. Eventually we're going to be interested in estimating the BSP and measuring the efficiency of certain prices at various points leading up to the race. Betfair markets work like all markets with bids and spreads. The market equilibrium forms around the best price offered on either side of the market to back or to lay. These top prices each have some inherent advantage built into it for the offerer. For example in early markets the best offers on either side of the market might be really wide (say 1.80 as a best back and 2.50 as a best lay). Given the price discovery process is still immature each bidder gets a large premium, backed into their offer price, which compensates them for providing betting opportunities with little to no information provided from other market participants. This spread will naturally get tighter and tighter as the market matures and more participants seek to get volume down and must be more and more competitive. But what's the price \"equilibrium\" in each case? Well it's up to you but I'll provide you two ways of finding the central mid-point of a bid-ask spread on betfair markets. The problem we're solving for here is the non-linearity of prices in odds space. We have some intuition for this: when we see a market spread of 10-100 in early trading we have an understanding that the true midpoint of this market is somewhere around 25-35 not the 55 you'd get if you simply took the (arithmetic) mean of those two numbers. Two techniques for accounting for that non-linearity are as follows. Ladder Midpoint The ladder midpoint method takes advantage of the fact that the Betfair price ladder itself accounts for the nonlinearity of prices in odds space. The method calculated the difference in number of rungs on the Betfair ladder, halves it, and shifts the best back or lay price that number of rungs towards the centre. This will generally provide a much better idea of the market midpoint than a simple arithmetic mean of the two prices. Geometric Mean Unfortunately the ladder method is a little computationally expensive. A good approximation for this approach is to take the geometric mean of the best back and best lay values. The geometric mean is a special kind of mean that you may have never used before that is more appropriate for purposes like this. It is calculated like: sqrt(x1 * x2 * ...) . This number will also provide a much better estimate of the market midpoint than the simple arithmetic mean. The latter calculation is trivial. The former requires a suite of Betfair tick arithmetic functions that I'll put below. It may seem like overkill for this exercise (and it is) but hopefully these functions might be of use to you for other purposes. # Define the betfair tick ladder def bfTickLadder (): tickIncrements = { 1.0 : 0.01 , 2.0 : 0.02 , 3.0 : 0.05 , 4.0 : 0.1 , 6.0 : 0.2 , 10.0 : 0.5 , 20.0 : 1.0 , 30.0 : 2.0 , 50.0 : 5.0 , 100.0 : 10.0 , 1000.0 : 1000 , } ladder = [] for index , key in enumerate ( tickIncrements ): increment = tickIncrements [ key ] if ( index + 1 ) == len ( tickIncrements ): ladder . append ( key ) else : key1 = [ * tickIncrements ][ index ] key2 = [ * tickIncrements ][ index + 1 ] steps = ( key2 - key1 ) / increment for i in range ( int ( steps )): ladder . append ( round ( key + i * increment , 2 )) return ( ladder ) bfticks = bfTickLadder () # Round a decimal to the betfair tick value below def bfTickFloor ( price , includeIndex = False ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () ind = [ n for n , i in enumerate ( bfticks ) if i >= price ][ 0 ] if includeIndex : if bfticks [ ind ] == price : return (( ind , price )) else : return (( ind - 1 , bfticks [ ind - 1 ])) else : if bfticks [ ind ] == price : return ( price ) else : return ( bfticks [ ind - 1 ]) # Calculate the numder of ticks between two tick values def bfTickDelta ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) x = bfTickFloor ( p1 , includeIndex = True ) y = bfTickFloor ( p2 , includeIndex = True ) return ( x [ 0 ] - y [ 0 ]) def bfTickShift ( p , rungs ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () flr = bfTickFloor ( p , includeIndex = True ) return ( bfticks [ flr [ 0 ] + rungs ]) def bfLadderMidPoint ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) delta = - 1 * bfTickDelta ( p1 , p2 ) if delta == 1 : return ( p1 ) elif delta % 2 != 0 : return ( bfTickShift ( p1 , math . ceil ( delta / 2 ))) else : return ( bfTickShift ( p1 , math . floor ( delta / 2 ))) # Let's test a midpoint using the ladder mid point method bfLadderMidPoint ( 10 , 100 ) 25.0 # And for illustrative purposes let's calculate the geomtric mean of these values np . sqrt ( 10 * 100 ) 31.622776601683793 Let's put this all together while stitching together our two core datasets. # Join and augment df = ( selection . merge ( prices , on = [ 'market_id' , 'selection_id' ]) . assign ( sbsj = lambda x : round (( x [ 'market_time' ] - x [ 'time' ]) . dt . total_seconds () / 10 ) * 10 ) . assign ( back_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atb_ladder' ]]) . assign ( lay_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atl_ladder' ]]) . assign ( geometric_mid_point = lambda x : round ( 1 / np . sqrt (( 1 / x [ 'back_best' ]) * ( 1 / x [ 'lay_best' ])), 3 )) . assign ( ladder_mid_point = lambda x : x . apply ( lambda x : bfLadderMidPoint ( x . back_best , x . lay_best ), axis = 1 )) . replace ([ np . inf , - np . inf ], np . nan ) ) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue market_time selection_name win bsp time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder sbsj back_best lay_best geometric_mid_point ladder_mid_point 0 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:00.058 2465 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [14.63, 13... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [22.86, 12... 120.0 6.2 6.6 6.397 6.4 1 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:10.077 2848 5.93 3.17 113 238 {'p': [6, 5.9, 5.8, 5.7, 5.6], 'v': [59.93, 36... {'p': [6.4, 6.6, 6.8, 7, 7.2], 'v': [28.79, 50... 110.0 6.0 6.4 6.197 6.2 2 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:20.161 2866 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [22.91, 88... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [55.19, 22... 100.0 6.2 6.6 6.397 6.4 2.3 Analysing the BSP Before we embark on our predictive exercise let's analyse the BSP to get a feel for it as an entity. 2.3.1 Volumes Ever wondered how much volume is traded on the BSP? How does it compare to limit bets? Well with our parsed stream data we can answer those questions! Now the BSP volume will be the bigger of the BSP back stake and the lay stake (which you can infer by the final BSP and the total lay liability). # Volume Traded # _________________________ # Extract the final time slice of data which includes the total preplay volumes traded across limit and BSP poools volumeDf = df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( - 1 )[[ 'market_id' , 'selection_id' , 'bsp' , 'traded_volume' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' ]] # Infer the biggest of the two BSP stakes volumeDf = ( volumeDf . assign ( lay_stake = lambda x : x [ 'bsp_lay_pool_liability' ] / ( x [ 'bsp' ] - 1 )) . assign ( bsp_stake = lambda x : x [[ 'lay_stake' , 'bsp_back_pool_stake' ]] . max ( axis = 1 )) ) ( volumeDf . groupby ( 'market_id' , as_index = False ) . agg ({ 'traded_volume' : 'sum' , 'bsp_stake' : 'sum' }) . agg ({ 'traded_volume' : 'mean' , 'bsp_stake' : 'mean' }) ) traded_volume 98025.705018 bsp_stake 7287.524766 dtype: float64 So in an average thoroughbred market there's about 98k traded limit volume and 7,300 BSP traded stake. So approximately 7% of thoroughbred volume is traded at the BSP at least for our sample of thoroughbred races. 2.3.2 Efficiency? Now you may have heard this story before: you can't beat the BSP it's too efficient! I'm not sure people really have a firm idea about what they're talking about when they say this. Typically what you'll see in a discussion about efficiency is the predicted vs observed scatterplot. Let's see if we can reproduce this chart. First let's assemble a dataframe that we can use for this chart as well as others. What we'll do is we'll extract the BSP and a price value at 5 different slices before the race starts. We could chose any price point (we'll analyse the difference between them in a subsequent step) but for this section I'm going to take the preplay market estimate as the geometric market midpoint (you'll have to trust me for now that this is a sensible decision). # Extract the geomtric market mid point at time slices: 120, 90, 60, 30, and 0 seconds from the scheduled off preplay = df [ df . sbsj . isin ([ 120 , 90 , 60 , 30 , 0 ])][[ 'market_id' , 'selection_id' , 'win' , 'sbsj' , 'geometric_mid_point' ]] . sort_values ([ 'market_id' , 'selection_id' , 'sbsj' ], ascending = [ True , True , False ]) . rename ( columns = { 'geometric_mid_point' : 'odds' }) . assign ( type = lambda x : \"seconds before off: \" + x [ 'sbsj' ] . astype ( int ) . astype ( str )) # Extract the BSP values bsp = df . sort_values ([ 'market_id' , 'selection_id' , 'time' ], ascending = [ True , True , False ]) . groupby ([ 'market_id' , 'selection_id' ]) . head ( 1 )[[ 'market_id' , 'selection_id' , 'win' , 'sbsj' , 'bsp' ]] . rename ( columns = { 'bsp' : 'odds' }) . assign ( type = \"bsp\" ) # Append them together accuracyFrame = pd . concat ([ preplay , bsp ]) . dropna () accuracyFrame . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id win sbsj odds type 2790008 1.171091071 10693094 0 120.0 134.164 seconds before off: 120 2790011 1.171091071 10693094 0 90.0 149.666 seconds before off: 90 2790014 1.171091071 10693094 0 60.0 200.000 seconds before off: 60 2790017 1.171091071 10693094 0 30.0 239.792 seconds before off: 30 2790020 1.171091071 10693094 0 -0.0 239.165 seconds before off: 0 Now we'll filter just on our BSP records and plot the observed vs actual scatterplot # BSP Scatter # __________________ winRates = ( accuracyFrame . query ( 'type == \"bsp\"' ) . assign ( implied_chance = lambda x : round ( 20 * ( 1 / x [ 'odds' ])) / 20 ) . groupby ( 'implied_chance' , as_index = False ) . agg ({ 'win' : 'mean' }) ) fig = px . scatter ( winRates , x = \"implied_chance\" , y = \"win\" , template = \"plotly_white\" , title = \"BSP: implied win vs actual win\" ) fig . add_trace ( go . Scatter ( x = winRates . implied_chance , y = winRates . implied_chance , name = 'no bias' , line_color = 'rgba(8,61,119, 0.3)' ) ) fig . show ( \"png\" ) Ok aside from some small sample noise at the top end (there's very few horses that run at sub 1.20 BSPs) we can see that the BSP is pretty perfectly.... efficient? Is that the right word? I'd argue that it's very much not the right word. Let me illustrate with a counter example. Let's plot the same chart for the BSP as well as our 5 other price points. # Bsp + Other Odds Scatter # __________________ winRates = ( accuracyFrame . assign ( implied_chance = lambda x : round ( 20 * ( 1 / x [ 'odds' ])) / 20 ) . groupby ([ 'type' , 'implied_chance' ], as_index = False ) . agg ({ 'win' : 'mean' }) ) fig = px . scatter ( winRates , x = \"implied_chance\" , y = \"win\" , color = 'type' , template = \"plotly_white\" , title = \"Comparing Price Points: implied win vs actual win\" ) fig . add_trace ( go . Scatter ( x = winRates . implied_chance , y = winRates . implied_chance , name = 'no bias' , line_color = 'rgba(8,61,119, 0.3)' ) ) fig . show ( \"png\" ) So they're all efficient? And indecernibly as efficient as one another? Well, to cut a long and possibly boring story short this isn't the right way to measure efficiency. What we're measure here is bias . All my scatter plot here tells me is if there's any systematic bias in the BSP, i.e. groups of BSPs that aren't well calibrated with actual outcomes. That is, for example, that perhaps randomly the group of horses that BSP around 2 don't happen to win 50% of the time maybe there was a sytemic bias that short favourites were underbet and these selections actually won 55% of the time. That would be a price bias in the BSP that someone could take advatange at just by looking at historical prices and outcomes alone. For an even simpler counter point: I could create a perfectly well calibrated estimate that assigned a single odds value to every horse which was the overall horse empirical win rate over our sample: 10.25% (which is merely a reflection of field sizes). This estimate would be unbiased, and would pass through our scatterplot method unscathed but would it be an efficient estimate? Clearly not. df . agg ({ 'win' : 'mean' }) win 0.102595 dtype: float64 Bias only tells us if there's a systematic way of exploiting the odds values themselves. I could have told you that this was unlikely but the scatterplot proves it. How else could we measure efficiency? I propose using the logloss metric. Let's calculate the logloss of the BSP # Logloss ++++++++++++++++++++++++++++++++++++++++ from sklearn.metrics import log_loss # Overall Logloss # _________________ bspLoss = log_loss ( y_pred = 1 / accuracyFrame . query ( 'type == \"bsp\"' )[ 'odds' ], y_true = accuracyFrame . query ( 'type == \"bsp\"' )[ 'win' ] ) print ( f 'The overall logloss of the BSP is { round ( bspLoss , 4 ) } ' ) The overall logloss of the BSP is 0.2757 Ok what does this mean? Well nothing really. This metric won't tell you anything by itself it's just useful for relative comparisons. Let's plot the logloss of our geometric midpoint at our various timeslices. # Logloss at Different Time Points # _________________ accuracyFrame . groupby ( 'type' , as_index = False ) . apply ( lambda x : log_loss ( y_pred = 1 / x [ 'odds' ], y_true = x [ 'win' ])) . rename ( columns = { None : 'logloss' }) . sort_values ( 'logloss' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type logloss 0 bsp 0.275679 1 seconds before off: 0 0.275824 3 seconds before off: 30 0.275999 4 seconds before off: 60 0.276044 5 seconds before off: 90 0.276206 2 seconds before off: 120 0.276256 # And in chart form fig = px . bar ( accuracyFrame . groupby ( 'type' , as_index = False ) . apply ( lambda x : log_loss ( y_pred = 1 / x [ 'odds' ], y_true = x [ 'win' ])) . rename ( columns = { None : 'logloss' }) . sort_values ( 'logloss' , ascending = False ), x = \"type\" , y = \"logloss\" , template = \"plotly_white\" , title = \"Logloss Of Odds At Various Time Points\" ) fig . update_yaxes ( range = [ .2755 , .2765 ]) fig . show ( \"png\" ) Now this is a cool graph. This is exactly like we would have intiuited. The market sharpens monotonically as we approach the market jump with the BSP being the most effiecient of all the prices! Hopefully you can now see the logical failing of measuring bias over market efficiency and it changes the way you think about your bet placement. Let's move on to what we're here for: is it possible to predict the BSP. 2.4 Predicting the BSP Ok so I'm interested in finding the answer to the question: which estimate of BSP should I use when betting on the exchange and is it possible to beat the projected SP provided on the website and through the API? Well the first thing we should recognise about this projection is that it's cached. What does that mean? It means it only updated every 60 seconds. This suprised me when I first learned it and it was actually causing issues in my bet placement logic for the SP. Let's have a look at a selection to see how this works in practice # Lets take a sample of a market and a selection dSlice = df . query ( 'market_id == \"1.182394184\" and selection_id == \"39243409\"' ) . dropna () def chartClosingPrices ( d ): fig = px . line ( pd . melt ( d [: - 1 ][[ 'sbsj' , 'back_best' , 'near_price' ]], id_vars = 'sbsj' , var_name = 'price' ), x = 'sbsj' , y = 'value' , color = 'price' , template = 'plotly_white' , title = \"Selection\" , labels = { 'sbsj' : \"Seconds Before Scheduled Jump\" } ) fig . update_layout ( font_family = \"Roboto\" ) fig . add_trace ( go . Line ( x = dSlice . sbsj , y = dSlice . bsp , name = 'BSP' , line_color = 'rgba(8,61,119, 0.3)' , mode = \"lines\" ) ) fig [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig . show ( \"png\" ) chartClosingPrices ( dSlice ) /home/tmbish/.local/lib/python3.9/site-packages/plotly/graph_objs/_deprecations.py:378: DeprecationWarning: plotly.graph_objs.Line is deprecated. Please replace it with one of the following more specific types - plotly.graph_objs.scatter.Line - plotly.graph_objs.layout.shape.Line - etc. The red line is the projected BSP, you can see that it's not very responsive. As the best back price comes in from ~3 to 2.9 leading up to the jump the projected SP doesn't move because it's cached. If you were relying on this number for something important and you were using it in that period you were using stale information and you'd be worse off for it. In this instance the final SP was 2.79 so you may have made the wrong betting decision. This is somewhat counter intuitive because the projected sp (the so called near price) should be a good estimate of the BSP because it synthetically runs the BSP algorithm on the current market state and produces and estimate, so you would think that it'd be a pretty good estimate. Let's widen our sample a bit and see how it performs across our entire sample. We'll slice the data at the exact scheduled off and see how accurate various price points are at predicting what the final BSP is. We'll use mean absolute error (MAE) as our error metric. We'll assess 6 price points: The near price (projected sp) The far price (projected sp excluding limit orders) The best back price The best lay price The ladder midpoint price The geometric midpoint price # Measurement # ________________________ estimatesDf = df [ df . sbsj == 0 ][[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] ( pd . melt ( estimatesDf , id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } estimate error 0 back_best 0.091702 3 ladder_mid_point 0.093181 2 geometric_mid_point 0.094405 4 lay_best 0.103142 5 near_price 0.121266 1 far_price 0.578425 So a bit surprisingly, in thoroughbred markets at the scheduled off your best to just use the current best back price as your estimate of the BSP. It significantly outperforms the projected SP and even some of our midpoint methods. Let's change the timeslice a little and take the very last moment before the market settles and see which performs best. lastEstimatesDf = df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( - 1 )[[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] ( pd . melt ( lastEstimatesDf , id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } estimate error 2 geometric_mid_point 0.039627 3 ladder_mid_point 0.042526 0 back_best 0.063622 5 near_price 0.077108 4 lay_best 0.098198 1 far_price 0.290777 First thing to notice is the estimates get a lot better than at the scheduled off, as we'd expect. A bit surprisingly the projected SP is still very weak due to the caching issue. In this scenario the geometric mid point beforms significantly better than the current best back price which suggests that as the late market is forming the back and lay spread with start converging to the fair price and eventual BSP. I personally use the geometric midpoint as my BSP estimate as it's a quick and easy metric that performs pretty well. What if you want more though? Is it possible to do better than these metrics? These simple price points use no information about what's in the BSP pools, surely if we used this information we'd be able to do better. Let's try to use machine learning to synthesise all this information at once. 2.3.4 Machine Learning We'll build a quick random forest model to estimate the BSP with current price and pool size information. This is a very simple application of machine learning so hopefully gives you an idea of its power without being too complex. Now we need an intelligent way of turning our pool and ladder information into a feature to insert into our model, how could we engineer this feature? Well what we'll do is calculate a WAP required to fill our pool stake on the back and lay side. What does that mean? Say we've got \\$200 sitting in the BSP back pool and \\$200 sitting on the top box of 2.5 on the back side, in this instance our WAP value would be exactly 2.5 cause we can fill it all at the top box. But if however, there was only $100 in the top box then we'd need to move down the ladder to fill the remaining \\$100 volume. Our feature will simulate this allocation logic and return the final weighted average price required to fill the total BSP pool. Here's the functions to do it on the back and lay side respectively: def wapToGetBack ( pool , ladder ): price = ladder [ 'p' ] volume = ladder [ 'v' ] try : indmax = min ([ i for ( i , j ) in enumerate ( cVolume ) if j > pool ]) + 1 except : indmax = len ( volume ) return ( round ( sum ([ a * b for a , b in zip ( price [: indmax ], volume [: indmax ])]) / sum ( volume [: indmax ]), 4 )) def wapToGetLay ( liability_pool , ladder ): price = ladder [ 'p' ] volume = ladder [ 'v' ] liability = [( a - 1 ) * b for a , b in zip ( price , volume )] cLiability = np . cumsum ( liability ) try : indmax = min ([ i for ( i , j ) in enumerate ( cLiability ) if j > liability_pool ]) + 1 except : indmax = len ( volume ) return ( round ( sum ([ a * b for a , b in zip ( price [: indmax ], volume [: indmax ])]) / sum ( volume [: indmax ]), 4 )) Now we'll set up our model matrix which will be the market state at the exact scheduled off. We'll also add our custom features. model_matrix = df [[ 'sbsj' , 'atb_ladder' , 'atl_ladder' , 'bsp' , 'traded_volume' , 'near_price' , 'far_price' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] # Filter at scheduled jump model_matrix = model_matrix [ model_matrix . sbsj == 0 ] . dropna () model_matrix = ( model_matrix . assign ( wap_to_get_back_pool = lambda x : x . apply ( lambda x : wapToGetBack ( x . bsp_back_pool_stake , x . atb_ladder ), axis = 1 )) . assign ( wap_to_get_lay_pool = lambda x : x . apply ( lambda x : wapToGetLay ( x . bsp_lay_pool_liability , x . atl_ladder ), axis = 1 )) ) # Drop other columns model_matrix . drop ( columns = [ 'sbsj' , 'atb_ladder' , 'atl_ladder' ], inplace = True ) model_matrix . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bsp traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability back_best lay_best geometric_mid_point ladder_mid_point wap_to_get_back_pool wap_to_get_lay_pool 12 6.20 6891 5.74 4.22 518 1580 6.00 6.2 6.099 6.00 5.7762 6.3731 28 3.60 13579 3.57 1.73 1023 1771 3.45 3.6 3.524 3.55 3.3010 3.7007 44 6.62 5911 5.81 1.59 845 1378 6.20 6.6 6.397 6.40 5.9156 7.1167 Now the machine learning. Sklearn makes this very simple, in our case it's a few lines only. We'll split our data into train and test sets and train a small random forrest to predict the BSP. from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split # Setup Train / Test train_features , test_features , train_labels , test_labels = train_test_split ( model_matrix . drop ( columns = [ 'bsp' ]), model_matrix [ 'bsp' ], test_size = 0.25 ) print ( 'Training Features Shape:' , train_features . shape ) print ( 'Training Labels Shape:' , train_labels . shape ) print ( 'Testing Features Shape:' , test_features . shape ) print ( 'Testing Labels Shape:' , test_labels . shape ) # Instantiate Model rf = RandomForestRegressor ( n_estimators = 100 ) # Train Model rf . fit ( train_features , train_labels ) Training Features Shape: (119822, 11) Training Labels Shape: (119822,) Testing Features Shape: (39941, 11) Testing Labels Shape: (39941,) RandomForestRegressor() Let's check out our predictions on the test set (remember our model hasn't seen any of this data so it should be a true reflection on how we'd perform on some new races that would happen this afternoon say) # Use the forest's predict method on the test data predicted_bsp = rf . predict ( test_features ) predicted_bsp array([268.9971, 4.9892, 24.2727, ..., 29.067 , 4.6216, 16.3991]) Seems reasonable. All well and good though is the prediction any good? Let's measure it using MAE in the same way as we did before. # Let's test our estimate vs our others in the same way as before testDf = test_features testDf [ 'bsp' ] = test_labels testDf [ 'rf_bsp_prediction' ] = predicted_bsp ( pd . melt ( testDf [[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' , 'rf_bsp_prediction' ]], id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } estimate error 6 rf_bsp_prediction 0.088898 0 back_best 0.091860 3 ladder_mid_point 0.093477 2 geometric_mid_point 0.094702 4 lay_best 0.103435 5 near_price 0.121456 1 far_price 0.575946 Nice that's significantly better than the best previous estimate at this time slice. To validate it further let's use the same model to predict the BSP using the market state 10 seconds after the scheduled jump instead of at the exact scheduled off. None of the rows (or samples) in this time slice have been seen by the model during the training step so it should provide a robust out of sample estimate of the models performance on unseen data. # Validate it on a completely different time point - 10 seconds after scheduled jump outOfSample = df [[ 'sbsj' , 'atb_ladder' , 'atl_ladder' , 'bsp' , 'traded_volume' , 'near_price' , 'far_price' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] outOfSample = outOfSample [ outOfSample . sbsj == - 10 ] . dropna () outOfSample = ( outOfSample . assign ( wap_to_get_back_pool = lambda x : x . apply ( lambda x : wapToGetBack ( x . bsp_back_pool_stake , x . atb_ladder ), axis = 1 )) . assign ( wap_to_get_lay_pool = lambda x : x . apply ( lambda x : wapToGetLay ( x . bsp_lay_pool_liability , x . atl_ladder ), axis = 1 )) ) # Produce Predictions outofsamplebspprediction = rf . predict ( outOfSample . drop ( columns = [ 'bsp' , 'sbsj' , 'atb_ladder' , 'atl_ladder' ])) outofsamplebspprediction array([ 6.395 , 3.5781, 6.3449, ..., 503.9829, 220.1171, 54.7511]) outOfSample [ 'rf_bsp_prediction' ] = outofsamplebspprediction ( pd . melt ( outOfSample [[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' , 'rf_bsp_prediction' ]], id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } estimate error 6 rf_bsp_prediction 0.079198 0 back_best 0.084658 3 ladder_mid_point 0.086128 2 geometric_mid_point 0.087326 4 lay_best 0.098311 5 near_price 0.109640 1 far_price 0.501813 Still significantly better on the out of sample set which is a really positive sign. 2.3.5 Next Steps To improve this model I'd include multiple time slices in the training sample and use the seconds before scheduled jump as a feature as I would estimate that the predictive dynamics of each of these features is dynamic and affected by how mature + how close to settlement the market is. To implement this model in your bet placement code you'd simply need to save the model object (some info about how to do this with sklearn can be found here here ). Your key challenge will be making sure you can produce the exact inputs you've created in this development process from the live stream or polling API responses, but if you've gotten this far it won't be a huge challenge for you. 3.0 Conclusion I've taken you through a quick crash course in the Betfair BSP including: What it is How it's created How it's traded on betfair Australian thoroughbred markets How efficient it is and a methodology for measuring its efficiency in different contexts The accuracy of the projected SP and how it compares with other estimates How to build your own custom projection that's better than anything available out of the box The analysis focused on thoroughbred markets but could easily be extended to other racing codes or sports markets that have BSP enabled. The custom SP projection methodology could be used for anything from staking your model more accurately or with some improvement maybe as part of a automated trading strategy. 3.1 Over to you We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support - There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space 3.2 Complete code Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github # %% [markdown] # # Analysing and Predicting The BSP # # # ## 0.1 Setup # # Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language. # # Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files. # # You'll also need `betfairlightweight` which you can install with something like `pip install betfairlightweight`. # %% import pandas as pd import numpy as np import requests import os import re import csv import plotly.express as px import plotly.graph_objects as go import math import logging import yaml import csv import tarfile import zipfile import bz2 import glob import ast from datetime import date , timedelta from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) # %% [markdown] # ## 0.2 Context # # The BSP is betting product offered by betfair (on large enough markets) that gives customers a chance to back or lay any selection at a \"fair\" price. Without getting too complex too quickly, the BSP allows you lock in a bet at any time after the market is openened and for as much stake as you can afford. The BSP is a good option for many different segments of customers: # # - Recreational punters that don't have a particular strategy for trying to get the best odds can lock in a price that is (in the aggregate) a lot better than what they'd get at a corporate book or they'd get by taking limit bets early in a market's trading # - Automated customers that don't want the hassle of managing live market trading can implement automated strategies a lot easier whilst also protecting them from edge cases like race reschedules # - Is perfect for simply backtesting fundemental models as it's a resiliant and robust single price # # Despite it being a good option for a lot of customers it's also a fairly contraversial topic for some other types of customers. Some people firmly believe that the BSP on big markets reflects the \"true chance\" of a selection so betting it is a fools errand that will simply lose you commission over the long run. You might have heard a version of this story before: given the large pool sizes, the 0% overround, the settlement at the exact moment the market is suspended the BSP perfectly synthesises all available public information and demand and arrives at a true fair odds. Some will attempt to prove this to you by showing you a predicted chance vs observed win rate scatterplot which shows a perfect correlation between chance implied by the BSP and a horses true chance. Whilst I don't disagree that the BSP is a **very strong** estimate of a selections chance it's pretty obviously not perfect. # # Furthermore, it presents some other tricky challenges to use in practical situations. It's not knowable perfectly before it's the exact moment of market suspension so many model or strategy builders make the mistake of unknowingly leaking it into their preplay model development or their theoretical staking calculations. Where the final number will land is actually another source of uncertainty in your processes which presents anothing forecasting / predictive modelling application as I'll explore later in this piece. I'll take you through how I'd measure the accuracy of the BSP, show you how it's traded on the exchange, and take you through a host of methods of estimating the BSP and build a custom machine learning approach that's better than each of them. # # ## 0.3 The Algorithm # # The actual logic of how betfair arrives at the final BSP number is quite complex and for a few reasons you won't be able to perfectly replicate it at home. However, the general gist of the BSP reconciliation algorithm that is executed just as the market suspended goes something like: # # - The algorithm combines 4 distinct groups of open bets for a given selection: # + Non price limited BSP orders on both the back and lay side (`market_on_close` orders) # + Price limited orders on both the back and lay side (`limit_on_close` orders) # + All non filled open lay orders # + All non filled open back orders # - It then combines them all together, passes a sophisticated balancing algorithm over the top of them and arrives at a single fair price for the BSP that balances the demand on either side of the ledger # # ## 0.4 This Example # # For this exercise we'll again take advantage of the betfair historical stream json files. The slice of betfair markets I'll be analysing is all thoroughbred races over July 2020 - June 2021. # # As an aside the projected BSP number you see on the betfair website isn't collected inside betfair's own internal database of orders, so any custom data request you may be able to get as a VIP won't include this number. So if you were planning to include it in any kind of projection or bet placement logic operation you were making the only way to anlayse it historically is to mine these data files. Another good reason to learn the skills to do so! # %% [markdown] # # 1.0 Data # # Like the previous tutorial we won't be able to collapse the stream data down into a single row per runner because I'm interested in anlaysing how the projected BSP moves late in betfair markets. I'm also interested in plotting the efficiency of certain odds values at certain distinct time points leading up the the races so I need multiple records per runner. # # Like in the previous tutorial I'll split out the selection metadata, BSP and win flag values as a seperate data file to reduce the size of the datafiles extracted for this analysis. # # For the preplay prices dataset I'll: # # - Start extraction at 2 mins before the scheduled off # - Extract prices every 10 seconds thereafter until the market is suspended # - I'll also extract the final market state the instant before the market is suspended # # ## 1.1 Sourcing Data # # First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Aask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder. # # ## 1.2 Utility functions # # First like always we'll need some general utility functions that you may have seen before: # %% # General Utility Functions # _________________________________ def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def pull_ladder ( availableLadder , n = 5 ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : n ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"p\" ] = price out [ \"v\" ] = volume return ( out ) def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # %% [markdown] # ## 1.3 Selection Metadata # # Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes. # %% def final_market_book ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): return ( None ) for market_book in market_books : last_market_book = market_book return ( last_market_book ) def parse_final_selection_meta ( dir , out_file ): with open ( out_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,venue,market_time,selection_name,win,bsp \\n \" ) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) last_market_book = final_market_book ( stream ) if last_market_book is None : continue # Extract Info ++++++++++++++++++++++++++++++++++ runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in last_market_book . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'win' : np . where ( r . status == \"WINNER\" , 1 , 0 ), 'sp' : r . sp . actual_sp } for r in last_market_book . runners ] # Return Info ++++++++++++++++++++++++++++++++++ for runnerMeta in runnerMeta : if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( last_market_book . market_id ), runnerMeta [ 'selection_id' ], last_market_book . market_definition . venue , last_market_book . market_definition . market_time , runnerMeta [ 'selection_name' ], runnerMeta [ 'win' ], runnerMeta [ 'sp' ] ) ) # %% selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) print ( \"__ Parsing Selection Metadata ___ \" ) # parse_final_selection_meta(stream_files, selection_meta) # %% [markdown] # ## 1.4 Preplay Prices and Projections # # In this set of preplay prices I'm interested in many of the same fields as we've extracted in previous tutorials as well as fields relating to the current state of the BSP. # # These objects sit under the `sp` slot within the returned `runner` object. The fields we'll extract are: # # - The so called \"near price\" # + The near price is the projected SP value you can see on the website # + It includes both bets already placed into the SP pools as well as open limit orders to estimate what the final BSP value will be # - The so called \"far price\" # + This is the same as the near price except it excludes limit orders on the exchange # + This makes it fairly redundant value and we'll see how poor of an estimator it is a bit later # - The volume currently bet into the BSP back pool # - The liability currently laid into the BSP lay pool # # We'll also extract the top 5 rungs of the available to back and available to lay ladders as well as the traded volume of limit bets. # # It's worth noting that I am discarding some key information about the BSP pools that I could have extracted if I wanted to. The current SP bets are laid out in a way that I could split out `limit_on_close` as well as `market_on_close` sp bets but I've rolled everything together in SP stake on the back side and sp liability on the lay side. This is just to reduce complexity of this article but including it would increase the predictive power of the BSP model in the final step. # %% def loop_preplay_prices ( s , o ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () marketID = None tradeVols = None time = None last_book_recorded = False prev_book = None for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): break for market_book in market_books : # Time Step Management ++++++++++++++++++++++++++++++++++ if marketID is None : # No market initialised marketID = market_book . market_id time = market_book . publish_time elif market_book . inplay and last_book_recorded : break else : seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () if seconds_to_start > 120 : # Too early before off to start logging prices prev_book = market_book continue else : # Update data at different time steps depending on seconds to off wait = 10 # New Market if market_book . market_id != marketID : last_book_recorded = False marketID = market_book . market_id time = market_book . publish_time continue # (wait) seconds elapsed since last write elif ( market_book . publish_time - time ) . total_seconds () > wait : time = market_book . publish_time # if current marketbook is inplay want to record the previous market book as it's the last preplay marketbook elif market_book . inplay : last_book_recorded = True market_book = prev_book # fewer than (wait) seconds elapsed continue to next loop else : prev_book = market_book continue # Execute Data Logging ++++++++++++++++++++++++++++++++++ for runner in market_book . runners : try : atb_ladder = pull_ladder ( runner . ex . available_to_back , n = 5 ) atl_ladder = pull_ladder ( runner . ex . available_to_lay , n = 5 ) except : atb_ladder = {} atl_ladder = {} limitTradedVol = sum ([ rung . size for rung in runner . ex . traded_volume ]) o . writerow ( ( market_book . market_id , runner . selection_id , market_book . publish_time , int ( limitTradedVol ), # SP Fields runner . sp . near_price , runner . sp . far_price , int ( sum ([ ps . size for ps in runner . sp . back_stake_taken ])), int ( sum ([ ps . size for ps in runner . sp . lay_liability_taken ])), # Limit bets available str ( atb_ladder ) . replace ( ' ' , '' ), str ( atl_ladder ) . replace ( ' ' , '' ) ) ) prev_book = market_book def parse_preplay_prices ( dir , out_file ): with open ( out_file , \"w+\" ) as output : writer = csv . writer ( output , delimiter = ',' , lineterminator = ' \\r\\n ' , quoting = csv . QUOTE_ALL ) writer . writerow (( \"market_id\" , \"selection_id\" , \"time\" , \"traded_volume\" , \"near_price\" , \"far_price\" , \"bsp_back_pool_stake\" , \"bsp_lay_pool_liability\" , \"atb_ladder\" , 'atl_ladder' )) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) loop_preplay_prices ( stream , writer ) # %% price = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) print ( \"__ Parsing Selection Prices ___ \" ) # parse_final_selection_meta(stream_files, price) # %% [markdown] # # 2.0 Analysis # # First step let's boot up the datasets we extracted in the previous steps and take a look at what we've managed to extract from the raw stream files. # # ## 2.1 Load and Inspect # # First we have the highlevel selection metadata as we have already seen in other tutorials # %% selection = pd . read_csv ( \"[PATH TO YOUR SELECTION METADATA FILE]\" , dtype = { 'market_id' : object , 'selection_id' : object }, parse_dates = [ 'market_time' ]) selection . head ( 3 ) # %% [markdown] # Now let's load the prices file. We'll apply some extra logic to parse the ladder columns into dictionaries and also remove the first odds record per group as it's the first record as the market was instantiated. # %% prices = pd . read_csv ( \"[PATH TO YOUR PRICES FILE]\" , quoting = csv . QUOTE_ALL , dtype = { 'market_id' : 'string' , 'selection_id' : 'string' , 'atb_ladder' : 'string' , 'atl_ladder' : 'string' }, parse_dates = [ 'time' ] ) # Parse ladder columns prices [ 'atb_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atb_ladder' ]] prices [ 'atl_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atl_ladder' ]] # Drop the first row within each group prices = prices . drop ( prices . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( 0 ) . index ) prices . head ( 3 ) # %% f 'The shape of the prices data file is { prices . shape [ 0 ] } rows and { prices . shape [ 1 ] } columns' # %% # Let's have a look at the prices datafile for a distinct market and selection prices . query ( 'market_id == \"1.183995724\" and selection_id == \"22832649\"' ) # %% [markdown] # We can see some expected behaviour as we zoom in on a particular selection # # - The traded volume increases on this selection as we get closer to the jump # - The projected BSP (the `near_price` column) stays constant for a number of increments as its update is cached for 60 seconds at a time # - The sizes in the BSP pools also increases as we get closer to the jump # - The prices offered and traded closer to the jump are closer to the BSP than those at the start of the 2 minute period # %% [markdown] # ## 2.2 Transform and Assemble # # We have our 2 core datasets, but we'd prefer to work with one now. We'd also like to add some key columns that will be reused throughout our analysis so we'll add those now too. # # # ### 2.2.1 Mid points # # The first semi-interesting thing we'll do in this analysis is add selection mid-points to our dataset. Eventually we're going to be interested in estimating the BSP and measuring the efficiency of certain prices at various points leading up to the race. # # Betfair markets work like all markets with bids and spreads. The market equilibrium forms around the best price offered on either side of the market to back or to lay. These top prices each have some inherent advantage built into it for the offerer. For example in early markets the best offers on either side of the market might be really wide (say 1.80 as a best back and 2.50 as a best lay). Given the price discovery process is still immature each bidder gets a large premium, backed into their offer price, which compensates them for providing betting opportunities with little to no information provided from other market participants. This spread will naturally get tighter and tighter as the market matures and more participants seek to get volume down and must be more and more competitive. But what's the price \"equilibrium\" in each case? # # Well it's up to you but I'll provide you two ways of finding the central mid-point of a bid-ask spread on betfair markets. The problem we're solving for here is the non-linearity of prices in odds space. We have some intuition for this: when we see a market spread of 10-100 in early trading we have an understanding that the true midpoint of this market is somewhere around 25-35 not the 55 you'd get if you simply took the (arithmetic) mean of those two numbers. # # Two techniquest for accounting for that non-linearity are as follows. # # **Ladder Midpoint** # # The ladder midpoint method takes advantage of the fact that the betfair price ladder itself accounts for the nonlinearity of prices in odds space. The method calculated the difference in number of rungs on the betfair ladder, halves it, and shifts the best back or lay price that number of rungs towards the centre. This will generally provide a much better idea of the market midpoint than a simple arithmetic mean of the two prices. # # **Geometric Mean** # # Unfortunately the ladder method is a little computationally expensive. A good approximation for this approach is to take the geometric mean of the best back and best lay values. The geometric mean is a special kind of mean that you may have never used before that is more appropriate for purposes like this. It is calculated like: `sqrt(x1 * x2 * ...)`. This number will also provide a much better estimate of the market midpoint than the simple arithmetic mean. # # The latter calculation is trivial. The former requires a suite of betfair tick arithmetic functions that I'll put below. It may seem like overkill for this exercise (and it is) but hopefully these functions might be of use to you for other purposes. # %% # Define the betfair tick ladder def bfTickLadder (): tickIncrements = { 1.0 : 0.01 , 2.0 : 0.02 , 3.0 : 0.05 , 4.0 : 0.1 , 6.0 : 0.2 , 10.0 : 0.5 , 20.0 : 1.0 , 30.0 : 2.0 , 50.0 : 5.0 , 100.0 : 10.0 , 1000.0 : 1000 , } ladder = [] for index , key in enumerate ( tickIncrements ): increment = tickIncrements [ key ] if ( index + 1 ) == len ( tickIncrements ): ladder . append ( key ) else : key1 = [ * tickIncrements ][ index ] key2 = [ * tickIncrements ][ index + 1 ] steps = ( key2 - key1 ) / increment for i in range ( int ( steps )): ladder . append ( round ( key + i * increment , 2 )) return ( ladder ) bfticks = bfTickLadder () # Round a decimal to the betfair tick value below def bfTickFloor ( price , includeIndex = False ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () ind = [ n for n , i in enumerate ( bfticks ) if i >= price ][ 0 ] if includeIndex : if bfticks [ ind ] == price : return (( ind , price )) else : return (( ind - 1 , bfticks [ ind - 1 ])) else : if bfticks [ ind ] == price : return ( price ) else : return ( bfticks [ ind - 1 ]) # Calculate the numder of ticks between two tick values def bfTickDelta ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) x = bfTickFloor ( p1 , includeIndex = True ) y = bfTickFloor ( p2 , includeIndex = True ) return ( x [ 0 ] - y [ 0 ]) def bfTickShift ( p , rungs ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () flr = bfTickFloor ( p , includeIndex = True ) return ( bfticks [ flr [ 0 ] + rungs ]) def bfLadderMidPoint ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) delta = - 1 * bfTickDelta ( p1 , p2 ) if delta == 1 : return ( p1 ) elif delta % 2 != 0 : return ( bfTickShift ( p1 , math . ceil ( delta / 2 ))) else : return ( bfTickShift ( p1 , math . floor ( delta / 2 ))) # %% # Let's test a midpoint using the ladder mid point method bfLadderMidPoint ( 10 , 100 ) # %% # And for illustrative purposes let's calculate the geomtric mean of these values np . sqrt ( 10 * 100 ) # %% [markdown] # Let's put this all together while stitching together our two core datasets. # %% # Join and augment df = ( selection . merge ( prices , on = [ 'market_id' , 'selection_id' ]) . assign ( sbsj = lambda x : round (( x [ 'market_time' ] - x [ 'time' ]) . dt . total_seconds () / 10 ) * 10 ) . assign ( back_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atb_ladder' ]]) . assign ( lay_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atl_ladder' ]]) . assign ( geometric_mid_point = lambda x : round ( 1 / np . sqrt (( 1 / x [ 'back_best' ]) * ( 1 / x [ 'lay_best' ])), 3 )) . assign ( ladder_mid_point = lambda x : x . apply ( lambda x : bfLadderMidPoint ( x . back_best , x . lay_best ), axis = 1 )) . replace ([ np . inf , - np . inf ], np . nan ) ) df . head ( 3 ) # %% [markdown] # ## 2.3 Analysing The BSP # # Before we embark on our predictive exercise let's analyse the BSP to get a feel for it as an entity. # # ### 2.3.1 Volumes # # Ever wondered how much volume is traded on the BSP? How does it compare to limit bets? Well with our parsed stream data we can answer those questions! Now the BSP volume will be the bigger of the BSP back stake and the lay stake (which you can infer by the final BSP and the total lay liability). # # # %% # Volume Traded # _________________________ # Extract the final time slice of data which includes the total preplay volumes traded across limit and BSP poools volumeDf = df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( - 1 )[[ 'market_id' , 'selection_id' , 'bsp' , 'traded_volume' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' ]] # Infer the biggest of the two BSP stakes volumeDf = ( volumeDf . assign ( lay_stake = lambda x : x [ 'bsp_lay_pool_liability' ] / ( x [ 'bsp' ] - 1 )) . assign ( bsp_stake = lambda x : x [[ 'lay_stake' , 'bsp_back_pool_stake' ]] . max ( axis = 1 )) ) ( volumeDf . groupby ( 'market_id' , as_index = False ) . agg ({ 'traded_volume' : 'sum' , 'bsp_stake' : 'sum' }) . agg ({ 'traded_volume' : 'mean' , 'bsp_stake' : 'mean' }) ) # %% [markdown] # So in an average thoroughbred market there's about 98k traded limit volume and 7,300 BSP traded stake. So approximately 7% of thoroughbred volume is traded at the BSP at least for our sample of thoroughbred races. # # # ## 2.3.2 Efficiency? # # Now you may have heard this story before: **you can't beat the BSP it's too efficient!**. I'm not sure people really have a firm idea about what they're talking about when they say this. # # Typically what you'll see in a discussion about efficiency is the predicted vs observed scatterplot. Let's see if we can reproduce this chart. # # First let's assemble a dataframe that we can use for this chart as well as others. What we'll do is we'll extract the BSP and a price value at 5 different slices before the race starts. We could chose any price point (we'll analyse the difference between them in a subsequent step) but for this section I'm going to take the preplay market estimate as the geometric market midpoint (you'll have to trust me for now that this is a sensible decision). # %% # Extract the geomtric market mid point at time slices: 120, 90, 60, 30, and 0 seconds from the scheduled off preplay = df [ df . sbsj . isin ([ 120 , 90 , 60 , 30 , 0 ])][[ 'market_id' , 'selection_id' , 'win' , 'sbsj' , 'geometric_mid_point' ]] . sort_values ([ 'market_id' , 'selection_id' , 'sbsj' ], ascending = [ True , True , False ]) . rename ( columns = { 'geometric_mid_point' : 'odds' }) . assign ( type = lambda x : \"seconds before off: \" + x [ 'sbsj' ] . astype ( int ) . astype ( str )) # Extract the BSP values bsp = df . sort_values ([ 'market_id' , 'selection_id' , 'time' ], ascending = [ True , True , False ]) . groupby ([ 'market_id' , 'selection_id' ]) . head ( 1 )[[ 'market_id' , 'selection_id' , 'win' , 'sbsj' , 'bsp' ]] . rename ( columns = { 'bsp' : 'odds' }) . assign ( type = \"bsp\" ) # Append them together accuracyFrame = pd . concat ([ preplay , bsp ]) . dropna () accuracyFrame . head ( 5 ) # %% [markdown] # Now we'll filter just on our BSP records and plot the observed vs actual scatterplot # %% # BSP Scatter # __________________ winRates = ( accuracyFrame . query ( 'type == \"bsp\"' ) . assign ( implied_chance = lambda x : round ( 20 * ( 1 / x [ 'odds' ])) / 20 ) . groupby ( 'implied_chance' , as_index = False ) . agg ({ 'win' : 'mean' }) ) fig = px . scatter ( winRates , x = \"implied_chance\" , y = \"win\" , template = \"plotly_white\" , title = \"BSP: implied win vs actual win\" ) fig . add_trace ( go . Scatter ( x = winRates . implied_chance , y = winRates . implied_chance , name = 'no bias' , line_color = 'rgba(8,61,119, 0.3)' ) ) fig . show ( \"png\" ) # %% [markdown] # Ok aside from some small sample noise at the top end (there's very few horses that run at sub 1.20 BSPs) we can see that the BSP is pretty perfectly.... efficient? Is that the right word? I'd argue that it's very much not the right word. Let me illustrate with a counter example. Let's plot the same chart for the BSP as well as our 5 other price points. # %% # Bsp + Other Odds Scatter # __________________ winRates = ( accuracyFrame . assign ( implied_chance = lambda x : round ( 20 * ( 1 / x [ 'odds' ])) / 20 ) . groupby ([ 'type' , 'implied_chance' ], as_index = False ) . agg ({ 'win' : 'mean' }) ) fig = px . scatter ( winRates , x = \"implied_chance\" , y = \"win\" , color = 'type' , template = \"plotly_white\" , title = \"Comparing Price Points: implied win vs actual win\" ) fig . add_trace ( go . Scatter ( x = winRates . implied_chance , y = winRates . implied_chance , name = 'no bias' , line_color = 'rgba(8,61,119, 0.3)' ) ) fig . show ( \"png\" ) # %% [markdown] # So they're all efficient? And indecernibly as efficient as one another? # # Well, to cut a long and possibly boring story short this isn't the right way to measure efficiency. What we're measure here is **bias**. All my scatter plot here tells me is if there's any systematic bias in the BSP, ie groups of BSPs that aren't well calibrated with actual outcomes. That is, for example, that perhaps randomly the group of horses that BSP around 2 don't happen to win 50% of the time maybe there was a sytemic bias that short favourites were underbet and these selections actually won 55% of the time. That would be a price bias in the BSP that someone could take advatange at just by looking at historical prices and outcomes alone. # # For and even simpler counter point: I could create a perfectly well calibrated estimate that assigned a single odds value to every horse which was the overall horse empirical win rate over our sample: 10.25% (which is merely a reflection of field sizes). This estimate would be unbiased, and would pass through our scatterplot method unscathed but would it be an efficient estimate? Clearly not. # %% df . agg ({ 'win' : 'mean' }) # %% [markdown] # Bias only tells us if there's a systematic way of exploiting the odds values themselves. I could have told you that this was unlikely but the scatterplot proves it. # # How else could we measure efficiency? I propose using the `logloss` metric. # # Let's calculate the logloss of the BSP # %% # Logloss ++++++++++++++++++++++++++++++++++++++++ from sklearn.metrics import log_loss # Overall Logloss # _________________ bspLoss = log_loss ( y_pred = 1 / accuracyFrame . query ( 'type == \"bsp\"' )[ 'odds' ], y_true = accuracyFrame . query ( 'type == \"bsp\"' )[ 'win' ] ) print ( f 'The overall logloss of the BSP is { round ( bspLoss , 4 ) } ' ) # %% [markdown] # Ok what does this mean? Well nothing really. This metric won't tell you anything by itself it's just useful for relative comparisons. Let's plot the logloss of our geometric midpoint at our various timeslices. # # %% # Logloss at Different Time Points # _________________ accuracyFrame . groupby ( 'type' , as_index = False ) . apply ( lambda x : log_loss ( y_pred = 1 / x [ 'odds' ], y_true = x [ 'win' ])) . rename ( columns = { None : 'logloss' }) . sort_values ( 'logloss' ) # %% # And in chart form fig = px . bar ( accuracyFrame . groupby ( 'type' , as_index = False ) . apply ( lambda x : log_loss ( y_pred = 1 / x [ 'odds' ], y_true = x [ 'win' ])) . rename ( columns = { None : 'logloss' }) . sort_values ( 'logloss' , ascending = False ), x = \"type\" , y = \"logloss\" , template = \"plotly_white\" , title = \"Logloss Of Odds At Various Time Points\" ) fig . update_yaxes ( range = [ .2755 , .2765 ]) fig . show ( \"png\" ) # %% [markdown] # Now this is a cool graph. This is exactly like we would have intiuited. The market sharpens monotonically as we approach the market jump with the BSP being the most effiecient of all the prices! # # Hopefully you can now see the logical failing of measuring bias over market efficiency and it changes the way you think about your bet placement. # # Let's move on to what we're here for: is it possible to predict the BSP. # %% [markdown] # ## 2.4 Predicting the BSP # # Ok so I'm interested in finding the answer to the question: which estimate of BSP should i use when betting on the exchange and is it possible to beat the projected SP provided on the website and through the API? # # Well the first thing we should recognise about this projection is that it's cached. What does that mean? It means it only updated every 60 seconds. This suprised me when i first learned it and it was actually causing issues in my bet placement logic for the SP. # # Let's have a look at a selection to see how this works in practice # %% # Lets take a sample of a market and a selection dSlice = df . query ( 'market_id == \"1.182394184\" and selection_id == \"39243409\"' ) . dropna () # %% def chartClosingPrices ( d ): fig = px . line ( pd . melt ( d [: - 1 ][[ 'sbsj' , 'back_best' , 'near_price' ]], id_vars = 'sbsj' , var_name = 'price' ), x = 'sbsj' , y = 'value' , color = 'price' , template = 'plotly_white' , title = \"Selection\" , labels = { 'sbsj' : \"Seconds Before Scheduled Jump\" } ) fig . update_layout ( font_family = \"Roboto\" ) fig . add_trace ( go . Line ( x = dSlice . sbsj , y = dSlice . bsp , name = 'BSP' , line_color = 'rgba(8,61,119, 0.3)' , mode = \"lines\" ) ) fig [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig . show ( \"png\" ) chartClosingPrices ( dSlice ) # %% [markdown] # The red line is the projected BSP, you can see that it's not very responsive. As the best back price comes in from ~3 to 2.9 leading up to the jump the projected SP doesn't move because it's cached. If you were relying on this number for something important and you were using it in that period you were using stale information and you'd be worse off for it. In this instance the final SP was 2.79 so you may have made the wrong betting decision. # # This is somewhat counter intuitive because the projected sp (the so called near price) should be a good estimate of the BSP because it synthetically runs the BSP algorithm on the current market state and produces and estimate, so you would think that it'd be a pretty good estimate. # # Let's widen our sample a bit and see how it performs across our entire sample. We'll slice the data at the exact scheduled off and see how accurate various price points are at predicting what the final BSP is. We'll use mean absolute error (MAE) as our error metric. We'll assess 6 price points: # # - The near price (projected sp) # - The far price (projected sp excluding limit orders) # - The best back price # - The best lay price # - The ladder midpoint price # - The geometric midpoint price # %% # Measurement # ________________________ estimatesDf = df [ df . sbsj == 0 ][[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] ( pd . melt ( estimatesDf , id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) # %% [markdown] # So a bit surprisingly, in thoroughbred markets at the scheduled off your best to just use the current best back price as your estimate of the BSP. It significantly outperforms the projected SP and even some of our midpoint methods. # # Let's change the timeslice a little and take the very last moment before the market settles and see which performs best. # %% lastEstimatesDf = df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( - 1 )[[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] ( pd . melt ( lastEstimatesDf , id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) # %% [markdown] # First thing to notice is the estimates get a lot better than at the scheduled off, as we'd expect. A bit surprisingly the projected SP is still very weak due to the caching issue. In this scenario the geometric mid point beforms significantly better than the current best back price which suggests that as the late market is forming the back and lay spread with start converging to the fair price and eventual BSP. I personally use the geometric midpoint as my BSP estimate as it's a quick and easy metric that performs pretty well. # # What if you want more though? Is it possible to do better than these metrics? These simple price points use no information about what's in the BSP pools, surely if we used this information we'd be able to do better. Let's try to use machine learning to synthesise all this information at once. # # ### 2.3.4 Machine Learning # # We'll build a quite random forest model to estimate the BSP with current price and pool size information. This is a very simple application of machine learning so hopefully gives you an idea of its power without being too complex. # # Now we need an intelligent way of turning our pool and ladder information into a feature to insert into our model, how could we engineer this feature? Well what we'll do is calculate a WAP required to fill our pool stake on the back and lay side. What does that mean? Say we've got $200 sitting in the BSP back pool and $200 sitting on the top box of 2.5 on the back side, in this instance our WAP value would be exactly 2.5 cause we can fill it all at the top box. But if however, there was only $100 in the top box then we'd need to move down the ladder to fill the remaining $100 volume. Our feature will simulate this allocation logic and return the final weighted average price required to fill the total BSP pool. Here's the functions to do it on the back and lay side respectively: # %% def wapToGetBack ( pool , ladder ): price = ladder [ 'p' ] volume = ladder [ 'v' ] try : indmax = min ([ i for ( i , j ) in enumerate ( cVolume ) if j > pool ]) + 1 except : indmax = len ( volume ) return ( round ( sum ([ a * b for a , b in zip ( price [: indmax ], volume [: indmax ])]) / sum ( volume [: indmax ]), 4 )) def wapToGetLay ( liability_pool , ladder ): price = ladder [ 'p' ] volume = ladder [ 'v' ] liability = [( a - 1 ) * b for a , b in zip ( price , volume )] cLiability = np . cumsum ( liability ) try : indmax = min ([ i for ( i , j ) in enumerate ( cLiability ) if j > liability_pool ]) + 1 except : indmax = len ( volume ) return ( round ( sum ([ a * b for a , b in zip ( price [: indmax ], volume [: indmax ])]) / sum ( volume [: indmax ]), 4 )) # %% [markdown] # Now we'll set up our model matrix which will be the market state at the exact scheduled off. We'll also add our custom features. # %% model_matrix = df [[ 'sbsj' , 'atb_ladder' , 'atl_ladder' , 'bsp' , 'traded_volume' , 'near_price' , 'far_price' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] # Filter at scheduled jump model_matrix = model_matrix [ model_matrix . sbsj == 0 ] . dropna () model_matrix = ( model_matrix . assign ( wap_to_get_back_pool = lambda x : x . apply ( lambda x : wapToGetBack ( x . bsp_back_pool_stake , x . atb_ladder ), axis = 1 )) . assign ( wap_to_get_lay_pool = lambda x : x . apply ( lambda x : wapToGetLay ( x . bsp_lay_pool_liability , x . atl_ladder ), axis = 1 )) ) # Drop other columns model_matrix . drop ( columns = [ 'sbsj' , 'atb_ladder' , 'atl_ladder' ], inplace = True ) model_matrix . head ( 3 ) # %% [markdown] # Now the machine learning. Sklearn make this very simple, in our case it's a few lines only. We'll split our data into train and test sets and train a small random forrest to predict the BSP. # %% from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split # Setup Train / Test train_features , test_features , train_labels , test_labels = train_test_split ( model_matrix . drop ( columns = [ 'bsp' ]), model_matrix [ 'bsp' ], test_size = 0.25 ) print ( 'Training Features Shape:' , train_features . shape ) print ( 'Training Labels Shape:' , train_labels . shape ) print ( 'Testing Features Shape:' , test_features . shape ) print ( 'Testing Labels Shape:' , test_labels . shape ) # Instantiate Model rf = RandomForestRegressor ( n_estimators = 100 ) # Train Model rf . fit ( train_features , train_labels ) # %% [markdown] # Let's check out our predictions on the test set (remember our model hasn't seen any of this data so it should be a true reflection on how we'd perform on some new races that would happen this afternoon say) # %% # Use the forest's predict method on the test data predicted_bsp = rf . predict ( test_features ) predicted_bsp # %% [markdown] # Seems reasonable. All well and good though is the prediction any good? Let's measure it using MAE in the same way as we did before. # %% # Let's test our estimate vs our others in the same way as before testDf = test_features testDf [ 'bsp' ] = test_labels testDf [ 'rf_bsp_prediction' ] = predicted_bsp ( pd . melt ( testDf [[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' , 'rf_bsp_prediction' ]], id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) # %% [markdown] # Nice that's significantly better than the best previous estimate at this time slice. To validate it further let's use the same model to predict the BSP using the market state 10 seconds after the scheduled jump instead of at the exact scheduled off. None of the rows (or samples) in this time slice have been seen by the model during the training step so it should provide a robust out of sample estimate of the models performance on unseen data. # %% # Validate it on a completely different time point - 10 seconds after scheduled jump outOfSample = df [[ 'sbsj' , 'atb_ladder' , 'atl_ladder' , 'bsp' , 'traded_volume' , 'near_price' , 'far_price' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] outOfSample = outOfSample [ outOfSample . sbsj == - 10 ] . dropna () outOfSample = ( outOfSample . assign ( wap_to_get_back_pool = lambda x : x . apply ( lambda x : wapToGetBack ( x . bsp_back_pool_stake , x . atb_ladder ), axis = 1 )) . assign ( wap_to_get_lay_pool = lambda x : x . apply ( lambda x : wapToGetLay ( x . bsp_lay_pool_liability , x . atl_ladder ), axis = 1 )) ) # Produce Predictions outofsamplebspprediction = rf . predict ( outOfSample . drop ( columns = [ 'bsp' , 'sbsj' , 'atb_ladder' , 'atl_ladder' ])) outofsamplebspprediction # %% outOfSample [ 'rf_bsp_prediction' ] = outofsamplebspprediction ( pd . melt ( outOfSample [[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' , 'rf_bsp_prediction' ]], id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) # %% [markdown] # Still significantly better on the out of sample set which is a really positive sign. # # ## 2.3.5 Next Steps # # To improve this model I'd include multiple time slices in the training sample and use the seconds before scheduled jump as a feature as I would estimate that the predictive dynamics of each of these features is dynamic and affected by how mature + how close to settlement the market is. # # To implement this model in your bet placement code you'd simply need to save the model object (some info about how to do this with sklearn can be found here [here](https://scikit-learn.org/stable/modules/model_persistence.html)). Your key challenge will be making sure you can produce the exact inputs you've created in this development process from the live stream or polling API responses, but if you've gotten this far it won't be a huge challenge for you. # # # 3.0 Conclusion # # I've taken you through a quick crash course in the Betfair BSP including: # # - What it is # - How it's created # - How it's traded on betfair Australian thoroughbred markets # - How efficient it is and a methodology for measuring its efficiency in different contexts # - The accuracy of the projected SP and how it compares with other estimates # - How to build your own custom projection that's better than anything available out of the box # # The analysis focused on thoroughbred markets but could easily be extended to other racing codes or sports markets that have BSP enabled. The custom SP projection methodology could be used for anything from staking your model more accurately or with some improvement maybe as part of a automated trading strategy. Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Wisdom of the crowd? Analysing & understanding BSP"},{"location":"historicData/analysingAndPredictingBSP/#wisdom-of-the-crowd-analysing-understanding-bsp","text":"This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the Market Movements tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!","title":"Wisdom of the crowd? Analysing &amp; understanding BSP"},{"location":"historicData/analysingAndPredictingBSP/#cheat-sheet","text":"This is presented as a Jupyter notebook as this format is interactive and lets you run snippets of code from wihtin the notebook. To use this functionality you'll need to download a copy of the ipynb file locally and open it in a text editor (i.e. VS code). If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo .","title":"Cheat sheet"},{"location":"historicData/analysingAndPredictingBSP/#00-setup","text":"","title":"0.0 Setup"},{"location":"historicData/analysingAndPredictingBSP/#01-importing-libraries","text":"Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language. Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files. You'll also need betfairlightweight which you can install with something like pip install betfairlightweight . import pandas as pd import numpy as np import requests import os import re import csv import plotly.express as px import plotly.graph_objects as go import math import logging import yaml import csv import tarfile import zipfile import bz2 import glob import ast from datetime import date , timedelta from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook )","title":"0.1 Importing libraries"},{"location":"historicData/analysingAndPredictingBSP/#02-context","text":"The Betfair Starting Price (BSP) is a starting price product offered by Betfair (on large enough markets, almost all racing and some sports) that gives customers a chance to back or lay any selection at a \"fair\" price. Without getting too complex too quickly, the BSP allows you lock in a bet at any time after the market is opened and for as much stake as you can afford. The BSP is a good option for many different segments of customers: Recreational punters that don't have a particular strategy for trying to get the best odds can lock in a price that is (in the aggregate) a lot better than what they'd get at a corporate book or they'd get by taking limit bets early in a market's trading Automated customers that don't want the hassle of managing live market trading can implement automated strategies a lot easier whilst also protecting them from edge cases like race reschedules Is perfect for simply backtesting fundamental models as it's a resilient and robust single price There\u2019s a lot of background reading available on BSP if you\u2019re keen to understand it better. Here are some resources that might be interesting if you want to learn more: Overview FAQ summary Detailed explanation Worked examples of the reconciliation process How the projected odds work Despite it being a good option for a lot of customers it's also a fairly contraversial topic for some other types of customers. Some people firmly believe that the BSP on big markets reflects the \"true chance\" of a selection so betting it is a fools errand that will simply lose you commission over the long run. You might have heard a version of this story before: given the large pool sizes, the 0% overround, the settlement at the exact moment the market is suspended the BSP perfectly synthesises all available public information and demand and arrives at a true fair odds. Some will attempt to prove this to you by showing you a predicted chance vs observed win rate scatterplot which shows a perfect correlation between chance implied by the BSP and a horses true chance. Whilst I don't disagree that the BSP is a very strong estimate of a selections chance it's pretty obviously not perfect. Furthermore, it presents some other tricky challenges to use in practical situations. It's not knowable perfectly before it's the exact moment of market suspension so many model or strategy builders make the mistake of unknowingly leaking it into their preplay model development or their theoretical staking calculations. Where the final number will land is actually another source of uncertainty in your processes which presents anothing forecasting / predictive modelling application as I'll explore later in this piece. I'll take you through how I'd measure the accuracy of the BSP, show you how it's traded on the exchange, and take you through a host of methods of estimating the BSP and build a custom machine learning approach that's better than each of them.","title":"0.2 Context"},{"location":"historicData/analysingAndPredictingBSP/#03-the-algorithm","text":"The actual logic of how betfair arrives at the final BSP number is quite complex and for a few reasons you won't be able to perfectly replicate it at home. However, the general gist of the BSP reconciliation algorithm that is executed just as the market suspended goes something like: The algorithm combines 4 distinct groups of open bets for a given selection: Non price limited BSP orders on both the back and lay side ( market_on_close orders) Price limited orders on both the back and lay side ( limit_on_close orders) All non filled open lay orders All non filled open back orders It then combines them all together, passes a sophisticated balancing algorithm over the top of them and arrives at a single fair price for the BSP that balances the demand on either side of the ledger","title":"0.3 The Algorithm"},{"location":"historicData/analysingAndPredictingBSP/#04-this-example","text":"For this exercise we'll again take advantage of the betfair historical stream json files. The slice of betfair markets I'll be analysing is all thoroughbred races over July 2020 - June 2021. As an aside the projected BSP number you see on the betfair website isn't collected inside betfair's own internal database of orders, so any custom data request you may be able to get as a VIP won't include this number. So if you were planning to include it in any kind of projection or bet placement logic operation you were making the only way to anlayse it historically is to mine these data files. Another good reason to learn the skills to do so!","title":"0.4 This Example"},{"location":"historicData/analysingAndPredictingBSP/#10-data","text":"Like the previous tutorial we won't be able to collapse the stream data down into a single row per runner because I'm interested in anlaysing how the projected BSP moves late in betfair markets. I'm also interested in plotting the efficiency of certain odds values at certain distinct time points leading up the the races so I need multiple records per runner. Like in the previous tutorial I'll split out the selection metadata, BSP and win flag values as a seperate data file to reduce the size of the datafiles extracted for this analysis. For the preplay prices dataset I'll: Start extraction at 2 mins before the scheduled off Extract prices every 10 seconds thereafter until the market is suspended I'll also extract the final market state the instant before the market is suspended","title":"1.0 Data"},{"location":"historicData/analysingAndPredictingBSP/#11-sourcing-data","text":"First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Ask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access, download them to your computer and store them together in a folder.","title":"1.1 Sourcing Data"},{"location":"historicData/analysingAndPredictingBSP/#12-utility-functions","text":"First like always we'll need some general utility functions that you may have seen before: # General Utility Functions # _________________________________ def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def pull_ladder ( availableLadder , n = 5 ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : n ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"p\" ] = price out [ \"v\" ] = volume return ( out ) def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' )","title":"1.2 Utility functions"},{"location":"historicData/analysingAndPredictingBSP/#13-selection-metadata","text":"Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes. def final_market_book ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): return ( None ) for market_book in market_books : last_market_book = market_book return ( last_market_book ) def parse_final_selection_meta ( dir , out_file ): with open ( out_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,venue,market_time,selection_name,win,bsp \\n \" ) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) last_market_book = final_market_book ( stream ) if last_market_book is None : continue # Extract Info ++++++++++++++++++++++++++++++++++ runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in last_market_book . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'win' : np . where ( r . status == \"WINNER\" , 1 , 0 ), 'sp' : r . sp . actual_sp } for r in last_market_book . runners ] # Return Info ++++++++++++++++++++++++++++++++++ for runnerMeta in runnerMeta : if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( last_market_book . market_id ), runnerMeta [ 'selection_id' ], last_market_book . market_definition . venue , last_market_book . market_definition . market_time , runnerMeta [ 'selection_name' ], runnerMeta [ 'win' ], runnerMeta [ 'sp' ] ) ) selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) print ( \"__ Parsing Selection Metadata ___ \" ) # parse_final_selection_meta(stream_files, selection_meta) __ Parsing Selection Metadata ___","title":"1.3 Selection Metadata"},{"location":"historicData/analysingAndPredictingBSP/#14-preplay-prices-and-projections","text":"In this set of preplay prices I'm interested in many of the same fields as we've extracted in previous tutorials as well as fields relating to the current state of the BSP. These objects sit under the sp slot within the returned runner object. The fields we'll extract are: The so called \"near price\" The near price is the projected SP value you can see on the website It includes both bets already placed into the SP pools as well as open limit orders to estimate what the final BSP value will be The so called \"far price\" This is the same as the near price except it excludes limit orders on the exchange This makes it fairly redundant value and we'll see how poor of an estimator it is a bit later The volume currently bet into the BSP back pool The liability currently laid into the BSP lay pool We'll also extract the top 5 rungs of the available to back and available to lay ladders as well as the traded volume of limit bets. It's worth noting that I am discarding some key information about the BSP pools that I could have extracted if I wanted to. The current SP bets are laid out in a way that I could split out limit_on_close as well as market_on_close sp bets but I've rolled everything together in SP stake on the back side and sp liability on the lay side. This is just to reduce complexity of this article but including it would increase the predictive power of the BSP model in the final step. def loop_preplay_prices ( s , o ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () marketID = None tradeVols = None time = None last_book_recorded = False prev_book = None for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): break for market_book in market_books : # Time Step Management ++++++++++++++++++++++++++++++++++ if marketID is None : # No market initialised marketID = market_book . market_id time = market_book . publish_time elif market_book . inplay and last_book_recorded : break else : seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () if seconds_to_start > 120 : # Too early before off to start logging prices prev_book = market_book continue else : # Update data at different time steps depending on seconds to off wait = 10 # New Market if market_book . market_id != marketID : last_book_recorded = False marketID = market_book . market_id time = market_book . publish_time continue # (wait) seconds elapsed since last write elif ( market_book . publish_time - time ) . total_seconds () > wait : time = market_book . publish_time # if current marketbook is inplay want to record the previous market book as it's the last preplay marketbook elif market_book . inplay : last_book_recorded = True market_book = prev_book # fewer than (wait) seconds elapsed continue to next loop else : prev_book = market_book continue # Execute Data Logging ++++++++++++++++++++++++++++++++++ for runner in market_book . runners : try : atb_ladder = pull_ladder ( runner . ex . available_to_back , n = 5 ) atl_ladder = pull_ladder ( runner . ex . available_to_lay , n = 5 ) except : atb_ladder = {} atl_ladder = {} limitTradedVol = sum ([ rung . size for rung in runner . ex . traded_volume ]) o . writerow ( ( market_book . market_id , runner . selection_id , market_book . publish_time , int ( limitTradedVol ), # SP Fields runner . sp . near_price , runner . sp . far_price , int ( sum ([ ps . size for ps in runner . sp . back_stake_taken ])), int ( sum ([ ps . size for ps in runner . sp . lay_liability_taken ])), # Limit bets available str ( atb_ladder ) . replace ( ' ' , '' ), str ( atl_ladder ) . replace ( ' ' , '' ) ) ) prev_book = market_book def parse_preplay_prices ( dir , out_file ): with open ( out_file , \"w+\" ) as output : writer = csv . writer ( output , delimiter = ',' , lineterminator = ' \\r\\n ' , quoting = csv . QUOTE_ALL ) writer . writerow (( \"market_id\" , \"selection_id\" , \"time\" , \"traded_volume\" , \"near_price\" , \"far_price\" , \"bsp_back_pool_stake\" , \"bsp_lay_pool_liability\" , \"atb_ladder\" , 'atl_ladder' )) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) loop_preplay_prices ( stream , writer ) price = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) print ( \"__ Parsing Selection Prices ___ \" ) # parse_final_selection_meta(stream_files, price) __ Parsing Selection Prices ___","title":"1.4 Preplay Prices and Projections"},{"location":"historicData/analysingAndPredictingBSP/#20-analysis","text":"First step let's boot up the datasets we extracted in the previous steps and take a look at what we've managed to extract from the raw stream files.","title":"2.0 Analysis"},{"location":"historicData/analysingAndPredictingBSP/#21-load-inspect","text":"First we have the highlevel selection metadata as we have already seen in other tutorials selection = pd . read_csv ( \"[PATH TO YOUR SELECTION METADATA FILE]\" , dtype = { 'market_id' : object , 'selection_id' : object }, parse_dates = [ 'market_time' ]) selection . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue market_time selection_name win bsp 0 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.20 1 1.179845158 16374800 Cowra 2021-03-01 04:15:00 3. Careering Away 1 3.60 2 1.179845158 19740699 Cowra 2021-03-01 04:15:00 4. Bells N Bows 0 6.62 Now let's load the prices file. We'll apply some extra logic to parse the ladder columns into dictionaries and also remove the first odds record per group as it's the first record as the market was instantiated. prices = pd . read_csv ( \"[PATH TO YOUR PRICES FILE]\" , quoting = csv . QUOTE_ALL , dtype = { 'market_id' : 'string' , 'selection_id' : 'string' , 'atb_ladder' : 'string' , 'atl_ladder' : 'string' }, parse_dates = [ 'time' ] ) # Parse ladder columns prices [ 'atb_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atb_ladder' ]] prices [ 'atl_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atl_ladder' ]] # Drop the first row within each group prices = prices . drop ( prices . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( 0 ) . index ) prices . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder 21 1.179845158 23493550 2021-03-01 04:13:00.058 2465 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [14.63, 13... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [22.86, 12... 22 1.179845158 16374800 2021-03-01 04:13:00.058 5046 3.35 1.70 449 300 {'p': [3.65, 3.6, 3.55, 3.5, 3.45], 'v': [0.45... {'p': [3.75, 3.8, 3.9, 4.1, 4.3], 'v': [5.14, ... 23 1.179845158 19740699 2021-03-01 04:13:00.058 1978 6.39 3.25 154 251 {'p': [6, 5.9, 5.8, 5.7, 5.6], 'v': [4.71, 89.... {'p': [6.4, 6.6, 6.8, 7, 7.2], 'v': [30.24, 2.... f 'The shape of the prices data file is { prices . shape [ 0 ] } rows and { prices . shape [ 1 ] } columns' 'The shape of the prices data file is 3937805 rows and 10 columns' # Let's have a look at the prices datafile for a distinct market and selection prices . query ( 'market_id == \"1.183995724\" and selection_id == \"22832649\"' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder 2762714 1.183995724 22832649 2021-06-01 01:38:00.062 1894 8.60 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [31.23, 37... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [63.98, 14... 2762724 1.183995724 22832649 2021-06-01 01:38:10.158 2082 8.60 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [25.54, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [37.63, 68... 2762734 1.183995724 22832649 2021-06-01 01:38:20.159 2094 8.00 7.69 27 184 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [94.52, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [37.63, 56... 2762744 1.183995724 22832649 2021-06-01 01:38:30.182 2229 8.00 7.69 27 190 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [62.88, 49... {'p': [8.2, 8.4, 8.6, 8.8, 9], 'v': [22.99, 56... 2762754 1.183995724 22832649 2021-06-01 01:38:40.221 2240 8.00 7.69 136 205 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [13.92, 12... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.28, 41... 2762764 1.183995724 22832649 2021-06-01 01:38:50.923 2294 8.00 7.69 136 205 {'p': [8, 7.8, 7.6, 7.4, 7.2], 'v': [129.4, 53... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [67.76, 41... 2762774 1.183995724 22832649 2021-06-01 01:39:00.955 2297 8.00 7.69 137 214 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [26.93, 13... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.28, 41... 2762784 1.183995724 22832649 2021-06-01 01:39:10.962 2417 8.00 7.69 137 229 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [29.2, 146... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [53.26, 23... 2762794 1.183995724 22832649 2021-06-01 01:39:20.966 2677 8.18 2.68 212 243 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [19.2, 89.... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [69.9, 16.... 2762804 1.183995724 22832649 2021-06-01 01:39:30.971 2795 8.18 2.68 212 243 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [79.45, 82... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [59.89, 65... 2762814 1.183995724 22832649 2021-06-01 01:39:41.018 3039 8.18 2.68 245 324 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [37.01, 12... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [73.69, 37... 2762824 1.183995724 22832649 2021-06-01 01:39:51.119 3290 8.18 2.68 324 459 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [63.46, 94... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [19.59, 18... 2762834 1.183995724 22832649 2021-06-01 01:40:01.123 3488 8.18 2.68 324 748 {'p': [8.4, 8.2, 8, 7.8, 7.6], 'v': [32.81, 97... {'p': [8.6, 8.8, 9, 9.2, 9.4], 'v': [0.96, 15.... 2762844 1.183995724 22832649 2021-06-01 01:40:11.136 3831 8.18 2.68 447 2285 {'p': [8.2, 8, 7.8, 7.6, 7.4], 'v': [46.15, 99... {'p': [8.4, 8.6, 8.8, 9, 9.2], 'v': [20.11, 47... 2762854 1.183995724 22832649 2021-06-01 01:40:18.201 3950 8.04 7.60 327 2469 {'p': [6.8, 6.4, 5.9, 5.1, 4.4], 'v': [4.01, 3... {'p': [10, 11, 11.5, 15, 15.5], 'v': [6.05, 0.... We can see some expected behaviour as we zoom in on a particular selection The traded volume increases on this selection as we get closer to the jump The projected BSP (the near_price column) stays constant for a number of increments as its update is cached for 60 seconds at a time The sizes in the BSP pools also increases as we get closer to the jump The prices offered and traded closer to the jump are closer to the BSP than those at the start of the 2 minute period","title":"2.1 Load &amp; Inspect"},{"location":"historicData/analysingAndPredictingBSP/#22-transform-and-assemble","text":"We have our 2 core datasets, but we'd prefer to work with one now. We'd also like to add some key columns that will be reused throughout our analysis so we'll add those now too.","title":"2.2 Transform and Assemble"},{"location":"historicData/analysingAndPredictingBSP/#221-mid-points","text":"The first semi-interesting thing we'll do in this analysis is add selection mid-points to our dataset. Eventually we're going to be interested in estimating the BSP and measuring the efficiency of certain prices at various points leading up to the race. Betfair markets work like all markets with bids and spreads. The market equilibrium forms around the best price offered on either side of the market to back or to lay. These top prices each have some inherent advantage built into it for the offerer. For example in early markets the best offers on either side of the market might be really wide (say 1.80 as a best back and 2.50 as a best lay). Given the price discovery process is still immature each bidder gets a large premium, backed into their offer price, which compensates them for providing betting opportunities with little to no information provided from other market participants. This spread will naturally get tighter and tighter as the market matures and more participants seek to get volume down and must be more and more competitive. But what's the price \"equilibrium\" in each case? Well it's up to you but I'll provide you two ways of finding the central mid-point of a bid-ask spread on betfair markets. The problem we're solving for here is the non-linearity of prices in odds space. We have some intuition for this: when we see a market spread of 10-100 in early trading we have an understanding that the true midpoint of this market is somewhere around 25-35 not the 55 you'd get if you simply took the (arithmetic) mean of those two numbers. Two techniques for accounting for that non-linearity are as follows. Ladder Midpoint The ladder midpoint method takes advantage of the fact that the Betfair price ladder itself accounts for the nonlinearity of prices in odds space. The method calculated the difference in number of rungs on the Betfair ladder, halves it, and shifts the best back or lay price that number of rungs towards the centre. This will generally provide a much better idea of the market midpoint than a simple arithmetic mean of the two prices. Geometric Mean Unfortunately the ladder method is a little computationally expensive. A good approximation for this approach is to take the geometric mean of the best back and best lay values. The geometric mean is a special kind of mean that you may have never used before that is more appropriate for purposes like this. It is calculated like: sqrt(x1 * x2 * ...) . This number will also provide a much better estimate of the market midpoint than the simple arithmetic mean. The latter calculation is trivial. The former requires a suite of Betfair tick arithmetic functions that I'll put below. It may seem like overkill for this exercise (and it is) but hopefully these functions might be of use to you for other purposes. # Define the betfair tick ladder def bfTickLadder (): tickIncrements = { 1.0 : 0.01 , 2.0 : 0.02 , 3.0 : 0.05 , 4.0 : 0.1 , 6.0 : 0.2 , 10.0 : 0.5 , 20.0 : 1.0 , 30.0 : 2.0 , 50.0 : 5.0 , 100.0 : 10.0 , 1000.0 : 1000 , } ladder = [] for index , key in enumerate ( tickIncrements ): increment = tickIncrements [ key ] if ( index + 1 ) == len ( tickIncrements ): ladder . append ( key ) else : key1 = [ * tickIncrements ][ index ] key2 = [ * tickIncrements ][ index + 1 ] steps = ( key2 - key1 ) / increment for i in range ( int ( steps )): ladder . append ( round ( key + i * increment , 2 )) return ( ladder ) bfticks = bfTickLadder () # Round a decimal to the betfair tick value below def bfTickFloor ( price , includeIndex = False ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () ind = [ n for n , i in enumerate ( bfticks ) if i >= price ][ 0 ] if includeIndex : if bfticks [ ind ] == price : return (( ind , price )) else : return (( ind - 1 , bfticks [ ind - 1 ])) else : if bfticks [ ind ] == price : return ( price ) else : return ( bfticks [ ind - 1 ]) # Calculate the numder of ticks between two tick values def bfTickDelta ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) x = bfTickFloor ( p1 , includeIndex = True ) y = bfTickFloor ( p2 , includeIndex = True ) return ( x [ 0 ] - y [ 0 ]) def bfTickShift ( p , rungs ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () flr = bfTickFloor ( p , includeIndex = True ) return ( bfticks [ flr [ 0 ] + rungs ]) def bfLadderMidPoint ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) delta = - 1 * bfTickDelta ( p1 , p2 ) if delta == 1 : return ( p1 ) elif delta % 2 != 0 : return ( bfTickShift ( p1 , math . ceil ( delta / 2 ))) else : return ( bfTickShift ( p1 , math . floor ( delta / 2 ))) # Let's test a midpoint using the ladder mid point method bfLadderMidPoint ( 10 , 100 ) 25.0 # And for illustrative purposes let's calculate the geomtric mean of these values np . sqrt ( 10 * 100 ) 31.622776601683793 Let's put this all together while stitching together our two core datasets. # Join and augment df = ( selection . merge ( prices , on = [ 'market_id' , 'selection_id' ]) . assign ( sbsj = lambda x : round (( x [ 'market_time' ] - x [ 'time' ]) . dt . total_seconds () / 10 ) * 10 ) . assign ( back_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atb_ladder' ]]) . assign ( lay_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atl_ladder' ]]) . assign ( geometric_mid_point = lambda x : round ( 1 / np . sqrt (( 1 / x [ 'back_best' ]) * ( 1 / x [ 'lay_best' ])), 3 )) . assign ( ladder_mid_point = lambda x : x . apply ( lambda x : bfLadderMidPoint ( x . back_best , x . lay_best ), axis = 1 )) . replace ([ np . inf , - np . inf ], np . nan ) ) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue market_time selection_name win bsp time traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability atb_ladder atl_ladder sbsj back_best lay_best geometric_mid_point ladder_mid_point 0 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:00.058 2465 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [14.63, 13... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [22.86, 12... 120.0 6.2 6.6 6.397 6.4 1 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:10.077 2848 5.93 3.17 113 238 {'p': [6, 5.9, 5.8, 5.7, 5.6], 'v': [59.93, 36... {'p': [6.4, 6.6, 6.8, 7, 7.2], 'v': [28.79, 50... 110.0 6.0 6.4 6.197 6.2 2 1.179845158 23493550 Cowra 2021-03-01 04:15:00 1. Larmour 0 6.2 2021-03-01 04:13:20.161 2866 5.93 3.17 113 238 {'p': [6.2, 6, 5.9, 5.8, 5.7], 'v': [22.91, 88... {'p': [6.6, 6.8, 7, 7.2, 7.4], 'v': [55.19, 22... 100.0 6.2 6.6 6.397 6.4","title":"2.2.1 Mid points"},{"location":"historicData/analysingAndPredictingBSP/#23-analysing-the-bsp","text":"Before we embark on our predictive exercise let's analyse the BSP to get a feel for it as an entity.","title":"2.3 Analysing the BSP"},{"location":"historicData/analysingAndPredictingBSP/#231-volumes","text":"Ever wondered how much volume is traded on the BSP? How does it compare to limit bets? Well with our parsed stream data we can answer those questions! Now the BSP volume will be the bigger of the BSP back stake and the lay stake (which you can infer by the final BSP and the total lay liability). # Volume Traded # _________________________ # Extract the final time slice of data which includes the total preplay volumes traded across limit and BSP poools volumeDf = df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( - 1 )[[ 'market_id' , 'selection_id' , 'bsp' , 'traded_volume' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' ]] # Infer the biggest of the two BSP stakes volumeDf = ( volumeDf . assign ( lay_stake = lambda x : x [ 'bsp_lay_pool_liability' ] / ( x [ 'bsp' ] - 1 )) . assign ( bsp_stake = lambda x : x [[ 'lay_stake' , 'bsp_back_pool_stake' ]] . max ( axis = 1 )) ) ( volumeDf . groupby ( 'market_id' , as_index = False ) . agg ({ 'traded_volume' : 'sum' , 'bsp_stake' : 'sum' }) . agg ({ 'traded_volume' : 'mean' , 'bsp_stake' : 'mean' }) ) traded_volume 98025.705018 bsp_stake 7287.524766 dtype: float64 So in an average thoroughbred market there's about 98k traded limit volume and 7,300 BSP traded stake. So approximately 7% of thoroughbred volume is traded at the BSP at least for our sample of thoroughbred races.","title":"2.3.1 Volumes"},{"location":"historicData/analysingAndPredictingBSP/#232-efficiency","text":"Now you may have heard this story before: you can't beat the BSP it's too efficient! I'm not sure people really have a firm idea about what they're talking about when they say this. Typically what you'll see in a discussion about efficiency is the predicted vs observed scatterplot. Let's see if we can reproduce this chart. First let's assemble a dataframe that we can use for this chart as well as others. What we'll do is we'll extract the BSP and a price value at 5 different slices before the race starts. We could chose any price point (we'll analyse the difference between them in a subsequent step) but for this section I'm going to take the preplay market estimate as the geometric market midpoint (you'll have to trust me for now that this is a sensible decision). # Extract the geomtric market mid point at time slices: 120, 90, 60, 30, and 0 seconds from the scheduled off preplay = df [ df . sbsj . isin ([ 120 , 90 , 60 , 30 , 0 ])][[ 'market_id' , 'selection_id' , 'win' , 'sbsj' , 'geometric_mid_point' ]] . sort_values ([ 'market_id' , 'selection_id' , 'sbsj' ], ascending = [ True , True , False ]) . rename ( columns = { 'geometric_mid_point' : 'odds' }) . assign ( type = lambda x : \"seconds before off: \" + x [ 'sbsj' ] . astype ( int ) . astype ( str )) # Extract the BSP values bsp = df . sort_values ([ 'market_id' , 'selection_id' , 'time' ], ascending = [ True , True , False ]) . groupby ([ 'market_id' , 'selection_id' ]) . head ( 1 )[[ 'market_id' , 'selection_id' , 'win' , 'sbsj' , 'bsp' ]] . rename ( columns = { 'bsp' : 'odds' }) . assign ( type = \"bsp\" ) # Append them together accuracyFrame = pd . concat ([ preplay , bsp ]) . dropna () accuracyFrame . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id win sbsj odds type 2790008 1.171091071 10693094 0 120.0 134.164 seconds before off: 120 2790011 1.171091071 10693094 0 90.0 149.666 seconds before off: 90 2790014 1.171091071 10693094 0 60.0 200.000 seconds before off: 60 2790017 1.171091071 10693094 0 30.0 239.792 seconds before off: 30 2790020 1.171091071 10693094 0 -0.0 239.165 seconds before off: 0 Now we'll filter just on our BSP records and plot the observed vs actual scatterplot # BSP Scatter # __________________ winRates = ( accuracyFrame . query ( 'type == \"bsp\"' ) . assign ( implied_chance = lambda x : round ( 20 * ( 1 / x [ 'odds' ])) / 20 ) . groupby ( 'implied_chance' , as_index = False ) . agg ({ 'win' : 'mean' }) ) fig = px . scatter ( winRates , x = \"implied_chance\" , y = \"win\" , template = \"plotly_white\" , title = \"BSP: implied win vs actual win\" ) fig . add_trace ( go . Scatter ( x = winRates . implied_chance , y = winRates . implied_chance , name = 'no bias' , line_color = 'rgba(8,61,119, 0.3)' ) ) fig . show ( \"png\" ) Ok aside from some small sample noise at the top end (there's very few horses that run at sub 1.20 BSPs) we can see that the BSP is pretty perfectly.... efficient? Is that the right word? I'd argue that it's very much not the right word. Let me illustrate with a counter example. Let's plot the same chart for the BSP as well as our 5 other price points. # Bsp + Other Odds Scatter # __________________ winRates = ( accuracyFrame . assign ( implied_chance = lambda x : round ( 20 * ( 1 / x [ 'odds' ])) / 20 ) . groupby ([ 'type' , 'implied_chance' ], as_index = False ) . agg ({ 'win' : 'mean' }) ) fig = px . scatter ( winRates , x = \"implied_chance\" , y = \"win\" , color = 'type' , template = \"plotly_white\" , title = \"Comparing Price Points: implied win vs actual win\" ) fig . add_trace ( go . Scatter ( x = winRates . implied_chance , y = winRates . implied_chance , name = 'no bias' , line_color = 'rgba(8,61,119, 0.3)' ) ) fig . show ( \"png\" ) So they're all efficient? And indecernibly as efficient as one another? Well, to cut a long and possibly boring story short this isn't the right way to measure efficiency. What we're measure here is bias . All my scatter plot here tells me is if there's any systematic bias in the BSP, i.e. groups of BSPs that aren't well calibrated with actual outcomes. That is, for example, that perhaps randomly the group of horses that BSP around 2 don't happen to win 50% of the time maybe there was a sytemic bias that short favourites were underbet and these selections actually won 55% of the time. That would be a price bias in the BSP that someone could take advatange at just by looking at historical prices and outcomes alone. For an even simpler counter point: I could create a perfectly well calibrated estimate that assigned a single odds value to every horse which was the overall horse empirical win rate over our sample: 10.25% (which is merely a reflection of field sizes). This estimate would be unbiased, and would pass through our scatterplot method unscathed but would it be an efficient estimate? Clearly not. df . agg ({ 'win' : 'mean' }) win 0.102595 dtype: float64 Bias only tells us if there's a systematic way of exploiting the odds values themselves. I could have told you that this was unlikely but the scatterplot proves it. How else could we measure efficiency? I propose using the logloss metric. Let's calculate the logloss of the BSP # Logloss ++++++++++++++++++++++++++++++++++++++++ from sklearn.metrics import log_loss # Overall Logloss # _________________ bspLoss = log_loss ( y_pred = 1 / accuracyFrame . query ( 'type == \"bsp\"' )[ 'odds' ], y_true = accuracyFrame . query ( 'type == \"bsp\"' )[ 'win' ] ) print ( f 'The overall logloss of the BSP is { round ( bspLoss , 4 ) } ' ) The overall logloss of the BSP is 0.2757 Ok what does this mean? Well nothing really. This metric won't tell you anything by itself it's just useful for relative comparisons. Let's plot the logloss of our geometric midpoint at our various timeslices. # Logloss at Different Time Points # _________________ accuracyFrame . groupby ( 'type' , as_index = False ) . apply ( lambda x : log_loss ( y_pred = 1 / x [ 'odds' ], y_true = x [ 'win' ])) . rename ( columns = { None : 'logloss' }) . sort_values ( 'logloss' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type logloss 0 bsp 0.275679 1 seconds before off: 0 0.275824 3 seconds before off: 30 0.275999 4 seconds before off: 60 0.276044 5 seconds before off: 90 0.276206 2 seconds before off: 120 0.276256 # And in chart form fig = px . bar ( accuracyFrame . groupby ( 'type' , as_index = False ) . apply ( lambda x : log_loss ( y_pred = 1 / x [ 'odds' ], y_true = x [ 'win' ])) . rename ( columns = { None : 'logloss' }) . sort_values ( 'logloss' , ascending = False ), x = \"type\" , y = \"logloss\" , template = \"plotly_white\" , title = \"Logloss Of Odds At Various Time Points\" ) fig . update_yaxes ( range = [ .2755 , .2765 ]) fig . show ( \"png\" ) Now this is a cool graph. This is exactly like we would have intiuited. The market sharpens monotonically as we approach the market jump with the BSP being the most effiecient of all the prices! Hopefully you can now see the logical failing of measuring bias over market efficiency and it changes the way you think about your bet placement. Let's move on to what we're here for: is it possible to predict the BSP.","title":"2.3.2 Efficiency?"},{"location":"historicData/analysingAndPredictingBSP/#24-predicting-the-bsp","text":"Ok so I'm interested in finding the answer to the question: which estimate of BSP should I use when betting on the exchange and is it possible to beat the projected SP provided on the website and through the API? Well the first thing we should recognise about this projection is that it's cached. What does that mean? It means it only updated every 60 seconds. This suprised me when I first learned it and it was actually causing issues in my bet placement logic for the SP. Let's have a look at a selection to see how this works in practice # Lets take a sample of a market and a selection dSlice = df . query ( 'market_id == \"1.182394184\" and selection_id == \"39243409\"' ) . dropna () def chartClosingPrices ( d ): fig = px . line ( pd . melt ( d [: - 1 ][[ 'sbsj' , 'back_best' , 'near_price' ]], id_vars = 'sbsj' , var_name = 'price' ), x = 'sbsj' , y = 'value' , color = 'price' , template = 'plotly_white' , title = \"Selection\" , labels = { 'sbsj' : \"Seconds Before Scheduled Jump\" } ) fig . update_layout ( font_family = \"Roboto\" ) fig . add_trace ( go . Line ( x = dSlice . sbsj , y = dSlice . bsp , name = 'BSP' , line_color = 'rgba(8,61,119, 0.3)' , mode = \"lines\" ) ) fig [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig . show ( \"png\" ) chartClosingPrices ( dSlice ) /home/tmbish/.local/lib/python3.9/site-packages/plotly/graph_objs/_deprecations.py:378: DeprecationWarning: plotly.graph_objs.Line is deprecated. Please replace it with one of the following more specific types - plotly.graph_objs.scatter.Line - plotly.graph_objs.layout.shape.Line - etc. The red line is the projected BSP, you can see that it's not very responsive. As the best back price comes in from ~3 to 2.9 leading up to the jump the projected SP doesn't move because it's cached. If you were relying on this number for something important and you were using it in that period you were using stale information and you'd be worse off for it. In this instance the final SP was 2.79 so you may have made the wrong betting decision. This is somewhat counter intuitive because the projected sp (the so called near price) should be a good estimate of the BSP because it synthetically runs the BSP algorithm on the current market state and produces and estimate, so you would think that it'd be a pretty good estimate. Let's widen our sample a bit and see how it performs across our entire sample. We'll slice the data at the exact scheduled off and see how accurate various price points are at predicting what the final BSP is. We'll use mean absolute error (MAE) as our error metric. We'll assess 6 price points: The near price (projected sp) The far price (projected sp excluding limit orders) The best back price The best lay price The ladder midpoint price The geometric midpoint price # Measurement # ________________________ estimatesDf = df [ df . sbsj == 0 ][[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] ( pd . melt ( estimatesDf , id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } estimate error 0 back_best 0.091702 3 ladder_mid_point 0.093181 2 geometric_mid_point 0.094405 4 lay_best 0.103142 5 near_price 0.121266 1 far_price 0.578425 So a bit surprisingly, in thoroughbred markets at the scheduled off your best to just use the current best back price as your estimate of the BSP. It significantly outperforms the projected SP and even some of our midpoint methods. Let's change the timeslice a little and take the very last moment before the market settles and see which performs best. lastEstimatesDf = df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( - 1 )[[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] ( pd . melt ( lastEstimatesDf , id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } estimate error 2 geometric_mid_point 0.039627 3 ladder_mid_point 0.042526 0 back_best 0.063622 5 near_price 0.077108 4 lay_best 0.098198 1 far_price 0.290777 First thing to notice is the estimates get a lot better than at the scheduled off, as we'd expect. A bit surprisingly the projected SP is still very weak due to the caching issue. In this scenario the geometric mid point beforms significantly better than the current best back price which suggests that as the late market is forming the back and lay spread with start converging to the fair price and eventual BSP. I personally use the geometric midpoint as my BSP estimate as it's a quick and easy metric that performs pretty well. What if you want more though? Is it possible to do better than these metrics? These simple price points use no information about what's in the BSP pools, surely if we used this information we'd be able to do better. Let's try to use machine learning to synthesise all this information at once.","title":"2.4 Predicting the BSP"},{"location":"historicData/analysingAndPredictingBSP/#234-machine-learning","text":"We'll build a quick random forest model to estimate the BSP with current price and pool size information. This is a very simple application of machine learning so hopefully gives you an idea of its power without being too complex. Now we need an intelligent way of turning our pool and ladder information into a feature to insert into our model, how could we engineer this feature? Well what we'll do is calculate a WAP required to fill our pool stake on the back and lay side. What does that mean? Say we've got \\$200 sitting in the BSP back pool and \\$200 sitting on the top box of 2.5 on the back side, in this instance our WAP value would be exactly 2.5 cause we can fill it all at the top box. But if however, there was only $100 in the top box then we'd need to move down the ladder to fill the remaining \\$100 volume. Our feature will simulate this allocation logic and return the final weighted average price required to fill the total BSP pool. Here's the functions to do it on the back and lay side respectively: def wapToGetBack ( pool , ladder ): price = ladder [ 'p' ] volume = ladder [ 'v' ] try : indmax = min ([ i for ( i , j ) in enumerate ( cVolume ) if j > pool ]) + 1 except : indmax = len ( volume ) return ( round ( sum ([ a * b for a , b in zip ( price [: indmax ], volume [: indmax ])]) / sum ( volume [: indmax ]), 4 )) def wapToGetLay ( liability_pool , ladder ): price = ladder [ 'p' ] volume = ladder [ 'v' ] liability = [( a - 1 ) * b for a , b in zip ( price , volume )] cLiability = np . cumsum ( liability ) try : indmax = min ([ i for ( i , j ) in enumerate ( cLiability ) if j > liability_pool ]) + 1 except : indmax = len ( volume ) return ( round ( sum ([ a * b for a , b in zip ( price [: indmax ], volume [: indmax ])]) / sum ( volume [: indmax ]), 4 )) Now we'll set up our model matrix which will be the market state at the exact scheduled off. We'll also add our custom features. model_matrix = df [[ 'sbsj' , 'atb_ladder' , 'atl_ladder' , 'bsp' , 'traded_volume' , 'near_price' , 'far_price' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] # Filter at scheduled jump model_matrix = model_matrix [ model_matrix . sbsj == 0 ] . dropna () model_matrix = ( model_matrix . assign ( wap_to_get_back_pool = lambda x : x . apply ( lambda x : wapToGetBack ( x . bsp_back_pool_stake , x . atb_ladder ), axis = 1 )) . assign ( wap_to_get_lay_pool = lambda x : x . apply ( lambda x : wapToGetLay ( x . bsp_lay_pool_liability , x . atl_ladder ), axis = 1 )) ) # Drop other columns model_matrix . drop ( columns = [ 'sbsj' , 'atb_ladder' , 'atl_ladder' ], inplace = True ) model_matrix . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bsp traded_volume near_price far_price bsp_back_pool_stake bsp_lay_pool_liability back_best lay_best geometric_mid_point ladder_mid_point wap_to_get_back_pool wap_to_get_lay_pool 12 6.20 6891 5.74 4.22 518 1580 6.00 6.2 6.099 6.00 5.7762 6.3731 28 3.60 13579 3.57 1.73 1023 1771 3.45 3.6 3.524 3.55 3.3010 3.7007 44 6.62 5911 5.81 1.59 845 1378 6.20 6.6 6.397 6.40 5.9156 7.1167 Now the machine learning. Sklearn makes this very simple, in our case it's a few lines only. We'll split our data into train and test sets and train a small random forrest to predict the BSP. from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split # Setup Train / Test train_features , test_features , train_labels , test_labels = train_test_split ( model_matrix . drop ( columns = [ 'bsp' ]), model_matrix [ 'bsp' ], test_size = 0.25 ) print ( 'Training Features Shape:' , train_features . shape ) print ( 'Training Labels Shape:' , train_labels . shape ) print ( 'Testing Features Shape:' , test_features . shape ) print ( 'Testing Labels Shape:' , test_labels . shape ) # Instantiate Model rf = RandomForestRegressor ( n_estimators = 100 ) # Train Model rf . fit ( train_features , train_labels ) Training Features Shape: (119822, 11) Training Labels Shape: (119822,) Testing Features Shape: (39941, 11) Testing Labels Shape: (39941,) RandomForestRegressor() Let's check out our predictions on the test set (remember our model hasn't seen any of this data so it should be a true reflection on how we'd perform on some new races that would happen this afternoon say) # Use the forest's predict method on the test data predicted_bsp = rf . predict ( test_features ) predicted_bsp array([268.9971, 4.9892, 24.2727, ..., 29.067 , 4.6216, 16.3991]) Seems reasonable. All well and good though is the prediction any good? Let's measure it using MAE in the same way as we did before. # Let's test our estimate vs our others in the same way as before testDf = test_features testDf [ 'bsp' ] = test_labels testDf [ 'rf_bsp_prediction' ] = predicted_bsp ( pd . melt ( testDf [[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' , 'rf_bsp_prediction' ]], id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } estimate error 6 rf_bsp_prediction 0.088898 0 back_best 0.091860 3 ladder_mid_point 0.093477 2 geometric_mid_point 0.094702 4 lay_best 0.103435 5 near_price 0.121456 1 far_price 0.575946 Nice that's significantly better than the best previous estimate at this time slice. To validate it further let's use the same model to predict the BSP using the market state 10 seconds after the scheduled jump instead of at the exact scheduled off. None of the rows (or samples) in this time slice have been seen by the model during the training step so it should provide a robust out of sample estimate of the models performance on unseen data. # Validate it on a completely different time point - 10 seconds after scheduled jump outOfSample = df [[ 'sbsj' , 'atb_ladder' , 'atl_ladder' , 'bsp' , 'traded_volume' , 'near_price' , 'far_price' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] outOfSample = outOfSample [ outOfSample . sbsj == - 10 ] . dropna () outOfSample = ( outOfSample . assign ( wap_to_get_back_pool = lambda x : x . apply ( lambda x : wapToGetBack ( x . bsp_back_pool_stake , x . atb_ladder ), axis = 1 )) . assign ( wap_to_get_lay_pool = lambda x : x . apply ( lambda x : wapToGetLay ( x . bsp_lay_pool_liability , x . atl_ladder ), axis = 1 )) ) # Produce Predictions outofsamplebspprediction = rf . predict ( outOfSample . drop ( columns = [ 'bsp' , 'sbsj' , 'atb_ladder' , 'atl_ladder' ])) outofsamplebspprediction array([ 6.395 , 3.5781, 6.3449, ..., 503.9829, 220.1171, 54.7511]) outOfSample [ 'rf_bsp_prediction' ] = outofsamplebspprediction ( pd . melt ( outOfSample [[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' , 'rf_bsp_prediction' ]], id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } estimate error 6 rf_bsp_prediction 0.079198 0 back_best 0.084658 3 ladder_mid_point 0.086128 2 geometric_mid_point 0.087326 4 lay_best 0.098311 5 near_price 0.109640 1 far_price 0.501813 Still significantly better on the out of sample set which is a really positive sign.","title":"2.3.4 Machine Learning"},{"location":"historicData/analysingAndPredictingBSP/#235-next-steps","text":"To improve this model I'd include multiple time slices in the training sample and use the seconds before scheduled jump as a feature as I would estimate that the predictive dynamics of each of these features is dynamic and affected by how mature + how close to settlement the market is. To implement this model in your bet placement code you'd simply need to save the model object (some info about how to do this with sklearn can be found here here ). Your key challenge will be making sure you can produce the exact inputs you've created in this development process from the live stream or polling API responses, but if you've gotten this far it won't be a huge challenge for you.","title":"2.3.5 Next Steps"},{"location":"historicData/analysingAndPredictingBSP/#30-conclusion","text":"I've taken you through a quick crash course in the Betfair BSP including: What it is How it's created How it's traded on betfair Australian thoroughbred markets How efficient it is and a methodology for measuring its efficiency in different contexts The accuracy of the projected SP and how it compares with other estimates How to build your own custom projection that's better than anything available out of the box The analysis focused on thoroughbred markets but could easily be extended to other racing codes or sports markets that have BSP enabled. The custom SP projection methodology could be used for anything from staking your model more accurately or with some improvement maybe as part of a automated trading strategy.","title":"3.0 Conclusion"},{"location":"historicData/analysingAndPredictingBSP/#31-over-to-you","text":"We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support - There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space","title":"3.1 Over to you"},{"location":"historicData/analysingAndPredictingBSP/#32-complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github # %% [markdown] # # Analysing and Predicting The BSP # # # ## 0.1 Setup # # Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language. # # Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files. # # You'll also need `betfairlightweight` which you can install with something like `pip install betfairlightweight`. # %% import pandas as pd import numpy as np import requests import os import re import csv import plotly.express as px import plotly.graph_objects as go import math import logging import yaml import csv import tarfile import zipfile import bz2 import glob import ast from datetime import date , timedelta from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) # %% [markdown] # ## 0.2 Context # # The BSP is betting product offered by betfair (on large enough markets) that gives customers a chance to back or lay any selection at a \"fair\" price. Without getting too complex too quickly, the BSP allows you lock in a bet at any time after the market is openened and for as much stake as you can afford. The BSP is a good option for many different segments of customers: # # - Recreational punters that don't have a particular strategy for trying to get the best odds can lock in a price that is (in the aggregate) a lot better than what they'd get at a corporate book or they'd get by taking limit bets early in a market's trading # - Automated customers that don't want the hassle of managing live market trading can implement automated strategies a lot easier whilst also protecting them from edge cases like race reschedules # - Is perfect for simply backtesting fundemental models as it's a resiliant and robust single price # # Despite it being a good option for a lot of customers it's also a fairly contraversial topic for some other types of customers. Some people firmly believe that the BSP on big markets reflects the \"true chance\" of a selection so betting it is a fools errand that will simply lose you commission over the long run. You might have heard a version of this story before: given the large pool sizes, the 0% overround, the settlement at the exact moment the market is suspended the BSP perfectly synthesises all available public information and demand and arrives at a true fair odds. Some will attempt to prove this to you by showing you a predicted chance vs observed win rate scatterplot which shows a perfect correlation between chance implied by the BSP and a horses true chance. Whilst I don't disagree that the BSP is a **very strong** estimate of a selections chance it's pretty obviously not perfect. # # Furthermore, it presents some other tricky challenges to use in practical situations. It's not knowable perfectly before it's the exact moment of market suspension so many model or strategy builders make the mistake of unknowingly leaking it into their preplay model development or their theoretical staking calculations. Where the final number will land is actually another source of uncertainty in your processes which presents anothing forecasting / predictive modelling application as I'll explore later in this piece. I'll take you through how I'd measure the accuracy of the BSP, show you how it's traded on the exchange, and take you through a host of methods of estimating the BSP and build a custom machine learning approach that's better than each of them. # # ## 0.3 The Algorithm # # The actual logic of how betfair arrives at the final BSP number is quite complex and for a few reasons you won't be able to perfectly replicate it at home. However, the general gist of the BSP reconciliation algorithm that is executed just as the market suspended goes something like: # # - The algorithm combines 4 distinct groups of open bets for a given selection: # + Non price limited BSP orders on both the back and lay side (`market_on_close` orders) # + Price limited orders on both the back and lay side (`limit_on_close` orders) # + All non filled open lay orders # + All non filled open back orders # - It then combines them all together, passes a sophisticated balancing algorithm over the top of them and arrives at a single fair price for the BSP that balances the demand on either side of the ledger # # ## 0.4 This Example # # For this exercise we'll again take advantage of the betfair historical stream json files. The slice of betfair markets I'll be analysing is all thoroughbred races over July 2020 - June 2021. # # As an aside the projected BSP number you see on the betfair website isn't collected inside betfair's own internal database of orders, so any custom data request you may be able to get as a VIP won't include this number. So if you were planning to include it in any kind of projection or bet placement logic operation you were making the only way to anlayse it historically is to mine these data files. Another good reason to learn the skills to do so! # %% [markdown] # # 1.0 Data # # Like the previous tutorial we won't be able to collapse the stream data down into a single row per runner because I'm interested in anlaysing how the projected BSP moves late in betfair markets. I'm also interested in plotting the efficiency of certain odds values at certain distinct time points leading up the the races so I need multiple records per runner. # # Like in the previous tutorial I'll split out the selection metadata, BSP and win flag values as a seperate data file to reduce the size of the datafiles extracted for this analysis. # # For the preplay prices dataset I'll: # # - Start extraction at 2 mins before the scheduled off # - Extract prices every 10 seconds thereafter until the market is suspended # - I'll also extract the final market state the instant before the market is suspended # # ## 1.1 Sourcing Data # # First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Aask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder. # # ## 1.2 Utility functions # # First like always we'll need some general utility functions that you may have seen before: # %% # General Utility Functions # _________________________________ def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def pull_ladder ( availableLadder , n = 5 ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : n ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"p\" ] = price out [ \"v\" ] = volume return ( out ) def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # %% [markdown] # ## 1.3 Selection Metadata # # Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes. # %% def final_market_book ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): return ( None ) for market_book in market_books : last_market_book = market_book return ( last_market_book ) def parse_final_selection_meta ( dir , out_file ): with open ( out_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,venue,market_time,selection_name,win,bsp \\n \" ) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) last_market_book = final_market_book ( stream ) if last_market_book is None : continue # Extract Info ++++++++++++++++++++++++++++++++++ runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in last_market_book . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'win' : np . where ( r . status == \"WINNER\" , 1 , 0 ), 'sp' : r . sp . actual_sp } for r in last_market_book . runners ] # Return Info ++++++++++++++++++++++++++++++++++ for runnerMeta in runnerMeta : if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( last_market_book . market_id ), runnerMeta [ 'selection_id' ], last_market_book . market_definition . venue , last_market_book . market_definition . market_time , runnerMeta [ 'selection_name' ], runnerMeta [ 'win' ], runnerMeta [ 'sp' ] ) ) # %% selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) print ( \"__ Parsing Selection Metadata ___ \" ) # parse_final_selection_meta(stream_files, selection_meta) # %% [markdown] # ## 1.4 Preplay Prices and Projections # # In this set of preplay prices I'm interested in many of the same fields as we've extracted in previous tutorials as well as fields relating to the current state of the BSP. # # These objects sit under the `sp` slot within the returned `runner` object. The fields we'll extract are: # # - The so called \"near price\" # + The near price is the projected SP value you can see on the website # + It includes both bets already placed into the SP pools as well as open limit orders to estimate what the final BSP value will be # - The so called \"far price\" # + This is the same as the near price except it excludes limit orders on the exchange # + This makes it fairly redundant value and we'll see how poor of an estimator it is a bit later # - The volume currently bet into the BSP back pool # - The liability currently laid into the BSP lay pool # # We'll also extract the top 5 rungs of the available to back and available to lay ladders as well as the traded volume of limit bets. # # It's worth noting that I am discarding some key information about the BSP pools that I could have extracted if I wanted to. The current SP bets are laid out in a way that I could split out `limit_on_close` as well as `market_on_close` sp bets but I've rolled everything together in SP stake on the back side and sp liability on the lay side. This is just to reduce complexity of this article but including it would increase the predictive power of the BSP model in the final step. # %% def loop_preplay_prices ( s , o ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () marketID = None tradeVols = None time = None last_book_recorded = False prev_book = None for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): break for market_book in market_books : # Time Step Management ++++++++++++++++++++++++++++++++++ if marketID is None : # No market initialised marketID = market_book . market_id time = market_book . publish_time elif market_book . inplay and last_book_recorded : break else : seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () if seconds_to_start > 120 : # Too early before off to start logging prices prev_book = market_book continue else : # Update data at different time steps depending on seconds to off wait = 10 # New Market if market_book . market_id != marketID : last_book_recorded = False marketID = market_book . market_id time = market_book . publish_time continue # (wait) seconds elapsed since last write elif ( market_book . publish_time - time ) . total_seconds () > wait : time = market_book . publish_time # if current marketbook is inplay want to record the previous market book as it's the last preplay marketbook elif market_book . inplay : last_book_recorded = True market_book = prev_book # fewer than (wait) seconds elapsed continue to next loop else : prev_book = market_book continue # Execute Data Logging ++++++++++++++++++++++++++++++++++ for runner in market_book . runners : try : atb_ladder = pull_ladder ( runner . ex . available_to_back , n = 5 ) atl_ladder = pull_ladder ( runner . ex . available_to_lay , n = 5 ) except : atb_ladder = {} atl_ladder = {} limitTradedVol = sum ([ rung . size for rung in runner . ex . traded_volume ]) o . writerow ( ( market_book . market_id , runner . selection_id , market_book . publish_time , int ( limitTradedVol ), # SP Fields runner . sp . near_price , runner . sp . far_price , int ( sum ([ ps . size for ps in runner . sp . back_stake_taken ])), int ( sum ([ ps . size for ps in runner . sp . lay_liability_taken ])), # Limit bets available str ( atb_ladder ) . replace ( ' ' , '' ), str ( atl_ladder ) . replace ( ' ' , '' ) ) ) prev_book = market_book def parse_preplay_prices ( dir , out_file ): with open ( out_file , \"w+\" ) as output : writer = csv . writer ( output , delimiter = ',' , lineterminator = ' \\r\\n ' , quoting = csv . QUOTE_ALL ) writer . writerow (( \"market_id\" , \"selection_id\" , \"time\" , \"traded_volume\" , \"near_price\" , \"far_price\" , \"bsp_back_pool_stake\" , \"bsp_lay_pool_liability\" , \"atb_ladder\" , 'atl_ladder' )) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) loop_preplay_prices ( stream , writer ) # %% price = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) print ( \"__ Parsing Selection Prices ___ \" ) # parse_final_selection_meta(stream_files, price) # %% [markdown] # # 2.0 Analysis # # First step let's boot up the datasets we extracted in the previous steps and take a look at what we've managed to extract from the raw stream files. # # ## 2.1 Load and Inspect # # First we have the highlevel selection metadata as we have already seen in other tutorials # %% selection = pd . read_csv ( \"[PATH TO YOUR SELECTION METADATA FILE]\" , dtype = { 'market_id' : object , 'selection_id' : object }, parse_dates = [ 'market_time' ]) selection . head ( 3 ) # %% [markdown] # Now let's load the prices file. We'll apply some extra logic to parse the ladder columns into dictionaries and also remove the first odds record per group as it's the first record as the market was instantiated. # %% prices = pd . read_csv ( \"[PATH TO YOUR PRICES FILE]\" , quoting = csv . QUOTE_ALL , dtype = { 'market_id' : 'string' , 'selection_id' : 'string' , 'atb_ladder' : 'string' , 'atl_ladder' : 'string' }, parse_dates = [ 'time' ] ) # Parse ladder columns prices [ 'atb_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atb_ladder' ]] prices [ 'atl_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atl_ladder' ]] # Drop the first row within each group prices = prices . drop ( prices . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( 0 ) . index ) prices . head ( 3 ) # %% f 'The shape of the prices data file is { prices . shape [ 0 ] } rows and { prices . shape [ 1 ] } columns' # %% # Let's have a look at the prices datafile for a distinct market and selection prices . query ( 'market_id == \"1.183995724\" and selection_id == \"22832649\"' ) # %% [markdown] # We can see some expected behaviour as we zoom in on a particular selection # # - The traded volume increases on this selection as we get closer to the jump # - The projected BSP (the `near_price` column) stays constant for a number of increments as its update is cached for 60 seconds at a time # - The sizes in the BSP pools also increases as we get closer to the jump # - The prices offered and traded closer to the jump are closer to the BSP than those at the start of the 2 minute period # %% [markdown] # ## 2.2 Transform and Assemble # # We have our 2 core datasets, but we'd prefer to work with one now. We'd also like to add some key columns that will be reused throughout our analysis so we'll add those now too. # # # ### 2.2.1 Mid points # # The first semi-interesting thing we'll do in this analysis is add selection mid-points to our dataset. Eventually we're going to be interested in estimating the BSP and measuring the efficiency of certain prices at various points leading up to the race. # # Betfair markets work like all markets with bids and spreads. The market equilibrium forms around the best price offered on either side of the market to back or to lay. These top prices each have some inherent advantage built into it for the offerer. For example in early markets the best offers on either side of the market might be really wide (say 1.80 as a best back and 2.50 as a best lay). Given the price discovery process is still immature each bidder gets a large premium, backed into their offer price, which compensates them for providing betting opportunities with little to no information provided from other market participants. This spread will naturally get tighter and tighter as the market matures and more participants seek to get volume down and must be more and more competitive. But what's the price \"equilibrium\" in each case? # # Well it's up to you but I'll provide you two ways of finding the central mid-point of a bid-ask spread on betfair markets. The problem we're solving for here is the non-linearity of prices in odds space. We have some intuition for this: when we see a market spread of 10-100 in early trading we have an understanding that the true midpoint of this market is somewhere around 25-35 not the 55 you'd get if you simply took the (arithmetic) mean of those two numbers. # # Two techniquest for accounting for that non-linearity are as follows. # # **Ladder Midpoint** # # The ladder midpoint method takes advantage of the fact that the betfair price ladder itself accounts for the nonlinearity of prices in odds space. The method calculated the difference in number of rungs on the betfair ladder, halves it, and shifts the best back or lay price that number of rungs towards the centre. This will generally provide a much better idea of the market midpoint than a simple arithmetic mean of the two prices. # # **Geometric Mean** # # Unfortunately the ladder method is a little computationally expensive. A good approximation for this approach is to take the geometric mean of the best back and best lay values. The geometric mean is a special kind of mean that you may have never used before that is more appropriate for purposes like this. It is calculated like: `sqrt(x1 * x2 * ...)`. This number will also provide a much better estimate of the market midpoint than the simple arithmetic mean. # # The latter calculation is trivial. The former requires a suite of betfair tick arithmetic functions that I'll put below. It may seem like overkill for this exercise (and it is) but hopefully these functions might be of use to you for other purposes. # %% # Define the betfair tick ladder def bfTickLadder (): tickIncrements = { 1.0 : 0.01 , 2.0 : 0.02 , 3.0 : 0.05 , 4.0 : 0.1 , 6.0 : 0.2 , 10.0 : 0.5 , 20.0 : 1.0 , 30.0 : 2.0 , 50.0 : 5.0 , 100.0 : 10.0 , 1000.0 : 1000 , } ladder = [] for index , key in enumerate ( tickIncrements ): increment = tickIncrements [ key ] if ( index + 1 ) == len ( tickIncrements ): ladder . append ( key ) else : key1 = [ * tickIncrements ][ index ] key2 = [ * tickIncrements ][ index + 1 ] steps = ( key2 - key1 ) / increment for i in range ( int ( steps )): ladder . append ( round ( key + i * increment , 2 )) return ( ladder ) bfticks = bfTickLadder () # Round a decimal to the betfair tick value below def bfTickFloor ( price , includeIndex = False ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () ind = [ n for n , i in enumerate ( bfticks ) if i >= price ][ 0 ] if includeIndex : if bfticks [ ind ] == price : return (( ind , price )) else : return (( ind - 1 , bfticks [ ind - 1 ])) else : if bfticks [ ind ] == price : return ( price ) else : return ( bfticks [ ind - 1 ]) # Calculate the numder of ticks between two tick values def bfTickDelta ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) x = bfTickFloor ( p1 , includeIndex = True ) y = bfTickFloor ( p2 , includeIndex = True ) return ( x [ 0 ] - y [ 0 ]) def bfTickShift ( p , rungs ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () flr = bfTickFloor ( p , includeIndex = True ) return ( bfticks [ flr [ 0 ] + rungs ]) def bfLadderMidPoint ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) delta = - 1 * bfTickDelta ( p1 , p2 ) if delta == 1 : return ( p1 ) elif delta % 2 != 0 : return ( bfTickShift ( p1 , math . ceil ( delta / 2 ))) else : return ( bfTickShift ( p1 , math . floor ( delta / 2 ))) # %% # Let's test a midpoint using the ladder mid point method bfLadderMidPoint ( 10 , 100 ) # %% # And for illustrative purposes let's calculate the geomtric mean of these values np . sqrt ( 10 * 100 ) # %% [markdown] # Let's put this all together while stitching together our two core datasets. # %% # Join and augment df = ( selection . merge ( prices , on = [ 'market_id' , 'selection_id' ]) . assign ( sbsj = lambda x : round (( x [ 'market_time' ] - x [ 'time' ]) . dt . total_seconds () / 10 ) * 10 ) . assign ( back_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atb_ladder' ]]) . assign ( lay_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atl_ladder' ]]) . assign ( geometric_mid_point = lambda x : round ( 1 / np . sqrt (( 1 / x [ 'back_best' ]) * ( 1 / x [ 'lay_best' ])), 3 )) . assign ( ladder_mid_point = lambda x : x . apply ( lambda x : bfLadderMidPoint ( x . back_best , x . lay_best ), axis = 1 )) . replace ([ np . inf , - np . inf ], np . nan ) ) df . head ( 3 ) # %% [markdown] # ## 2.3 Analysing The BSP # # Before we embark on our predictive exercise let's analyse the BSP to get a feel for it as an entity. # # ### 2.3.1 Volumes # # Ever wondered how much volume is traded on the BSP? How does it compare to limit bets? Well with our parsed stream data we can answer those questions! Now the BSP volume will be the bigger of the BSP back stake and the lay stake (which you can infer by the final BSP and the total lay liability). # # # %% # Volume Traded # _________________________ # Extract the final time slice of data which includes the total preplay volumes traded across limit and BSP poools volumeDf = df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( - 1 )[[ 'market_id' , 'selection_id' , 'bsp' , 'traded_volume' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' ]] # Infer the biggest of the two BSP stakes volumeDf = ( volumeDf . assign ( lay_stake = lambda x : x [ 'bsp_lay_pool_liability' ] / ( x [ 'bsp' ] - 1 )) . assign ( bsp_stake = lambda x : x [[ 'lay_stake' , 'bsp_back_pool_stake' ]] . max ( axis = 1 )) ) ( volumeDf . groupby ( 'market_id' , as_index = False ) . agg ({ 'traded_volume' : 'sum' , 'bsp_stake' : 'sum' }) . agg ({ 'traded_volume' : 'mean' , 'bsp_stake' : 'mean' }) ) # %% [markdown] # So in an average thoroughbred market there's about 98k traded limit volume and 7,300 BSP traded stake. So approximately 7% of thoroughbred volume is traded at the BSP at least for our sample of thoroughbred races. # # # ## 2.3.2 Efficiency? # # Now you may have heard this story before: **you can't beat the BSP it's too efficient!**. I'm not sure people really have a firm idea about what they're talking about when they say this. # # Typically what you'll see in a discussion about efficiency is the predicted vs observed scatterplot. Let's see if we can reproduce this chart. # # First let's assemble a dataframe that we can use for this chart as well as others. What we'll do is we'll extract the BSP and a price value at 5 different slices before the race starts. We could chose any price point (we'll analyse the difference between them in a subsequent step) but for this section I'm going to take the preplay market estimate as the geometric market midpoint (you'll have to trust me for now that this is a sensible decision). # %% # Extract the geomtric market mid point at time slices: 120, 90, 60, 30, and 0 seconds from the scheduled off preplay = df [ df . sbsj . isin ([ 120 , 90 , 60 , 30 , 0 ])][[ 'market_id' , 'selection_id' , 'win' , 'sbsj' , 'geometric_mid_point' ]] . sort_values ([ 'market_id' , 'selection_id' , 'sbsj' ], ascending = [ True , True , False ]) . rename ( columns = { 'geometric_mid_point' : 'odds' }) . assign ( type = lambda x : \"seconds before off: \" + x [ 'sbsj' ] . astype ( int ) . astype ( str )) # Extract the BSP values bsp = df . sort_values ([ 'market_id' , 'selection_id' , 'time' ], ascending = [ True , True , False ]) . groupby ([ 'market_id' , 'selection_id' ]) . head ( 1 )[[ 'market_id' , 'selection_id' , 'win' , 'sbsj' , 'bsp' ]] . rename ( columns = { 'bsp' : 'odds' }) . assign ( type = \"bsp\" ) # Append them together accuracyFrame = pd . concat ([ preplay , bsp ]) . dropna () accuracyFrame . head ( 5 ) # %% [markdown] # Now we'll filter just on our BSP records and plot the observed vs actual scatterplot # %% # BSP Scatter # __________________ winRates = ( accuracyFrame . query ( 'type == \"bsp\"' ) . assign ( implied_chance = lambda x : round ( 20 * ( 1 / x [ 'odds' ])) / 20 ) . groupby ( 'implied_chance' , as_index = False ) . agg ({ 'win' : 'mean' }) ) fig = px . scatter ( winRates , x = \"implied_chance\" , y = \"win\" , template = \"plotly_white\" , title = \"BSP: implied win vs actual win\" ) fig . add_trace ( go . Scatter ( x = winRates . implied_chance , y = winRates . implied_chance , name = 'no bias' , line_color = 'rgba(8,61,119, 0.3)' ) ) fig . show ( \"png\" ) # %% [markdown] # Ok aside from some small sample noise at the top end (there's very few horses that run at sub 1.20 BSPs) we can see that the BSP is pretty perfectly.... efficient? Is that the right word? I'd argue that it's very much not the right word. Let me illustrate with a counter example. Let's plot the same chart for the BSP as well as our 5 other price points. # %% # Bsp + Other Odds Scatter # __________________ winRates = ( accuracyFrame . assign ( implied_chance = lambda x : round ( 20 * ( 1 / x [ 'odds' ])) / 20 ) . groupby ([ 'type' , 'implied_chance' ], as_index = False ) . agg ({ 'win' : 'mean' }) ) fig = px . scatter ( winRates , x = \"implied_chance\" , y = \"win\" , color = 'type' , template = \"plotly_white\" , title = \"Comparing Price Points: implied win vs actual win\" ) fig . add_trace ( go . Scatter ( x = winRates . implied_chance , y = winRates . implied_chance , name = 'no bias' , line_color = 'rgba(8,61,119, 0.3)' ) ) fig . show ( \"png\" ) # %% [markdown] # So they're all efficient? And indecernibly as efficient as one another? # # Well, to cut a long and possibly boring story short this isn't the right way to measure efficiency. What we're measure here is **bias**. All my scatter plot here tells me is if there's any systematic bias in the BSP, ie groups of BSPs that aren't well calibrated with actual outcomes. That is, for example, that perhaps randomly the group of horses that BSP around 2 don't happen to win 50% of the time maybe there was a sytemic bias that short favourites were underbet and these selections actually won 55% of the time. That would be a price bias in the BSP that someone could take advatange at just by looking at historical prices and outcomes alone. # # For and even simpler counter point: I could create a perfectly well calibrated estimate that assigned a single odds value to every horse which was the overall horse empirical win rate over our sample: 10.25% (which is merely a reflection of field sizes). This estimate would be unbiased, and would pass through our scatterplot method unscathed but would it be an efficient estimate? Clearly not. # %% df . agg ({ 'win' : 'mean' }) # %% [markdown] # Bias only tells us if there's a systematic way of exploiting the odds values themselves. I could have told you that this was unlikely but the scatterplot proves it. # # How else could we measure efficiency? I propose using the `logloss` metric. # # Let's calculate the logloss of the BSP # %% # Logloss ++++++++++++++++++++++++++++++++++++++++ from sklearn.metrics import log_loss # Overall Logloss # _________________ bspLoss = log_loss ( y_pred = 1 / accuracyFrame . query ( 'type == \"bsp\"' )[ 'odds' ], y_true = accuracyFrame . query ( 'type == \"bsp\"' )[ 'win' ] ) print ( f 'The overall logloss of the BSP is { round ( bspLoss , 4 ) } ' ) # %% [markdown] # Ok what does this mean? Well nothing really. This metric won't tell you anything by itself it's just useful for relative comparisons. Let's plot the logloss of our geometric midpoint at our various timeslices. # # %% # Logloss at Different Time Points # _________________ accuracyFrame . groupby ( 'type' , as_index = False ) . apply ( lambda x : log_loss ( y_pred = 1 / x [ 'odds' ], y_true = x [ 'win' ])) . rename ( columns = { None : 'logloss' }) . sort_values ( 'logloss' ) # %% # And in chart form fig = px . bar ( accuracyFrame . groupby ( 'type' , as_index = False ) . apply ( lambda x : log_loss ( y_pred = 1 / x [ 'odds' ], y_true = x [ 'win' ])) . rename ( columns = { None : 'logloss' }) . sort_values ( 'logloss' , ascending = False ), x = \"type\" , y = \"logloss\" , template = \"plotly_white\" , title = \"Logloss Of Odds At Various Time Points\" ) fig . update_yaxes ( range = [ .2755 , .2765 ]) fig . show ( \"png\" ) # %% [markdown] # Now this is a cool graph. This is exactly like we would have intiuited. The market sharpens monotonically as we approach the market jump with the BSP being the most effiecient of all the prices! # # Hopefully you can now see the logical failing of measuring bias over market efficiency and it changes the way you think about your bet placement. # # Let's move on to what we're here for: is it possible to predict the BSP. # %% [markdown] # ## 2.4 Predicting the BSP # # Ok so I'm interested in finding the answer to the question: which estimate of BSP should i use when betting on the exchange and is it possible to beat the projected SP provided on the website and through the API? # # Well the first thing we should recognise about this projection is that it's cached. What does that mean? It means it only updated every 60 seconds. This suprised me when i first learned it and it was actually causing issues in my bet placement logic for the SP. # # Let's have a look at a selection to see how this works in practice # %% # Lets take a sample of a market and a selection dSlice = df . query ( 'market_id == \"1.182394184\" and selection_id == \"39243409\"' ) . dropna () # %% def chartClosingPrices ( d ): fig = px . line ( pd . melt ( d [: - 1 ][[ 'sbsj' , 'back_best' , 'near_price' ]], id_vars = 'sbsj' , var_name = 'price' ), x = 'sbsj' , y = 'value' , color = 'price' , template = 'plotly_white' , title = \"Selection\" , labels = { 'sbsj' : \"Seconds Before Scheduled Jump\" } ) fig . update_layout ( font_family = \"Roboto\" ) fig . add_trace ( go . Line ( x = dSlice . sbsj , y = dSlice . bsp , name = 'BSP' , line_color = 'rgba(8,61,119, 0.3)' , mode = \"lines\" ) ) fig [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig . show ( \"png\" ) chartClosingPrices ( dSlice ) # %% [markdown] # The red line is the projected BSP, you can see that it's not very responsive. As the best back price comes in from ~3 to 2.9 leading up to the jump the projected SP doesn't move because it's cached. If you were relying on this number for something important and you were using it in that period you were using stale information and you'd be worse off for it. In this instance the final SP was 2.79 so you may have made the wrong betting decision. # # This is somewhat counter intuitive because the projected sp (the so called near price) should be a good estimate of the BSP because it synthetically runs the BSP algorithm on the current market state and produces and estimate, so you would think that it'd be a pretty good estimate. # # Let's widen our sample a bit and see how it performs across our entire sample. We'll slice the data at the exact scheduled off and see how accurate various price points are at predicting what the final BSP is. We'll use mean absolute error (MAE) as our error metric. We'll assess 6 price points: # # - The near price (projected sp) # - The far price (projected sp excluding limit orders) # - The best back price # - The best lay price # - The ladder midpoint price # - The geometric midpoint price # %% # Measurement # ________________________ estimatesDf = df [ df . sbsj == 0 ][[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] ( pd . melt ( estimatesDf , id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) # %% [markdown] # So a bit surprisingly, in thoroughbred markets at the scheduled off your best to just use the current best back price as your estimate of the BSP. It significantly outperforms the projected SP and even some of our midpoint methods. # # Let's change the timeslice a little and take the very last moment before the market settles and see which performs best. # %% lastEstimatesDf = df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . nth ( - 1 )[[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] ( pd . melt ( lastEstimatesDf , id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) # %% [markdown] # First thing to notice is the estimates get a lot better than at the scheduled off, as we'd expect. A bit surprisingly the projected SP is still very weak due to the caching issue. In this scenario the geometric mid point beforms significantly better than the current best back price which suggests that as the late market is forming the back and lay spread with start converging to the fair price and eventual BSP. I personally use the geometric midpoint as my BSP estimate as it's a quick and easy metric that performs pretty well. # # What if you want more though? Is it possible to do better than these metrics? These simple price points use no information about what's in the BSP pools, surely if we used this information we'd be able to do better. Let's try to use machine learning to synthesise all this information at once. # # ### 2.3.4 Machine Learning # # We'll build a quite random forest model to estimate the BSP with current price and pool size information. This is a very simple application of machine learning so hopefully gives you an idea of its power without being too complex. # # Now we need an intelligent way of turning our pool and ladder information into a feature to insert into our model, how could we engineer this feature? Well what we'll do is calculate a WAP required to fill our pool stake on the back and lay side. What does that mean? Say we've got $200 sitting in the BSP back pool and $200 sitting on the top box of 2.5 on the back side, in this instance our WAP value would be exactly 2.5 cause we can fill it all at the top box. But if however, there was only $100 in the top box then we'd need to move down the ladder to fill the remaining $100 volume. Our feature will simulate this allocation logic and return the final weighted average price required to fill the total BSP pool. Here's the functions to do it on the back and lay side respectively: # %% def wapToGetBack ( pool , ladder ): price = ladder [ 'p' ] volume = ladder [ 'v' ] try : indmax = min ([ i for ( i , j ) in enumerate ( cVolume ) if j > pool ]) + 1 except : indmax = len ( volume ) return ( round ( sum ([ a * b for a , b in zip ( price [: indmax ], volume [: indmax ])]) / sum ( volume [: indmax ]), 4 )) def wapToGetLay ( liability_pool , ladder ): price = ladder [ 'p' ] volume = ladder [ 'v' ] liability = [( a - 1 ) * b for a , b in zip ( price , volume )] cLiability = np . cumsum ( liability ) try : indmax = min ([ i for ( i , j ) in enumerate ( cLiability ) if j > liability_pool ]) + 1 except : indmax = len ( volume ) return ( round ( sum ([ a * b for a , b in zip ( price [: indmax ], volume [: indmax ])]) / sum ( volume [: indmax ]), 4 )) # %% [markdown] # Now we'll set up our model matrix which will be the market state at the exact scheduled off. We'll also add our custom features. # %% model_matrix = df [[ 'sbsj' , 'atb_ladder' , 'atl_ladder' , 'bsp' , 'traded_volume' , 'near_price' , 'far_price' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] # Filter at scheduled jump model_matrix = model_matrix [ model_matrix . sbsj == 0 ] . dropna () model_matrix = ( model_matrix . assign ( wap_to_get_back_pool = lambda x : x . apply ( lambda x : wapToGetBack ( x . bsp_back_pool_stake , x . atb_ladder ), axis = 1 )) . assign ( wap_to_get_lay_pool = lambda x : x . apply ( lambda x : wapToGetLay ( x . bsp_lay_pool_liability , x . atl_ladder ), axis = 1 )) ) # Drop other columns model_matrix . drop ( columns = [ 'sbsj' , 'atb_ladder' , 'atl_ladder' ], inplace = True ) model_matrix . head ( 3 ) # %% [markdown] # Now the machine learning. Sklearn make this very simple, in our case it's a few lines only. We'll split our data into train and test sets and train a small random forrest to predict the BSP. # %% from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split # Setup Train / Test train_features , test_features , train_labels , test_labels = train_test_split ( model_matrix . drop ( columns = [ 'bsp' ]), model_matrix [ 'bsp' ], test_size = 0.25 ) print ( 'Training Features Shape:' , train_features . shape ) print ( 'Training Labels Shape:' , train_labels . shape ) print ( 'Testing Features Shape:' , test_features . shape ) print ( 'Testing Labels Shape:' , test_labels . shape ) # Instantiate Model rf = RandomForestRegressor ( n_estimators = 100 ) # Train Model rf . fit ( train_features , train_labels ) # %% [markdown] # Let's check out our predictions on the test set (remember our model hasn't seen any of this data so it should be a true reflection on how we'd perform on some new races that would happen this afternoon say) # %% # Use the forest's predict method on the test data predicted_bsp = rf . predict ( test_features ) predicted_bsp # %% [markdown] # Seems reasonable. All well and good though is the prediction any good? Let's measure it using MAE in the same way as we did before. # %% # Let's test our estimate vs our others in the same way as before testDf = test_features testDf [ 'bsp' ] = test_labels testDf [ 'rf_bsp_prediction' ] = predicted_bsp ( pd . melt ( testDf [[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' , 'rf_bsp_prediction' ]], id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) # %% [markdown] # Nice that's significantly better than the best previous estimate at this time slice. To validate it further let's use the same model to predict the BSP using the market state 10 seconds after the scheduled jump instead of at the exact scheduled off. None of the rows (or samples) in this time slice have been seen by the model during the training step so it should provide a robust out of sample estimate of the models performance on unseen data. # %% # Validate it on a completely different time point - 10 seconds after scheduled jump outOfSample = df [[ 'sbsj' , 'atb_ladder' , 'atl_ladder' , 'bsp' , 'traded_volume' , 'near_price' , 'far_price' , 'bsp_back_pool_stake' , 'bsp_lay_pool_liability' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' ]] outOfSample = outOfSample [ outOfSample . sbsj == - 10 ] . dropna () outOfSample = ( outOfSample . assign ( wap_to_get_back_pool = lambda x : x . apply ( lambda x : wapToGetBack ( x . bsp_back_pool_stake , x . atb_ladder ), axis = 1 )) . assign ( wap_to_get_lay_pool = lambda x : x . apply ( lambda x : wapToGetLay ( x . bsp_lay_pool_liability , x . atl_ladder ), axis = 1 )) ) # Produce Predictions outofsamplebspprediction = rf . predict ( outOfSample . drop ( columns = [ 'bsp' , 'sbsj' , 'atb_ladder' , 'atl_ladder' ])) outofsamplebspprediction # %% outOfSample [ 'rf_bsp_prediction' ] = outofsamplebspprediction ( pd . melt ( outOfSample [[ 'bsp' , 'near_price' , 'far_price' , 'back_best' , 'lay_best' , 'geometric_mid_point' , 'ladder_mid_point' , 'rf_bsp_prediction' ]], id_vars = 'bsp' , var_name = 'estimate' ) . assign ( error = lambda x : abs ( x [ 'value' ] - x [ 'bsp' ]) / x [ 'bsp' ]) . groupby ( 'estimate' , as_index = False ) . agg ({ 'error' : 'mean' }) . sort_values ( 'error' ) ) # %% [markdown] # Still significantly better on the out of sample set which is a really positive sign. # # ## 2.3.5 Next Steps # # To improve this model I'd include multiple time slices in the training sample and use the seconds before scheduled jump as a feature as I would estimate that the predictive dynamics of each of these features is dynamic and affected by how mature + how close to settlement the market is. # # To implement this model in your bet placement code you'd simply need to save the model object (some info about how to do this with sklearn can be found here [here](https://scikit-learn.org/stable/modules/model_persistence.html)). Your key challenge will be making sure you can produce the exact inputs you've created in this development process from the live stream or polling API responses, but if you've gotten this far it won't be a huge challenge for you. # # # 3.0 Conclusion # # I've taken you through a quick crash course in the Betfair BSP including: # # - What it is # - How it's created # - How it's traded on betfair Australian thoroughbred markets # - How efficient it is and a methodology for measuring its efficiency in different contexts # - The accuracy of the projected SP and how it compares with other estimates # - How to build your own custom projection that's better than anything available out of the box # # The analysis focused on thoroughbred markets but could easily be extended to other racing codes or sports markets that have BSP enabled. The custom SP projection methodology could be used for anything from staking your model more accurately or with some improvement maybe as part of a automated trading strategy.","title":"3.2 Complete code"},{"location":"historicData/analysingAndPredictingBSP/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"historicData/analysingAndPredictingMarketMovements/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Do #theyknow ? Analysing Betfair market formation & market movements This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the Automated betting angles in Python we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo . 0.0 Setup 0.1 Importing libraries Once again I'll be presenting the analysis in a Jupyter notebook and will be using Python as a programming language. Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files. You'll also need betfairlightweight which you can install with something like pip install betfairlightweight . import pandas as pd import numpy as np import requests import os import re import csv import plotly.express as px import plotly.graph_objects as go import math import logging import yaml import csv import tarfile import zipfile import bz2 import glob import ast from datetime import date , timedelta from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) 0.2 Context You may have seen the hashtag if you're on Australian racing twitter #theyknow following a dramatic late market move for a horse that's followed by a decisive race victory. Sometimes it can seem eerie how accurate these moves are after the fact. In Betfair racing markets there's usually a flurry of activity leading up to the race start as players look to get their bets down at the best price without tipping their hand (opinion on the race) too much. Large moves can happen when large players rally around a selection who's implied chance in early trading isn't close to what it's true chance is in the upcoming race. Large moves can also happen when there's some inside information - not able to be gleaned from analysis of the horses previous races - that slowly filters out of the stable or training group. This creates opportunity in the secondary market as punters try to read these movements to make bets themselves. The task often becomes identifying which movements are caused by these sophisticated players or represent real signals of strength and which aren't. So do #theyknow generally? Before even looking at the data I can assure you that yes they do know pretty well. Strong movements in betting markets are usually pretty reliable indicators about what is about to happen. However, these moves can be overvalued by recreationals. When observing a horse plumet in from \\$3.50 to \\$2 you are usually suprised if the horse loses, but the general efficiency of late prices would suggest that this horse is going to still lose 50% of time. If you simply observe the large moves after the sharp players have corrected the market landscape you're in no better a position to bet than before the move happened. On the other hand what if would could reliably identify the move as it was happening or about to happen? That would be a recipe for successful trading of horse racing markets and no doubt this is what many players in this secondary market (analysis of Betfair markets rather than the races themselves) try to do. If you were to build up a manual qualitative strategy for this kind of market trading you need to understand: - Who the participants are - How sophisticated they are at the top end - What types of races do they bet on and for how much - When the different types of participants typically enter the market - What do bet placement patterns look like for these participants - etc. This is the kind of task that takes a lot research, years of experience watching markets, a lot of trial and error, and a lot of industry know-how. Given I'm a lazy quantitative person I'll try to see if I can uncover any of these patterns in the data alone. Put simply the central question for the second half of this piece will be: How should you think about large price moves in Betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data I'll just be analysing historical thoroughbred markets but the same approach could be applied to any sport or racing code of your interest. 0.3 This example Building market based trading strategies is a broad and fertile ground for many quantitative Betfair customers; too big to cover in a single article. I'll try to zero in on a small slice of thoroughbred markets and analyse how these markets form and how I'd start the process of trying to find the patterns in the market movements. Again hopefully this is some inspiration for you and you can pick up some of the ideas and build them out. Given volume of data (when analysing second by second slices of market data) I'll be looking at a year's worth of thoroughbred races from the 5 largest Victorian tracks: Flemington, Caulfield, Moonee Valley, Bendigo and Sandown. 1.0 Data Unlike in some of the previous tutorials we aren't going to collapse the stream data into a single row per runner. In those examples we were interested in analysing some discrete things about selections in Betfair markets like: Their final odds (bsp or last traded price) Their odds at some fixed time point or time points before the scheduled race start Other single number descriptors of the trading activity on a selection (eg total traded volume) In this analysis I want to analyse how markets form and prices move for selections as markets evolve. So we'll need to pull out multiple price points per runner - so we'll have multiple rows per runner in our parsed output dataset. To output a row for every stream update for every selection in every thoroughbred race over the last 12 months would produce a dataset far too big too analyse using normal data analysis tools - we're about 10s to 100s of billions of rows. To chop our sample down into a manageable slice I'm going to filter on some select tracks of interest (as mentioned above) and I'm also going to have 3 sections of data granularity: I won't log any of the odds or traded volumes > 30mins before the scheduled off In thoroughbreds there is non-trivial action before this point you may want to study, but it's not what I want to study here Between 30 and 10 minutes before the scheduled off I'll log data every 60 seconds 10 minutes or less to the scheuled off I'll log prices every second The code to manage this windowed granularity is in the below parsing code tweak as you wish if you want to tighten or broaden the analysis. 1.1 Sourcing data First you'll need to source the Stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Ask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder. 1.2 Utility functions # General Utility Functions # _________________________________ def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def slicePrice ( l , n ): try : x = l [ n ] . price except : x = \"\" return ( x ) def sliceSize ( l , n ): try : x = l [ n ] . size except : x = \"\" return ( x ) def pull_ladder ( availableLadder , n = 5 ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : n ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"p\" ] = price out [ \"v\" ] = volume return ( out ) Slicing the tracks we want we'll just adjust the market filter function used before to include some logic on the venue name def filter_market ( market : MarketBook ) -> bool : d = market . market_definition track_filter = [ 'Bendigo' , 'Sandown' , 'Flemington' , 'Caulfield' , 'Moonee Valley' ] return ( d . country_code == 'AU' and d . venue in track_filter and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) 1.3 Selection metadata Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes. This means we'll have to parse over the data twice but our outputs will be much smaller than if we duplicated the selection name 800 times for example. def final_market_book ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): return ( None ) for market_book in market_books : last_market_book = market_book return ( last_market_book ) def parse_final_selection_meta ( dir , out_file ): with open ( out_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,venue,market_time,selection_name,win,bsp \\n \" ) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) last_market_book = final_market_book ( stream ) if last_market_book is None : continue # Extract Info ++++++++++++++++++++++++++++++++++ runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in last_market_book . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'win' : np . where ( r . status == \"WINNER\" , 1 , 0 ), 'sp' : r . sp . actual_sp } for r in last_market_book . runners ] # Return Info ++++++++++++++++++++++++++++++++++ for runnerMeta in runnerMeta : if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( last_market_book . market_id ), runnerMeta [ 'selection_id' ], last_market_book . market_definition . venue , last_market_book . market_definition . market_time , runnerMeta [ 'selection_name' ], runnerMeta [ 'win' ], runnerMeta [ 'sp' ] ) ) selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) trading = betfairlightweight . APIClient ( username = \"username\" , password = \"password\" , app_key = \"app_key\" ) listener = StreamListener ( max_latency = None ) print ( \"__ Parsing Selection Metadata ___ \" ) # parse_final_selection_meta(stream_files, selection_meta) __ Parsing Selection Metadata ___ 1.4 Detailed preplay odds Like mentioned above there will be some time control logic injected to control the time granularity that odds are recorded in each step. Instead of widening the available to bet price ladder I'll extract the top 10 rungs of the available to back (atb) and available to lay (atl) ladders and write them both to the output file. That will give me more flexibility during the analysis to pull out things that interest me. So in total I'll extract: Top 10 ATB Ladder Top 10 ATL Ladder Total Traded Volume Volume weighted average traded price up till the current time Last Traded price def loop_preplay_prices ( s , o ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () marketID = None tradeVols = None time = None for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): break for market_book in market_books : # Time Step Management ++++++++++++++++++++++++++++++++++ if marketID is None : # No market initialised marketID = market_book . market_id time = market_book . publish_time elif market_book . inplay : # Stop once market goes inplay break else : seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () if seconds_to_start > log1_Start : # Too early before off to start logging prices continue else : # Update data at different time steps depending on seconds to off wait = np . where ( seconds_to_start <= log2_Start , log2_Step , log1_Step ) # New Market if market_book . market_id != marketID : marketID = market_book . market_id time = market_book . publish_time # (wait) seconds elapsed since last write elif ( market_book . publish_time - time ) . total_seconds () > wait : time = market_book . publish_time # fewer than (wait) seconds elapsed continue to next loop else : continue # Execute Data Logging ++++++++++++++++++++++++++++++++++ for runner in market_book . runners : try : atb_ladder = pull_ladder ( runner . ex . available_to_back , n = 10 ) atl_ladder = pull_ladder ( runner . ex . available_to_lay , n = 10 ) except : atb_ladder = {} atl_ladder = {} # Calculate Current Traded Volume + Tradedd WAP limitTradedVol = sum ([ rung . size for rung in runner . ex . traded_volume ]) if limitTradedVol == 0 : limitWAP = \"\" else : limitWAP = sum ([ rung . size * rung . price for rung in runner . ex . traded_volume ]) / limitTradedVol limitWAP = round ( limitWAP , 2 ) o . writerow ( ( market_book . market_id , runner . selection_id , market_book . publish_time , limitTradedVol , limitWAP , runner . last_price_traded or \"\" , str ( atb_ladder ) . replace ( ' ' , '' ), str ( atl_ladder ) . replace ( ' ' , '' ) ) ) def parse_preplay_prices ( dir , out_file ): with open ( out_file , \"w+\" ) as output : writer = csv . writer ( output , delimiter = ',' , lineterminator = ' \\r\\n ' , quoting = csv . QUOTE_ALL ) writer . writerow (( \"market_id\" , \"selection_id\" , \"time\" , \"traded_volume\" , \"wap\" , \"ltp\" , \"atb_ladder\" , \"atl_ladder\" )) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) loop_preplay_prices ( stream , writer ) preplay_price_file = \"[OUTPUT PATH TO CSV FOR PREPLAY PRICES]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) log1_Start = 60 * 30 # Seconds before scheduled off to start recording data for data segment one log1_Step = 60 # Seconds between log steps for first data segment log2_Start = 60 * 10 # Seconds before scheduled off to start recording data for data segment two log2_Step = 1 # Seconds between log steps for second data segment print ( \"__ Parsing Detailed Preplay Prices ___ \" ) # parse_preplay_prices(stream_files, preplay_price_file) __ Parsing Detailed Preplay Prices ___ 2.0 Analysis 2.1 Load and Assemble First let's load the raw datafiles we created in the previous step. 2.1.1 Load We have the highlevel selection metadata (1 row per selection): selection = pd . read_csv ( \"[PATH TO METADATA FILE]\" , dtype = { 'market_id' : object , 'selection_id' : object }, parse_dates = [ 'market_time' ]) selection . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue market_time selection_name win bsp 0 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 1 1.179904683 3593031 Bendigo 2021-03-02 03:00:00 2. Cornucopia 0 6.64 2 1.179904683 38629714 Bendigo 2021-03-02 03:00:00 3. Danejararose 0 21.66 And we have the detailed preplay price data for these markets + selections: prices = pd . read_csv ( \"[PATH TO PRICES FILE]\" , quoting = csv . QUOTE_ALL , dtype = { 'market_id' : 'string' , 'selection_id' : 'string' , 'atb_ladder' : 'string' , 'atl_ladder' : 'string' }, parse_dates = [ 'time' ] ) prices . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id time traded_volume wap ltp atb_ladder atl_ladder 0 1.179904683 38629713 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} 1 1.179904683 3593031 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} 2 1.179904683 38629714 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} Now it's important to observe how much data we have here. prices . shape (7774392, 8) We've got 7 million rows of price data here just for races at 5 thoroughbred tracks over the last year. Now it's not really \"big data\" in the sense you might have heard before but it's certainly a lot of rows and we'll have to think about the performance of our code a little bit more than we would if we were dealining with 1 row per selection style datasets. We need pandas to correctly interpret the dictionary columns as dictionaries so we'll run this code: # To get pandas to correctly recognise the ladder columns as dictionaries prices [ 'atb_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atb_ladder' ]] prices [ 'atl_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atl_ladder' ]] 2.1.2 Assemble Now we'll join the 2 data sets together to form a nice normalised dataframe: # Simple join on market and selection_id initially df = selection . merge ( prices , on = [ 'market_id' , 'selection_id' ]) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue market_time selection_name win bsp time traded_volume wap ltp atb_ladder atl_ladder 0 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-01 05:15:09.480 0.00 NaN NaN {} {} 1 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-02 02:30:00.552 15.95 15.44 22.0 {'p': [18.5, 18, 17.5, 17, 16, 15, 14, 13.5, 1... {'p': [24, 25, 29, 30, 38, 55, 65, 70, 90, 140... 2 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-02 02:31:06.716 15.95 15.44 22.0 {'p': [19, 18.5, 18, 17, 16, 15, 14, 13.5, 13,... {'p': [25, 29, 30, 38, 55, 65, 70, 90, 140, 24... 2.1.3 Transform Next we'll do some processing on the joined dataframe to add some columns that we can use in our analysis including calculating a numeric #seconds before the scheduled jump field that we'll use extensively. df = ( df . assign ( back_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atb_ladder' ]]) . assign ( lay_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atl_ladder' ]]) . assign ( back_vwap = lambda x : [ np . nan if d . get ( 'p' ) is None else round ( sum ([ a * b for a , b in zip ( d . get ( 'p' )[ 0 : 3 ], d . get ( 'v' )[ 0 : 3 ])]) / sum ( d . get ( 'v' )[ 0 : 3 ]), 3 ) for d in x [ 'atb_ladder' ]]) . assign ( lay_vwap = lambda x : [ np . nan if d . get ( 'p' ) is None else round ( sum ([ a * b for a , b in zip ( d . get ( 'p' )[ 0 : 3 ], d . get ( 'v' )[ 0 : 3 ])]) / sum ( d . get ( 'v' )[ 0 : 3 ]), 3 ) for d in x [ 'atl_ladder' ]]) . assign ( seconds_before_scheduled_jump = lambda x : round (( x [ 'market_time' ] - x [ 'time' ]) . dt . total_seconds ())) . query ( 'seconds_before_scheduled_jump < 1800 and seconds_before_scheduled_jump > -120' ) ) 2.2 Market formation Before we analyse how prices for selections move let's understand some basic things about how thoroughbred markets form. 2.2.1 Traded Volumes Let's look at how a typical market (at one of these 5 tracks) trades leading up to the scheduled race start. To make some of the analysis a little bit cleaner we need to pad out missing odds updates. For example for a given market we might have a market update 140 seconds before the jump but not another one till 132 seconds before the jump. We'll add rows for those interim 8 seconds by filling the values from the the previous row, this is required to iron out some idiosyncracies in the aggregations, it's not that important to follow if you don't understand it. traded_volume_values = df [[ 'market_id' , 'selection_id' , 'venue' , 'bsp' , 'seconds_before_scheduled_jump' , 'traded_volume' ]] all_sbj = pd . DataFrame ( data = { 'seconds_before_scheduled_jump' : traded_volume_values . seconds_before_scheduled_jump . unique ()}) . assign ( join = 1 ) traded_volume_explode = traded_volume_values [[ 'market_id' , 'selection_id' , 'venue' , 'bsp' ]] . drop_duplicates () . assign ( join = 1 ) . merge ( all_sbj ) . drop ( 'join' , 1 ) traded_volume_df = traded_volume_explode . merge ( traded_volume_values , how = \"left\" ) traded_volume_df = traded_volume_df . sort_values ([ 'market_id' , 'selection_id' , 'venue' , 'seconds_before_scheduled_jump' ], ascending = [ True , True , True , False ]) traded_volume_df . update ( traded_volume_df . groupby ([ 'market_id' , 'selection_id' , 'venue' ])[[ 'seconds_before_scheduled_jump' , 'traded_volume' ]] . ffill () . fillna ( 0 )) /tmp/ipykernel_327971/677704215.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only # Group by market, sum volume over selections at a given time, average over times for total def chunkSBJ ( sbj ): if sbj < 600 : return ( sbj ) else : return ( int ( math . floor ( sbj / 60 ) * 60 )) tradedVolumes_1 = ( traded_volume_df . groupby ([ \"market_id\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . assign ( seconds_before_scheduled_jump = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . groupby ( \"seconds_before_scheduled_jump\" , as_index = False ) . agg ({ 'traded_volume' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) ) fig = px . area ( tradedVolumes_1 , x = 'seconds_before_scheduled_jump' , y = 'traded_volume' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } #subtitle = \"Top 5 Biggest Vic Track Sample\" ) fig . update_layout ( font_family = \"Roboto\" ) fig [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig . show ( \"png\" ) The discontinuity in the chart highlights the switch point between the two time granularities that we extracted from the stream. Pre 600 seconds (10 minutes) before the scheduled off I can plot 1 data point per minute and after I'm plotting 60 data points per minute. The traded volume chart looks like an exponential chart: the total traded volume doubles from 10 minutes out to 4 minutes out, then it doubles again between then and 1 minute out, then nearly doubling again in the last minute and a bit of trading. Even a simple visual like this can help you with your bet placement on Betfair markets. For example if you're planning to get large volumes down on Betfair thoroughbred markets it's probably best to view prices >10 minutes out with a skeptical eye even if the market is tight - because you won't find the requisite lay volume that early as the majority of traded volume happens in the last 2-5 minutes of trading. Now like most analysis the average is definitely hiding lots of interesting things about this data. Let's split out this data by our 5 tracks: tradedVolumes_2 = ( traded_volume_df . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . groupby ([ \"market_id\" , \"venue\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . groupby ([ \"venue\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'traded_volume' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump_chunk' , ascending = False ) ) fig_2 = px . line ( tradedVolumes_2 , x = 'seconds_before_scheduled_jump_chunk' , y = 'traded_volume' , color = 'venue' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } ) fig_2 . update_layout ( font_family = \"Roboto\" ) fig_2 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_2 . show ( \"png\" ) As expected an average Flemington race trades nearly 500k whereas an average Bendigo race trades only ~120k volume. How about if we split our selections by odds range. Intuitively we know that odds-on horses will trade significantly more volume than a \\$50 shot but let's visualise the difference. We'll chunk the BSP of each horse into 5 groups: - Odds on (<50% chance of winning) - Odds between 2 and 5 - Odds between 5 and 15 - Odds between 15 and 50 - Odds 50+ def chunkBsp ( bsp ): if bsp <= 2 : return ( \"1. Odds On\" ) elif bsp <= 5 : return ( \"2. (2, 5]\" ) elif bsp <= 15 : return ( \"3. (5, 15]\" ) elif bsp <= 50 : return ( \"4. (15, 50]\" ) else : return ( \"5. 50+\" ) # Group by odds range tradedVolumes_3 = ( traded_volume_df . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( bsp = lambda x : x [ 'bsp' ] . apply ( chunkBsp )) . groupby ([ \"market_id\" , \"bsp\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . groupby ([ \"bsp\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'traded_volume' : 'mean' }) ) fig_3 = px . line ( tradedVolumes_3 , x = 'seconds_before_scheduled_jump_chunk' , y = 'traded_volume' , color = 'bsp' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } ) fig_3 . update_layout ( font_family = \"Roboto\" ) fig_3 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_3 . show ( \"png\" ) Again as expected the traded volume is strongly inversely proportional to the implied chance. There's a few reasons for this: - Inherently exposure is inversely proportional to odds so the same stake can produce widely different exposures for lay betting - Non model based participants have limited resources to manually analyse the form and thus focus on the top end of the market - Higher chance events reduce variance which is captured in staking schemes like the kelly criterion (which overweight stakes on larged percieved advantages on high probability events) that sophisticated participants tend to use Knowing where a majority of the traded volume is concentrated is another thing you should be aware of whether your betting on horse racing or elections and everything in between. 2.2.2 Market Tightness Understanding how the market tightens before the off is also another key conceptual component to market formation. I will consider two different variations on this concept: Market overround or Market Percentage The sum of probabilities across all outcomes Back % are always above 1 (else there exists an arbitrage opportunity) Lay % are always below 1 (else there exists an arbitrage opportunity) Market Spread The # of ticks / rungs between the best available back price and the best available lay price The market % is the value displayed on the Betfair website here: averageMarketPct = ( df [[ 'market_id' , 'selection_id' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . query ( 'seconds_before_scheduled_jump >= -20' ) . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( back_best = lambda x : 1 / x [ 'back_best' ], lay_best = lambda x : 1 / x [ 'lay_best' ] ) . groupby ([ \"market_id\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'back_best' : 'sum' , 'lay_best' : 'sum' }) . groupby ( \"seconds_before_scheduled_jump_chunk\" , as_index = False ) . agg ({ 'back_best' : 'mean' , 'lay_best' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump_chunk' , ascending = False ) ) fig_4 = go . Figure () fig_4 . add_trace ( go . Scatter ( x = averageMarketPct [ 'seconds_before_scheduled_jump_chunk' ], y = averageMarketPct [ 'back_best' ], mode = 'lines' , name = 'Back Market Overround' )) fig_4 . add_trace ( go . Scatter ( x = averageMarketPct [ 'seconds_before_scheduled_jump_chunk' ], y = averageMarketPct [ 'lay_best' ], mode = 'lines' , name = 'Lay Market Overround' )) fig_4 . update_layout ( font_family = \"Roboto\" , template = \"plotly_white\" , title = 'Average Back + Lay Market Overound Vic Thoroughbreds' ) fig_4 . update_xaxes ( title = \"Seconds Before Scheduled Jump\" ) fig_4 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_4 . show ( \"png\" ) As you can see the back and lay market % converge to 1 as the market gets closer to the jump. However, these are generally great markets even 30 mins before the off to have overrounds of only 4% is very good for racing markets. If you were analysing different kinds of racing markets, however, (harness or greyhound markets or thoroughbred races for country meets) you may need to conduct this kind of analysis to see when the earliest time you're likely to be able to get fair prices on either side of the market. Another way we can measure the tightness of Betfair markets is the market spread. I'm going to define the market spread as the number of ticks between the best back and best lay prices. This can give some extra granularity when measuring the market tightness for an individual selection In this market for example we can see that the first selection has a spread of 5 ticks between 11 and 13.5 (where ticks are 0.5 apart) whereas there's only 2 ticks between the best back and lay for the market favourite 3.4 and 3.5 (where ticks are 0.05 apart). First we'll need to create some custom functions that will create the Betfair ladder and do Betfair \"tick arithmetic\" for us. Part of the reason that I'm creating a different view for market spread is as a reason to introduce this Betfair tick ladder concept. Measuring odds differences and movement between odds values can be tricky because prices are fairly non-linear in probability space (you see far more horses between 2 and 3 than you do between 802 and 803 for example). Converting prices to a rank on the Betfair tick ladder creates a nice output mapping that can be used for all kinds of other purposes. Betfair actually has some Betfair arithmetic tick functions available on Github # Define the betfair tick ladder def bfTickLadder (): tickIncrements = { 1.0 : 0.01 , 2.0 : 0.02 , 3.0 : 0.05 , 4.0 : 0.1 , 6.0 : 0.2 , 10.0 : 0.5 , 20.0 : 1.0 , 30.0 : 2.0 , 50.0 : 5.0 , 100.0 : 10.0 , 1000.0 : 1000 , } ladder = [] for index , key in enumerate ( tickIncrements ): increment = tickIncrements [ key ] if ( index + 1 ) == len ( tickIncrements ): ladder . append ( key ) else : key1 = [ * tickIncrements ][ index ] key2 = [ * tickIncrements ][ index + 1 ] steps = ( key2 - key1 ) / increment for i in range ( int ( steps )): ladder . append ( round ( key + i * increment , 2 )) return ( ladder ) bfticks = bfTickLadder () # Round a decimal to the betfair tick value below def bfTickFloor ( price , includeIndex = False ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () ind = [ n for n , i in enumerate ( bfticks ) if i >= price ][ 0 ] if includeIndex : if bfticks [ ind ] == price : return (( ind , price )) else : return (( ind - 1 , bfticks [ ind - 1 ])) else : if bfticks [ ind ] == price : return ( price ) else : return ( bfticks [ ind - 1 ]) # Calculate the numder of ticks between two tick values def bfTickDelta ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) x = bfTickFloor ( p1 , includeIndex = True ) y = bfTickFloor ( p2 , includeIndex = True ) return ( x [ 0 ] - y [ 0 ]) bfTickDelta ( 13.5 , 11 ) 5 bfTickDelta ( 3.5 , 3.4 ) 2 Now that we have our functions let's plot the average market spread leading up to the jump. # Group by odds range averageMarketSpread = ( df [[ 'market_id' , 'selection_id' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( market_spread = lambda x : x . apply ( lambda x : bfTickDelta ( x . lay_best , x . back_best ), axis = 1 )) . groupby ([ \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'market_spread' : 'mean' }) ) fig_5 = px . line ( averageMarketSpread , x = 'seconds_before_scheduled_jump_chunk' , y = 'market_spread' , template = 'plotly_white' , title = \"Market Spread Leading Up To Jump\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"market_spread\" : \"Average Spread (b/w best back and lay)\" } ) fig_5 . update_layout ( font_family = \"Roboto\" ) fig_5 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_5 . show ( \"png\" ) So much the same story as before, the spread tightens pretty dramatically leading all the way up to the jump. Now that we have a measure for market tightness on a selection level we can split it by odds range to see how tightness varies across odds range. # Market Spread By Odds Group # _______________________ # Group by odds range averageMarketSpreadOddsgrp = ( df [[ 'market_id' , 'selection_id' , 'bsp' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( bsp = lambda x : x [ 'bsp' ] . apply ( chunkBsp )) . assign ( market_spread = lambda x : x . apply ( lambda x : bfTickDelta ( x . lay_best , x . back_best ), axis = 1 )) . groupby ([ \"bsp\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'market_spread' : 'mean' }) ) fig_6 = px . line ( averageMarketSpreadOddsgrp , x = 'seconds_before_scheduled_jump_chunk' , y = 'market_spread' , color = \"bsp\" , template = 'plotly_white' , title = \"Market Spread Leading Up To Jump\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"market_spread\" : \"Average Spread (b/w best back and lay)\" } ) fig_6 . update_layout ( font_family = \"Roboto\" ) fig_6 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_6 . show ( \"png\" ) Now this gives us visibility of how the market tightens across groups of selections. Clearly the bottom end of the market takes a bit longer to tighten up which fits our understanding that there's generally less money, and less very sharp money on these selections. 2.3 Market Moves Circling back to the motivation of this article: How should you think about large price moves in Betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data Now that we have a good grasp on the data we've collected on how Betfair markets form let's try to analyse big price moves. Here's a sample of our largest price moves in our race sample: # Big Movers # ____________________ ( df . groupby ([ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'bsp' ], as_index = False ) . agg ({ 'ltp' : 'max' }) . rename ( columns = { 'ltp' : 'max_traded' }) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . max_traded , x . bsp ), axis = 1 )) . query ( 'max_traded < 500' ) . sort_values ( 'ticks_in' , ascending = False ) . head ( 10 ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue selection_name win bsp max_traded ticks_in 9209 1.182760338 39317622 Bendigo 6. San Fabrizio 0 2.81 10.50 71 2277 1.173069469 35981261 Bendigo 13. Shotmaker 1 2.32 4.20 56 4853 1.176708684 37456112 Bendigo 6. Zoutons 1 1.94 3.00 56 7795 1.180291241 38744938 Sandown 4. Summit Reach 0 2.42 4.50 54 4404 1.175952680 36995397 Bendigo 2. Nikau Spur 0 1.68 2.40 52 6413 1.178722039 27445196 Sandown 6. Miss Five Hundred 0 2.05 3.20 52 10216 1.184305249 39800435 Sandown 18. Lucabelle 0 48.00 470.00 48 5872 1.177927858 38197699 Flemington 2. Long Arm 0 9.90 80.00 47 1519 1.172217104 35609408 Moonee Valley 4. Tailleur 1 1.61 2.14 46 8856 1.182235083 25370687 Bendigo 3. Cernan 0 2.34 3.65 46 Some of those moves are astounding: San Fabrizio traded at 10.50 in the last 30 mins before the jump and had a final BSP of 2.80, but lost. Like we discussed before observing a market plunge in hindsight may seem like a more powerful predictor than it actually is. For example, if we had have flat staked these 10 giant moves after they happened (say at the BSP) we'd be running at ~70% POT even though our intuition would have told you at the time that these horses couldn't lose! 2.3.1 Reacting to moves after they happen Bookies, race callers and other services will often tell you what selection has been strong in the market; who the \"money\" has come for. But once you hear this is it already too late to do anything about? Let's extend the sample in the previous section to see if we can draw any broader conclusions. I'll take a sample of the all selections 10 minutes before the scheduled jump I'll also take those same selections at the scheduled jump exactly I'll then calculate the number of ticks between the best back price at each time point to measure the movement in the last 10 minutes of trading I'll then calculate a back and lay profit using the top box at the scheduled off m10 = ( df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . apply ( lambda g : None if g [ g [ 'seconds_before_scheduled_jump' ] < 600 ] . shape [ 0 ] == 0 else g [ g [ 'seconds_before_scheduled_jump' ] < 600 ] . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) . iloc [ 0 ]) . filter ( items = [ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'back_best' , 'lay_best' ]) . rename ( columns = { 'back_best' : 'back_best_10m' , 'lay_best' : 'lay_best_10m' }) ) m0 = ( df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . apply ( lambda g : None if g [ g [ 'seconds_before_scheduled_jump' ] < 0 ] . shape [ 0 ] == 0 else g [ g [ 'seconds_before_scheduled_jump' ] < 0 ] . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) . iloc [ 0 ]) . filter ( items = [ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'back_best' , 'lay_best' ]) ) # Back or lay according to large plunges pd . DataFrame ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 stake 241.000 back_pl -4.824 lay_pl -17.620 # Back or lay according to large plunges - grouped by track ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . groupby ( 'venue' , as_index = False ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } venue stake back_pl lay_pl 0 Bendigo 73 -14.7225 8.64 1 Caulfield 32 21.1305 -25.20 2 Flemington 36 -8.0095 5.13 3 Moonee Valley 34 -18.8730 16.62 4 Sandown 66 15.6505 -22.81 As you can see there's not much of a pattern here to take advantage of here. At least for this sample of tracks, and for this time slice, the value has been sucked dry from the market on these selections. To illustrate this let's check what the backers would have profited if they were the ones who could identify these selections prior to their big market moves # Backing before the plunge? ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best_10m' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best_10m' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) stake 241.0000 back_pl 108.6155 lay_pl -154.6100 dtype: float64 I could sign up for winning at 50% POT! Now I just have to pick 100% of the large thoroughbred plunges before they happen... Obviously no one individual person or group is predicting these movements 100%. Even the sharpest players will have movements go against them but this highlights the essence of the problem and the contradiction of retrospective price movement analysis. These movements are important. There is a lot to be gained by purely trading Betfair markets and how they move. However, identifting large moves after they occur isn't good enough. The large players sharpen the odds down to a good and fair implied chance and leave those watching on the sidelines with little to no value either side. To profit then we must capture some of the value as they are and jump on momentum as it's happening not after it's come to a halt. 2.3.2 Visualising the moves In the final section I'll frame how I think you could go about identifying these moves as they're happening or about to happen. But first to illustrate some of dynamics let's visualise some of these large moves. How do selections firm and do they do it with different characteristics in different patterns? Candlestick charts are a good way to visualise the evolution of prices in markets and are often used in financial markets to do technical analysis. I'll first create a function to create a candlestick chart for a market / selection slice of our dataframe using plotly charts. # Candlestick Plotly Chart With Plotly # _____________________________ def priceCandlestickPlotly ( d ): d [ 'time_chunk' ] = d [ 'time' ] . dt . round ( '2min' ) selectionName = d . selection_name . iloc [ 0 ] track = d . venue . iloc [ 0 ] startTime = d . market_time . iloc [ 0 ] candleStickInput = d . groupby ( 'time_chunk' , as_index = False ) . agg ({ \"ltp\" : [ 'first' , 'last' , 'min' , 'max' ]}) candleStickInput . columns = [ \"_\" . join ( pair ) for pair in candleStickInput . columns ] fig = go . Figure ( data = [ go . Candlestick ( x = candleStickInput [ 'time_chunk_' ], open = candleStickInput [ 'ltp_first' ], high = candleStickInput [ 'ltp_max' ], low = candleStickInput [ 'ltp_min' ], close = candleStickInput [ 'ltp_last' ])]) fig . update_layout ( template = \"plotly_white\" , xaxis_rangeslider_visible = False , title = f ' { selectionName } at { track } race started at { startTime } UTC' ) fig . show ( \"png\" ) Let's visualise the 2 biggest moves with this chart function. priceCandlestickPlotly ( df . query ( 'market_id == \"1.182760338\" and selection_id == \"39317622\"' )) /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This looks exactly like what we'd expect from a extreme plunge: consistent, sustained and unrelenting support on the back side as the horse is pushed from 8-10 odds down to it's fair odds of around 3. Let's look at the second big plunge: priceCandlestickPlotly ( df . query ( 'market_id == \"1.173069469\" and selection_id == \"35981261\"' )) This movement chart is a bit different. The market has 4 distinct segments: About 30 minutes from the off the market changes and the horse is supported from 4 into 3 It trades at around 3 for the next 25 minutes It then drifts back out sharply to a bit over 4 It then gets crunched back in all the way into a BSP of 2.30 Clearly different segments of the participants had very different perceptions of this horses chances. How do we make sense of it? Let's try a different candlestick visualistion, which includes the traded volume. import mplfinance as fplt def priceCandlestickMpl ( d ): d [ 'time_chunk' ] = d [ 'time' ] . dt . round ( '2min' ) selectionName = d . selection_name . iloc [ 0 ] track = d . venue . iloc [ 0 ] startTime = d . market_time . iloc [ 0 ] # Add traded volume in last interval d = d . groupby ([ 'market_id' , 'selection_id' ]) . apply ( lambda x : x . assign ( traded_volume_delta = lambda y : ( y [ 'traded_volume' ] - y [ 'traded_volume' ] . shift ( 1 )) . mask ( pd . isnull , 0 ))) candleStickInput = d . groupby ( 'time_chunk' , as_index = False ) . agg ({ \"ltp\" : [ 'first' , 'last' , 'min' , 'max' ], \"traded_volume_delta\" : 'sum' }) candleStickInput . columns = [ \"_\" . join ( pair ) for pair in candleStickInput . columns ] candleStickInput = candleStickInput . rename ( columns = { 'time_chunk_' : 'date' , 'ltp_first' : 'open' , 'ltp_last' : 'close' , 'ltp_min' : 'low' , 'ltp_max' : 'high' , 'traded_volume_delta_sum' : 'volume' }) candleStickInput = candleStickInput . set_index ( 'date' ) fplt . plot ( candleStickInput , type = 'candle' , style = 'yahoo' , title = f ' { selectionName } at { track } race started at { startTime } UTC' , ylabel = 'Odds' , volume = True , ylabel_lower = 'Volume Traded' , ) First the first horse: priceCandlestickMpl ( df . query ( 'market_id == \"1.182760338\" and selection_id == \"39317622\"' )) /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy 2021-08-30T11:43:18.549484 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Now the second selection: priceCandlestickMpl ( df . query ( 'market_id == \"1.173069469\" and selection_id == \"35981261\"' )) /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy 2021-08-30T11:43:22.999632 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} So as the majority of the volume was traded right before the jump, the price was hammered into 2.30. This is a pattern that you might see a lot in Betfair markets: prices may oscillate around or even drift consistently in one direction until a point at which certain large participants or groups of participants enter the market and push the selection in another direction completely. So anticipating a move won't be as simple as analysing the price as a time series, it's one piece of the puzzle. Really sophisticated market trading algorithms will need to historically analyse a raft of market metrics and correlate them with these movements historically within the market type of interest. In the next section I'll start off down that road to give you an idea of how I'd tackle the problem but it's more complicated than we can uncover in a single article. 2.3.3 Anticipating a move Our retrospective analysis was good to help us understand the dynamics of movements, the efficiency of markets, how they form, and what things we could put in a predictive analysis. The next step is to create forward looking estimates about where the market is headed next. You get setup your problem in lots of different ways including creating rules based strategies like discussed in a previous piece . Or you could go down a formalised machine learning approach. The first step to both is to build up some key features and test their predictive power. The factors I'll consider in this section will be: The movement over the last 30 seconds The weight of money on the back side The weight of money on the lay side The current best back / volume weighted average top 3 back (top box support on back side) The current best lay / volume weighted average top 3 lay (top box support on lay side) The current best back / The volume weighted traded price over the last increment The current best lay / The volume weighted traded price over the last increment And im interested in correlating these variables with the number + direction of ticks moved over the next 30 seconds to see if I can find anything interesting. # Start Analysis # ______________________________ dfPredict = df . query ( 'seconds_before_scheduled_jump <= 600 and seconds_before_scheduled_jump >= 0' ) # Target # ____________________________ dfPredict [ 'best_back_30s_in_future' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( 30 ) dfPredict [ 'best_lay_30s_in_future' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( 30 ) dfPredict [ 'back_target' ] = dfPredict . apply ( lambda x : bfTickDelta ( x . back_best , x . best_back_30s_in_future ), axis = 1 ) dfPredict [ 'lay_target' ] = dfPredict . apply ( lambda x : bfTickDelta ( x . lay_best , x . best_lay_30s_in_future ), axis = 1 ) # Movement # ____________________________ dfPredict [ 'back_best_30s_ago' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( - 30 ) dfPredict [ 'back_lay_30s_ago' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'lay_best' ] . shift ( - 30 ) dfPredict = ( dfPredict . assign ( back_ticks_in_30s = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_30s_ago , x . back_best ), axis = 1 )) . assign ( lay_ticks_in_30s = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_lay_30s_ago , x . lay_best ), axis = 1 )) ) # Weight Of Money # ____________________________ atb_ladder = dfPredict . atb_ladder . iloc [ 0 ] atl_ladder = dfPredict . atl_ladder . iloc [ 0 ] def wom ( back_ladder , lay_ladder ): if not back_ladder or not lay_ladder : return (( None , None )) total_volume = round ( sum ( back_ladder [ 'v' ]) + sum ( lay_ladder [ 'v' ]), 2 ) return (( round ( sum ( back_ladder [ 'v' ]) / total_volume , 3 ), round ( sum ( lay_ladder [ 'v' ]) / total_volume , 3 ))) dfPredict [ 'wom' ] = dfPredict . apply ( lambda x : wom ( x [ 'atb_ladder' ], x [ 'atl_ladder' ]), axis = 1 ) dfPredict [[ 'back_wom' , 'lay_wom' ]] = pd . DataFrame ( dfPredict [ 'wom' ] . tolist (), index = dfPredict . index ) # Top Box Support # ________________________ dfPredict [ 'back_best_support' ] = dfPredict [ 'back_vwap' ] / dfPredict [ 'back_best' ] dfPredict [ 'lay_best_support' ] = dfPredict [ 'lay_best' ] / dfPredict [ 'lay_vwap' ] # Recent Movement # _____________________ dfPredict [ 'wap_movement_10s' ] = dfPredict [ 'wap' ] / dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'wap' ] . shift ( - 10 ) /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Now that we've created the candidate factors let's use a spearman correlation coefficient to analyse their correlation (positive or negative we're mostly focussed on the absolute size of the coefficient). corrMatrix = dfPredict [[ 'back_target' , 'back_ticks_in_30s' , 'lay_ticks_in_30s' , 'back_wom' , 'lay_wom' , 'back_best_support' , 'lay_best_support' , 'wap_movement_10s' ]] . dropna () . corr ( method = \"spearman\" ) corrMatrix [[ 'back_target' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } back_target back_target 1.000000 back_ticks_in_30s -0.080772 lay_ticks_in_30s -0.036798 back_wom 0.259926 lay_wom -0.259927 back_best_support -0.142726 lay_best_support 0.013173 wap_movement_10s -0.316167 import seaborn as sns corPlot = sns . heatmap ( corrMatrix , vmin =- 1 , vmax = 1 , center = 0 , cmap = sns . diverging_palette ( 20 , 220 , n = 200 ), square = True ) corPlot . set_xticklabels ( corPlot . get_xticklabels (), rotation = 45 , horizontalalignment = 'right' ) [Text(0.5, 0, 'back_target'), Text(1.5, 0, 'back_ticks_in_30s'), Text(2.5, 0, 'lay_ticks_in_30s'), Text(3.5, 0, 'back_wom'), Text(4.5, 0, 'lay_wom'), Text(5.5, 0, 'back_best_support'), Text(6.5, 0, 'lay_best_support'), Text(7.5, 0, 'wap_movement_10s')] 2021-08-30T12:26:29.189464 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} We're entirely interested in the back_target column in the matrix and the heatmap. We can see that some of our candidate features have decent correlation with the number of ticks the price will move over the course of the next 30 seconds. 2.3.4 Next Steps This marks the end of this analysis however if you're interested in turning this kind of analysis / approach into something more I'd suggest there's two main paths you could go down from here. Angles + Rules You could dig into the data a little more a find specific situations where some of these features (or ones like them) preceeded certain things happening in the market (movements or increases in trading volatility etc) You could then construct trading rules based on these findings an try to automate them or bake them into trading rules inside a 3rd party tool. Predictive Modelling If you were comfortable with statistical models or machine learning you could easily feed this data into a predictive modelling workflow. Once honed, the predictions from this workflow could be turned into recommended betting decisions which could form a part of an automated algorithmic betting framework. The work to fill out a project like this would be significant but so would the reward. 3.0 Conclusion Like previous articles this analysis is a sketch of what can be accomplished analysing the historical stream files. In this instance we focussed on the markets themselves, with a particular focussing on a small slice of markets: victorian thoroughbred markets. We analysed how these markets form, how to interpret the price movements on certain selections and the first steps to building out automated strategies based on these features alone. Building automated betting strategies based on the markets alone is path that plenty of quantitatively inclined Betfair customers go down as it minimises complexity in source data (there's only one!) and there's plenty of value in it left untapped for you to try to capture. Complete code Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github # %% [markdown] # # Do #theyknow? Analysing betfair market formation and market movements # %% [markdown] # ## 0.1 Setup # # Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language. # # Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files. # # You'll also need `betfairlightweight` which you can install with something like `pip install betfairlightweight`. # %% import pandas as pd import numpy as np import requests import os import re import csv import plotly.express as px import plotly.graph_objects as go import math import logging import yaml import csv import tarfile import zipfile import bz2 import glob import ast from datetime import date , timedelta from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) # %% [markdown] # ## 0.2 Context # # You may have seen the hashtag if you're on australian racing twitter #theyknow following a dramatic late market move for a horse that's followed by a decisive race victory. Sometimes it can seem eerie how accurate these moves are after the fact. In betfair racing markets there's usually a flurry of activity leading up to the race start as players look to get their bets down at the best price without tipping their hand (opinion on the race) too much. Large moves can happend when large players rally around a selection who's implied chance in early trading isn't close to what it's true chance is in the upcoming race. Large moves can also happen when there's some inside information - not able to be gleaned from analysis of the horses previous races - that slowly filters out of the stable or training group. # # This creates opportunity in the secondary market as punters try to read these movements to make bets themselves. The task often becomes identifying which movements are caused by these sophisticated players or represent real signals of strength and which aren't. # # So do #theyknow generally? Before even looking at the data I can assure you that yes they do know pretty well. Strong movements in betting markets are usually pretty reliable indicators about what is about to happen. However, these moves can be overvalued by recreationals. When observing a horse plumet in from $3.50 to $2 you are usually suprised if the horse loses, but the general efficiency of late prices would suggest that this horse is going to still lose 50% of time. If you simply observe the large moves after the sharp players have corrected the market landscape you're in no better a position to bet than before the move happened. On the other hand what if would could reliably identify the move as it was happening or about to happen? That would be a recipe for successful trading of horse racing markets and no doubt this is what many players in this secondary market (analysis of betfair markets rather than the races themselves) try to do. # # If you were to build up a manual qualitative strategy for this kind of market trading you need to understand: # - Who the participants are # - How sophisticated they are at the top end # - What types of races do they bet on and for how much # - When the different types of participants typically enter the market # - What do bet placement patterns look like for these participants # - etc. # # This is the kind of task that takes a lot research, years of experience watching markets, a lot of trial and error, and a lot of industry know-how. Given I'm a lazy quantitative person I'll try to see if I can uncover any of these patterns in the data alone. # # Put simply the central question for the second half of this piece will be: # # > How should you think about large price moves in betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data # # I'll just be analysing historical thoroughbred markets but the same approach could be applied to any sport or racing code of your interest. # %% [markdown] # ## 0.3 This Example # # Building market based trading strategies is a broad and fertile ground for many quantitative betfair customers; too big to cover in a single article. I'll try to zero in on a small slice of thoroughbred markets and analyse how these markets form and how I'd start the process of trying to find the patterns in the market movements. Again hopefully this is some inspiration for you and you can pick up some of the ideas and build them out. # # Given volume of data (when analysing second by second slices of market data) I'll be looking at a years worth of thoroughbred races from the 5 largest Victorian tracks: Flemington, Caulfield, Moonee Valley, Bendigo and Sandown. # %% [markdown] # # 1.0 Data # # Unlike in some of the previous tutorials we aren't going to collapse the stream data into a single row per runner. In those examples we were interested in analysing some discrete things about selections in betfair markets like: # # - Their final odds (bsp or last traded price) # - Their odds at some fixed time point or time points before the scheduled race start # - Other single number descriptors of the trading activity on a selection (eg total traded volume) # # # In this analysis I want to analyse how markets form and prices move for selections as markets evolve. So we'll need to pull out multiple price points per runner - so we'll have multiple rows per runner in our parsed output dataset. # # To output a row for every stream update for every selection in every thoroughbred race over the last 12 months would produce a dataset far too big too analyse using normal data analysis tools - we're about 10s to 100s of billions of rows. # # To chop our sample down into a manageable slice I'm going to filter on some select tracks of interest (as mentioned above) and I'm also going to have 3 sections of data granularity: # # - I won't log any of the odds or traded volumes > 30mins before the scheduled off # + In thoroughbreds there is non-trivial action before this point you may want to study, but it's not what I want to study here # - Between 30 and 10 minutes before the scheduled off I'll log data every 60 seconds # - 10 minutes or less to the scheuled off I'll log prices every second # # The code to manage this windowed granularity is in the below parsing code tweak as you wish if you want to tighten or broaden the analysis. # %% [markdown] # ## 1.1 Sourcing Data # # First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Aask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder. # %% [markdown] # ## 1.2 Utility Functions # %% # General Utility Functions # _________________________________ def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def slicePrice ( l , n ): try : x = l [ n ] . price except : x = \"\" return ( x ) def sliceSize ( l , n ): try : x = l [ n ] . size except : x = \"\" return ( x ) def pull_ladder ( availableLadder , n = 5 ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : n ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"p\" ] = price out [ \"v\" ] = volume return ( out ) # %% [markdown] # Slicing the tracks we want we'll just adjust the market filter function used before to include some logic on the venue name # %% def filter_market ( market : MarketBook ) -> bool : d = market . market_definition track_filter = [ 'Bendigo' , 'Sandown' , 'Flemington' , 'Caulfield' , 'Moonee Valley' ] return ( d . country_code == 'AU' and d . venue in track_filter and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # %% [markdown] # ## 1.3 Selection Metadata # # Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes. # # This means we'll have to parse over the data twice but our outputs will be much smaller than if we duplicated the selection name 800 times for example. # %% def final_market_book ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): return ( None ) for market_book in market_books : last_market_book = market_book return ( last_market_book ) def parse_final_selection_meta ( dir , out_file ): with open ( out_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,venue,market_time,selection_name,win,bsp \\n \" ) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) last_market_book = final_market_book ( stream ) if last_market_book is None : continue # Extract Info ++++++++++++++++++++++++++++++++++ runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in last_market_book . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'win' : np . where ( r . status == \"WINNER\" , 1 , 0 ), 'sp' : r . sp . actual_sp } for r in last_market_book . runners ] # Return Info ++++++++++++++++++++++++++++++++++ for runnerMeta in runnerMeta : if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( last_market_book . market_id ), runnerMeta [ 'selection_id' ], last_market_book . market_definition . venue , last_market_book . market_definition . market_time , runnerMeta [ 'selection_name' ], runnerMeta [ 'win' ], runnerMeta [ 'sp' ] ) ) # %% selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) trading = betfairlightweight . APIClient ( username = \"username\" , password = \"password\" , app_key = \"app_key\" ) listener = StreamListener ( max_latency = None ) print ( \"__ Parsing Selection Metadata ___ \" ) # parse_final_selection_meta(stream_files, selection_meta) # %% [markdown] # ## 1.4 Detailed Preplay Odds # # Like mentioned above there will be some time control logic injected to control the time granularity that odds are recorded in each step. # # Instead of widening the available to bet price ladder I'll extract the top 10 rungs of the available to back (atb) and available to lay (atl) ladders and write them both to the output file. That will give me more flexibility during the analysis to pull out things that interest me. So in total I'll extract: # # - Top 10 ATB Ladder # - Top 10 ATL Ladder # - Total Traded Volume # - Volume weighted average traded price up till the current time # - Last Traded price # # %% def loop_preplay_prices ( s , o ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () marketID = None tradeVols = None time = None for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): break for market_book in market_books : # Time Step Management ++++++++++++++++++++++++++++++++++ if marketID is None : # No market initialised marketID = market_book . market_id time = market_book . publish_time elif market_book . inplay : # Stop once market goes inplay break else : seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () if seconds_to_start > log1_Start : # Too early before off to start logging prices continue else : # Update data at different time steps depending on seconds to off wait = np . where ( seconds_to_start <= log2_Start , log2_Step , log1_Step ) # New Market if market_book . market_id != marketID : marketID = market_book . market_id time = market_book . publish_time # (wait) seconds elapsed since last write elif ( market_book . publish_time - time ) . total_seconds () > wait : time = market_book . publish_time # fewer than (wait) seconds elapsed continue to next loop else : continue # Execute Data Logging ++++++++++++++++++++++++++++++++++ for runner in market_book . runners : try : atb_ladder = pull_ladder ( runner . ex . available_to_back , n = 10 ) atl_ladder = pull_ladder ( runner . ex . available_to_lay , n = 10 ) except : atb_ladder = {} atl_ladder = {} # Calculate Current Traded Volume + Tradedd WAP limitTradedVol = sum ([ rung . size for rung in runner . ex . traded_volume ]) if limitTradedVol == 0 : limitWAP = \"\" else : limitWAP = sum ([ rung . size * rung . price for rung in runner . ex . traded_volume ]) / limitTradedVol limitWAP = round ( limitWAP , 2 ) o . writerow ( ( market_book . market_id , runner . selection_id , market_book . publish_time , limitTradedVol , limitWAP , runner . last_price_traded or \"\" , str ( atb_ladder ) . replace ( ' ' , '' ), str ( atl_ladder ) . replace ( ' ' , '' ) ) ) def parse_preplay_prices ( dir , out_file ): with open ( out_file , \"w+\" ) as output : writer = csv . writer ( output , delimiter = ',' , lineterminator = ' \\r\\n ' , quoting = csv . QUOTE_ALL ) writer . writerow (( \"market_id\" , \"selection_id\" , \"time\" , \"traded_volume\" , \"wap\" , \"ltp\" , \"atb_ladder\" , \"atl_ladder\" )) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) loop_preplay_prices ( stream , writer ) # %% preplay_price_file = \"[OUTPUT PATH TO CSV FOR PREPLAY PRICES]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) log1_Start = 60 * 30 # Seconds before scheduled off to start recording data for data segment one log1_Step = 60 # Seconds between log steps for first data segment log2_Start = 60 * 10 # Seconds before scheduled off to start recording data for data segment two log2_Step = 1 # Seconds between log steps for second data segment print ( \"__ Parsing Detailed Preplay Prices ___ \" ) # parse_preplay_prices(stream_files, preplay_price_file) # %% [markdown] # # 2.0 Analysis # %% [markdown] # ## 2.1 Load and Assemble # # First let's load the raw datafiles we created in the previous step. # # ## 2.1.1 Load # %% [markdown] # We have the highlevel selection metadata (1 row per selection): # # %% selection_meta_path = \"[PATH TO METADATA FILE]\" selection = pd . read_csv ( selection_meta_path , dtype = { 'market_id' : object , 'selection_id' : object }, parse_dates = [ 'market_time' ]) selection . head ( 3 ) # %% [markdown] # And we have the detailed preplay price data for these markets + selections: # %% prices_path = \"[PATH TO PRICES FILE]\" prices = pd . read_csv ( prices_path , quoting = csv . QUOTE_ALL , dtype = { 'market_id' : 'string' , 'selection_id' : 'string' , 'atb_ladder' : 'string' , 'atl_ladder' : 'string' }, parse_dates = [ 'time' ] ) prices . head ( 3 ) # %% [markdown] # Now it's important to observe how much data we have here. # %% prices . shape # %% [markdown] # We've got 7 million rows of price data here just for races at 5 thoroughbred tracks over the last year. Now it's not really \"big data\" in the sense you might have heard before but it's certainly a lot of rows and we'll have to think about the performance of our code a little bit more than we would if we were dealining with 1 row per selection style datasets. # %% [markdown] # We need pandas to correctly interpret the dictionary columns as dictionaries so we'll run this code: # %% # To get pandas to correctly recognise the ladder columns as dictionaries prices [ 'atb_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atb_ladder' ]] prices [ 'atl_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atl_ladder' ]] # %% [markdown] # ## 2.1.2 Assemble # %% [markdown] # Now we'll join the 2 data sets together to form a nice normalised dataframe: # %% # Simple join on market and selection_id initially df = selection . merge ( prices , on = [ 'market_id' , 'selection_id' ]) df . head ( 3 ) # %% [markdown] # ## 2.1.3 Transform # # Next we'll do some processing on the joined dataframe to add some columns that we can use in our analysis including calculating a numeric #seconds before the scheduled jump field that we'll use extensively. # %% df = ( df . assign ( back_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atb_ladder' ]]) . assign ( lay_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atl_ladder' ]]) . assign ( back_vwap = lambda x : [ np . nan if d . get ( 'p' ) is None else round ( sum ([ a * b for a , b in zip ( d . get ( 'p' )[ 0 : 3 ], d . get ( 'v' )[ 0 : 3 ])]) / sum ( d . get ( 'v' )[ 0 : 3 ]), 3 ) for d in x [ 'atb_ladder' ]]) . assign ( lay_vwap = lambda x : [ np . nan if d . get ( 'p' ) is None else round ( sum ([ a * b for a , b in zip ( d . get ( 'p' )[ 0 : 3 ], d . get ( 'v' )[ 0 : 3 ])]) / sum ( d . get ( 'v' )[ 0 : 3 ]), 3 ) for d in x [ 'atl_ladder' ]]) . assign ( seconds_before_scheduled_jump = lambda x : round (( x [ 'market_time' ] - x [ 'time' ]) . dt . total_seconds ())) . query ( 'seconds_before_scheduled_jump < 1800 and seconds_before_scheduled_jump > -120' ) ) # %% [markdown] # ## 2.2 Market Formation # # Before we analyse how prices for selections move let's understand some basic things about how thoroughbred markets form. # %% [markdown] # ## 2.2.1 Traded Volumes # # Let's look at how a typical market (at one of these 5 tracks) trades leading up to the scheduled race start. # # To make some of the analysis a little bit cleaner we need to pad out missing odds updates. For example for a given market we might have a market update 140 seconds before the jump but not another one till 132 seconds before the jump. We'll add rows for those interim 8 seconds by filling the values from the the previous row, this is required to iron out some idiosyncracies in the aggregations, it's not that important to follow if you don't understand it. # %% traded_volume_values = df [[ 'market_id' , 'selection_id' , 'venue' , 'bsp' , 'seconds_before_scheduled_jump' , 'traded_volume' ]] all_sbj = pd . DataFrame ( data = { 'seconds_before_scheduled_jump' : traded_volume_values . seconds_before_scheduled_jump . unique ()}) . assign ( join = 1 ) traded_volume_explode = traded_volume_values [[ 'market_id' , 'selection_id' , 'venue' , 'bsp' ]] . drop_duplicates () . assign ( join = 1 ) . merge ( all_sbj ) . drop ( 'join' , 1 ) traded_volume_df = traded_volume_explode . merge ( traded_volume_values , how = \"left\" ) traded_volume_df = traded_volume_df . sort_values ([ 'market_id' , 'selection_id' , 'venue' , 'seconds_before_scheduled_jump' ], ascending = [ True , True , True , False ]) traded_volume_df . update ( traded_volume_df . groupby ([ 'market_id' , 'selection_id' , 'venue' ])[[ 'seconds_before_scheduled_jump' , 'traded_volume' ]] . ffill () . fillna ( 0 )) # %% # Group by market, sum volume over selections at a given time, average over times for total def chunkSBJ ( sbj ): if sbj < 600 : return ( sbj ) else : return ( int ( math . floor ( sbj / 60 ) * 60 )) tradedVolumes_1 = ( traded_volume_df . groupby ([ \"market_id\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . assign ( seconds_before_scheduled_jump = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . groupby ( \"seconds_before_scheduled_jump\" , as_index = False ) . agg ({ 'traded_volume' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) ) fig = px . area ( tradedVolumes_1 , x = 'seconds_before_scheduled_jump' , y = 'traded_volume' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } #subtitle = \"Top 5 Biggest Vic Track Sample\" ) fig . update_layout ( font_family = \"Roboto\" ) fig [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig . show ( \"png\" ) # %% [markdown] # The discontinuity in the chart highlights the switch point between the two time granularities that we extracted from the stream. Pre 600 seconds (10 minutes) before the scheduled off I can plot 1 data point per minute and after I'm plotting 60 data points per minute. # # The traded volume chart looks like an exponential chart: the total traded volume doubles from 10 minutes out to 4 minutes out, then it doubles again between then and 1 minute out, then nearly doubling again in the last minute and a bit of trading. Even a simple visual like this can help you with your bet placement on betfair markets. For example if you're planning to get large volumes down on betfair thoroughbred markets it's probably best to view prices >10 minutes out with a skeptical eye even if the market is tight - because you won't find the requisite lay volume that early as the majority of traded volume happens in the last 2-5 minutes of trading. # # Now like most analysis the average is definitely hiding lots of interesting things about this data. Let's split out this data by our 5 tracks: # %% tradedVolumes_2 = ( traded_volume_df . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . groupby ([ \"market_id\" , \"venue\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . groupby ([ \"venue\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'traded_volume' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump_chunk' , ascending = False ) ) fig_2 = px . line ( tradedVolumes_2 , x = 'seconds_before_scheduled_jump_chunk' , y = 'traded_volume' , color = 'venue' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } ) fig_2 . update_layout ( font_family = \"Roboto\" ) fig_2 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_2 . show ( \"png\" ) # %% [markdown] # As expected an average Flemington race trades nearly 500k whereas an average Bendigo race trades only ~120k volume. # # How about if we split our selections by odds range. Intuitively we know that odds-on horses will trade significantly more volume than a $50 shot but let's visualise the difference. # # We'll chunk the BSP of each horse into 5 groups: # - Odds on (<50% chance of winning) # - Odds between 2 and 5 # - Odds between 5 and 15 # - Odds between 15 and 50 # - Odds 50+ # %% def chunkBsp ( bsp ): if bsp <= 2 : return ( \"1. Odds On\" ) elif bsp <= 5 : return ( \"2. (2, 5]\" ) elif bsp <= 15 : return ( \"3. (5, 15]\" ) elif bsp <= 50 : return ( \"4. (15, 50]\" ) else : return ( \"5. 50+\" ) # Group by odds range tradedVolumes_3 = ( traded_volume_df . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( bsp = lambda x : x [ 'bsp' ] . apply ( chunkBsp )) . groupby ([ \"market_id\" , \"bsp\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . groupby ([ \"bsp\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'traded_volume' : 'mean' }) ) fig_3 = px . line ( tradedVolumes_3 , x = 'seconds_before_scheduled_jump_chunk' , y = 'traded_volume' , color = 'bsp' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } ) fig_3 . update_layout ( font_family = \"Roboto\" ) fig_3 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_3 . show ( \"png\" ) # %% [markdown] # Again as expected the traded volume is strongly inversely proportional to the implied chance. There's a few reasons for this: # - Inherently exposure is inversely proportional to odds so the same stake can produce widely different exposures for lay betting # - Non model based participants have limited resources to manually analyse the form and thus focus on the top end of the market # - Higher chance events reduce variance which is captured in staking schemes like the kelly criterion (which overweight stakes on larged percieved advantages on high probability events) that sophisticated participants tend to use # # Knowing where a majority of the traded volume is concentrated is another thing you should be aware of whether your betting on horse racing or elections and everything in between. # %% [markdown] # ## 2.2.2 Market Tightness # # Understanding how the market tightens before the off is also another key conceptual component to market formation. I will consider two different variations on this concept: # # - Market overround or Market Percentage # + The sum of probabilities across all outcomes # + Back % are always above 1 (else there exists an arbitrage opportunity) # + Lay % are always below 1 (else there exists an arbitrage opportunity) # - Market Spread # + The # of ticks / rungs between the best available back price and the best available lay price # # The market % is the value displayed on the betfair website here # %% [markdown] # ![](img/overround.png) # %% averageMarketPct = ( df [[ 'market_id' , 'selection_id' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . query ( 'seconds_before_scheduled_jump >= -20' ) . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( back_best = lambda x : 1 / x [ 'back_best' ], lay_best = lambda x : 1 / x [ 'lay_best' ] ) . groupby ([ \"market_id\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'back_best' : 'sum' , 'lay_best' : 'sum' }) . groupby ( \"seconds_before_scheduled_jump_chunk\" , as_index = False ) . agg ({ 'back_best' : 'mean' , 'lay_best' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump_chunk' , ascending = False ) ) fig_4 = go . Figure () fig_4 . add_trace ( go . Scatter ( x = averageMarketPct [ 'seconds_before_scheduled_jump_chunk' ], y = averageMarketPct [ 'back_best' ], mode = 'lines' , name = 'Back Market Overround' )) fig_4 . add_trace ( go . Scatter ( x = averageMarketPct [ 'seconds_before_scheduled_jump_chunk' ], y = averageMarketPct [ 'lay_best' ], mode = 'lines' , name = 'Lay Market Overround' )) fig_4 . update_layout ( font_family = \"Roboto\" , template = \"plotly_white\" , title = 'Average Back + Lay Market Overound Vic Thoroughbreds' ) fig_4 . update_xaxes ( title = \"Seconds Before Scheduled Jump\" ) fig_4 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_4 . show ( \"png\" ) # %% [markdown] # As you can see the back and lay market % converge to 1 as the market gets closer to the jump. However, these are generally great markets even 30 mins before the off to have overrounds of only 4% is very good for racing markets. # # If you were analysing different kinds of racing markets, however, (harness or greyhound markets or thoroughbred races for country meets) you may need to conduct this kind of analysis to see when the earliest time you're likely to be able to get fair prices on either side of the market. # # Another way we can measure the tightness of betfair markets is the market spread. I'm going to define the market spread as the number of ticks between the best back and best lay prices. This can give some extra granularity when measuring the market tightness for an individual selection # %% [markdown] # ![](img/market-spread.png) # %% [markdown] # In this market for example we can see that the first selection has a spread of 5 ticks between 11 and 13.5 (where ticks are 0.5 apart) whereas there's only 2 ticks between the best back and lay for the market favourite 3.4 and 3.5 (where ticks are 0.05 apart). # # First we'll need to create some custom functions that will create the betfair ladder and do betfair \"tick arithmetic\" for us. Part of the reason that I'm creating a different view for market spread is as a reason to introduce this betfair tick ladder concept. Measuring odds differences and movement between odds values can be tricky because prices are fairly non-linear in probability space (you see far more horses between 2 and 3 than you do between 802 and 803 for example). Converting prices to a rank on the betfair tick ladder creates a nice output mapping that can be used for all kinds of other purposes. # %% # Define the betfair tick ladder def bfTickLadder (): tickIncrements = { 1.0 : 0.01 , 2.0 : 0.02 , 3.0 : 0.05 , 4.0 : 0.1 , 6.0 : 0.2 , 10.0 : 0.5 , 20.0 : 1.0 , 30.0 : 2.0 , 50.0 : 5.0 , 100.0 : 10.0 , 1000.0 : 1000 , } ladder = [] for index , key in enumerate ( tickIncrements ): increment = tickIncrements [ key ] if ( index + 1 ) == len ( tickIncrements ): ladder . append ( key ) else : key1 = [ * tickIncrements ][ index ] key2 = [ * tickIncrements ][ index + 1 ] steps = ( key2 - key1 ) / increment for i in range ( int ( steps )): ladder . append ( round ( key + i * increment , 2 )) return ( ladder ) bfticks = bfTickLadder () # Round a decimal to the betfair tick value below def bfTickFloor ( price , includeIndex = False ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () ind = [ n for n , i in enumerate ( bfticks ) if i >= price ][ 0 ] if includeIndex : if bfticks [ ind ] == price : return (( ind , price )) else : return (( ind - 1 , bfticks [ ind - 1 ])) else : if bfticks [ ind ] == price : return ( price ) else : return ( bfticks [ ind - 1 ]) # Calculate the numder of ticks between two tick values def bfTickDelta ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) x = bfTickFloor ( p1 , includeIndex = True ) y = bfTickFloor ( p2 , includeIndex = True ) return ( x [ 0 ] - y [ 0 ]) # %% bfTickDelta ( 13.5 , 11 ) # %% bfTickDelta ( 3.5 , 3.4 ) # %% [markdown] # Now that we have our functions let's plot the average market spread leading up to the jump. # %% # Group by odds range averageMarketSpread = ( df [[ 'market_id' , 'selection_id' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( market_spread = lambda x : x . apply ( lambda x : bfTickDelta ( x . lay_best , x . back_best ), axis = 1 )) . groupby ([ \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'market_spread' : 'mean' }) ) # %% fig_5 = px . line ( averageMarketSpread , x = 'seconds_before_scheduled_jump_chunk' , y = 'market_spread' , template = 'plotly_white' , title = \"Market Spread Leading Up To Jump\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"market_spread\" : \"Average Spread (b/w best back and lay)\" } ) fig_5 . update_layout ( font_family = \"Roboto\" ) fig_5 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_5 . show ( \"png\" ) # %% [markdown] # So much the same story as before, the spread tightens pretty dramatically leading all the way up to the jump. Now that we have a measure for market tightness on a selection level we can split it by odds range to see how tightness varies across odds range. # %% # Market Spread By Odds Group # _______________________ # Group by odds range averageMarketSpreadOddsgrp = ( df [[ 'market_id' , 'selection_id' , 'bsp' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( bsp = lambda x : x [ 'bsp' ] . apply ( chunkBsp )) . assign ( market_spread = lambda x : x . apply ( lambda x : bfTickDelta ( x . lay_best , x . back_best ), axis = 1 )) . groupby ([ \"bsp\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'market_spread' : 'mean' }) ) fig_6 = px . line ( averageMarketSpreadOddsgrp , x = 'seconds_before_scheduled_jump_chunk' , y = 'market_spread' , color = \"bsp\" , template = 'plotly_white' , title = \"Market Spread Leading Up To Jump\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"market_spread\" : \"Average Spread (b/w best back and lay)\" } ) fig_6 . update_layout ( font_family = \"Roboto\" ) fig_6 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_6 . show ( \"png\" ) # %% [markdown] # Now this gives us visibility of how the market tightens across groups of selections. Clearly the bottom end of the market takes a bit longer to tighten up which fits our understanding that there's generally less money, and less very sharp money on these selections. # %% [markdown] # ## 2.3 Market Moves # # Circling back to the motivation of this article: # # > How should you think about large price moves in betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data # # Now that we have a good grasp on the data we've collected on how betfair markets form let's try to analyse big price moves. Here's a sample of our largest price moves in our race sample: # # # %% # Big Movers # ____________________ ( df . groupby ([ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'bsp' ], as_index = False ) . agg ({ 'ltp' : 'max' }) . rename ( columns = { 'ltp' : 'max_traded' }) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . max_traded , x . bsp ), axis = 1 )) . query ( 'max_traded < 500' ) . sort_values ( 'ticks_in' , ascending = False ) . head ( 10 ) ) # %% [markdown] # Some of those moves are astounding: San Fabrizio traded at 10.50 in the last 30 mins before the jump and had a final BSP of 2.80, but lost. # # Like we discussed before observing a market plunge in hindsight may seem like a more powerful predictor than it actually is. For example, if we had have flat staked these 10 giant moves after they happened (say at the BSP) we'd be running at ~70% POT even though our intuition would have told you at the time that these horses couldn't lose! # %% [markdown] # ## 2.3.1 Reacting to moves after they happen # # Bookies, race callers and other services will often tell you what selection has been strong in the market; who the \"money\" has come for. But once you hear this is it already too late to do anything about? # # Let's extend the sample in the previous section to see if we can draw any broader conclusions. # # - I'll take a sample of the all selections 10 minutes before the scheduled jump # - I'll also take those same selections at the scheduled jump exactly # - I'll then calculate the number of ticks between the best back price at each time point to measure the movement in the last 10 minutes of trading # - I'll then calculate a back and lay profit using the top box at the scheduled off # %% m10 = ( df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . apply ( lambda g : None if g [ g [ 'seconds_before_scheduled_jump' ] < 600 ] . shape [ 0 ] == 0 else g [ g [ 'seconds_before_scheduled_jump' ] < 600 ] . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) . iloc [ 0 ]) . filter ( items = [ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'back_best' , 'lay_best' ]) . rename ( columns = { 'back_best' : 'back_best_10m' , 'lay_best' : 'lay_best_10m' }) ) m0 = ( df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . apply ( lambda g : None if g [ g [ 'seconds_before_scheduled_jump' ] < 0 ] . shape [ 0 ] == 0 else g [ g [ 'seconds_before_scheduled_jump' ] < 0 ] . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) . iloc [ 0 ]) . filter ( items = [ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'back_best' , 'lay_best' ]) ) # %% # Back or lay according to large plunges pd . DataFrame ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) # %% # Back or lay according to large plunges - grouped by track ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . groupby ( 'venue' , as_index = False ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) # %% [markdown] # As you can see there's not much of a pattern here to take advantage of here. At least for this sample of tracks, and for this time slice, the value has been sucked dry from the market on these selections. # To illustrate this let's check what the backers would have profited if they were the ones who could identify these selections prior to their big market moves # %% # Backing before the plunge? ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best_10m' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best_10m' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) # %% [markdown] # I could sign up for winning at 50% POT! Now I just have to pick 100% of the large thoroughbred plunges before they happen... Obviously no one individual person or group is predicting these movements 100%. Even the sharpest players will have movements go against them but this highlights the essence of the problem and the contradiction of retrospective price movement analysis. # # These movements are important. There is a lot to be gained by purely trading betfair markets and how they move. However, identifting large moves **after** they occur isn't good enough. The large players sharpen the odds down to a good and fair implied chance and leave those watching on the sidelines with little to no value either side. To profit then we must capture some of the value as they are and jump on momentum as it's happening not after it's come to a halt. # %% [markdown] # ## 2.3.2 Visualising the moves # # In the final section I'll frame how I think you could go about identifying these moves as they're happening or about to happen. But first to illustrate some of dynamics let's visualise some of these large moves. How do selections firm and do they do it with different characteristics in different patterns? Candlestick charts are a good way to visualise the evolution of prices in markets and are often used in financial markets to do technical analysis. # # I'll first create a function to create a candlestick chart for a market / selection slice of our dataframe using plotly charts. # # %% # Candlestick Plotly Chart With Plotly # _____________________________ def priceCandlestickPlotly ( d ): d [ 'time_chunk' ] = d [ 'time' ] . dt . round ( '2min' ) selectionName = d . selection_name . iloc [ 0 ] track = d . venue . iloc [ 0 ] startTime = d . market_time . iloc [ 0 ] candleStickInput = d . groupby ( 'time_chunk' , as_index = False ) . agg ({ \"ltp\" : [ 'first' , 'last' , 'min' , 'max' ]}) candleStickInput . columns = [ \"_\" . join ( pair ) for pair in candleStickInput . columns ] fig = go . Figure ( data = [ go . Candlestick ( x = candleStickInput [ 'time_chunk_' ], open = candleStickInput [ 'ltp_first' ], high = candleStickInput [ 'ltp_max' ], low = candleStickInput [ 'ltp_min' ], close = candleStickInput [ 'ltp_last' ])]) fig . update_layout ( template = \"plotly_white\" , xaxis_rangeslider_visible = False , title = f ' { selectionName } at { track } race started at { startTime } UTC' ) fig . show ( \"png\" ) # %% [markdown] # Let's visualise the 2 biggest moves with this chart function. # %% priceCandlestickPlotly ( df . query ( 'market_id == \"1.182760338\" and selection_id == \"39317622\"' )) # %% [markdown] # This looks exactly like what we'd expect from a extreme plunge: consistent, sustained and unrelenting support on the back side as the horse is pushed from 8-10 odds down to it's fair odds of around 3. # # Let's look at the second big plunge: # %% priceCandlestickPlotly ( df . query ( 'market_id == \"1.173069469\" and selection_id == \"35981261\"' )) # %% [markdown] # This movement chart is a bit different. The market has 4 distinct segments: # # - About 30 minutes from the off the market changes and the horse is supported from 4 into 3 # - It trades at around 3 for the next 25 minutes # - It then drifts back out sharply to a bit over 4 # - It then gets crunched back in all the way into a BSP of 2.30 # # Clearly different segments of the participants had very different perceptions of this horses chances. How do we make sense of it? # # Let's try a different candlestick visualistion, which includes the traded volume. # %% import mplfinance as fplt def priceCandlestickMpl ( d ): d [ 'time_chunk' ] = d [ 'time' ] . dt . round ( '2min' ) selectionName = d . selection_name . iloc [ 0 ] track = d . venue . iloc [ 0 ] startTime = d . market_time . iloc [ 0 ] # Add traded volume in last interval d = d . groupby ([ 'market_id' , 'selection_id' ]) . apply ( lambda x : x . assign ( traded_volume_delta = lambda y : ( y [ 'traded_volume' ] - y [ 'traded_volume' ] . shift ( 1 )) . mask ( pd . isnull , 0 ))) candleStickInput = d . groupby ( 'time_chunk' , as_index = False ) . agg ({ \"ltp\" : [ 'first' , 'last' , 'min' , 'max' ], \"traded_volume_delta\" : 'sum' }) candleStickInput . columns = [ \"_\" . join ( pair ) for pair in candleStickInput . columns ] candleStickInput = candleStickInput . rename ( columns = { 'time_chunk_' : 'date' , 'ltp_first' : 'open' , 'ltp_last' : 'close' , 'ltp_min' : 'low' , 'ltp_max' : 'high' , 'traded_volume_delta_sum' : 'volume' }) candleStickInput = candleStickInput . set_index ( 'date' ) fplt . plot ( candleStickInput , type = 'candle' , style = 'yahoo' , title = f ' { selectionName } at { track } race started at { startTime } UTC' , ylabel = 'Odds' , volume = True , ylabel_lower = 'Volume Traded' , ) # %% [markdown] # First the first horse: # %% priceCandlestickMpl ( df . query ( 'market_id == \"1.182760338\" and selection_id == \"39317622\"' )) # %% [markdown] # Now the second selection: # %% priceCandlestickMpl ( df . query ( 'market_id == \"1.173069469\" and selection_id == \"35981261\"' )) # %% [markdown] # So as the majority of the volume was traded right before the jump, the price was hammered into 2.30. This is a pattern that you might see a lot in betfair markets: prices may oscillate around or even drift consistently in one direction until a point at which certain large participants or groups of participants enter the market and push the selection in another direction completely. # # So anticipating a move won't be as simple as analysing the price as a time series, it's one piece of the puzzle. Really sophisticated market trading algorithms will need to historically analyse a raft of market metrics and correlate them with these movements historically within the market type of interest. In the next section I'll start off down that road to give you an idea of how I'd tackle the problem but it's more complicated than we can uncover in a single article. # %% [markdown] # ## 2.3.3 Anticipating a move # # Our retrospective analysis was good to help us understand the dynamics of movements, the efficiency of markets, how they form, and what things we could put in a predictive analysis. The next step is to create forward looking estimates about where the market is headed next. # # You get setup your problem in lots of different ways including creating rules based strategies like discussed in a [previous piece](https://betfair-datascientists.github.io/historicData/automatedBettingAnglesTutorial/). Or you could go down a formalised machine learning approach. The first step to both is to build up some key features and test their predictive power. # # The factors I'll consider in this section will be: # # - The movement over the last 30 seconds # - The weight of money on the back side # - The weight of money on the lay side # - The current best back / volume weighted average top 3 back (top box support on back side) # - The current best lay / volume weighted average top 3 lay (top box support on lay side) # - The current best back / The volume weighted traded price over the last increment # - The current best lay / The volume weighted traded price over the last increment # # And im interested in correlating these variables with the number + direction of ticks moved over the **next 30 seconds** to see if I can find anything interesting. # # %% # Start Analysis # ______________________________ dfPredict = df . query ( 'seconds_before_scheduled_jump <= 600 and seconds_before_scheduled_jump >= 0' ) # Target # ____________________________ dfPredict [ 'best_back_30s_in_future' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( 30 ) dfPredict [ 'best_lay_30s_in_future' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( 30 ) dfPredict [ 'back_target' ] = dfPredict . apply ( lambda x : bfTickDelta ( x . back_best , x . best_back_30s_in_future ), axis = 1 ) dfPredict [ 'lay_target' ] = dfPredict . apply ( lambda x : bfTickDelta ( x . lay_best , x . best_lay_30s_in_future ), axis = 1 ) # Movement # ____________________________ dfPredict [ 'back_best_30s_ago' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( - 30 ) dfPredict [ 'back_lay_30s_ago' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'lay_best' ] . shift ( - 30 ) dfPredict = ( dfPredict . assign ( back_ticks_in_30s = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_30s_ago , x . back_best ), axis = 1 )) . assign ( lay_ticks_in_30s = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_lay_30s_ago , x . lay_best ), axis = 1 )) ) # Weight Of Money # ____________________________ atb_ladder = dfPredict . atb_ladder . iloc [ 0 ] atl_ladder = dfPredict . atl_ladder . iloc [ 0 ] def wom ( back_ladder , lay_ladder ): if not back_ladder or not lay_ladder : return (( None , None )) total_volume = round ( sum ( back_ladder [ 'v' ]) + sum ( lay_ladder [ 'v' ]), 2 ) return (( round ( sum ( back_ladder [ 'v' ]) / total_volume , 3 ), round ( sum ( lay_ladder [ 'v' ]) / total_volume , 3 ))) dfPredict [ 'wom' ] = dfPredict . apply ( lambda x : wom ( x [ 'atb_ladder' ], x [ 'atl_ladder' ]), axis = 1 ) dfPredict [[ 'back_wom' , 'lay_wom' ]] = pd . DataFrame ( dfPredict [ 'wom' ] . tolist (), index = dfPredict . index ) # Top Box Support # ________________________ dfPredict [ 'back_best_support' ] = dfPredict [ 'back_vwap' ] / dfPredict [ 'back_best' ] dfPredict [ 'lay_best_support' ] = dfPredict [ 'lay_best' ] / dfPredict [ 'lay_vwap' ] # Recent Movement # _____________________ dfPredict [ 'wap_movement_10s' ] = dfPredict [ 'wap' ] / dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'wap' ] . shift ( - 10 ) # %% [markdown] # Now that we've created the candidate factors let's use a spearman correlation coefficient to analyse their correlation (positive or negative we're mostly focussed on the absolute size of the coefficient). # %% corrMatrix = dfPredict [[ 'back_target' , 'back_ticks_in_30s' , 'lay_ticks_in_30s' , 'back_wom' , 'lay_wom' , 'back_best_support' , 'lay_best_support' , 'wap_movement_10s' ]] . dropna () . corr ( method = \"spearman\" ) corrMatrix [[ 'back_target' ]] # %% import seaborn as sns corPlot = sns . heatmap ( corrMatrix , vmin =- 1 , vmax = 1 , center = 0 , cmap = sns . diverging_palette ( 20 , 220 , n = 200 ), square = True ) corPlot . set_xticklabels ( corPlot . get_xticklabels (), rotation = 45 , horizontalalignment = 'right' ) # %% [markdown] # We're entirely interested in the `back_target` column in the matrix and the heatmap. We can see that some of our candidate features have decent correlation with the number of ticks the price will move over the course of the next 30 seconds. # # # ## 2.3.4 Next Steps # # This marks the end of this analysis however if you're interested in turning this kind of analysis / approach into something more I'd suggest there's 2 main paths you could go down from here. # # 1. Angles + Rules # # You could dig into the data a little more a find specific situations where some of these features (or ones like them) preceeded certain things happening in the market (movements or increases in trading volatility etc) You could then construct trading rules based on these findings an try to automate them or bake them into trading rules inside a 3rd party tool. # # 2. Predictive Modelling # # If you were comfortable with statistical models or machine learning you could easily feed this data into a predictive modelling workflow. Once honed, the predictions from this workflow could be turned into recommended betting decisions which could form a part of an automated algorithmic betting framework. The work to fill out a project like this would be significant but so would the reward. # # # # 3.0 Conclusion # # Like previous articles this analysis is a sketch of what can be accomplished analysing the historical stream files. In this instance we focussed on the markets themselves, with a particular focussing on a small slice of markets: victorian thoroughbred markets. We analysed how these markets form, how to interpret the price movements on certain selections and the first steps to building out automated strategies based on these features alone. # # Building automated betting strategies based on the markets alone is path that plenty of quantitatively inclined betfair customers go down as it minimises complexity in source data (there's only 1!) and there's plenty of value in it left untapped for you to try to capture. # # Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Do &#35theyknow? Analysing Betfair market formation & market movements"},{"location":"historicData/analysingAndPredictingMarketMovements/#do-theyknow-analysing-betfair-market-formation-market-movements","text":"This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the Automated betting angles in Python we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!","title":"Do #theyknow? Analysing Betfair market formation &amp; market movements"},{"location":"historicData/analysingAndPredictingMarketMovements/#cheat-sheet","text":"If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo .","title":"Cheat sheet"},{"location":"historicData/analysingAndPredictingMarketMovements/#00-setup","text":"","title":"0.0 Setup"},{"location":"historicData/analysingAndPredictingMarketMovements/#01-importing-libraries","text":"Once again I'll be presenting the analysis in a Jupyter notebook and will be using Python as a programming language. Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files. You'll also need betfairlightweight which you can install with something like pip install betfairlightweight . import pandas as pd import numpy as np import requests import os import re import csv import plotly.express as px import plotly.graph_objects as go import math import logging import yaml import csv import tarfile import zipfile import bz2 import glob import ast from datetime import date , timedelta from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook )","title":"0.1 Importing libraries"},{"location":"historicData/analysingAndPredictingMarketMovements/#02-context","text":"You may have seen the hashtag if you're on Australian racing twitter #theyknow following a dramatic late market move for a horse that's followed by a decisive race victory. Sometimes it can seem eerie how accurate these moves are after the fact. In Betfair racing markets there's usually a flurry of activity leading up to the race start as players look to get their bets down at the best price without tipping their hand (opinion on the race) too much. Large moves can happen when large players rally around a selection who's implied chance in early trading isn't close to what it's true chance is in the upcoming race. Large moves can also happen when there's some inside information - not able to be gleaned from analysis of the horses previous races - that slowly filters out of the stable or training group. This creates opportunity in the secondary market as punters try to read these movements to make bets themselves. The task often becomes identifying which movements are caused by these sophisticated players or represent real signals of strength and which aren't. So do #theyknow generally? Before even looking at the data I can assure you that yes they do know pretty well. Strong movements in betting markets are usually pretty reliable indicators about what is about to happen. However, these moves can be overvalued by recreationals. When observing a horse plumet in from \\$3.50 to \\$2 you are usually suprised if the horse loses, but the general efficiency of late prices would suggest that this horse is going to still lose 50% of time. If you simply observe the large moves after the sharp players have corrected the market landscape you're in no better a position to bet than before the move happened. On the other hand what if would could reliably identify the move as it was happening or about to happen? That would be a recipe for successful trading of horse racing markets and no doubt this is what many players in this secondary market (analysis of Betfair markets rather than the races themselves) try to do. If you were to build up a manual qualitative strategy for this kind of market trading you need to understand: - Who the participants are - How sophisticated they are at the top end - What types of races do they bet on and for how much - When the different types of participants typically enter the market - What do bet placement patterns look like for these participants - etc. This is the kind of task that takes a lot research, years of experience watching markets, a lot of trial and error, and a lot of industry know-how. Given I'm a lazy quantitative person I'll try to see if I can uncover any of these patterns in the data alone. Put simply the central question for the second half of this piece will be: How should you think about large price moves in Betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data I'll just be analysing historical thoroughbred markets but the same approach could be applied to any sport or racing code of your interest.","title":"0.2 Context"},{"location":"historicData/analysingAndPredictingMarketMovements/#03-this-example","text":"Building market based trading strategies is a broad and fertile ground for many quantitative Betfair customers; too big to cover in a single article. I'll try to zero in on a small slice of thoroughbred markets and analyse how these markets form and how I'd start the process of trying to find the patterns in the market movements. Again hopefully this is some inspiration for you and you can pick up some of the ideas and build them out. Given volume of data (when analysing second by second slices of market data) I'll be looking at a year's worth of thoroughbred races from the 5 largest Victorian tracks: Flemington, Caulfield, Moonee Valley, Bendigo and Sandown.","title":"0.3 This example"},{"location":"historicData/analysingAndPredictingMarketMovements/#10-data","text":"Unlike in some of the previous tutorials we aren't going to collapse the stream data into a single row per runner. In those examples we were interested in analysing some discrete things about selections in Betfair markets like: Their final odds (bsp or last traded price) Their odds at some fixed time point or time points before the scheduled race start Other single number descriptors of the trading activity on a selection (eg total traded volume) In this analysis I want to analyse how markets form and prices move for selections as markets evolve. So we'll need to pull out multiple price points per runner - so we'll have multiple rows per runner in our parsed output dataset. To output a row for every stream update for every selection in every thoroughbred race over the last 12 months would produce a dataset far too big too analyse using normal data analysis tools - we're about 10s to 100s of billions of rows. To chop our sample down into a manageable slice I'm going to filter on some select tracks of interest (as mentioned above) and I'm also going to have 3 sections of data granularity: I won't log any of the odds or traded volumes > 30mins before the scheduled off In thoroughbreds there is non-trivial action before this point you may want to study, but it's not what I want to study here Between 30 and 10 minutes before the scheduled off I'll log data every 60 seconds 10 minutes or less to the scheuled off I'll log prices every second The code to manage this windowed granularity is in the below parsing code tweak as you wish if you want to tighten or broaden the analysis.","title":"1.0 Data"},{"location":"historicData/analysingAndPredictingMarketMovements/#11-sourcing-data","text":"First you'll need to source the Stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Ask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder.","title":"1.1 Sourcing data"},{"location":"historicData/analysingAndPredictingMarketMovements/#12-utility-functions","text":"# General Utility Functions # _________________________________ def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def slicePrice ( l , n ): try : x = l [ n ] . price except : x = \"\" return ( x ) def sliceSize ( l , n ): try : x = l [ n ] . size except : x = \"\" return ( x ) def pull_ladder ( availableLadder , n = 5 ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : n ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"p\" ] = price out [ \"v\" ] = volume return ( out ) Slicing the tracks we want we'll just adjust the market filter function used before to include some logic on the venue name def filter_market ( market : MarketBook ) -> bool : d = market . market_definition track_filter = [ 'Bendigo' , 'Sandown' , 'Flemington' , 'Caulfield' , 'Moonee Valley' ] return ( d . country_code == 'AU' and d . venue in track_filter and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' )","title":"1.2 Utility functions"},{"location":"historicData/analysingAndPredictingMarketMovements/#13-selection-metadata","text":"Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes. This means we'll have to parse over the data twice but our outputs will be much smaller than if we duplicated the selection name 800 times for example. def final_market_book ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): return ( None ) for market_book in market_books : last_market_book = market_book return ( last_market_book ) def parse_final_selection_meta ( dir , out_file ): with open ( out_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,venue,market_time,selection_name,win,bsp \\n \" ) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) last_market_book = final_market_book ( stream ) if last_market_book is None : continue # Extract Info ++++++++++++++++++++++++++++++++++ runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in last_market_book . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'win' : np . where ( r . status == \"WINNER\" , 1 , 0 ), 'sp' : r . sp . actual_sp } for r in last_market_book . runners ] # Return Info ++++++++++++++++++++++++++++++++++ for runnerMeta in runnerMeta : if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( last_market_book . market_id ), runnerMeta [ 'selection_id' ], last_market_book . market_definition . venue , last_market_book . market_definition . market_time , runnerMeta [ 'selection_name' ], runnerMeta [ 'win' ], runnerMeta [ 'sp' ] ) ) selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) trading = betfairlightweight . APIClient ( username = \"username\" , password = \"password\" , app_key = \"app_key\" ) listener = StreamListener ( max_latency = None ) print ( \"__ Parsing Selection Metadata ___ \" ) # parse_final_selection_meta(stream_files, selection_meta) __ Parsing Selection Metadata ___","title":"1.3 Selection metadata"},{"location":"historicData/analysingAndPredictingMarketMovements/#14-detailed-preplay-odds","text":"Like mentioned above there will be some time control logic injected to control the time granularity that odds are recorded in each step. Instead of widening the available to bet price ladder I'll extract the top 10 rungs of the available to back (atb) and available to lay (atl) ladders and write them both to the output file. That will give me more flexibility during the analysis to pull out things that interest me. So in total I'll extract: Top 10 ATB Ladder Top 10 ATL Ladder Total Traded Volume Volume weighted average traded price up till the current time Last Traded price def loop_preplay_prices ( s , o ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () marketID = None tradeVols = None time = None for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): break for market_book in market_books : # Time Step Management ++++++++++++++++++++++++++++++++++ if marketID is None : # No market initialised marketID = market_book . market_id time = market_book . publish_time elif market_book . inplay : # Stop once market goes inplay break else : seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () if seconds_to_start > log1_Start : # Too early before off to start logging prices continue else : # Update data at different time steps depending on seconds to off wait = np . where ( seconds_to_start <= log2_Start , log2_Step , log1_Step ) # New Market if market_book . market_id != marketID : marketID = market_book . market_id time = market_book . publish_time # (wait) seconds elapsed since last write elif ( market_book . publish_time - time ) . total_seconds () > wait : time = market_book . publish_time # fewer than (wait) seconds elapsed continue to next loop else : continue # Execute Data Logging ++++++++++++++++++++++++++++++++++ for runner in market_book . runners : try : atb_ladder = pull_ladder ( runner . ex . available_to_back , n = 10 ) atl_ladder = pull_ladder ( runner . ex . available_to_lay , n = 10 ) except : atb_ladder = {} atl_ladder = {} # Calculate Current Traded Volume + Tradedd WAP limitTradedVol = sum ([ rung . size for rung in runner . ex . traded_volume ]) if limitTradedVol == 0 : limitWAP = \"\" else : limitWAP = sum ([ rung . size * rung . price for rung in runner . ex . traded_volume ]) / limitTradedVol limitWAP = round ( limitWAP , 2 ) o . writerow ( ( market_book . market_id , runner . selection_id , market_book . publish_time , limitTradedVol , limitWAP , runner . last_price_traded or \"\" , str ( atb_ladder ) . replace ( ' ' , '' ), str ( atl_ladder ) . replace ( ' ' , '' ) ) ) def parse_preplay_prices ( dir , out_file ): with open ( out_file , \"w+\" ) as output : writer = csv . writer ( output , delimiter = ',' , lineterminator = ' \\r\\n ' , quoting = csv . QUOTE_ALL ) writer . writerow (( \"market_id\" , \"selection_id\" , \"time\" , \"traded_volume\" , \"wap\" , \"ltp\" , \"atb_ladder\" , \"atl_ladder\" )) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) loop_preplay_prices ( stream , writer ) preplay_price_file = \"[OUTPUT PATH TO CSV FOR PREPLAY PRICES]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) log1_Start = 60 * 30 # Seconds before scheduled off to start recording data for data segment one log1_Step = 60 # Seconds between log steps for first data segment log2_Start = 60 * 10 # Seconds before scheduled off to start recording data for data segment two log2_Step = 1 # Seconds between log steps for second data segment print ( \"__ Parsing Detailed Preplay Prices ___ \" ) # parse_preplay_prices(stream_files, preplay_price_file) __ Parsing Detailed Preplay Prices ___","title":"1.4 Detailed preplay odds"},{"location":"historicData/analysingAndPredictingMarketMovements/#20-analysis","text":"","title":"2.0 Analysis"},{"location":"historicData/analysingAndPredictingMarketMovements/#21-load-and-assemble","text":"First let's load the raw datafiles we created in the previous step.","title":"2.1 Load and Assemble"},{"location":"historicData/analysingAndPredictingMarketMovements/#211-load","text":"We have the highlevel selection metadata (1 row per selection): selection = pd . read_csv ( \"[PATH TO METADATA FILE]\" , dtype = { 'market_id' : object , 'selection_id' : object }, parse_dates = [ 'market_time' ]) selection . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue market_time selection_name win bsp 0 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 1 1.179904683 3593031 Bendigo 2021-03-02 03:00:00 2. Cornucopia 0 6.64 2 1.179904683 38629714 Bendigo 2021-03-02 03:00:00 3. Danejararose 0 21.66 And we have the detailed preplay price data for these markets + selections: prices = pd . read_csv ( \"[PATH TO PRICES FILE]\" , quoting = csv . QUOTE_ALL , dtype = { 'market_id' : 'string' , 'selection_id' : 'string' , 'atb_ladder' : 'string' , 'atl_ladder' : 'string' }, parse_dates = [ 'time' ] ) prices . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id time traded_volume wap ltp atb_ladder atl_ladder 0 1.179904683 38629713 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} 1 1.179904683 3593031 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} 2 1.179904683 38629714 2021-03-01 05:15:09.480 0.0 NaN NaN {} {} Now it's important to observe how much data we have here. prices . shape (7774392, 8) We've got 7 million rows of price data here just for races at 5 thoroughbred tracks over the last year. Now it's not really \"big data\" in the sense you might have heard before but it's certainly a lot of rows and we'll have to think about the performance of our code a little bit more than we would if we were dealining with 1 row per selection style datasets. We need pandas to correctly interpret the dictionary columns as dictionaries so we'll run this code: # To get pandas to correctly recognise the ladder columns as dictionaries prices [ 'atb_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atb_ladder' ]] prices [ 'atl_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atl_ladder' ]]","title":"2.1.1 Load"},{"location":"historicData/analysingAndPredictingMarketMovements/#212-assemble","text":"Now we'll join the 2 data sets together to form a nice normalised dataframe: # Simple join on market and selection_id initially df = selection . merge ( prices , on = [ 'market_id' , 'selection_id' ]) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue market_time selection_name win bsp time traded_volume wap ltp atb_ladder atl_ladder 0 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-01 05:15:09.480 0.00 NaN NaN {} {} 1 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-02 02:30:00.552 15.95 15.44 22.0 {'p': [18.5, 18, 17.5, 17, 16, 15, 14, 13.5, 1... {'p': [24, 25, 29, 30, 38, 55, 65, 70, 90, 140... 2 1.179904683 38629713 Bendigo 2021-03-02 03:00:00 1. Bigdoorprize 0 19.37 2021-03-02 02:31:06.716 15.95 15.44 22.0 {'p': [19, 18.5, 18, 17, 16, 15, 14, 13.5, 13,... {'p': [25, 29, 30, 38, 55, 65, 70, 90, 140, 24...","title":"2.1.2 Assemble"},{"location":"historicData/analysingAndPredictingMarketMovements/#213-transform","text":"Next we'll do some processing on the joined dataframe to add some columns that we can use in our analysis including calculating a numeric #seconds before the scheduled jump field that we'll use extensively. df = ( df . assign ( back_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atb_ladder' ]]) . assign ( lay_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atl_ladder' ]]) . assign ( back_vwap = lambda x : [ np . nan if d . get ( 'p' ) is None else round ( sum ([ a * b for a , b in zip ( d . get ( 'p' )[ 0 : 3 ], d . get ( 'v' )[ 0 : 3 ])]) / sum ( d . get ( 'v' )[ 0 : 3 ]), 3 ) for d in x [ 'atb_ladder' ]]) . assign ( lay_vwap = lambda x : [ np . nan if d . get ( 'p' ) is None else round ( sum ([ a * b for a , b in zip ( d . get ( 'p' )[ 0 : 3 ], d . get ( 'v' )[ 0 : 3 ])]) / sum ( d . get ( 'v' )[ 0 : 3 ]), 3 ) for d in x [ 'atl_ladder' ]]) . assign ( seconds_before_scheduled_jump = lambda x : round (( x [ 'market_time' ] - x [ 'time' ]) . dt . total_seconds ())) . query ( 'seconds_before_scheduled_jump < 1800 and seconds_before_scheduled_jump > -120' ) )","title":"2.1.3 Transform"},{"location":"historicData/analysingAndPredictingMarketMovements/#22-market-formation","text":"Before we analyse how prices for selections move let's understand some basic things about how thoroughbred markets form.","title":"2.2 Market formation"},{"location":"historicData/analysingAndPredictingMarketMovements/#221-traded-volumes","text":"Let's look at how a typical market (at one of these 5 tracks) trades leading up to the scheduled race start. To make some of the analysis a little bit cleaner we need to pad out missing odds updates. For example for a given market we might have a market update 140 seconds before the jump but not another one till 132 seconds before the jump. We'll add rows for those interim 8 seconds by filling the values from the the previous row, this is required to iron out some idiosyncracies in the aggregations, it's not that important to follow if you don't understand it. traded_volume_values = df [[ 'market_id' , 'selection_id' , 'venue' , 'bsp' , 'seconds_before_scheduled_jump' , 'traded_volume' ]] all_sbj = pd . DataFrame ( data = { 'seconds_before_scheduled_jump' : traded_volume_values . seconds_before_scheduled_jump . unique ()}) . assign ( join = 1 ) traded_volume_explode = traded_volume_values [[ 'market_id' , 'selection_id' , 'venue' , 'bsp' ]] . drop_duplicates () . assign ( join = 1 ) . merge ( all_sbj ) . drop ( 'join' , 1 ) traded_volume_df = traded_volume_explode . merge ( traded_volume_values , how = \"left\" ) traded_volume_df = traded_volume_df . sort_values ([ 'market_id' , 'selection_id' , 'venue' , 'seconds_before_scheduled_jump' ], ascending = [ True , True , True , False ]) traded_volume_df . update ( traded_volume_df . groupby ([ 'market_id' , 'selection_id' , 'venue' ])[[ 'seconds_before_scheduled_jump' , 'traded_volume' ]] . ffill () . fillna ( 0 )) /tmp/ipykernel_327971/677704215.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only # Group by market, sum volume over selections at a given time, average over times for total def chunkSBJ ( sbj ): if sbj < 600 : return ( sbj ) else : return ( int ( math . floor ( sbj / 60 ) * 60 )) tradedVolumes_1 = ( traded_volume_df . groupby ([ \"market_id\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . assign ( seconds_before_scheduled_jump = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . groupby ( \"seconds_before_scheduled_jump\" , as_index = False ) . agg ({ 'traded_volume' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) ) fig = px . area ( tradedVolumes_1 , x = 'seconds_before_scheduled_jump' , y = 'traded_volume' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } #subtitle = \"Top 5 Biggest Vic Track Sample\" ) fig . update_layout ( font_family = \"Roboto\" ) fig [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig . show ( \"png\" ) The discontinuity in the chart highlights the switch point between the two time granularities that we extracted from the stream. Pre 600 seconds (10 minutes) before the scheduled off I can plot 1 data point per minute and after I'm plotting 60 data points per minute. The traded volume chart looks like an exponential chart: the total traded volume doubles from 10 minutes out to 4 minutes out, then it doubles again between then and 1 minute out, then nearly doubling again in the last minute and a bit of trading. Even a simple visual like this can help you with your bet placement on Betfair markets. For example if you're planning to get large volumes down on Betfair thoroughbred markets it's probably best to view prices >10 minutes out with a skeptical eye even if the market is tight - because you won't find the requisite lay volume that early as the majority of traded volume happens in the last 2-5 minutes of trading. Now like most analysis the average is definitely hiding lots of interesting things about this data. Let's split out this data by our 5 tracks: tradedVolumes_2 = ( traded_volume_df . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . groupby ([ \"market_id\" , \"venue\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . groupby ([ \"venue\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'traded_volume' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump_chunk' , ascending = False ) ) fig_2 = px . line ( tradedVolumes_2 , x = 'seconds_before_scheduled_jump_chunk' , y = 'traded_volume' , color = 'venue' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } ) fig_2 . update_layout ( font_family = \"Roboto\" ) fig_2 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_2 . show ( \"png\" ) As expected an average Flemington race trades nearly 500k whereas an average Bendigo race trades only ~120k volume. How about if we split our selections by odds range. Intuitively we know that odds-on horses will trade significantly more volume than a \\$50 shot but let's visualise the difference. We'll chunk the BSP of each horse into 5 groups: - Odds on (<50% chance of winning) - Odds between 2 and 5 - Odds between 5 and 15 - Odds between 15 and 50 - Odds 50+ def chunkBsp ( bsp ): if bsp <= 2 : return ( \"1. Odds On\" ) elif bsp <= 5 : return ( \"2. (2, 5]\" ) elif bsp <= 15 : return ( \"3. (5, 15]\" ) elif bsp <= 50 : return ( \"4. (15, 50]\" ) else : return ( \"5. 50+\" ) # Group by odds range tradedVolumes_3 = ( traded_volume_df . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( bsp = lambda x : x [ 'bsp' ] . apply ( chunkBsp )) . groupby ([ \"market_id\" , \"bsp\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . groupby ([ \"bsp\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'traded_volume' : 'mean' }) ) fig_3 = px . line ( tradedVolumes_3 , x = 'seconds_before_scheduled_jump_chunk' , y = 'traded_volume' , color = 'bsp' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } ) fig_3 . update_layout ( font_family = \"Roboto\" ) fig_3 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_3 . show ( \"png\" ) Again as expected the traded volume is strongly inversely proportional to the implied chance. There's a few reasons for this: - Inherently exposure is inversely proportional to odds so the same stake can produce widely different exposures for lay betting - Non model based participants have limited resources to manually analyse the form and thus focus on the top end of the market - Higher chance events reduce variance which is captured in staking schemes like the kelly criterion (which overweight stakes on larged percieved advantages on high probability events) that sophisticated participants tend to use Knowing where a majority of the traded volume is concentrated is another thing you should be aware of whether your betting on horse racing or elections and everything in between.","title":"2.2.1 Traded Volumes"},{"location":"historicData/analysingAndPredictingMarketMovements/#222-market-tightness","text":"Understanding how the market tightens before the off is also another key conceptual component to market formation. I will consider two different variations on this concept: Market overround or Market Percentage The sum of probabilities across all outcomes Back % are always above 1 (else there exists an arbitrage opportunity) Lay % are always below 1 (else there exists an arbitrage opportunity) Market Spread The # of ticks / rungs between the best available back price and the best available lay price The market % is the value displayed on the Betfair website here: averageMarketPct = ( df [[ 'market_id' , 'selection_id' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . query ( 'seconds_before_scheduled_jump >= -20' ) . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( back_best = lambda x : 1 / x [ 'back_best' ], lay_best = lambda x : 1 / x [ 'lay_best' ] ) . groupby ([ \"market_id\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'back_best' : 'sum' , 'lay_best' : 'sum' }) . groupby ( \"seconds_before_scheduled_jump_chunk\" , as_index = False ) . agg ({ 'back_best' : 'mean' , 'lay_best' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump_chunk' , ascending = False ) ) fig_4 = go . Figure () fig_4 . add_trace ( go . Scatter ( x = averageMarketPct [ 'seconds_before_scheduled_jump_chunk' ], y = averageMarketPct [ 'back_best' ], mode = 'lines' , name = 'Back Market Overround' )) fig_4 . add_trace ( go . Scatter ( x = averageMarketPct [ 'seconds_before_scheduled_jump_chunk' ], y = averageMarketPct [ 'lay_best' ], mode = 'lines' , name = 'Lay Market Overround' )) fig_4 . update_layout ( font_family = \"Roboto\" , template = \"plotly_white\" , title = 'Average Back + Lay Market Overound Vic Thoroughbreds' ) fig_4 . update_xaxes ( title = \"Seconds Before Scheduled Jump\" ) fig_4 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_4 . show ( \"png\" ) As you can see the back and lay market % converge to 1 as the market gets closer to the jump. However, these are generally great markets even 30 mins before the off to have overrounds of only 4% is very good for racing markets. If you were analysing different kinds of racing markets, however, (harness or greyhound markets or thoroughbred races for country meets) you may need to conduct this kind of analysis to see when the earliest time you're likely to be able to get fair prices on either side of the market. Another way we can measure the tightness of Betfair markets is the market spread. I'm going to define the market spread as the number of ticks between the best back and best lay prices. This can give some extra granularity when measuring the market tightness for an individual selection In this market for example we can see that the first selection has a spread of 5 ticks between 11 and 13.5 (where ticks are 0.5 apart) whereas there's only 2 ticks between the best back and lay for the market favourite 3.4 and 3.5 (where ticks are 0.05 apart). First we'll need to create some custom functions that will create the Betfair ladder and do Betfair \"tick arithmetic\" for us. Part of the reason that I'm creating a different view for market spread is as a reason to introduce this Betfair tick ladder concept. Measuring odds differences and movement between odds values can be tricky because prices are fairly non-linear in probability space (you see far more horses between 2 and 3 than you do between 802 and 803 for example). Converting prices to a rank on the Betfair tick ladder creates a nice output mapping that can be used for all kinds of other purposes. Betfair actually has some Betfair arithmetic tick functions available on Github # Define the betfair tick ladder def bfTickLadder (): tickIncrements = { 1.0 : 0.01 , 2.0 : 0.02 , 3.0 : 0.05 , 4.0 : 0.1 , 6.0 : 0.2 , 10.0 : 0.5 , 20.0 : 1.0 , 30.0 : 2.0 , 50.0 : 5.0 , 100.0 : 10.0 , 1000.0 : 1000 , } ladder = [] for index , key in enumerate ( tickIncrements ): increment = tickIncrements [ key ] if ( index + 1 ) == len ( tickIncrements ): ladder . append ( key ) else : key1 = [ * tickIncrements ][ index ] key2 = [ * tickIncrements ][ index + 1 ] steps = ( key2 - key1 ) / increment for i in range ( int ( steps )): ladder . append ( round ( key + i * increment , 2 )) return ( ladder ) bfticks = bfTickLadder () # Round a decimal to the betfair tick value below def bfTickFloor ( price , includeIndex = False ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () ind = [ n for n , i in enumerate ( bfticks ) if i >= price ][ 0 ] if includeIndex : if bfticks [ ind ] == price : return (( ind , price )) else : return (( ind - 1 , bfticks [ ind - 1 ])) else : if bfticks [ ind ] == price : return ( price ) else : return ( bfticks [ ind - 1 ]) # Calculate the numder of ticks between two tick values def bfTickDelta ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) x = bfTickFloor ( p1 , includeIndex = True ) y = bfTickFloor ( p2 , includeIndex = True ) return ( x [ 0 ] - y [ 0 ]) bfTickDelta ( 13.5 , 11 ) 5 bfTickDelta ( 3.5 , 3.4 ) 2 Now that we have our functions let's plot the average market spread leading up to the jump. # Group by odds range averageMarketSpread = ( df [[ 'market_id' , 'selection_id' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( market_spread = lambda x : x . apply ( lambda x : bfTickDelta ( x . lay_best , x . back_best ), axis = 1 )) . groupby ([ \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'market_spread' : 'mean' }) ) fig_5 = px . line ( averageMarketSpread , x = 'seconds_before_scheduled_jump_chunk' , y = 'market_spread' , template = 'plotly_white' , title = \"Market Spread Leading Up To Jump\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"market_spread\" : \"Average Spread (b/w best back and lay)\" } ) fig_5 . update_layout ( font_family = \"Roboto\" ) fig_5 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_5 . show ( \"png\" ) So much the same story as before, the spread tightens pretty dramatically leading all the way up to the jump. Now that we have a measure for market tightness on a selection level we can split it by odds range to see how tightness varies across odds range. # Market Spread By Odds Group # _______________________ # Group by odds range averageMarketSpreadOddsgrp = ( df [[ 'market_id' , 'selection_id' , 'bsp' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( bsp = lambda x : x [ 'bsp' ] . apply ( chunkBsp )) . assign ( market_spread = lambda x : x . apply ( lambda x : bfTickDelta ( x . lay_best , x . back_best ), axis = 1 )) . groupby ([ \"bsp\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'market_spread' : 'mean' }) ) fig_6 = px . line ( averageMarketSpreadOddsgrp , x = 'seconds_before_scheduled_jump_chunk' , y = 'market_spread' , color = \"bsp\" , template = 'plotly_white' , title = \"Market Spread Leading Up To Jump\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"market_spread\" : \"Average Spread (b/w best back and lay)\" } ) fig_6 . update_layout ( font_family = \"Roboto\" ) fig_6 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_6 . show ( \"png\" ) Now this gives us visibility of how the market tightens across groups of selections. Clearly the bottom end of the market takes a bit longer to tighten up which fits our understanding that there's generally less money, and less very sharp money on these selections.","title":"2.2.2 Market Tightness"},{"location":"historicData/analysingAndPredictingMarketMovements/#23-market-moves","text":"Circling back to the motivation of this article: How should you think about large price moves in Betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data Now that we have a good grasp on the data we've collected on how Betfair markets form let's try to analyse big price moves. Here's a sample of our largest price moves in our race sample: # Big Movers # ____________________ ( df . groupby ([ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'bsp' ], as_index = False ) . agg ({ 'ltp' : 'max' }) . rename ( columns = { 'ltp' : 'max_traded' }) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . max_traded , x . bsp ), axis = 1 )) . query ( 'max_traded < 500' ) . sort_values ( 'ticks_in' , ascending = False ) . head ( 10 ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id venue selection_name win bsp max_traded ticks_in 9209 1.182760338 39317622 Bendigo 6. San Fabrizio 0 2.81 10.50 71 2277 1.173069469 35981261 Bendigo 13. Shotmaker 1 2.32 4.20 56 4853 1.176708684 37456112 Bendigo 6. Zoutons 1 1.94 3.00 56 7795 1.180291241 38744938 Sandown 4. Summit Reach 0 2.42 4.50 54 4404 1.175952680 36995397 Bendigo 2. Nikau Spur 0 1.68 2.40 52 6413 1.178722039 27445196 Sandown 6. Miss Five Hundred 0 2.05 3.20 52 10216 1.184305249 39800435 Sandown 18. Lucabelle 0 48.00 470.00 48 5872 1.177927858 38197699 Flemington 2. Long Arm 0 9.90 80.00 47 1519 1.172217104 35609408 Moonee Valley 4. Tailleur 1 1.61 2.14 46 8856 1.182235083 25370687 Bendigo 3. Cernan 0 2.34 3.65 46 Some of those moves are astounding: San Fabrizio traded at 10.50 in the last 30 mins before the jump and had a final BSP of 2.80, but lost. Like we discussed before observing a market plunge in hindsight may seem like a more powerful predictor than it actually is. For example, if we had have flat staked these 10 giant moves after they happened (say at the BSP) we'd be running at ~70% POT even though our intuition would have told you at the time that these horses couldn't lose!","title":"2.3 Market Moves"},{"location":"historicData/analysingAndPredictingMarketMovements/#231-reacting-to-moves-after-they-happen","text":"Bookies, race callers and other services will often tell you what selection has been strong in the market; who the \"money\" has come for. But once you hear this is it already too late to do anything about? Let's extend the sample in the previous section to see if we can draw any broader conclusions. I'll take a sample of the all selections 10 minutes before the scheduled jump I'll also take those same selections at the scheduled jump exactly I'll then calculate the number of ticks between the best back price at each time point to measure the movement in the last 10 minutes of trading I'll then calculate a back and lay profit using the top box at the scheduled off m10 = ( df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . apply ( lambda g : None if g [ g [ 'seconds_before_scheduled_jump' ] < 600 ] . shape [ 0 ] == 0 else g [ g [ 'seconds_before_scheduled_jump' ] < 600 ] . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) . iloc [ 0 ]) . filter ( items = [ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'back_best' , 'lay_best' ]) . rename ( columns = { 'back_best' : 'back_best_10m' , 'lay_best' : 'lay_best_10m' }) ) m0 = ( df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . apply ( lambda g : None if g [ g [ 'seconds_before_scheduled_jump' ] < 0 ] . shape [ 0 ] == 0 else g [ g [ 'seconds_before_scheduled_jump' ] < 0 ] . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) . iloc [ 0 ]) . filter ( items = [ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'back_best' , 'lay_best' ]) ) # Back or lay according to large plunges pd . DataFrame ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 stake 241.000 back_pl -4.824 lay_pl -17.620 # Back or lay according to large plunges - grouped by track ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . groupby ( 'venue' , as_index = False ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } venue stake back_pl lay_pl 0 Bendigo 73 -14.7225 8.64 1 Caulfield 32 21.1305 -25.20 2 Flemington 36 -8.0095 5.13 3 Moonee Valley 34 -18.8730 16.62 4 Sandown 66 15.6505 -22.81 As you can see there's not much of a pattern here to take advantage of here. At least for this sample of tracks, and for this time slice, the value has been sucked dry from the market on these selections. To illustrate this let's check what the backers would have profited if they were the ones who could identify these selections prior to their big market moves # Backing before the plunge? ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best_10m' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best_10m' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) stake 241.0000 back_pl 108.6155 lay_pl -154.6100 dtype: float64 I could sign up for winning at 50% POT! Now I just have to pick 100% of the large thoroughbred plunges before they happen... Obviously no one individual person or group is predicting these movements 100%. Even the sharpest players will have movements go against them but this highlights the essence of the problem and the contradiction of retrospective price movement analysis. These movements are important. There is a lot to be gained by purely trading Betfair markets and how they move. However, identifting large moves after they occur isn't good enough. The large players sharpen the odds down to a good and fair implied chance and leave those watching on the sidelines with little to no value either side. To profit then we must capture some of the value as they are and jump on momentum as it's happening not after it's come to a halt.","title":"2.3.1 Reacting to moves after they happen"},{"location":"historicData/analysingAndPredictingMarketMovements/#232-visualising-the-moves","text":"In the final section I'll frame how I think you could go about identifying these moves as they're happening or about to happen. But first to illustrate some of dynamics let's visualise some of these large moves. How do selections firm and do they do it with different characteristics in different patterns? Candlestick charts are a good way to visualise the evolution of prices in markets and are often used in financial markets to do technical analysis. I'll first create a function to create a candlestick chart for a market / selection slice of our dataframe using plotly charts. # Candlestick Plotly Chart With Plotly # _____________________________ def priceCandlestickPlotly ( d ): d [ 'time_chunk' ] = d [ 'time' ] . dt . round ( '2min' ) selectionName = d . selection_name . iloc [ 0 ] track = d . venue . iloc [ 0 ] startTime = d . market_time . iloc [ 0 ] candleStickInput = d . groupby ( 'time_chunk' , as_index = False ) . agg ({ \"ltp\" : [ 'first' , 'last' , 'min' , 'max' ]}) candleStickInput . columns = [ \"_\" . join ( pair ) for pair in candleStickInput . columns ] fig = go . Figure ( data = [ go . Candlestick ( x = candleStickInput [ 'time_chunk_' ], open = candleStickInput [ 'ltp_first' ], high = candleStickInput [ 'ltp_max' ], low = candleStickInput [ 'ltp_min' ], close = candleStickInput [ 'ltp_last' ])]) fig . update_layout ( template = \"plotly_white\" , xaxis_rangeslider_visible = False , title = f ' { selectionName } at { track } race started at { startTime } UTC' ) fig . show ( \"png\" ) Let's visualise the 2 biggest moves with this chart function. priceCandlestickPlotly ( df . query ( 'market_id == \"1.182760338\" and selection_id == \"39317622\"' )) /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This looks exactly like what we'd expect from a extreme plunge: consistent, sustained and unrelenting support on the back side as the horse is pushed from 8-10 odds down to it's fair odds of around 3. Let's look at the second big plunge: priceCandlestickPlotly ( df . query ( 'market_id == \"1.173069469\" and selection_id == \"35981261\"' )) This movement chart is a bit different. The market has 4 distinct segments: About 30 minutes from the off the market changes and the horse is supported from 4 into 3 It trades at around 3 for the next 25 minutes It then drifts back out sharply to a bit over 4 It then gets crunched back in all the way into a BSP of 2.30 Clearly different segments of the participants had very different perceptions of this horses chances. How do we make sense of it? Let's try a different candlestick visualistion, which includes the traded volume. import mplfinance as fplt def priceCandlestickMpl ( d ): d [ 'time_chunk' ] = d [ 'time' ] . dt . round ( '2min' ) selectionName = d . selection_name . iloc [ 0 ] track = d . venue . iloc [ 0 ] startTime = d . market_time . iloc [ 0 ] # Add traded volume in last interval d = d . groupby ([ 'market_id' , 'selection_id' ]) . apply ( lambda x : x . assign ( traded_volume_delta = lambda y : ( y [ 'traded_volume' ] - y [ 'traded_volume' ] . shift ( 1 )) . mask ( pd . isnull , 0 ))) candleStickInput = d . groupby ( 'time_chunk' , as_index = False ) . agg ({ \"ltp\" : [ 'first' , 'last' , 'min' , 'max' ], \"traded_volume_delta\" : 'sum' }) candleStickInput . columns = [ \"_\" . join ( pair ) for pair in candleStickInput . columns ] candleStickInput = candleStickInput . rename ( columns = { 'time_chunk_' : 'date' , 'ltp_first' : 'open' , 'ltp_last' : 'close' , 'ltp_min' : 'low' , 'ltp_max' : 'high' , 'traded_volume_delta_sum' : 'volume' }) candleStickInput = candleStickInput . set_index ( 'date' ) fplt . plot ( candleStickInput , type = 'candle' , style = 'yahoo' , title = f ' { selectionName } at { track } race started at { startTime } UTC' , ylabel = 'Odds' , volume = True , ylabel_lower = 'Volume Traded' , ) First the first horse: priceCandlestickMpl ( df . query ( 'market_id == \"1.182760338\" and selection_id == \"39317622\"' )) /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy 2021-08-30T11:43:18.549484 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} Now the second selection: priceCandlestickMpl ( df . query ( 'market_id == \"1.173069469\" and selection_id == \"35981261\"' )) /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy 2021-08-30T11:43:22.999632 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} So as the majority of the volume was traded right before the jump, the price was hammered into 2.30. This is a pattern that you might see a lot in Betfair markets: prices may oscillate around or even drift consistently in one direction until a point at which certain large participants or groups of participants enter the market and push the selection in another direction completely. So anticipating a move won't be as simple as analysing the price as a time series, it's one piece of the puzzle. Really sophisticated market trading algorithms will need to historically analyse a raft of market metrics and correlate them with these movements historically within the market type of interest. In the next section I'll start off down that road to give you an idea of how I'd tackle the problem but it's more complicated than we can uncover in a single article.","title":"2.3.2 Visualising the moves"},{"location":"historicData/analysingAndPredictingMarketMovements/#233-anticipating-a-move","text":"Our retrospective analysis was good to help us understand the dynamics of movements, the efficiency of markets, how they form, and what things we could put in a predictive analysis. The next step is to create forward looking estimates about where the market is headed next. You get setup your problem in lots of different ways including creating rules based strategies like discussed in a previous piece . Or you could go down a formalised machine learning approach. The first step to both is to build up some key features and test their predictive power. The factors I'll consider in this section will be: The movement over the last 30 seconds The weight of money on the back side The weight of money on the lay side The current best back / volume weighted average top 3 back (top box support on back side) The current best lay / volume weighted average top 3 lay (top box support on lay side) The current best back / The volume weighted traded price over the last increment The current best lay / The volume weighted traded price over the last increment And im interested in correlating these variables with the number + direction of ticks moved over the next 30 seconds to see if I can find anything interesting. # Start Analysis # ______________________________ dfPredict = df . query ( 'seconds_before_scheduled_jump <= 600 and seconds_before_scheduled_jump >= 0' ) # Target # ____________________________ dfPredict [ 'best_back_30s_in_future' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( 30 ) dfPredict [ 'best_lay_30s_in_future' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( 30 ) dfPredict [ 'back_target' ] = dfPredict . apply ( lambda x : bfTickDelta ( x . back_best , x . best_back_30s_in_future ), axis = 1 ) dfPredict [ 'lay_target' ] = dfPredict . apply ( lambda x : bfTickDelta ( x . lay_best , x . best_lay_30s_in_future ), axis = 1 ) # Movement # ____________________________ dfPredict [ 'back_best_30s_ago' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( - 30 ) dfPredict [ 'back_lay_30s_ago' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'lay_best' ] . shift ( - 30 ) dfPredict = ( dfPredict . assign ( back_ticks_in_30s = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_30s_ago , x . back_best ), axis = 1 )) . assign ( lay_ticks_in_30s = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_lay_30s_ago , x . lay_best ), axis = 1 )) ) # Weight Of Money # ____________________________ atb_ladder = dfPredict . atb_ladder . iloc [ 0 ] atl_ladder = dfPredict . atl_ladder . iloc [ 0 ] def wom ( back_ladder , lay_ladder ): if not back_ladder or not lay_ladder : return (( None , None )) total_volume = round ( sum ( back_ladder [ 'v' ]) + sum ( lay_ladder [ 'v' ]), 2 ) return (( round ( sum ( back_ladder [ 'v' ]) / total_volume , 3 ), round ( sum ( lay_ladder [ 'v' ]) / total_volume , 3 ))) dfPredict [ 'wom' ] = dfPredict . apply ( lambda x : wom ( x [ 'atb_ladder' ], x [ 'atl_ladder' ]), axis = 1 ) dfPredict [[ 'back_wom' , 'lay_wom' ]] = pd . DataFrame ( dfPredict [ 'wom' ] . tolist (), index = dfPredict . index ) # Top Box Support # ________________________ dfPredict [ 'back_best_support' ] = dfPredict [ 'back_vwap' ] / dfPredict [ 'back_best' ] dfPredict [ 'lay_best_support' ] = dfPredict [ 'lay_best' ] / dfPredict [ 'lay_vwap' ] # Recent Movement # _____________________ dfPredict [ 'wap_movement_10s' ] = dfPredict [ 'wap' ] / dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'wap' ] . shift ( - 10 ) /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Now that we've created the candidate factors let's use a spearman correlation coefficient to analyse their correlation (positive or negative we're mostly focussed on the absolute size of the coefficient). corrMatrix = dfPredict [[ 'back_target' , 'back_ticks_in_30s' , 'lay_ticks_in_30s' , 'back_wom' , 'lay_wom' , 'back_best_support' , 'lay_best_support' , 'wap_movement_10s' ]] . dropna () . corr ( method = \"spearman\" ) corrMatrix [[ 'back_target' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } back_target back_target 1.000000 back_ticks_in_30s -0.080772 lay_ticks_in_30s -0.036798 back_wom 0.259926 lay_wom -0.259927 back_best_support -0.142726 lay_best_support 0.013173 wap_movement_10s -0.316167 import seaborn as sns corPlot = sns . heatmap ( corrMatrix , vmin =- 1 , vmax = 1 , center = 0 , cmap = sns . diverging_palette ( 20 , 220 , n = 200 ), square = True ) corPlot . set_xticklabels ( corPlot . get_xticklabels (), rotation = 45 , horizontalalignment = 'right' ) [Text(0.5, 0, 'back_target'), Text(1.5, 0, 'back_ticks_in_30s'), Text(2.5, 0, 'lay_ticks_in_30s'), Text(3.5, 0, 'back_wom'), Text(4.5, 0, 'lay_wom'), Text(5.5, 0, 'back_best_support'), Text(6.5, 0, 'lay_best_support'), Text(7.5, 0, 'wap_movement_10s')] 2021-08-30T12:26:29.189464 image/svg+xml Matplotlib v3.4.3, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} We're entirely interested in the back_target column in the matrix and the heatmap. We can see that some of our candidate features have decent correlation with the number of ticks the price will move over the course of the next 30 seconds.","title":"2.3.3 Anticipating a move"},{"location":"historicData/analysingAndPredictingMarketMovements/#234-next-steps","text":"This marks the end of this analysis however if you're interested in turning this kind of analysis / approach into something more I'd suggest there's two main paths you could go down from here. Angles + Rules You could dig into the data a little more a find specific situations where some of these features (or ones like them) preceeded certain things happening in the market (movements or increases in trading volatility etc) You could then construct trading rules based on these findings an try to automate them or bake them into trading rules inside a 3rd party tool. Predictive Modelling If you were comfortable with statistical models or machine learning you could easily feed this data into a predictive modelling workflow. Once honed, the predictions from this workflow could be turned into recommended betting decisions which could form a part of an automated algorithmic betting framework. The work to fill out a project like this would be significant but so would the reward.","title":"2.3.4 Next Steps"},{"location":"historicData/analysingAndPredictingMarketMovements/#30-conclusion","text":"Like previous articles this analysis is a sketch of what can be accomplished analysing the historical stream files. In this instance we focussed on the markets themselves, with a particular focussing on a small slice of markets: victorian thoroughbred markets. We analysed how these markets form, how to interpret the price movements on certain selections and the first steps to building out automated strategies based on these features alone. Building automated betting strategies based on the markets alone is path that plenty of quantitatively inclined Betfair customers go down as it minimises complexity in source data (there's only one!) and there's plenty of value in it left untapped for you to try to capture.","title":"3.0 Conclusion"},{"location":"historicData/analysingAndPredictingMarketMovements/#complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github # %% [markdown] # # Do #theyknow? Analysing betfair market formation and market movements # %% [markdown] # ## 0.1 Setup # # Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language. # # Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer locally where you want to store the intermediate data files. # # You'll also need `betfairlightweight` which you can install with something like `pip install betfairlightweight`. # %% import pandas as pd import numpy as np import requests import os import re import csv import plotly.express as px import plotly.graph_objects as go import math import logging import yaml import csv import tarfile import zipfile import bz2 import glob import ast from datetime import date , timedelta from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) # %% [markdown] # ## 0.2 Context # # You may have seen the hashtag if you're on australian racing twitter #theyknow following a dramatic late market move for a horse that's followed by a decisive race victory. Sometimes it can seem eerie how accurate these moves are after the fact. In betfair racing markets there's usually a flurry of activity leading up to the race start as players look to get their bets down at the best price without tipping their hand (opinion on the race) too much. Large moves can happend when large players rally around a selection who's implied chance in early trading isn't close to what it's true chance is in the upcoming race. Large moves can also happen when there's some inside information - not able to be gleaned from analysis of the horses previous races - that slowly filters out of the stable or training group. # # This creates opportunity in the secondary market as punters try to read these movements to make bets themselves. The task often becomes identifying which movements are caused by these sophisticated players or represent real signals of strength and which aren't. # # So do #theyknow generally? Before even looking at the data I can assure you that yes they do know pretty well. Strong movements in betting markets are usually pretty reliable indicators about what is about to happen. However, these moves can be overvalued by recreationals. When observing a horse plumet in from $3.50 to $2 you are usually suprised if the horse loses, but the general efficiency of late prices would suggest that this horse is going to still lose 50% of time. If you simply observe the large moves after the sharp players have corrected the market landscape you're in no better a position to bet than before the move happened. On the other hand what if would could reliably identify the move as it was happening or about to happen? That would be a recipe for successful trading of horse racing markets and no doubt this is what many players in this secondary market (analysis of betfair markets rather than the races themselves) try to do. # # If you were to build up a manual qualitative strategy for this kind of market trading you need to understand: # - Who the participants are # - How sophisticated they are at the top end # - What types of races do they bet on and for how much # - When the different types of participants typically enter the market # - What do bet placement patterns look like for these participants # - etc. # # This is the kind of task that takes a lot research, years of experience watching markets, a lot of trial and error, and a lot of industry know-how. Given I'm a lazy quantitative person I'll try to see if I can uncover any of these patterns in the data alone. # # Put simply the central question for the second half of this piece will be: # # > How should you think about large price moves in betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data # # I'll just be analysing historical thoroughbred markets but the same approach could be applied to any sport or racing code of your interest. # %% [markdown] # ## 0.3 This Example # # Building market based trading strategies is a broad and fertile ground for many quantitative betfair customers; too big to cover in a single article. I'll try to zero in on a small slice of thoroughbred markets and analyse how these markets form and how I'd start the process of trying to find the patterns in the market movements. Again hopefully this is some inspiration for you and you can pick up some of the ideas and build them out. # # Given volume of data (when analysing second by second slices of market data) I'll be looking at a years worth of thoroughbred races from the 5 largest Victorian tracks: Flemington, Caulfield, Moonee Valley, Bendigo and Sandown. # %% [markdown] # # 1.0 Data # # Unlike in some of the previous tutorials we aren't going to collapse the stream data into a single row per runner. In those examples we were interested in analysing some discrete things about selections in betfair markets like: # # - Their final odds (bsp or last traded price) # - Their odds at some fixed time point or time points before the scheduled race start # - Other single number descriptors of the trading activity on a selection (eg total traded volume) # # # In this analysis I want to analyse how markets form and prices move for selections as markets evolve. So we'll need to pull out multiple price points per runner - so we'll have multiple rows per runner in our parsed output dataset. # # To output a row for every stream update for every selection in every thoroughbred race over the last 12 months would produce a dataset far too big too analyse using normal data analysis tools - we're about 10s to 100s of billions of rows. # # To chop our sample down into a manageable slice I'm going to filter on some select tracks of interest (as mentioned above) and I'm also going to have 3 sections of data granularity: # # - I won't log any of the odds or traded volumes > 30mins before the scheduled off # + In thoroughbreds there is non-trivial action before this point you may want to study, but it's not what I want to study here # - Between 30 and 10 minutes before the scheduled off I'll log data every 60 seconds # - 10 minutes or less to the scheuled off I'll log prices every second # # The code to manage this windowed granularity is in the below parsing code tweak as you wish if you want to tighten or broaden the analysis. # %% [markdown] # ## 1.1 Sourcing Data # # First you'll need to source the stream file TAR archive files. I'll be analysing 12 months of Australian thoroughbred Pro files. Aask automation@betfair.com.au for more info if you don't know how to do this. Once you've gotten access download them to your computer and store them together in a folder. # %% [markdown] # ## 1.2 Utility Functions # %% # General Utility Functions # _________________________________ def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def slicePrice ( l , n ): try : x = l [ n ] . price except : x = \"\" return ( x ) def sliceSize ( l , n ): try : x = l [ n ] . size except : x = \"\" return ( x ) def pull_ladder ( availableLadder , n = 5 ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : n ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"p\" ] = price out [ \"v\" ] = volume return ( out ) # %% [markdown] # Slicing the tracks we want we'll just adjust the market filter function used before to include some logic on the venue name # %% def filter_market ( market : MarketBook ) -> bool : d = market . market_definition track_filter = [ 'Bendigo' , 'Sandown' , 'Flemington' , 'Caulfield' , 'Moonee Valley' ] return ( d . country_code == 'AU' and d . venue in track_filter and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # %% [markdown] # ## 1.3 Selection Metadata # # Given that the detailed price data will have so many records we will split out the selection metadata (including the selection win outcome flag and the bsp) into it's own dataset much you would do in a relational database to manage data volumes. # # This means we'll have to parse over the data twice but our outputs will be much smaller than if we duplicated the selection name 800 times for example. # %% def final_market_book ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): return ( None ) for market_book in market_books : last_market_book = market_book return ( last_market_book ) def parse_final_selection_meta ( dir , out_file ): with open ( out_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,venue,market_time,selection_name,win,bsp \\n \" ) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) last_market_book = final_market_book ( stream ) if last_market_book is None : continue # Extract Info ++++++++++++++++++++++++++++++++++ runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in last_market_book . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'win' : np . where ( r . status == \"WINNER\" , 1 , 0 ), 'sp' : r . sp . actual_sp } for r in last_market_book . runners ] # Return Info ++++++++++++++++++++++++++++++++++ for runnerMeta in runnerMeta : if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( last_market_book . market_id ), runnerMeta [ 'selection_id' ], last_market_book . market_definition . venue , last_market_book . market_definition . market_time , runnerMeta [ 'selection_name' ], runnerMeta [ 'win' ], runnerMeta [ 'sp' ] ) ) # %% selection_meta = \"[OUTPUT PATH TO CSV FOR SELECTION METADATA]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) trading = betfairlightweight . APIClient ( username = \"username\" , password = \"password\" , app_key = \"app_key\" ) listener = StreamListener ( max_latency = None ) print ( \"__ Parsing Selection Metadata ___ \" ) # parse_final_selection_meta(stream_files, selection_meta) # %% [markdown] # ## 1.4 Detailed Preplay Odds # # Like mentioned above there will be some time control logic injected to control the time granularity that odds are recorded in each step. # # Instead of widening the available to bet price ladder I'll extract the top 10 rungs of the available to back (atb) and available to lay (atl) ladders and write them both to the output file. That will give me more flexibility during the analysis to pull out things that interest me. So in total I'll extract: # # - Top 10 ATB Ladder # - Top 10 ATL Ladder # - Total Traded Volume # - Volume weighted average traded price up till the current time # - Last Traded price # # %% def loop_preplay_prices ( s , o ): with patch ( \"builtins.open\" , lambda f , _ : f ): gen = s . get_generator () marketID = None tradeVols = None time = None for market_books in gen (): # Check if this market book meets our market filter ++++++++++++++++++++++++++++++++++ if (( evaluate_market := filter_market ( market_books [ 0 ])) == False ): break for market_book in market_books : # Time Step Management ++++++++++++++++++++++++++++++++++ if marketID is None : # No market initialised marketID = market_book . market_id time = market_book . publish_time elif market_book . inplay : # Stop once market goes inplay break else : seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () if seconds_to_start > log1_Start : # Too early before off to start logging prices continue else : # Update data at different time steps depending on seconds to off wait = np . where ( seconds_to_start <= log2_Start , log2_Step , log1_Step ) # New Market if market_book . market_id != marketID : marketID = market_book . market_id time = market_book . publish_time # (wait) seconds elapsed since last write elif ( market_book . publish_time - time ) . total_seconds () > wait : time = market_book . publish_time # fewer than (wait) seconds elapsed continue to next loop else : continue # Execute Data Logging ++++++++++++++++++++++++++++++++++ for runner in market_book . runners : try : atb_ladder = pull_ladder ( runner . ex . available_to_back , n = 10 ) atl_ladder = pull_ladder ( runner . ex . available_to_lay , n = 10 ) except : atb_ladder = {} atl_ladder = {} # Calculate Current Traded Volume + Tradedd WAP limitTradedVol = sum ([ rung . size for rung in runner . ex . traded_volume ]) if limitTradedVol == 0 : limitWAP = \"\" else : limitWAP = sum ([ rung . size * rung . price for rung in runner . ex . traded_volume ]) / limitTradedVol limitWAP = round ( limitWAP , 2 ) o . writerow ( ( market_book . market_id , runner . selection_id , market_book . publish_time , limitTradedVol , limitWAP , runner . last_price_traded or \"\" , str ( atb_ladder ) . replace ( ' ' , '' ), str ( atl_ladder ) . replace ( ' ' , '' ) ) ) def parse_preplay_prices ( dir , out_file ): with open ( out_file , \"w+\" ) as output : writer = csv . writer ( output , delimiter = ',' , lineterminator = ' \\r\\n ' , quoting = csv . QUOTE_ALL ) writer . writerow (( \"market_id\" , \"selection_id\" , \"time\" , \"traded_volume\" , \"wap\" , \"ltp\" , \"atb_ladder\" , \"atl_ladder\" )) for file_obj in load_markets ( dir ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) loop_preplay_prices ( stream , writer ) # %% preplay_price_file = \"[OUTPUT PATH TO CSV FOR PREPLAY PRICES]\" stream_files = glob . glob ( \"[PATH TO STREAM FILES]*.tar\" ) log1_Start = 60 * 30 # Seconds before scheduled off to start recording data for data segment one log1_Step = 60 # Seconds between log steps for first data segment log2_Start = 60 * 10 # Seconds before scheduled off to start recording data for data segment two log2_Step = 1 # Seconds between log steps for second data segment print ( \"__ Parsing Detailed Preplay Prices ___ \" ) # parse_preplay_prices(stream_files, preplay_price_file) # %% [markdown] # # 2.0 Analysis # %% [markdown] # ## 2.1 Load and Assemble # # First let's load the raw datafiles we created in the previous step. # # ## 2.1.1 Load # %% [markdown] # We have the highlevel selection metadata (1 row per selection): # # %% selection_meta_path = \"[PATH TO METADATA FILE]\" selection = pd . read_csv ( selection_meta_path , dtype = { 'market_id' : object , 'selection_id' : object }, parse_dates = [ 'market_time' ]) selection . head ( 3 ) # %% [markdown] # And we have the detailed preplay price data for these markets + selections: # %% prices_path = \"[PATH TO PRICES FILE]\" prices = pd . read_csv ( prices_path , quoting = csv . QUOTE_ALL , dtype = { 'market_id' : 'string' , 'selection_id' : 'string' , 'atb_ladder' : 'string' , 'atl_ladder' : 'string' }, parse_dates = [ 'time' ] ) prices . head ( 3 ) # %% [markdown] # Now it's important to observe how much data we have here. # %% prices . shape # %% [markdown] # We've got 7 million rows of price data here just for races at 5 thoroughbred tracks over the last year. Now it's not really \"big data\" in the sense you might have heard before but it's certainly a lot of rows and we'll have to think about the performance of our code a little bit more than we would if we were dealining with 1 row per selection style datasets. # %% [markdown] # We need pandas to correctly interpret the dictionary columns as dictionaries so we'll run this code: # %% # To get pandas to correctly recognise the ladder columns as dictionaries prices [ 'atb_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atb_ladder' ]] prices [ 'atl_ladder' ] = [ ast . literal_eval ( x ) for x in prices [ 'atl_ladder' ]] # %% [markdown] # ## 2.1.2 Assemble # %% [markdown] # Now we'll join the 2 data sets together to form a nice normalised dataframe: # %% # Simple join on market and selection_id initially df = selection . merge ( prices , on = [ 'market_id' , 'selection_id' ]) df . head ( 3 ) # %% [markdown] # ## 2.1.3 Transform # # Next we'll do some processing on the joined dataframe to add some columns that we can use in our analysis including calculating a numeric #seconds before the scheduled jump field that we'll use extensively. # %% df = ( df . assign ( back_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atb_ladder' ]]) . assign ( lay_best = lambda x : [ np . nan if d . get ( 'p' ) is None else d . get ( 'p' )[ 0 ] for d in x [ 'atl_ladder' ]]) . assign ( back_vwap = lambda x : [ np . nan if d . get ( 'p' ) is None else round ( sum ([ a * b for a , b in zip ( d . get ( 'p' )[ 0 : 3 ], d . get ( 'v' )[ 0 : 3 ])]) / sum ( d . get ( 'v' )[ 0 : 3 ]), 3 ) for d in x [ 'atb_ladder' ]]) . assign ( lay_vwap = lambda x : [ np . nan if d . get ( 'p' ) is None else round ( sum ([ a * b for a , b in zip ( d . get ( 'p' )[ 0 : 3 ], d . get ( 'v' )[ 0 : 3 ])]) / sum ( d . get ( 'v' )[ 0 : 3 ]), 3 ) for d in x [ 'atl_ladder' ]]) . assign ( seconds_before_scheduled_jump = lambda x : round (( x [ 'market_time' ] - x [ 'time' ]) . dt . total_seconds ())) . query ( 'seconds_before_scheduled_jump < 1800 and seconds_before_scheduled_jump > -120' ) ) # %% [markdown] # ## 2.2 Market Formation # # Before we analyse how prices for selections move let's understand some basic things about how thoroughbred markets form. # %% [markdown] # ## 2.2.1 Traded Volumes # # Let's look at how a typical market (at one of these 5 tracks) trades leading up to the scheduled race start. # # To make some of the analysis a little bit cleaner we need to pad out missing odds updates. For example for a given market we might have a market update 140 seconds before the jump but not another one till 132 seconds before the jump. We'll add rows for those interim 8 seconds by filling the values from the the previous row, this is required to iron out some idiosyncracies in the aggregations, it's not that important to follow if you don't understand it. # %% traded_volume_values = df [[ 'market_id' , 'selection_id' , 'venue' , 'bsp' , 'seconds_before_scheduled_jump' , 'traded_volume' ]] all_sbj = pd . DataFrame ( data = { 'seconds_before_scheduled_jump' : traded_volume_values . seconds_before_scheduled_jump . unique ()}) . assign ( join = 1 ) traded_volume_explode = traded_volume_values [[ 'market_id' , 'selection_id' , 'venue' , 'bsp' ]] . drop_duplicates () . assign ( join = 1 ) . merge ( all_sbj ) . drop ( 'join' , 1 ) traded_volume_df = traded_volume_explode . merge ( traded_volume_values , how = \"left\" ) traded_volume_df = traded_volume_df . sort_values ([ 'market_id' , 'selection_id' , 'venue' , 'seconds_before_scheduled_jump' ], ascending = [ True , True , True , False ]) traded_volume_df . update ( traded_volume_df . groupby ([ 'market_id' , 'selection_id' , 'venue' ])[[ 'seconds_before_scheduled_jump' , 'traded_volume' ]] . ffill () . fillna ( 0 )) # %% # Group by market, sum volume over selections at a given time, average over times for total def chunkSBJ ( sbj ): if sbj < 600 : return ( sbj ) else : return ( int ( math . floor ( sbj / 60 ) * 60 )) tradedVolumes_1 = ( traded_volume_df . groupby ([ \"market_id\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . assign ( seconds_before_scheduled_jump = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . groupby ( \"seconds_before_scheduled_jump\" , as_index = False ) . agg ({ 'traded_volume' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) ) fig = px . area ( tradedVolumes_1 , x = 'seconds_before_scheduled_jump' , y = 'traded_volume' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } #subtitle = \"Top 5 Biggest Vic Track Sample\" ) fig . update_layout ( font_family = \"Roboto\" ) fig [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig . show ( \"png\" ) # %% [markdown] # The discontinuity in the chart highlights the switch point between the two time granularities that we extracted from the stream. Pre 600 seconds (10 minutes) before the scheduled off I can plot 1 data point per minute and after I'm plotting 60 data points per minute. # # The traded volume chart looks like an exponential chart: the total traded volume doubles from 10 minutes out to 4 minutes out, then it doubles again between then and 1 minute out, then nearly doubling again in the last minute and a bit of trading. Even a simple visual like this can help you with your bet placement on betfair markets. For example if you're planning to get large volumes down on betfair thoroughbred markets it's probably best to view prices >10 minutes out with a skeptical eye even if the market is tight - because you won't find the requisite lay volume that early as the majority of traded volume happens in the last 2-5 minutes of trading. # # Now like most analysis the average is definitely hiding lots of interesting things about this data. Let's split out this data by our 5 tracks: # %% tradedVolumes_2 = ( traded_volume_df . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . groupby ([ \"market_id\" , \"venue\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . groupby ([ \"venue\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'traded_volume' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump_chunk' , ascending = False ) ) fig_2 = px . line ( tradedVolumes_2 , x = 'seconds_before_scheduled_jump_chunk' , y = 'traded_volume' , color = 'venue' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } ) fig_2 . update_layout ( font_family = \"Roboto\" ) fig_2 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_2 . show ( \"png\" ) # %% [markdown] # As expected an average Flemington race trades nearly 500k whereas an average Bendigo race trades only ~120k volume. # # How about if we split our selections by odds range. Intuitively we know that odds-on horses will trade significantly more volume than a $50 shot but let's visualise the difference. # # We'll chunk the BSP of each horse into 5 groups: # - Odds on (<50% chance of winning) # - Odds between 2 and 5 # - Odds between 5 and 15 # - Odds between 15 and 50 # - Odds 50+ # %% def chunkBsp ( bsp ): if bsp <= 2 : return ( \"1. Odds On\" ) elif bsp <= 5 : return ( \"2. (2, 5]\" ) elif bsp <= 15 : return ( \"3. (5, 15]\" ) elif bsp <= 50 : return ( \"4. (15, 50]\" ) else : return ( \"5. 50+\" ) # Group by odds range tradedVolumes_3 = ( traded_volume_df . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( bsp = lambda x : x [ 'bsp' ] . apply ( chunkBsp )) . groupby ([ \"market_id\" , \"bsp\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'traded_volume' : 'sum' }) . groupby ([ \"bsp\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'traded_volume' : 'mean' }) ) fig_3 = px . line ( tradedVolumes_3 , x = 'seconds_before_scheduled_jump_chunk' , y = 'traded_volume' , color = 'bsp' , template = 'plotly_white' , title = \"Traded Volume Leading Up To Race Jump (Major Victorian Tracks)\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"traded_volume\" : \"Average Cumulative Traded Volume\" } ) fig_3 . update_layout ( font_family = \"Roboto\" ) fig_3 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_3 . show ( \"png\" ) # %% [markdown] # Again as expected the traded volume is strongly inversely proportional to the implied chance. There's a few reasons for this: # - Inherently exposure is inversely proportional to odds so the same stake can produce widely different exposures for lay betting # - Non model based participants have limited resources to manually analyse the form and thus focus on the top end of the market # - Higher chance events reduce variance which is captured in staking schemes like the kelly criterion (which overweight stakes on larged percieved advantages on high probability events) that sophisticated participants tend to use # # Knowing where a majority of the traded volume is concentrated is another thing you should be aware of whether your betting on horse racing or elections and everything in between. # %% [markdown] # ## 2.2.2 Market Tightness # # Understanding how the market tightens before the off is also another key conceptual component to market formation. I will consider two different variations on this concept: # # - Market overround or Market Percentage # + The sum of probabilities across all outcomes # + Back % are always above 1 (else there exists an arbitrage opportunity) # + Lay % are always below 1 (else there exists an arbitrage opportunity) # - Market Spread # + The # of ticks / rungs between the best available back price and the best available lay price # # The market % is the value displayed on the betfair website here # %% [markdown] # ![](img/overround.png) # %% averageMarketPct = ( df [[ 'market_id' , 'selection_id' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . query ( 'seconds_before_scheduled_jump >= -20' ) . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( back_best = lambda x : 1 / x [ 'back_best' ], lay_best = lambda x : 1 / x [ 'lay_best' ] ) . groupby ([ \"market_id\" , \"seconds_before_scheduled_jump_chunk\" , \"seconds_before_scheduled_jump\" ], as_index = False ) . agg ({ 'back_best' : 'sum' , 'lay_best' : 'sum' }) . groupby ( \"seconds_before_scheduled_jump_chunk\" , as_index = False ) . agg ({ 'back_best' : 'mean' , 'lay_best' : 'mean' }) . sort_values ( 'seconds_before_scheduled_jump_chunk' , ascending = False ) ) fig_4 = go . Figure () fig_4 . add_trace ( go . Scatter ( x = averageMarketPct [ 'seconds_before_scheduled_jump_chunk' ], y = averageMarketPct [ 'back_best' ], mode = 'lines' , name = 'Back Market Overround' )) fig_4 . add_trace ( go . Scatter ( x = averageMarketPct [ 'seconds_before_scheduled_jump_chunk' ], y = averageMarketPct [ 'lay_best' ], mode = 'lines' , name = 'Lay Market Overround' )) fig_4 . update_layout ( font_family = \"Roboto\" , template = \"plotly_white\" , title = 'Average Back + Lay Market Overound Vic Thoroughbreds' ) fig_4 . update_xaxes ( title = \"Seconds Before Scheduled Jump\" ) fig_4 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_4 . show ( \"png\" ) # %% [markdown] # As you can see the back and lay market % converge to 1 as the market gets closer to the jump. However, these are generally great markets even 30 mins before the off to have overrounds of only 4% is very good for racing markets. # # If you were analysing different kinds of racing markets, however, (harness or greyhound markets or thoroughbred races for country meets) you may need to conduct this kind of analysis to see when the earliest time you're likely to be able to get fair prices on either side of the market. # # Another way we can measure the tightness of betfair markets is the market spread. I'm going to define the market spread as the number of ticks between the best back and best lay prices. This can give some extra granularity when measuring the market tightness for an individual selection # %% [markdown] # ![](img/market-spread.png) # %% [markdown] # In this market for example we can see that the first selection has a spread of 5 ticks between 11 and 13.5 (where ticks are 0.5 apart) whereas there's only 2 ticks between the best back and lay for the market favourite 3.4 and 3.5 (where ticks are 0.05 apart). # # First we'll need to create some custom functions that will create the betfair ladder and do betfair \"tick arithmetic\" for us. Part of the reason that I'm creating a different view for market spread is as a reason to introduce this betfair tick ladder concept. Measuring odds differences and movement between odds values can be tricky because prices are fairly non-linear in probability space (you see far more horses between 2 and 3 than you do between 802 and 803 for example). Converting prices to a rank on the betfair tick ladder creates a nice output mapping that can be used for all kinds of other purposes. # %% # Define the betfair tick ladder def bfTickLadder (): tickIncrements = { 1.0 : 0.01 , 2.0 : 0.02 , 3.0 : 0.05 , 4.0 : 0.1 , 6.0 : 0.2 , 10.0 : 0.5 , 20.0 : 1.0 , 30.0 : 2.0 , 50.0 : 5.0 , 100.0 : 10.0 , 1000.0 : 1000 , } ladder = [] for index , key in enumerate ( tickIncrements ): increment = tickIncrements [ key ] if ( index + 1 ) == len ( tickIncrements ): ladder . append ( key ) else : key1 = [ * tickIncrements ][ index ] key2 = [ * tickIncrements ][ index + 1 ] steps = ( key2 - key1 ) / increment for i in range ( int ( steps )): ladder . append ( round ( key + i * increment , 2 )) return ( ladder ) bfticks = bfTickLadder () # Round a decimal to the betfair tick value below def bfTickFloor ( price , includeIndex = False ): if 'bfticks' in globals (): global bfticks else : bfticks = bfTickLadder () ind = [ n for n , i in enumerate ( bfticks ) if i >= price ][ 0 ] if includeIndex : if bfticks [ ind ] == price : return (( ind , price )) else : return (( ind - 1 , bfticks [ ind - 1 ])) else : if bfticks [ ind ] == price : return ( price ) else : return ( bfticks [ ind - 1 ]) # Calculate the numder of ticks between two tick values def bfTickDelta ( p1 , p2 ): if np . isnan ( p1 ) or np . isnan ( p2 ): return ( np . nan ) x = bfTickFloor ( p1 , includeIndex = True ) y = bfTickFloor ( p2 , includeIndex = True ) return ( x [ 0 ] - y [ 0 ]) # %% bfTickDelta ( 13.5 , 11 ) # %% bfTickDelta ( 3.5 , 3.4 ) # %% [markdown] # Now that we have our functions let's plot the average market spread leading up to the jump. # %% # Group by odds range averageMarketSpread = ( df [[ 'market_id' , 'selection_id' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( market_spread = lambda x : x . apply ( lambda x : bfTickDelta ( x . lay_best , x . back_best ), axis = 1 )) . groupby ([ \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'market_spread' : 'mean' }) ) # %% fig_5 = px . line ( averageMarketSpread , x = 'seconds_before_scheduled_jump_chunk' , y = 'market_spread' , template = 'plotly_white' , title = \"Market Spread Leading Up To Jump\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"market_spread\" : \"Average Spread (b/w best back and lay)\" } ) fig_5 . update_layout ( font_family = \"Roboto\" ) fig_5 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_5 . show ( \"png\" ) # %% [markdown] # So much the same story as before, the spread tightens pretty dramatically leading all the way up to the jump. Now that we have a measure for market tightness on a selection level we can split it by odds range to see how tightness varies across odds range. # %% # Market Spread By Odds Group # _______________________ # Group by odds range averageMarketSpreadOddsgrp = ( df [[ 'market_id' , 'selection_id' , 'bsp' , 'seconds_before_scheduled_jump' , 'back_best' , 'lay_best' ]] . assign ( seconds_before_scheduled_jump_chunk = lambda x : x [ 'seconds_before_scheduled_jump' ] . apply ( chunkSBJ )) . assign ( bsp = lambda x : x [ 'bsp' ] . apply ( chunkBsp )) . assign ( market_spread = lambda x : x . apply ( lambda x : bfTickDelta ( x . lay_best , x . back_best ), axis = 1 )) . groupby ([ \"bsp\" , \"seconds_before_scheduled_jump_chunk\" ], as_index = False ) . agg ({ 'market_spread' : 'mean' }) ) fig_6 = px . line ( averageMarketSpreadOddsgrp , x = 'seconds_before_scheduled_jump_chunk' , y = 'market_spread' , color = \"bsp\" , template = 'plotly_white' , title = \"Market Spread Leading Up To Jump\" , labels = { 'seconds_before_scheduled_jump_chunk' : \"Seconds Before Scheduled Jump\" , \"market_spread\" : \"Average Spread (b/w best back and lay)\" } ) fig_6 . update_layout ( font_family = \"Roboto\" ) fig_6 [ 'layout' ][ 'xaxis' ][ 'autorange' ] = \"reversed\" fig_6 . show ( \"png\" ) # %% [markdown] # Now this gives us visibility of how the market tightens across groups of selections. Clearly the bottom end of the market takes a bit longer to tighten up which fits our understanding that there's generally less money, and less very sharp money on these selections. # %% [markdown] # ## 2.3 Market Moves # # Circling back to the motivation of this article: # # > How should you think about large price moves in betfair markets and how could you look to build secondary betting strategies by quantitatively analysing historical data # # Now that we have a good grasp on the data we've collected on how betfair markets form let's try to analyse big price moves. Here's a sample of our largest price moves in our race sample: # # # %% # Big Movers # ____________________ ( df . groupby ([ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'bsp' ], as_index = False ) . agg ({ 'ltp' : 'max' }) . rename ( columns = { 'ltp' : 'max_traded' }) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . max_traded , x . bsp ), axis = 1 )) . query ( 'max_traded < 500' ) . sort_values ( 'ticks_in' , ascending = False ) . head ( 10 ) ) # %% [markdown] # Some of those moves are astounding: San Fabrizio traded at 10.50 in the last 30 mins before the jump and had a final BSP of 2.80, but lost. # # Like we discussed before observing a market plunge in hindsight may seem like a more powerful predictor than it actually is. For example, if we had have flat staked these 10 giant moves after they happened (say at the BSP) we'd be running at ~70% POT even though our intuition would have told you at the time that these horses couldn't lose! # %% [markdown] # ## 2.3.1 Reacting to moves after they happen # # Bookies, race callers and other services will often tell you what selection has been strong in the market; who the \"money\" has come for. But once you hear this is it already too late to do anything about? # # Let's extend the sample in the previous section to see if we can draw any broader conclusions. # # - I'll take a sample of the all selections 10 minutes before the scheduled jump # - I'll also take those same selections at the scheduled jump exactly # - I'll then calculate the number of ticks between the best back price at each time point to measure the movement in the last 10 minutes of trading # - I'll then calculate a back and lay profit using the top box at the scheduled off # %% m10 = ( df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . apply ( lambda g : None if g [ g [ 'seconds_before_scheduled_jump' ] < 600 ] . shape [ 0 ] == 0 else g [ g [ 'seconds_before_scheduled_jump' ] < 600 ] . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) . iloc [ 0 ]) . filter ( items = [ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'back_best' , 'lay_best' ]) . rename ( columns = { 'back_best' : 'back_best_10m' , 'lay_best' : 'lay_best_10m' }) ) m0 = ( df . groupby ([ 'market_id' , 'selection_id' ], as_index = False ) . apply ( lambda g : None if g [ g [ 'seconds_before_scheduled_jump' ] < 0 ] . shape [ 0 ] == 0 else g [ g [ 'seconds_before_scheduled_jump' ] < 0 ] . sort_values ( 'seconds_before_scheduled_jump' , ascending = False ) . iloc [ 0 ]) . filter ( items = [ 'market_id' , 'selection_id' , 'venue' , 'selection_name' , 'win' , 'back_best' , 'lay_best' ]) ) # %% # Back or lay according to large plunges pd . DataFrame ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) # %% # Back or lay according to large plunges - grouped by track ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . groupby ( 'venue' , as_index = False ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) # %% [markdown] # As you can see there's not much of a pattern here to take advantage of here. At least for this sample of tracks, and for this time slice, the value has been sucked dry from the market on these selections. # To illustrate this let's check what the backers would have profited if they were the ones who could identify these selections prior to their big market moves # %% # Backing before the plunge? ( m10 . merge ( m0 ) . assign ( ticks_in = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_10m , x . back_best ), axis = 1 )) . query ( 'ticks_in >= 15' ) . assign ( back_pl = lambda x : np . where ( x [ 'win' ] == 1 , 0.95 * ( x [ 'back_best_10m' ] - 1 ), - 1 )) . assign ( lay_pl = lambda x : np . where ( x [ 'win' ] == 1 , - 1 * ( x [ 'lay_best_10m' ] - 1 ), 0.95 )) . assign ( stake = 1 ) . agg ({ 'stake' : 'sum' , 'back_pl' : 'sum' , 'lay_pl' : 'sum' }) ) # %% [markdown] # I could sign up for winning at 50% POT! Now I just have to pick 100% of the large thoroughbred plunges before they happen... Obviously no one individual person or group is predicting these movements 100%. Even the sharpest players will have movements go against them but this highlights the essence of the problem and the contradiction of retrospective price movement analysis. # # These movements are important. There is a lot to be gained by purely trading betfair markets and how they move. However, identifting large moves **after** they occur isn't good enough. The large players sharpen the odds down to a good and fair implied chance and leave those watching on the sidelines with little to no value either side. To profit then we must capture some of the value as they are and jump on momentum as it's happening not after it's come to a halt. # %% [markdown] # ## 2.3.2 Visualising the moves # # In the final section I'll frame how I think you could go about identifying these moves as they're happening or about to happen. But first to illustrate some of dynamics let's visualise some of these large moves. How do selections firm and do they do it with different characteristics in different patterns? Candlestick charts are a good way to visualise the evolution of prices in markets and are often used in financial markets to do technical analysis. # # I'll first create a function to create a candlestick chart for a market / selection slice of our dataframe using plotly charts. # # %% # Candlestick Plotly Chart With Plotly # _____________________________ def priceCandlestickPlotly ( d ): d [ 'time_chunk' ] = d [ 'time' ] . dt . round ( '2min' ) selectionName = d . selection_name . iloc [ 0 ] track = d . venue . iloc [ 0 ] startTime = d . market_time . iloc [ 0 ] candleStickInput = d . groupby ( 'time_chunk' , as_index = False ) . agg ({ \"ltp\" : [ 'first' , 'last' , 'min' , 'max' ]}) candleStickInput . columns = [ \"_\" . join ( pair ) for pair in candleStickInput . columns ] fig = go . Figure ( data = [ go . Candlestick ( x = candleStickInput [ 'time_chunk_' ], open = candleStickInput [ 'ltp_first' ], high = candleStickInput [ 'ltp_max' ], low = candleStickInput [ 'ltp_min' ], close = candleStickInput [ 'ltp_last' ])]) fig . update_layout ( template = \"plotly_white\" , xaxis_rangeslider_visible = False , title = f ' { selectionName } at { track } race started at { startTime } UTC' ) fig . show ( \"png\" ) # %% [markdown] # Let's visualise the 2 biggest moves with this chart function. # %% priceCandlestickPlotly ( df . query ( 'market_id == \"1.182760338\" and selection_id == \"39317622\"' )) # %% [markdown] # This looks exactly like what we'd expect from a extreme plunge: consistent, sustained and unrelenting support on the back side as the horse is pushed from 8-10 odds down to it's fair odds of around 3. # # Let's look at the second big plunge: # %% priceCandlestickPlotly ( df . query ( 'market_id == \"1.173069469\" and selection_id == \"35981261\"' )) # %% [markdown] # This movement chart is a bit different. The market has 4 distinct segments: # # - About 30 minutes from the off the market changes and the horse is supported from 4 into 3 # - It trades at around 3 for the next 25 minutes # - It then drifts back out sharply to a bit over 4 # - It then gets crunched back in all the way into a BSP of 2.30 # # Clearly different segments of the participants had very different perceptions of this horses chances. How do we make sense of it? # # Let's try a different candlestick visualistion, which includes the traded volume. # %% import mplfinance as fplt def priceCandlestickMpl ( d ): d [ 'time_chunk' ] = d [ 'time' ] . dt . round ( '2min' ) selectionName = d . selection_name . iloc [ 0 ] track = d . venue . iloc [ 0 ] startTime = d . market_time . iloc [ 0 ] # Add traded volume in last interval d = d . groupby ([ 'market_id' , 'selection_id' ]) . apply ( lambda x : x . assign ( traded_volume_delta = lambda y : ( y [ 'traded_volume' ] - y [ 'traded_volume' ] . shift ( 1 )) . mask ( pd . isnull , 0 ))) candleStickInput = d . groupby ( 'time_chunk' , as_index = False ) . agg ({ \"ltp\" : [ 'first' , 'last' , 'min' , 'max' ], \"traded_volume_delta\" : 'sum' }) candleStickInput . columns = [ \"_\" . join ( pair ) for pair in candleStickInput . columns ] candleStickInput = candleStickInput . rename ( columns = { 'time_chunk_' : 'date' , 'ltp_first' : 'open' , 'ltp_last' : 'close' , 'ltp_min' : 'low' , 'ltp_max' : 'high' , 'traded_volume_delta_sum' : 'volume' }) candleStickInput = candleStickInput . set_index ( 'date' ) fplt . plot ( candleStickInput , type = 'candle' , style = 'yahoo' , title = f ' { selectionName } at { track } race started at { startTime } UTC' , ylabel = 'Odds' , volume = True , ylabel_lower = 'Volume Traded' , ) # %% [markdown] # First the first horse: # %% priceCandlestickMpl ( df . query ( 'market_id == \"1.182760338\" and selection_id == \"39317622\"' )) # %% [markdown] # Now the second selection: # %% priceCandlestickMpl ( df . query ( 'market_id == \"1.173069469\" and selection_id == \"35981261\"' )) # %% [markdown] # So as the majority of the volume was traded right before the jump, the price was hammered into 2.30. This is a pattern that you might see a lot in betfair markets: prices may oscillate around or even drift consistently in one direction until a point at which certain large participants or groups of participants enter the market and push the selection in another direction completely. # # So anticipating a move won't be as simple as analysing the price as a time series, it's one piece of the puzzle. Really sophisticated market trading algorithms will need to historically analyse a raft of market metrics and correlate them with these movements historically within the market type of interest. In the next section I'll start off down that road to give you an idea of how I'd tackle the problem but it's more complicated than we can uncover in a single article. # %% [markdown] # ## 2.3.3 Anticipating a move # # Our retrospective analysis was good to help us understand the dynamics of movements, the efficiency of markets, how they form, and what things we could put in a predictive analysis. The next step is to create forward looking estimates about where the market is headed next. # # You get setup your problem in lots of different ways including creating rules based strategies like discussed in a [previous piece](https://betfair-datascientists.github.io/historicData/automatedBettingAnglesTutorial/). Or you could go down a formalised machine learning approach. The first step to both is to build up some key features and test their predictive power. # # The factors I'll consider in this section will be: # # - The movement over the last 30 seconds # - The weight of money on the back side # - The weight of money on the lay side # - The current best back / volume weighted average top 3 back (top box support on back side) # - The current best lay / volume weighted average top 3 lay (top box support on lay side) # - The current best back / The volume weighted traded price over the last increment # - The current best lay / The volume weighted traded price over the last increment # # And im interested in correlating these variables with the number + direction of ticks moved over the **next 30 seconds** to see if I can find anything interesting. # # %% # Start Analysis # ______________________________ dfPredict = df . query ( 'seconds_before_scheduled_jump <= 600 and seconds_before_scheduled_jump >= 0' ) # Target # ____________________________ dfPredict [ 'best_back_30s_in_future' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( 30 ) dfPredict [ 'best_lay_30s_in_future' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( 30 ) dfPredict [ 'back_target' ] = dfPredict . apply ( lambda x : bfTickDelta ( x . back_best , x . best_back_30s_in_future ), axis = 1 ) dfPredict [ 'lay_target' ] = dfPredict . apply ( lambda x : bfTickDelta ( x . lay_best , x . best_lay_30s_in_future ), axis = 1 ) # Movement # ____________________________ dfPredict [ 'back_best_30s_ago' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'back_best' ] . shift ( - 30 ) dfPredict [ 'back_lay_30s_ago' ] = dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'lay_best' ] . shift ( - 30 ) dfPredict = ( dfPredict . assign ( back_ticks_in_30s = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_best_30s_ago , x . back_best ), axis = 1 )) . assign ( lay_ticks_in_30s = lambda x : x . apply ( lambda x : bfTickDelta ( x . back_lay_30s_ago , x . lay_best ), axis = 1 )) ) # Weight Of Money # ____________________________ atb_ladder = dfPredict . atb_ladder . iloc [ 0 ] atl_ladder = dfPredict . atl_ladder . iloc [ 0 ] def wom ( back_ladder , lay_ladder ): if not back_ladder or not lay_ladder : return (( None , None )) total_volume = round ( sum ( back_ladder [ 'v' ]) + sum ( lay_ladder [ 'v' ]), 2 ) return (( round ( sum ( back_ladder [ 'v' ]) / total_volume , 3 ), round ( sum ( lay_ladder [ 'v' ]) / total_volume , 3 ))) dfPredict [ 'wom' ] = dfPredict . apply ( lambda x : wom ( x [ 'atb_ladder' ], x [ 'atl_ladder' ]), axis = 1 ) dfPredict [[ 'back_wom' , 'lay_wom' ]] = pd . DataFrame ( dfPredict [ 'wom' ] . tolist (), index = dfPredict . index ) # Top Box Support # ________________________ dfPredict [ 'back_best_support' ] = dfPredict [ 'back_vwap' ] / dfPredict [ 'back_best' ] dfPredict [ 'lay_best_support' ] = dfPredict [ 'lay_best' ] / dfPredict [ 'lay_vwap' ] # Recent Movement # _____________________ dfPredict [ 'wap_movement_10s' ] = dfPredict [ 'wap' ] / dfPredict . groupby ([ 'market_id' , 'selection_id' ])[ 'wap' ] . shift ( - 10 ) # %% [markdown] # Now that we've created the candidate factors let's use a spearman correlation coefficient to analyse their correlation (positive or negative we're mostly focussed on the absolute size of the coefficient). # %% corrMatrix = dfPredict [[ 'back_target' , 'back_ticks_in_30s' , 'lay_ticks_in_30s' , 'back_wom' , 'lay_wom' , 'back_best_support' , 'lay_best_support' , 'wap_movement_10s' ]] . dropna () . corr ( method = \"spearman\" ) corrMatrix [[ 'back_target' ]] # %% import seaborn as sns corPlot = sns . heatmap ( corrMatrix , vmin =- 1 , vmax = 1 , center = 0 , cmap = sns . diverging_palette ( 20 , 220 , n = 200 ), square = True ) corPlot . set_xticklabels ( corPlot . get_xticklabels (), rotation = 45 , horizontalalignment = 'right' ) # %% [markdown] # We're entirely interested in the `back_target` column in the matrix and the heatmap. We can see that some of our candidate features have decent correlation with the number of ticks the price will move over the course of the next 30 seconds. # # # ## 2.3.4 Next Steps # # This marks the end of this analysis however if you're interested in turning this kind of analysis / approach into something more I'd suggest there's 2 main paths you could go down from here. # # 1. Angles + Rules # # You could dig into the data a little more a find specific situations where some of these features (or ones like them) preceeded certain things happening in the market (movements or increases in trading volatility etc) You could then construct trading rules based on these findings an try to automate them or bake them into trading rules inside a 3rd party tool. # # 2. Predictive Modelling # # If you were comfortable with statistical models or machine learning you could easily feed this data into a predictive modelling workflow. Once honed, the predictions from this workflow could be turned into recommended betting decisions which could form a part of an automated algorithmic betting framework. The work to fill out a project like this would be significant but so would the reward. # # # # 3.0 Conclusion # # Like previous articles this analysis is a sketch of what can be accomplished analysing the historical stream files. In this instance we focussed on the markets themselves, with a particular focussing on a small slice of markets: victorian thoroughbred markets. We analysed how these markets form, how to interpret the price movements on certain selections and the first steps to building out automated strategies based on these features alone. # # Building automated betting strategies based on the markets alone is path that plenty of quantitatively inclined betfair customers go down as it minimises complexity in source data (there's only 1!) and there's plenty of value in it left untapped for you to try to capture. # #","title":"Complete code"},{"location":"historicData/analysingAndPredictingMarketMovements/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"historicData/automatedBettingAnglesTutorial/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Automated betting angles in Python | Betting strategies based on your existing insights: no modelling required Workshop This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the JSON to CSV tutorial and backteseting ratings in Python tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at those tutorials before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet This is presented as a Jupyter notebook as this format is interactive and lets you run snippets of code from wihtin the notebook. To use this functionality you'll need to download a copy of the ipynb file locally and open it in a text editor (i.e. VS code). If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo . 0.1 Setup Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language. Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer where you want to locally store the intermediate data files. You'll also need betfairlightweight which you can install with something like pip install betfairlightweight . import requests import pandas as pd from datetime import date , timedelta import numpy as np import os import re import tarfile import zipfile import bz2 import glob import logging import yaml from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) from scipy.stats import t import plotly.express as px 0.2 Context Formulating betting angles (or \"strategies\" as some call them) is quite a common pasttime for some. These angles can range all the way from very simple to quite sophisticated, and could include things like: Laying NBA teams playing on the second night of a back-to-back Laying AFL team coming off a bye when matched against a team who played last week Backing a greyhound in boxes 1 or 2 in short sprint style races Backing a horse pre-race who typically runs at the front of the field and placing an order to lay the same horse if it shortens to some lower price in-play, locking in a profit Beyond the complexity of the actual concept what really seperates these angles is evidence. You might have heard TV personalities and betting ads suggest a certain strategy (resembling one of the above) are real-world predictive trends but they rarely are. They are rarely derived from the right historical data or are concluded without the necessary statistical rigour. Most simply formulated their angles off intuition or observing a trend across a small sample of data. There are many users on betting exchanges who profit off these angles. In fact, when most people talk about automated or sophisticated exchange betting they are often talking about automating these kind of betting angles, as opposed to betting ratings produced from a sophisticated bottom-up fundemental modelling. That's because profitable fundemental modelling (where your model which arrives at some estimation of fair value from first principles) is very hard. The reason this approach is so much easier is that you assume the market odds are right except x and go from there, applying small top-down adjustments for factors that haven't historically been incorporated in the market opinion. The challenge lies in finding those factors and making sure you aren't tricking yourself in thinking you've found one that you can profit off in the future. Once again this is another example of the uses of the Betfair historical stream data. To get cracking - as always - we need historical odds and the best place to get that is to self serve the historical stream files. 0.3 Examples I'll go through an end-to-end example of 3 different betting angles on Australian thoroughbred racing. Which will include: Which will include: Sourcing data Assembling data Formulating hypotheses Testing Hypotheses Discussion about implementation 1.0 Data 1.1 Betfair Odds Data We'll follow a very similar template as other tutorials extracting key information from the betfair stream data. It's important to note that given the volume of data you need to handle with these stream files, your workflow will probably involve choosing some methods of aggregation / summary that you'll reconsider after your first cut of analysis. Parsing and saving a dataset, using it to test some hypotheses which likely results in more questions that need to be examined by reparsing the stream files in a slightly different way. Your workflow will likely follow something like this diagram. For the purposes of this article I'm interested in backtesting some betting angles at the BSP, using some indication of price momentum/market support in some angles, and testing some back to lay strategies so we'll need to pull out some information about each runners in-play trading. So we'll extract the following for each runner: - BSP - Last Traded Price - Volume Weighted Avg Price (top 3 boxes) 5 mins before the scheduled jump time - Volume Weighted Avg Price (top 3 boxes) 30 seconds before the scheduled jump time - The volume traded on the selection - The minimum \"best available to lay\" price offered inplay (which is a measure of how low the selection traded during the race) First we'll establish some utility functions needed to parse the data. Most of these were discussed in the previous backtest your ratings tutorial. # Utility Functions For Stream Parsing # _________________________________ def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def slicePrice ( l , n ): try : x = l [ n ] . price except : x = np . nan return ( x ) def sliceSize ( l , n ): try : x = l [ n ] . size except : x = np . nan return ( x ) def wapPrice ( l , n ): try : x = round ( sum ( [ rung . price * rung . size for rung in l [ 0 :( n - 1 )] ] ) / sum ( [ rung . size for rung in l [ 0 :( n - 1 )] ]), 2 ) except : x = np . nan return ( x ) def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) Then we'll create our core execution functions that will scan over the historical stream files and use betfairlightweight to recreate the state of the exchange for each thoroughbred market and extract key information for each selection # Core Execution Fucntions # _________________________________ def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): evaluate_market = None prev_market = None postplay = None preplay = None t5m = None t30s = None inplay_min_lay = None gen = s . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 30 seconds before scheduled off if t30s is None and seconds_to_start < 30 : t30s = market_book # Market at 5 mins before scheduled off if t5m is None and seconds_to_start < 5 * 60 : t5m = market_book # Manage Inplay Vectors if market_book . inplay : if inplay_min_lay is None : inplay_min_lay = [ slicePrice ( runner . ex . available_to_lay , 0 ) for runner in market_book . runners ] else : inplay_min_lay = np . fmin ( inplay_min_lay , [ slicePrice ( runner . ex . available_to_lay , 0 ) for runner in market_book . runners ]) # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay is not None and preplay is None : preplay = postplay inplay_min_lay = [ \"\" for runner in market_book . runners ] return ( t5m , t30s , preplay , postplay , inplay_min_lay , prev_market ) # Final market is last prev_market def parse_stream ( stream_files , output_file ): with open ( output_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,selection_name,wap_5m,wap_30s,bsp,ltp,traded_vol,inplay_min_lay \\n \" ) for file_obj in load_markets ( stream_files ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) ( t5m , t30s , preplay , postplay , inplayMin , final ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay is None or final is None or t30s is None : continue ; # All runner removed if all ( runner . status == \"REMOVED\" for runner in final . runners ): continue runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final . runners ] ltp = [ runner . last_price_traded for runner in preplay . runners ] tradedVol = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay . runners ] wapBack30s = [ wapPrice ( runner . ex . available_to_back , 3 ) for runner in t30s . runners ] wapBack5m = [ wapPrice ( runner . ex . available_to_back , 3 ) for runner in t5m . runners ] # Writing To CSV # ______________________ for ( runnerMeta , ltp , tradedVol , inplayMin , wapBack5m , wapBack30s ) in zip ( runnerMeta , ltp , tradedVol , inplayMin , wapBack5m , wapBack30s ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final . market_id ), runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], wapBack5m , wapBack30s , runnerMeta [ 'sp' ], ltp , round ( tradedVol ), inplayMin ) ) Finally, after sourcing and downloading 12 months of stream files (ask automation@betfair.com.au for more info if you don't know how to do this) we'll use the above code to parse each file and write to a single csv file to be used for analysis. # Description: # Will loop through a set of stream data archive files and extract a few key pricing measures for each selection # Estimated Time: # ~6 hours # Parameters # _________________________________ # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) # stream_files = glob.glob(\"[PATH TO LOCAL FOLDER STORING ARCHIVE FILES]*.tar\") # output_file = \"[SOME OUTPUT DIRECTORY]/thoroughbred-odds-2021.csv\" # Run # _________________________________ # if __name__ == '__main__': # parse_stream(stream_files, output_file) 1.2 Race Data If you're building a fundamental bottom-up model, finding and managing ETL from an appropriate data source is a large part of the exercise. If your needs are simpler (for these types of automated strategies for example) there's plenty of good information that's available right inside the betfair API itself. The RUNNER_METADATA slot inside the listMarketCatalogue response for example will return a pretty good slice of metadata about the horses racing in upcoming races including but not limited to: the trainer, the jockey, the horses age, and a class rating. The documentaion for this endpoint will give you the full extent of this what's inside this response. Our problem for this exercise is that the historical stream files don't include this RUNNER_METADATA so we weren't able to extract it in the previous step. However, a sneaky workaround is to use an unsuppoerted back-end endpoint, one which Betfair use for the Hub racing results page . These API endpoints are: Market result data: https://apigateway.betfair.com.au/hub/raceevent/1.154620281 Day\u2019s markets: https://apigateway.betfair.com.au/hub/racecard?date=2018-12-18 Extract Betfair Racing Markets for a Given Date First we'll hit the https://apigateway.betfair.com.au/hub/racecard endpoint to get the racing markets available on Betfair for a given day in the past: def getBfMarkets ( dte ): url = 'https://apigateway.betfair.com.au/hub/racecard?date= {} ' . format ( dte ) responseJson = requests . get ( url ) . json () marketList = [] for meeting in responseJson [ 'MEETINGS' ]: for markets in meeting [ 'MARKETS' ]: marketList . append ( { 'date' : dte , 'track' : meeting [ 'VENUE_NAME' ], 'country' : meeting [ 'COUNTRY' ], 'race_type' : meeting [ 'RACE_TYPE' ], 'race_number' : markets [ 'RACE_NO' ], 'market_id' : str ( '1.' + markets [ 'MARKET_ID' ]), 'start_time' : markets [ 'START_TIME' ] } ) marketDf = pd . DataFrame ( marketList ) return ( marketDf ) Extract Key Race Metadata Then (for one of these market_id s) we'll hit the https://apigateway.betfair.com.au/hub/raceevent/ enpdoint to get some key runner metadata for the runners in this race. It's important to note that this information is available through the Betfair API so we won't need to go to a secondary datasource to find it at the point of implementation, this would add a large layer of complexity to the project including things like string cleaning and matching. def getBfRaceMeta ( market_id ): url = 'https://apigateway.betfair.com.au/hub/raceevent/ {} ' . format ( market_id ) responseJson = requests . get ( url ) . json () if 'error' in responseJson : return ( pd . DataFrame ()) raceList = [] for runner in responseJson [ 'runners' ]: if 'isScratched' in runner and runner [ 'isScratched' ]: continue # Jockey not always populated try : jockey = runner [ 'jockeyName' ] except : jockey = \"\" # Place not always populated try : placeResult = runner [ 'placedResult' ] except : placeResult = \"\" # Place not always populated try : trainer = runner [ 'trainerName' ] except : trainer = \"\" raceList . append ( { 'market_id' : market_id , 'weather' : responseJson [ 'weather' ], 'track_condition' : responseJson [ 'trackCondition' ], 'race_distance' : responseJson [ 'raceLength' ], 'selection_id' : runner [ 'selectionId' ], 'selection_name' : runner [ 'runnerName' ], 'barrier' : runner [ 'barrierNo' ], 'place' : placeResult , 'trainer' : trainer , 'jockey' : jockey , 'weight' : runner [ 'weight' ] } ) raceDf = pd . DataFrame ( raceList ) return ( raceDf ) Wrapper Function Stiching these two functions together we can create a wrapper function that hits both endpoints for all the thoroughbred races in a given day and extract all the runner metadata and results. def scrapeThoroughbredBfDate ( dte ): markets = getBfMarkets ( dte ) if markets . shape [ 0 ] == 0 : return ( pd . DataFrame ()) thoMarkets = markets . query ( 'country == \"AUS\" and race_type == \"R\"' ) if thoMarkets . shape [ 0 ] == 0 : return ( pd . DataFrame ()) raceMetaList = [] for market in thoMarkets . market_id : raceMetaList . append ( getBfRaceMeta ( market )) raceMeta = pd . concat ( raceMetaList ) return ( markets . merge ( raceMeta , on = 'market_id' )) # Executing the wrapper for an example date scrapeThoroughbredBfDate ( date ( 2021 , 2 , 10 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date track country race_type race_number market_id start_time weather track_condition race_distance selection_id selection_name barrier place trainer jockey weight 0 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 38448397 Triple Missile 3 1 Todd Harvey Paul Harvey 60.0 1 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 28763768 Shock Result 5 4 P H Jordan Craig Staples 59.5 2 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 8772321 Secret Plan 6 3 G & A Williams William Pike 59.0 3 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 9021011 Command Force 2 0 Daniel & Ben Pearce J Azzopardi 58.0 4 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 38448398 Fish Hook 7 2 M P Allan Madi Derrick 57.5 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 458 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 133456 Sedition 12 2 Richard Litt Ms Rachel King 58.0 459 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447782 Amusez Moi 9 6 Richard Litt Josh Parr 57.0 460 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 25388274 Savoury 1 5 Bjorn Baker Jason Collett 57.0 461 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447783 Born A Warrior 7 3 Michael & Wayne & John Hawkes Tommy Berry 56.5 462 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447784 Newsreader 10 1 John O'shea James Mcdonald 55.5 463 rows \u00d7 17 columns Then to produce a historical slice of all races between two dates we could just loop over a set of dates and append each results set # Description: # Will loop through a set of dates (starting July 2020 in this instance) and return race metadata from betfair # Estimated Time: # ~60 mins # # dataList = [] # dateList = pd.date_range(date(2020,7,1),date.today()-timedelta(days=1),freq='d') # for dte in dateList: # dte = dte.date() # print(dte) # races = scrapeThoroughbredBfDate(dte) # dataList.append(races) # data = pd.concat(dataList) # data.to_csv(\"[LOCAL PATH SOMEWHERE]\", index=False) 2.0 Analysis I'll be running through 3 simple betting angles, one easy, one medium, and one hard to illustrate different types of angles you might want to try at home. The process I lay out is very similar (if not identical) but the implementation might be a bit trickier in each case and might take a little more programming skill to get up and running. We'll use a simple evaluation function (POT and strike rate) to evaluate each of these strategies. def bet_eval_metrics ( d , side = False ): metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" , \"win\" : \"mean\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) 2.1 Assemble Data Now that we have our 2 core datasets (odds + race / runner metadata) we can join them together and do some analysis # Local Paths (will be different on your machine) path_odds_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-odds-2021.csv\" path_race_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-race-data.csv\" odds = pd . read_csv ( path_odds_local , dtype = { 'market_id' : object , 'selection_id' : object }) race = pd . read_csv ( path_race_local , dtype = { 'market_id' : object , 'selection_id' : object }) odds . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id selection_name wap_5m wap_30s bsp ltp traded_vol inplay_min_lay 0 1.179845158 23493550 1. Larmour 6.27 5.84 6.20 6.2 8277 1.19 1 1.179845158 16374800 3. Careering Away 3.31 3.67 3.60 3.65 18592 1.08 2 1.179845158 19740699 4. Bells N Bows 6.87 6.36 6.62 6.4 7413 1.42 race . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date track country race_type race_number market_id start_time weather track_condition race_distance selection_id selection_name barrier place trainer jockey weight 0 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 19674744 Baldy 2 4.0 Peter Nolan Karl Zechner 59.0 1 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 401615 Nostrovia 4 7.0 Dennis O'leary Margaret Collett 59.0 2 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 26789410 Ammo Loco 5 1.0 John Hickmott Barend Vorster 58.5 # Joining two datasets df = race . merge ( odds . loc [:, odds . columns != 'selection_name' ], how = \"inner\" , on = [ 'market_id' , 'selection_id' ]) # I'll also add columns for the net profit from backing and laying each selection to be picked up in subsequent sections df [ 'back_npl' ] = np . where ( df [ 'place' ] == 1 , 0.95 * ( df [ 'bsp' ] - 1 ), - 1 ) df [ 'lay_npl' ] = np . where ( df [ 'place' ] == 1 , - 1 * ( df [ 'bsp' ] - 1 ), 0.95 ) 2.2 Methodology Looping back around to the context discussion in part 0.2 we need to decide on how to set up our analysis that will help us: find angles, formulate strategies, and test them with enough rigour that will give us a good estimate of our forward looking profitability on any that we choose to implement and automate. The 3 key tricks I'll lay out in this piece are: - Using a statistical estimate to quantify the robustness of historical profitibalility - Using out-of-sample validation (much like a you would in a model building exercise) to get an accurate view of forward looking profitability - Using domain knowledge to chunk selections to get broader sample for more stable estimate of profitability 2.3.1 Chunking This is a technique you can use to group together variables in conceptually similar groups. For example, thoroughbred races are run over many different exact distances (800m, 810m, 850m, 860m etc) which - using a domain overlay - are all very short sprint style races for a horse race. Similarly, barriers 1, 2 and 3 being on the very inside of the race field and closest to the rail all present similar early race challenges and advantages for horses jumping from those barriers. So formulating your betting angles you may want to overlay semantically similar variable groups to test your betting hypothesis. I'll add variable chunks for race distance and barrier for now but you may want to test more (for example horse experience, trainer stable size etc) def distance_group ( distance ): if distance is None : return ( \"missing\" ) elif distance < 1100 : return ( \"sprint\" ) elif distance < 1400 : return ( \"mid_short\" ) elif distance < 1800 : return ( \"mid_long\" ) else : return ( \"long\" ) def barrier_group ( barrier ): if barrier is None : return ( \"missing\" ) elif barrier < 4 : return ( \"inside\" ) elif barrier < 9 : return ( \"mid_field\" ) else : return ( \"outside\" ) df [ 'distance_group' ] = pd . to_numeric ( df . race_distance , errors = \"coerce\" ) . apply ( distance_group ) df [ 'barrier_group' ] = pd . to_numeric ( df . barrier , errors = \"coerce\" ) . apply ( barrier_group ) 2.3.2 In Sample vs Out of Sample The first thing I'm going to do is to split off a largish chunk of my data before even looking at it. I'll ultimately use it to paper trade some of my candidate angles but I want it to be as seperate from the idea generation process as possible. I'll use the model building nomenclature \"train\" and \"test\" even though I'm not really doing any \"training\". My data contains all AUS thoroughbred races from July 2020 until end of June 2021 so I'll cut off the period Apr-June 2021 as my \"test\" set. dfTrain = df . query ( 'date < \"2021-04-01\"' ) dfTest = df . query ( 'date >= \"2021-04-01\"' ) ' {} rows in the \"training\" set and {} rows in the \"test\" data' . format ( dfTrain . shape [ 0 ], dfTest . shape [ 0 ]) '119244 rows in the \"training\" set and 40783 rows in the \"test\" data' 2.3.3 Statistically Measuring Profit Betting outcomes, and the randomness associated with them, at their core are the types of things the discipline of statistics was created to solve. Concepts like sample size, expected value, and variance are terms you might hear from sophisticated (and some novice) bettors and they are all drawn from the field of statistics. Though you don't need to become a PHD of statistics every little extra technique or concept you can glean from the field will help your betting if you want it to. To illustrate with an example, let's group by net backing profit on turnover for a horse to see which horses have the highest historical back POT: ( dfTrain . assign ( stake = 1 ) . groupby ( 'selection_name' , as_index = False ) . agg ({ 'back_npl' : 'sum' , 'stake' : 'sum' }) . assign ( pot = lambda x : x [ 'back_npl' ] / x [ 'stake' ]) . sort_values ( 'pot' , ascending = False ) . head ( 3 ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } selection_name back_npl stake pot 12247 Little Vulpine 274.550 1 274.550000 15384 Not Tonight Dear 130.701 1 130.701000 9987 Im Cheeky 617.307 7 88.186714 So back Little Vulpine whenever it races? We all know intuitively what's wrong with that betting angle - it's raced one time in our sample and happened to win at a bsp of 270. Sample size and variance are dominating this simple measure of historical POT. Instead what we can do is treat the historical betting outcomes as a random variable and apply some statistical tests of signifance to them. A more detailed discussion of this particular test can be found here as can an excel calculator you can input your stats into. I'll simply translate the test to python to enable it's use when formulating our betting angles. The TLDR version of this test is that; based on your bet sample size, your profit, and the average odds across that sample of bets, the calculation produces a p value which estimates the probability your profit (or loss) happened by pure chance (where chance would be an expectation of breakeven betting simply at fair odds). def pl_pValue ( number_bets , npl , stake , average_odds ): pot = npl / stake tStatistic = ( pot * np . sqrt ( number_bets )) / np . sqrt ( ( 1 + pot ) * ( average_odds - 1 - pot ) ) pValue = 2 * t . cdf ( - abs ( tStatistic ), number_bets - 1 ) return ( np . where ( np . logical_or ( np . isnan ( pValue ), pValue == 0 ), 1 , pValue )) That doesn't mean we can formulate our angles and use this metric (and this metric alone) to validate their profitability. You'll find that it will give you misleading results in some instances. As analysts we're also prone to finding infinite different ways to unintentionally overfit our analysis as you might have heard elsewhere described as the concept of p-hacking, but it does give us an extra filter to cast over our hypotheses before really validating them with out-of-sample testing. 2.4 Angle 1: Track | Distance | Barrier The first thing I'll test is whether or not there are any combinations of track/distance/barrier where backing or laying could produce robust long term profit. This probably fits within the types of betting angles people before you have already sucked all the value out of long before you started reading this article. That's not to say that you shouldn't test them though, as people have made livings on betting angles as simple as these. # Calculate the profit (back and lay) and average odds across all track / distance / barrier group combos trackDistanceBarrier = ( dfTrain . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . groupby ([ 'track' , 'race_distance' , 'barrier_group' ], as_index = False ) . agg ({ 'back_npl' : 'sum' , 'lay_npl' : 'sum' , 'stake' : 'sum' , 'odds' : 'mean' }) ) trackDistanceBarrier .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track race_distance barrier_group back_npl lay_npl stake odds 0 Albany 1000 inside 11.2550 -11.95 2 15.450000 1 Albany 1000 mid_field -5.0000 4.75 5 101.136000 2 Albany 1000 outside -5.0000 4.75 5 88.374000 3 Albany 1100 inside -3.0525 2.70 6 29.430000 4 Albany 1100 mid_field -6.4040 5.92 9 37.483333 ... ... ... ... ... ... ... ... 6325 York 1500 inside 1.8995 -2.41 6 41.195000 6326 York 1500 mid_field -7.0000 6.65 7 32.472857 6327 York 1920 inside -3.0000 2.85 3 21.883333 6328 York 1920 mid_field -0.3520 -0.04 5 20.978000 6329 York 1920 outside -2.0000 1.90 2 21.450000 6330 rows \u00d7 7 columns So it looks like over 2 selections jumping from the inside 3 barriers at Albany 1000m you would have made a healthy profit if you'd decide to back them historically. Let's use our lense of statistical significance to view these profit figures trackDistanceBarrier = ( trackDistanceBarrier . assign ( backPL_pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'back_npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) . assign ( layPL_pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'lay_npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) ) trackDistanceBarrier /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt result = getattr(ufunc, method)(*inputs, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track race_distance barrier_group back_npl lay_npl stake odds backPL_pValue layPL_pValue 0 Albany 1000 inside 11.2550 -11.95 2 15.450000 0.487280 1.000000 1 Albany 1000 mid_field -5.0000 4.75 5 101.136000 1.000000 0.885995 2 Albany 1000 outside -5.0000 4.75 5 88.374000 1.000000 0.877954 3 Albany 1100 inside -3.0525 2.70 6 29.430000 0.754412 0.869397 4 Albany 1100 mid_field -6.4040 5.92 9 37.483333 0.532857 0.804366 ... ... ... ... ... ... ... ... ... ... 6325 York 1500 inside 1.8995 -2.41 6 41.195000 0.918934 0.849635 6326 York 1500 mid_field -7.0000 6.65 7 32.472857 1.000000 0.755643 6327 York 1920 inside -3.0000 2.85 3 21.883333 1.000000 0.816546 6328 York 1920 mid_field -0.3520 -0.04 5 20.978000 0.972659 0.996987 6329 York 1920 outside -2.0000 1.90 2 21.450000 1.000000 0.863432 6330 rows \u00d7 9 columns So as you can see, whilst having a back POT of nearly 500% because the results were generated over 2 runners at quite high odds the p value (50%) suggest that it's quite likely we could have seen these exact results due to randomness, which is very intuitive. Let's have a look to see if there's any statistically significant edge to be gained on the lay side # Top 5 lay combos Track | Distance | Barrier (TDB) TDB_bestLay = trackDistanceBarrier . query ( 'lay_npl>0' ) . sort_values ( 'layPL_pValue' ) . head ( 5 ) TDB_bestLay .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track race_distance barrier_group back_npl lay_npl stake odds backPL_pValue layPL_pValue 188 Ascot 1000 inside -83.6395 77.06 115 24.616870 0.003054 0.248157 3619 Moonee Valley 1200 inside -52.2195 48.81 64 17.725313 0.000565 0.254399 6299 Yeppoon 1400 inside -11.0000 10.45 11 6.022727 1.000000 0.289686 959 Caulfield 1400 mid_field -74.3150 67.45 114 24.828772 0.018780 0.301137 1366 Darwin 1200 mid_field -47.3980 43.94 64 19.354531 0.009844 0.318178 So despite high lay POT none of these angles suggest an irrefutablely profitable angle laying these combinations. However, that doesn't mean we shouldn't test them on our out of sample set of races. These are our statistically most promising examples, we'll just take the top 5 for now and see how we would have performed if we had of started betting them on April first 2021. Keep in mind this should give us a pretty good indication of what we could get over the next 3 months into the future if we started today because we haven't contaminated/leaked any data from the post April period into our angle formulation. # First let's test laying on the train set (by definition we know these will be profitable) train_TDB_bestLay = ( dfTrain . merge ( TDB_bestLay [[ 'track' , 'race_distance' ]]) . assign ( npl = lambda x : x [ 'lay_npl' ]) . assign ( stake = 1 ) . assign ( win = lambda x : np . where ( x [ 'lay_npl' ] > 0 , 1 , 0 )) ) # This is the key test (non of the races has been part of analysis to this point) test_TDB_bestLay = ( dfTest . merge ( TDB_bestLay [[ 'track' , 'race_distance' ]]) . assign ( npl = lambda x : x [ 'lay_npl' ]) . assign ( stake = 1 ) . assign ( win = lambda x : np . where ( x [ 'lay_npl' ] > 0 , 1 , 0 )) ) # Peaking at the bets in the test set test_TDB_bestLay [[ 'track' , 'race_distance' , 'barrier' , 'barrier_group' , 'bsp' , 'lay_npl' , 'win' , 'stake' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track race_distance barrier barrier_group bsp lay_npl win stake 0 Ascot 1000 4 mid_field 11.08 0.95 1 1 1 Ascot 1000 11 outside 5.41 0.95 1 1 2 Ascot 1000 1 inside 4.73 -3.73 0 1 3 Ascot 1000 5 mid_field 7.35 0.95 1 1 4 Ascot 1000 10 outside 4.97 0.95 1 1 ... ... ... ... ... ... ... ... ... 219 Darwin 1200 10 outside 18.31 0.95 1 1 220 Darwin 1200 8 mid_field 42.00 0.95 1 1 221 Darwin 1200 6 mid_field 29.91 0.95 1 1 222 Darwin 1200 11 outside 6.60 -5.60 0 1 223 Darwin 1200 7 mid_field 5.74 0.95 1 1 224 rows \u00d7 8 columns # Let's run our evaluation on the training set bet_eval_metrics ( train_TDB_bestLay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 98.1 1047.0 0.892073 0.093696 # And on the test set bet_eval_metrics ( test_TDB_bestLay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 18.44 224.0 0.870536 0.082321 That's promising results. Our test set shows similar betting performance as our training set and we're still seeing a profitble trend. These are lay strategies so they aren't as robust as backing strategies as your profit distribution is lots of small wins and some large losses, but this is potentially a profitble betting angle! 2.5 Angle 2: Jockeys + Market Opinion Moving up slightly in level of difficulty our angles could include different kinds of reference points. Jockeys seem to be a divisive form factor in thoroughbred racing, and their quality can be hard to isolate relative to the quality of the horse and its preperation etc. I'm going to look at isolating jockeys that are either favoured or unfavoured by the market to see if I can formulate a betting angle that could generate me expected profit. The metric I'm going to use to determine market favour will be the ratio between back price 5 minutes before the scheduled jump and 30 seconds before the scheduled jump. Plotting this ratio for jockeys in our training set we can see which jockeys tend to have high market support by a high ratio (horses they are riding tend to shorten before the off) ( dfTrain . assign ( market_support = lambda x : x [ 'wap_5m' ] / x [ 'wap_30s' ]) . assign ( races = 1 ) . groupby ( 'jockey' ) . agg ({ 'market_support' : 'mean' , 'races' : 'count' }) . query ( 'races > 10' ) . sort_values ( 'market_support' , ascending = False ) . head () ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_support races jockey Scott Sheargold 1.133095 192 Lorelle Crow 1.056582 106 Chris Mc Carthy 1.051022 26 Anthony Darmanin 1.048931 142 James Winks 1.048893 12 Bob El-Issa 1.046756 196 Elyce Smith 1.043593 164 Jessica Gray 1.043376 108 Paul Francis Hamblin 1.042248 61 Alana Livesey 1.042188 32 Next, let's split the sample of each jockey's races between two scenarios a) the market firmed for their horse b) their horse drifted in the market in the last 5 minutes of trading. We then calculate the same summary table of inputs (profit, average odds etc) for backing these jockeys at the BSP given some market move. We can then feed these metrics into our statistical significance test to get an idea of the profitability of each combination. # Group By Jockey and Market Support jockeys = ( dfTrain . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . groupby ([ 'jockey' , 'market_support' ], as_index = False ) . agg ({ 'odds' : 'mean' , 'stake' : 'sum' , 'npl' : 'sum' }) . assign ( pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) ) jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } jockey market_support odds stake npl pValue 624 K Jennings Y 18.106118 85 178.6955 0.005643 496 Jade Darose Y 87.343333 39 579.4265 0.008942 263 Clayton Gallagher Y 24.225338 148 226.7145 0.012994 906 Ms T Harrison Y 27.944125 160 241.3065 0.018095 615 Justin P Stanley N 13.084502 231 155.7305 0.019913 802 Michael Dee N 36.338213 263 299.7255 0.031634 753 Madeleine Wishart Y 25.249872 78 156.5065 0.033329 433 Hannah Fitzgerald Y 32.171944 72 170.1830 0.045334 937 Nick Heywood N 17.172857 98 111.8905 0.049176 745 M Pateman N 22.690808 260 189.2050 0.052283 You can think of each of these scenarios representing different cases. If profitable: - Under market support this could indicate the jockey is being correctly favoured to maximise their horse's chances of winning the race or perhaps even some kind of insider knowledge coming out of certain stables - Under market drift this could indicate some incorrect skepticism about the jockeys ability and thus their horse has been overlayed Either way we're interested to see how these combinations would perform paper trading in our out of sample set # First evaluate on our training set train_jockeyMarket = ( dfTrain . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . merge ( jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 )[[ 'jockey' , 'market_support' ]]) . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] > 0 , 1 , 0 )) ) # And on the test set test_jockeyMarket = ( dfTest . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . merge ( jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 )[[ 'jockey' , 'market_support' ]]) . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] > 0 , 1 , 0 )) ) bet_eval_metrics ( train_jockeyMarket ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 2309.384 1434.0 0.154812 1.610449 bet_eval_metrics ( test_jockeyMarket ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 36.329 375.0 0.109333 0.096877 You can see overfitting in full effect here with the train set performance. However, our out-of-sample performance is still decently profitable. We might have found another profitable betting angle! It's worth noting that implementing this strategy would be slightly more complex than implementing our first strategy. Our code (or third party tool) would need to be able to check whether the market had firmed between 2 distinct time points before the jump of the race and cross reference that with the jockey name. Trivial for someone who is comfortable with bet placement and the betfair API but a little more involved for the uninitiated. It's important to formulate angles that you would know how and are capable of implementing. 2.6 Angle 3: Backing To Lay Now let's try to use some of our inplay price data we extracted from the stream files. I'm interested in testing some back-to-lay strategies where a horse is backed preplay with the intention to get some tradeout lay order filled during the race. The types of scenarios where this could be conceivably profitable would be on certain kinds of horses or jockeys that show promise or strength early in the race but generally fade late and might not convert those early advantages often. Things we could look at here are: - Horses that typically trade lower than their preplay odds but don't win often - Jockeys that typically trade lower than their preplay odds but don't win often - Certain combinations of jockey / trainer / horse / race distance that meet these criteria # First Investigate The Average Inplay Minimums And Loss Rates of Certain Jockeys tradeOutIndex = ( dfTrain . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . assign ( inplay_odds_ratio = lambda x : x [ 'inplay_min_lay' ] / x [ 'bsp' ]) . assign ( win = lambda x : np . where ( x [ 'place' ] == 1 , 1 , 0 )) . assign ( races = lambda x : 1 ) . groupby ([ 'jockey' ], as_index = False ) . agg ({ 'inplay_odds_ratio' : 'mean' , 'win' : 'mean' , 'races' : 'sum' }) . sort_values ( 'inplay_odds_ratio' ) . query ( 'races >= 5' ) ) tradeOutIndex .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } jockey inplay_odds_ratio win races 291 John Rudd 0.352796 0.000000 8 457 Natalie M Morton 0.357216 0.142857 7 451 Murray Henderson 0.455943 0.166667 6 92 Bridget Grylls 0.474635 0.000000 11 431 Ms Heather Poland 0.478529 0.000000 5 ... ... ... ... ... 438 Ms K Stanley 0.898819 0.000000 21 619 Yasuhiro Nishitani 0.902459 0.043478 23 99 Cameron Quilty 0.907503 0.000000 20 87 Brett Fliedner 0.923814 0.000000 20 169 Desiree Stra 0.949329 0.000000 5 558 rows \u00d7 4 columns Ok so what we have here is a list of all jockeys with over 5 races on long and mid-long race distance groups (over 1800m) ordered by their average ratio of inplay minimum traded price compared with their jump price. If this trend is predictive we could assume that these jockeys tend to have an agressive race style and like to get out and lead the race. We'd like to capitalise on that race style by backing the jockeys pre-play and putting in a lay order which we'll leave inplay hoping to get matched during the race. For simplicity let's just assume we're flat staking on both sides so that our payoff profile looks like this: - Horse never trades at <50% of it's BSP our lay bet never get's matched and we lose 1 unit - Horse trades at <50% of it's BSP but loses (our lay bet gets filled) we're breakeven for the market - Horse trades wins (our lay bet get's filled) and we profit on our back bet and lose our lay bet so our profit is: (BSP-1) - (0.5*BSP-1) Let's run this backtest on the top 20 jockeys in our tradeOutIndex dataframe to see how we'd perform on the train and test set. targetTradeoutFraction = 0.5 train_JockeyBackToLay = ( dfTrain . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . merge ( tradeOutIndex . head ( 20 )[ 'jockey' ]) . assign ( npl = lambda x : np . where ( x [ 'inplay_min_lay' ] <= targetTradeoutFraction * x [ 'bsp' ], np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'bsp' ] - 1 - ( 0.5 * x [ 'bsp' ] - 1 )), 0 ), - 1 )) . assign ( stake = lambda x : np . where ( x [ 'npl' ] != - 1 , 2 , 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] >= 0 , 1 , 0 )) ) bet_eval_metrics ( train_JockeyBackToLay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 23.797 671.0 0.5181 0.035465 test_JockeyBackToLay = ( dfTest . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . merge ( tradeOutIndex . head ( 20 )[ 'jockey' ]) . assign ( npl = lambda x : np . where ( x [ 'inplay_min_lay' ] <= targetTradeoutFraction * x [ 'bsp' ], np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'bsp' ] - 1 - ( 0.5 * x [ 'bsp' ] - 1 )), 0 ), - 1 )) . assign ( stake = lambda x : np . where ( x [ 'npl' ] != - 1 , 2 , 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] >= 0 , 1 , 0 )) ) bet_eval_metrics ( test_JockeyBackToLay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 45.62475 255.0 0.342105 0.178921 Not bad! Looks like we found another possibly promising lead. Again it's worth noting that this is probably another step up in implementation complexity again from previous angles. It's not very hard when you're familiar with betfair order types and placing them through the API but it requires some additional API savviness. But the documentation is quite good and there's plenty of resources available online to help you understand how to automate something like this. 3.0 Conclusion This analysis is just a sketch. Hopefully it helps inspire you to think about what kinds of betting angles you could test for a sport or racing code you're interested in. It should give you a framework for thinking about this kind of automated betting, and how it differs from fundamental modelling. It should also give you a few tricks for coming up with your own angles and testing them with the rigour needed to have any realistic expectations of profit. Most of the betting angles you're sold are faulty or have long evaporated from the market by people long before you even knew the rules of the sport. You'll need to be creative and scientific to create your own profitable betting angles, but it's certainly worth it to try. Complete code Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github import requests import pandas as pd from datetime import date , timedelta import numpy as np import os import re import tarfile import zipfile import bz2 import glob import logging import yaml from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) from scipy.stats import t import plotly.express as px # Utility Functions # + Stream Parsing # + Betfair Race Data Scraping # + Various utilities # _________________________________ def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def slicePrice ( l , n ): try : x = l [ n ] . price except : x = np . nan return ( x ) def sliceSize ( l , n ): try : x = l [ n ] . size except : x = np . nan return ( x ) def wapPrice ( l , n ): try : x = round ( sum ( [ rung . price * rung . size for rung in l [ 0 :( n - 1 )] ] ) / sum ( [ rung . size for rung in l [ 0 :( n - 1 )] ]), 2 ) except : x = np . nan return ( x ) def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) # Core Execution Fucntions # _________________________________ def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): evaluate_market = None prev_market = None postplay = None preplay = None t5m = None t30s = None inplay_min_lay = None gen = s . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 30 seconds before scheduled off if t30s is None and seconds_to_start < 30 : t30s = market_book # Market at 5 mins before scheduled off if t5m is None and seconds_to_start < 5 * 60 : t5m = market_book # Manage Inplay Vectors if market_book . inplay : if inplay_min_lay is None : inplay_min_lay = [ slicePrice ( runner . ex . available_to_lay , 0 ) for runner in market_book . runners ] else : inplay_min_lay = np . fmin ( inplay_min_lay , [ slicePrice ( runner . ex . available_to_lay , 0 ) for runner in market_book . runners ]) # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay is not None and preplay is None : preplay = postplay inplay_min_lay = [ \"\" for runner in market_book . runners ] return ( t5m , t30s , preplay , postplay , inplay_min_lay , prev_market ) # Final market is last prev_market def parse_stream ( stream_files , output_file ): with open ( output_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,selection_name,wap_5m,wap_30s,bsp,ltp,traded_vol,inplay_min_lay \\n \" ) for file_obj in load_markets ( stream_files ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) ( t5m , t30s , preplay , postplay , inplayMin , final ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay is None or final is None or t30s is None : continue ; # All runner removed if all ( runner . status == \"REMOVED\" for runner in final . runners ): continue runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final . runners ] ltp = [ runner . last_price_traded for runner in preplay . runners ] tradedVol = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay . runners ] wapBack30s = [ wapPrice ( runner . ex . available_to_back , 3 ) for runner in t30s . runners ] wapBack5m = [ wapPrice ( runner . ex . available_to_back , 3 ) for runner in t5m . runners ] # Writing To CSV # ______________________ for ( runnerMeta , ltp , tradedVol , inplayMin , wapBack5m , wapBack30s ) in zip ( runnerMeta , ltp , tradedVol , inplayMin , wapBack5m , wapBack30s ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final . market_id ), runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], wapBack5m , wapBack30s , runnerMeta [ 'sp' ], ltp , round ( tradedVol ), inplayMin ) ) def get_bf_markets ( dte ): url = 'https://apigateway.betfair.com.au/hub/racecard?date= {} ' . format ( dte ) responseJson = requests . get ( url ) . json () marketList = [] for meeting in responseJson [ 'MEETINGS' ]: for markets in meeting [ 'MARKETS' ]: marketList . append ( { 'date' : dte , 'track' : meeting [ 'VENUE_NAME' ], 'country' : meeting [ 'COUNTRY' ], 'race_type' : meeting [ 'RACE_TYPE' ], 'race_number' : markets [ 'RACE_NO' ], 'market_id' : str ( '1.' + markets [ 'MARKET_ID' ]), 'start_time' : markets [ 'START_TIME' ] } ) marketDf = pd . DataFrame ( marketList ) return ( marketDf ) def get_bf_race_meta ( market_id ): url = 'https://apigateway.betfair.com.au/hub/raceevent/ {} ' . format ( market_id ) responseJson = requests . get ( url ) . json () if 'error' in responseJson : return ( pd . DataFrame ()) raceList = [] for runner in responseJson [ 'runners' ]: if 'isScratched' in runner and runner [ 'isScratched' ]: continue # Jockey not always populated try : jockey = runner [ 'jockeyName' ] except : jockey = \"\" # Place not always populated try : placeResult = runner [ 'placedResult' ] except : placeResult = \"\" # Place not always populated try : trainer = runner [ 'trainerName' ] except : trainer = \"\" raceList . append ( { 'market_id' : market_id , 'weather' : responseJson [ 'weather' ], 'track_condition' : responseJson [ 'trackCondition' ], 'race_distance' : responseJson [ 'raceLength' ], 'selection_id' : runner [ 'selectionId' ], 'selection_name' : runner [ 'runnerName' ], 'barrier' : runner [ 'barrierNo' ], 'place' : placeResult , 'trainer' : trainer , 'jockey' : jockey , 'weight' : runner [ 'weight' ] } ) raceDf = pd . DataFrame ( raceList ) return ( raceDf ) def scrape_thoroughbred_bf_date ( dte ): markets = get_bf_markets ( dte ) if markets . shape [ 0 ] == 0 : return ( pd . DataFrame ()) thoMarkets = markets . query ( 'country == \"AUS\" and race_type == \"R\"' ) if thoMarkets . shape [ 0 ] == 0 : return ( pd . DataFrame ()) raceMetaList = [] for market in thoMarkets . market_id : raceMetaList . append ( get_bf_race_meta ( market )) raceMeta = pd . concat ( raceMetaList ) return ( markets . merge ( raceMeta , on = 'market_id' )) # Execute Data Pipeline # _________________________________ # Description: # Will loop through a set of dates (starting July 2020 in this instance) and return race metadata from betfair # Estimated Time: # ~60 mins # # if __name__ == '__main__': # dataList = [] # dateList = pd.date_range(date(2020,7,1),date.today()-timedelta(days=1),freq='d') # for dte in dateList: # dte = dte.date() # print(dte) # races = scrapeThoroughbredBfDate(dte) # dataList.append(races) # data = pd.concat(dataList) # data.to_csv(\"[LOCAL PATH SOMEWHERE]\", index=False) # Description: # Will loop through a set of stream data archive files and extract a few key pricing measures for each selection # Estimated Time: # ~6 hours # # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) # stream_files = glob.glob(\"[PATH TO LOCAL FOLDER STORING ARCHIVE FILES]*.tar\") # output_file = \"[SOME OUTPUT DIRECTORY]/thoroughbred-odds-2021.csv\" # if __name__ == '__main__': # parse_stream(stream_files, output_file) # Analysis # _________________________________ # Functions ++++++++ def bet_eval_metrics ( d , side = False ): metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" , \"win\" : \"mean\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) def pl_pValue ( number_bets , npl , stake , average_odds ): pot = npl / stake tStatistic = ( pot * np . sqrt ( number_bets )) / np . sqrt ( ( 1 + pot ) * ( average_odds - 1 - pot ) ) pValue = 2 * t . cdf ( - abs ( tStatistic ), number_bets - 1 ) return ( np . where ( np . logical_or ( np . isnan ( pValue ), pValue == 0 ), 1 , pValue )) def distance_group ( distance ): if distance is None : return ( \"missing\" ) elif distance < 1100 : return ( \"sprint\" ) elif distance < 1400 : return ( \"mid_short\" ) elif distance < 1800 : return ( \"mid_long\" ) else : return ( \"long\" ) def barrier_group ( barrier ): if barrier is None : return ( \"missing\" ) elif barrier < 4 : return ( \"inside\" ) elif barrier < 9 : return ( \"mid_field\" ) else : return ( \"outside\" ) # Analysis ++++++++ # Local Paths (will be different on your machine) path_odds_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-odds-2021.csv\" path_race_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-race-data.csv\" odds = pd . read_csv ( path_odds_local , dtype = { 'market_id' : object , 'selection_id' : object }) race = pd . read_csv ( path_race_local , dtype = { 'market_id' : object , 'selection_id' : object }) # Joining two datasets df = race . merge ( odds . loc [:, odds . columns != 'selection_name' ], how = \"inner\" , on = [ 'market_id' , 'selection_id' ]) # I'll also add columns for the net profit from backing and laying each selection to be picked up in subsequent sections df [ 'back_npl' ] = np . where ( df [ 'place' ] == 1 , 0.95 * ( df [ 'bsp' ] - 1 ), - 1 ) df [ 'lay_npl' ] = np . where ( df [ 'place' ] == 1 , - 1 * ( df [ 'bsp' ] - 1 ), 0.95 ) # Adding Variable Chunks df [ 'distance_group' ] = pd . to_numeric ( df . race_distance , errors = \"coerce\" ) . apply ( distance_group ) df [ 'barrier_group' ] = pd . to_numeric ( df . barrier , errors = \"coerce\" ) . apply ( barrier_group ) # Data Partitioning dfTrain = df . query ( 'date < \"2021-04-01\"' ) dfTest = df . query ( 'date >= \"2021-04-01\"' ) ' {} rows in the \"training\" set and {} rows in the \"test\" data' . format ( dfTrain . shape [ 0 ], dfTest . shape [ 0 ]) # Angle 1 ++++++++++++++++++++++++++++++++++++++++++++++ ( dfTrain . assign ( stake = 1 ) . groupby ( 'selection_name' , as_index = False ) . agg ({ 'back_npl' : 'sum' , 'stake' : 'sum' }) . assign ( pot = lambda x : x [ 'back_npl' ] / x [ 'stake' ]) . sort_values ( 'pot' , ascending = False ) . head ( 3 ) ) # Calculate the profit (back and lay) and average odds across all track / distance / barrier group combos trackDistanceBarrier = ( dfTrain . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . groupby ([ 'track' , 'race_distance' , 'barrier_group' ], as_index = False ) . agg ({ 'back_npl' : 'sum' , 'lay_npl' : 'sum' , 'stake' : 'sum' , 'odds' : 'mean' }) ) trackDistanceBarrier trackDistanceBarrier = ( trackDistanceBarrier . assign ( backPL_pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'back_npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) . assign ( layPL_pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'lay_npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) ) trackDistanceBarrier # Top 5 lay combos Track | Distance | Barrier (TDB) TDB_bestLay = trackDistanceBarrier . query ( 'lay_npl>0' ) . sort_values ( 'layPL_pValue' ) . head ( 5 ) TDB_bestLay # First let's test laying on the train set (by definition we know these will be profitable) train_TDB_bestLay = ( dfTrain . merge ( TDB_bestLay [[ 'track' , 'race_distance' ]]) . assign ( npl = lambda x : x [ 'lay_npl' ]) . assign ( stake = 1 ) . assign ( win = lambda x : np . where ( x [ 'lay_npl' ] > 0 , 1 , 0 )) ) # This is the key test (non of the races has been part of analysis to this point) test_TDB_bestLay = ( dfTest . merge ( TDB_bestLay [[ 'track' , 'race_distance' ]]) . assign ( npl = lambda x : x [ 'lay_npl' ]) . assign ( stake = 1 ) . assign ( win = lambda x : np . where ( x [ 'lay_npl' ] > 0 , 1 , 0 )) ) # Peaking at the bets in the test set test_TDB_bestLay [[ 'track' , 'race_distance' , 'barrier' , 'barrier_group' , 'bsp' , 'lay_npl' , 'win' , 'stake' ]] # Let's run our evaluation on the training set bet_eval_metrics ( train_TDB_bestLay ) # And on the test set bet_eval_metrics ( test_TDB_bestLay ) # Angle 2 ++++++++++++++++++++++++++++++++++++++++++++++ ( dfTrain . assign ( market_support = lambda x : x [ 'wap_5m' ] / x [ 'wap_30s' ]) . assign ( races = 1 ) . groupby ( 'jockey' ) . agg ({ 'market_support' : 'mean' , 'races' : 'count' }) . query ( 'races > 10' ) . sort_values ( 'market_support' , ascending = False ) . head () ) # Group By Jockey and Market Support jockeys = ( dfTrain . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . groupby ([ 'jockey' , 'market_support' ], as_index = False ) . agg ({ 'odds' : 'mean' , 'stake' : 'sum' , 'npl' : 'sum' }) . assign ( pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) ) jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 ) # First evaluate on our training set train_jockeyMarket = ( dfTrain . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . merge ( jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 )[[ 'jockey' , 'market_support' ]]) . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] > 0 , 1 , 0 )) ) # And on the test set test_jockeyMarket = ( dfTest . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . merge ( jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 )[[ 'jockey' , 'market_support' ]]) . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] > 0 , 1 , 0 )) ) bet_eval_metrics ( train_jockeyMarket ) bet_eval_metrics ( test_jockeyMarket ) # Angle 3 ++++++++++++++++++++++++++++++++++++++++++++++ # First Investigate The Average Inplay Minimums And Loss Rates of Certain Jockeys tradeOutIndex = ( dfTrain . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . assign ( inplay_odds_ratio = lambda x : x [ 'inplay_min_lay' ] / x [ 'bsp' ]) . assign ( win = lambda x : np . where ( x [ 'place' ] == 1 , 1 , 0 )) . assign ( races = lambda x : 1 ) . groupby ([ 'jockey' ], as_index = False ) . agg ({ 'inplay_odds_ratio' : 'mean' , 'win' : 'mean' , 'races' : 'sum' }) . sort_values ( 'inplay_odds_ratio' ) . query ( 'races >= 5' ) ) tradeOutIndex targetTradeoutFraction = 0.5 train_JockeyBackToLay = ( dfTrain . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . merge ( tradeOutIndex . head ( 20 )[ 'jockey' ]) . assign ( npl = lambda x : np . where ( x [ 'inplay_min_lay' ] <= targetTradeoutFraction * x [ 'bsp' ], np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'bsp' ] - 1 - ( 0.5 * x [ 'bsp' ] - 1 )), 0 ), - 1 )) . assign ( stake = lambda x : np . where ( x [ 'npl' ] != - 1 , 2 , 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] >= 0 , 1 , 0 )) ) bet_eval_metrics ( train_JockeyBackToLay ) test_JockeyBackToLay = ( dfTest . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . merge ( tradeOutIndex . head ( 20 )[ 'jockey' ]) . assign ( npl = lambda x : np . where ( x [ 'inplay_min_lay' ] <= targetTradeoutFraction * x [ 'bsp' ], np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'bsp' ] - 1 - ( 0.5 * x [ 'bsp' ] - 1 )), 0 ), - 1 )) . assign ( stake = lambda x : np . where ( x [ 'npl' ] != - 1 , 2 , 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] >= 0 , 1 , 0 )) ) bet_eval_metrics ( test_JockeyBackToLay ) Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Automated betting angles in Python"},{"location":"historicData/automatedBettingAnglesTutorial/#automated-betting-angles-in-python","text":"| Betting strategies based on your existing insights: no modelling required","title":"Automated betting angles in Python"},{"location":"historicData/automatedBettingAnglesTutorial/#workshop","text":"This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the JSON to CSV tutorial and backteseting ratings in Python tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at those tutorials before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!","title":"Workshop"},{"location":"historicData/automatedBettingAnglesTutorial/#cheat-sheet","text":"This is presented as a Jupyter notebook as this format is interactive and lets you run snippets of code from wihtin the notebook. To use this functionality you'll need to download a copy of the ipynb file locally and open it in a text editor (i.e. VS code). If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo .","title":"Cheat sheet"},{"location":"historicData/automatedBettingAnglesTutorial/#01-setup","text":"Once again I'll be presenting the analysis in a jupyter notebook and will be using python as a programming language. Some of the data processing code takes a while to execute - that code will be in cells that are commented out - and will require a bit of adjustment to point to places on your computer where you want to locally store the intermediate data files. You'll also need betfairlightweight which you can install with something like pip install betfairlightweight . import requests import pandas as pd from datetime import date , timedelta import numpy as np import os import re import tarfile import zipfile import bz2 import glob import logging import yaml from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) from scipy.stats import t import plotly.express as px","title":"0.1 Setup"},{"location":"historicData/automatedBettingAnglesTutorial/#02-context","text":"Formulating betting angles (or \"strategies\" as some call them) is quite a common pasttime for some. These angles can range all the way from very simple to quite sophisticated, and could include things like: Laying NBA teams playing on the second night of a back-to-back Laying AFL team coming off a bye when matched against a team who played last week Backing a greyhound in boxes 1 or 2 in short sprint style races Backing a horse pre-race who typically runs at the front of the field and placing an order to lay the same horse if it shortens to some lower price in-play, locking in a profit Beyond the complexity of the actual concept what really seperates these angles is evidence. You might have heard TV personalities and betting ads suggest a certain strategy (resembling one of the above) are real-world predictive trends but they rarely are. They are rarely derived from the right historical data or are concluded without the necessary statistical rigour. Most simply formulated their angles off intuition or observing a trend across a small sample of data. There are many users on betting exchanges who profit off these angles. In fact, when most people talk about automated or sophisticated exchange betting they are often talking about automating these kind of betting angles, as opposed to betting ratings produced from a sophisticated bottom-up fundemental modelling. That's because profitable fundemental modelling (where your model which arrives at some estimation of fair value from first principles) is very hard. The reason this approach is so much easier is that you assume the market odds are right except x and go from there, applying small top-down adjustments for factors that haven't historically been incorporated in the market opinion. The challenge lies in finding those factors and making sure you aren't tricking yourself in thinking you've found one that you can profit off in the future. Once again this is another example of the uses of the Betfair historical stream data. To get cracking - as always - we need historical odds and the best place to get that is to self serve the historical stream files.","title":"0.2 Context"},{"location":"historicData/automatedBettingAnglesTutorial/#03-examples","text":"I'll go through an end-to-end example of 3 different betting angles on Australian thoroughbred racing. Which will include: Which will include: Sourcing data Assembling data Formulating hypotheses Testing Hypotheses Discussion about implementation","title":"0.3 Examples"},{"location":"historicData/automatedBettingAnglesTutorial/#10-data","text":"","title":"1.0 Data"},{"location":"historicData/automatedBettingAnglesTutorial/#11-betfair-odds-data","text":"We'll follow a very similar template as other tutorials extracting key information from the betfair stream data. It's important to note that given the volume of data you need to handle with these stream files, your workflow will probably involve choosing some methods of aggregation / summary that you'll reconsider after your first cut of analysis. Parsing and saving a dataset, using it to test some hypotheses which likely results in more questions that need to be examined by reparsing the stream files in a slightly different way. Your workflow will likely follow something like this diagram. For the purposes of this article I'm interested in backtesting some betting angles at the BSP, using some indication of price momentum/market support in some angles, and testing some back to lay strategies so we'll need to pull out some information about each runners in-play trading. So we'll extract the following for each runner: - BSP - Last Traded Price - Volume Weighted Avg Price (top 3 boxes) 5 mins before the scheduled jump time - Volume Weighted Avg Price (top 3 boxes) 30 seconds before the scheduled jump time - The volume traded on the selection - The minimum \"best available to lay\" price offered inplay (which is a measure of how low the selection traded during the race) First we'll establish some utility functions needed to parse the data. Most of these were discussed in the previous backtest your ratings tutorial. # Utility Functions For Stream Parsing # _________________________________ def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def slicePrice ( l , n ): try : x = l [ n ] . price except : x = np . nan return ( x ) def sliceSize ( l , n ): try : x = l [ n ] . size except : x = np . nan return ( x ) def wapPrice ( l , n ): try : x = round ( sum ( [ rung . price * rung . size for rung in l [ 0 :( n - 1 )] ] ) / sum ( [ rung . size for rung in l [ 0 :( n - 1 )] ]), 2 ) except : x = np . nan return ( x ) def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) Then we'll create our core execution functions that will scan over the historical stream files and use betfairlightweight to recreate the state of the exchange for each thoroughbred market and extract key information for each selection # Core Execution Fucntions # _________________________________ def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): evaluate_market = None prev_market = None postplay = None preplay = None t5m = None t30s = None inplay_min_lay = None gen = s . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 30 seconds before scheduled off if t30s is None and seconds_to_start < 30 : t30s = market_book # Market at 5 mins before scheduled off if t5m is None and seconds_to_start < 5 * 60 : t5m = market_book # Manage Inplay Vectors if market_book . inplay : if inplay_min_lay is None : inplay_min_lay = [ slicePrice ( runner . ex . available_to_lay , 0 ) for runner in market_book . runners ] else : inplay_min_lay = np . fmin ( inplay_min_lay , [ slicePrice ( runner . ex . available_to_lay , 0 ) for runner in market_book . runners ]) # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay is not None and preplay is None : preplay = postplay inplay_min_lay = [ \"\" for runner in market_book . runners ] return ( t5m , t30s , preplay , postplay , inplay_min_lay , prev_market ) # Final market is last prev_market def parse_stream ( stream_files , output_file ): with open ( output_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,selection_name,wap_5m,wap_30s,bsp,ltp,traded_vol,inplay_min_lay \\n \" ) for file_obj in load_markets ( stream_files ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) ( t5m , t30s , preplay , postplay , inplayMin , final ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay is None or final is None or t30s is None : continue ; # All runner removed if all ( runner . status == \"REMOVED\" for runner in final . runners ): continue runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final . runners ] ltp = [ runner . last_price_traded for runner in preplay . runners ] tradedVol = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay . runners ] wapBack30s = [ wapPrice ( runner . ex . available_to_back , 3 ) for runner in t30s . runners ] wapBack5m = [ wapPrice ( runner . ex . available_to_back , 3 ) for runner in t5m . runners ] # Writing To CSV # ______________________ for ( runnerMeta , ltp , tradedVol , inplayMin , wapBack5m , wapBack30s ) in zip ( runnerMeta , ltp , tradedVol , inplayMin , wapBack5m , wapBack30s ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final . market_id ), runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], wapBack5m , wapBack30s , runnerMeta [ 'sp' ], ltp , round ( tradedVol ), inplayMin ) ) Finally, after sourcing and downloading 12 months of stream files (ask automation@betfair.com.au for more info if you don't know how to do this) we'll use the above code to parse each file and write to a single csv file to be used for analysis. # Description: # Will loop through a set of stream data archive files and extract a few key pricing measures for each selection # Estimated Time: # ~6 hours # Parameters # _________________________________ # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) # stream_files = glob.glob(\"[PATH TO LOCAL FOLDER STORING ARCHIVE FILES]*.tar\") # output_file = \"[SOME OUTPUT DIRECTORY]/thoroughbred-odds-2021.csv\" # Run # _________________________________ # if __name__ == '__main__': # parse_stream(stream_files, output_file)","title":"1.1 Betfair Odds Data"},{"location":"historicData/automatedBettingAnglesTutorial/#12-race-data","text":"If you're building a fundamental bottom-up model, finding and managing ETL from an appropriate data source is a large part of the exercise. If your needs are simpler (for these types of automated strategies for example) there's plenty of good information that's available right inside the betfair API itself. The RUNNER_METADATA slot inside the listMarketCatalogue response for example will return a pretty good slice of metadata about the horses racing in upcoming races including but not limited to: the trainer, the jockey, the horses age, and a class rating. The documentaion for this endpoint will give you the full extent of this what's inside this response. Our problem for this exercise is that the historical stream files don't include this RUNNER_METADATA so we weren't able to extract it in the previous step. However, a sneaky workaround is to use an unsuppoerted back-end endpoint, one which Betfair use for the Hub racing results page . These API endpoints are: Market result data: https://apigateway.betfair.com.au/hub/raceevent/1.154620281 Day\u2019s markets: https://apigateway.betfair.com.au/hub/racecard?date=2018-12-18","title":"1.2 Race Data"},{"location":"historicData/automatedBettingAnglesTutorial/#extract-betfair-racing-markets-for-a-given-date","text":"First we'll hit the https://apigateway.betfair.com.au/hub/racecard endpoint to get the racing markets available on Betfair for a given day in the past: def getBfMarkets ( dte ): url = 'https://apigateway.betfair.com.au/hub/racecard?date= {} ' . format ( dte ) responseJson = requests . get ( url ) . json () marketList = [] for meeting in responseJson [ 'MEETINGS' ]: for markets in meeting [ 'MARKETS' ]: marketList . append ( { 'date' : dte , 'track' : meeting [ 'VENUE_NAME' ], 'country' : meeting [ 'COUNTRY' ], 'race_type' : meeting [ 'RACE_TYPE' ], 'race_number' : markets [ 'RACE_NO' ], 'market_id' : str ( '1.' + markets [ 'MARKET_ID' ]), 'start_time' : markets [ 'START_TIME' ] } ) marketDf = pd . DataFrame ( marketList ) return ( marketDf )","title":"Extract Betfair Racing Markets for a Given Date"},{"location":"historicData/automatedBettingAnglesTutorial/#extract-key-race-metadata","text":"Then (for one of these market_id s) we'll hit the https://apigateway.betfair.com.au/hub/raceevent/ enpdoint to get some key runner metadata for the runners in this race. It's important to note that this information is available through the Betfair API so we won't need to go to a secondary datasource to find it at the point of implementation, this would add a large layer of complexity to the project including things like string cleaning and matching. def getBfRaceMeta ( market_id ): url = 'https://apigateway.betfair.com.au/hub/raceevent/ {} ' . format ( market_id ) responseJson = requests . get ( url ) . json () if 'error' in responseJson : return ( pd . DataFrame ()) raceList = [] for runner in responseJson [ 'runners' ]: if 'isScratched' in runner and runner [ 'isScratched' ]: continue # Jockey not always populated try : jockey = runner [ 'jockeyName' ] except : jockey = \"\" # Place not always populated try : placeResult = runner [ 'placedResult' ] except : placeResult = \"\" # Place not always populated try : trainer = runner [ 'trainerName' ] except : trainer = \"\" raceList . append ( { 'market_id' : market_id , 'weather' : responseJson [ 'weather' ], 'track_condition' : responseJson [ 'trackCondition' ], 'race_distance' : responseJson [ 'raceLength' ], 'selection_id' : runner [ 'selectionId' ], 'selection_name' : runner [ 'runnerName' ], 'barrier' : runner [ 'barrierNo' ], 'place' : placeResult , 'trainer' : trainer , 'jockey' : jockey , 'weight' : runner [ 'weight' ] } ) raceDf = pd . DataFrame ( raceList ) return ( raceDf )","title":"Extract Key Race Metadata"},{"location":"historicData/automatedBettingAnglesTutorial/#wrapper-function","text":"Stiching these two functions together we can create a wrapper function that hits both endpoints for all the thoroughbred races in a given day and extract all the runner metadata and results. def scrapeThoroughbredBfDate ( dte ): markets = getBfMarkets ( dte ) if markets . shape [ 0 ] == 0 : return ( pd . DataFrame ()) thoMarkets = markets . query ( 'country == \"AUS\" and race_type == \"R\"' ) if thoMarkets . shape [ 0 ] == 0 : return ( pd . DataFrame ()) raceMetaList = [] for market in thoMarkets . market_id : raceMetaList . append ( getBfRaceMeta ( market )) raceMeta = pd . concat ( raceMetaList ) return ( markets . merge ( raceMeta , on = 'market_id' )) # Executing the wrapper for an example date scrapeThoroughbredBfDate ( date ( 2021 , 2 , 10 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date track country race_type race_number market_id start_time weather track_condition race_distance selection_id selection_name barrier place trainer jockey weight 0 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 38448397 Triple Missile 3 1 Todd Harvey Paul Harvey 60.0 1 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 28763768 Shock Result 5 4 P H Jordan Craig Staples 59.5 2 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 8772321 Secret Plan 6 3 G & A Williams William Pike 59.0 3 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 9021011 Command Force 2 0 Daniel & Ben Pearce J Azzopardi 58.0 4 2021-02-10 Ascot AUS R 1 1.179077389 2021-02-10 04:34:00 None None 1000 38448398 Fish Hook 7 2 M P Allan Madi Derrick 57.5 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 458 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 133456 Sedition 12 2 Richard Litt Ms Rachel King 58.0 459 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447782 Amusez Moi 9 6 Richard Litt Josh Parr 57.0 460 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 25388274 Savoury 1 5 Bjorn Baker Jason Collett 57.0 461 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447783 Born A Warrior 7 3 Michael & Wayne & John Hawkes Tommy Berry 56.5 462 2021-02-10 Warwick Farm AUS R 7 1.179081635 2021-02-10 06:50:00 None None 1200 38447784 Newsreader 10 1 John O'shea James Mcdonald 55.5 463 rows \u00d7 17 columns Then to produce a historical slice of all races between two dates we could just loop over a set of dates and append each results set # Description: # Will loop through a set of dates (starting July 2020 in this instance) and return race metadata from betfair # Estimated Time: # ~60 mins # # dataList = [] # dateList = pd.date_range(date(2020,7,1),date.today()-timedelta(days=1),freq='d') # for dte in dateList: # dte = dte.date() # print(dte) # races = scrapeThoroughbredBfDate(dte) # dataList.append(races) # data = pd.concat(dataList) # data.to_csv(\"[LOCAL PATH SOMEWHERE]\", index=False)","title":"Wrapper Function"},{"location":"historicData/automatedBettingAnglesTutorial/#20-analysis","text":"I'll be running through 3 simple betting angles, one easy, one medium, and one hard to illustrate different types of angles you might want to try at home. The process I lay out is very similar (if not identical) but the implementation might be a bit trickier in each case and might take a little more programming skill to get up and running. We'll use a simple evaluation function (POT and strike rate) to evaluate each of these strategies. def bet_eval_metrics ( d , side = False ): metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" , \"win\" : \"mean\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ])","title":"2.0 Analysis"},{"location":"historicData/automatedBettingAnglesTutorial/#21-assemble-data","text":"Now that we have our 2 core datasets (odds + race / runner metadata) we can join them together and do some analysis # Local Paths (will be different on your machine) path_odds_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-odds-2021.csv\" path_race_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-race-data.csv\" odds = pd . read_csv ( path_odds_local , dtype = { 'market_id' : object , 'selection_id' : object }) race = pd . read_csv ( path_race_local , dtype = { 'market_id' : object , 'selection_id' : object }) odds . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id selection_id selection_name wap_5m wap_30s bsp ltp traded_vol inplay_min_lay 0 1.179845158 23493550 1. Larmour 6.27 5.84 6.20 6.2 8277 1.19 1 1.179845158 16374800 3. Careering Away 3.31 3.67 3.60 3.65 18592 1.08 2 1.179845158 19740699 4. Bells N Bows 6.87 6.36 6.62 6.4 7413 1.42 race . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date track country race_type race_number market_id start_time weather track_condition race_distance selection_id selection_name barrier place trainer jockey weight 0 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 19674744 Baldy 2 4.0 Peter Nolan Karl Zechner 59.0 1 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 401615 Nostrovia 4 7.0 Dennis O'leary Margaret Collett 59.0 2 2020-07-01 Balaklava AUS R 1 1.171091087 2020-07-01 02:40:00 FINE GOOD4 2200 26789410 Ammo Loco 5 1.0 John Hickmott Barend Vorster 58.5 # Joining two datasets df = race . merge ( odds . loc [:, odds . columns != 'selection_name' ], how = \"inner\" , on = [ 'market_id' , 'selection_id' ]) # I'll also add columns for the net profit from backing and laying each selection to be picked up in subsequent sections df [ 'back_npl' ] = np . where ( df [ 'place' ] == 1 , 0.95 * ( df [ 'bsp' ] - 1 ), - 1 ) df [ 'lay_npl' ] = np . where ( df [ 'place' ] == 1 , - 1 * ( df [ 'bsp' ] - 1 ), 0.95 )","title":"2.1 Assemble Data"},{"location":"historicData/automatedBettingAnglesTutorial/#22-methodology","text":"Looping back around to the context discussion in part 0.2 we need to decide on how to set up our analysis that will help us: find angles, formulate strategies, and test them with enough rigour that will give us a good estimate of our forward looking profitability on any that we choose to implement and automate. The 3 key tricks I'll lay out in this piece are: - Using a statistical estimate to quantify the robustness of historical profitibalility - Using out-of-sample validation (much like a you would in a model building exercise) to get an accurate view of forward looking profitability - Using domain knowledge to chunk selections to get broader sample for more stable estimate of profitability","title":"2.2 Methodology"},{"location":"historicData/automatedBettingAnglesTutorial/#231-chunking","text":"This is a technique you can use to group together variables in conceptually similar groups. For example, thoroughbred races are run over many different exact distances (800m, 810m, 850m, 860m etc) which - using a domain overlay - are all very short sprint style races for a horse race. Similarly, barriers 1, 2 and 3 being on the very inside of the race field and closest to the rail all present similar early race challenges and advantages for horses jumping from those barriers. So formulating your betting angles you may want to overlay semantically similar variable groups to test your betting hypothesis. I'll add variable chunks for race distance and barrier for now but you may want to test more (for example horse experience, trainer stable size etc) def distance_group ( distance ): if distance is None : return ( \"missing\" ) elif distance < 1100 : return ( \"sprint\" ) elif distance < 1400 : return ( \"mid_short\" ) elif distance < 1800 : return ( \"mid_long\" ) else : return ( \"long\" ) def barrier_group ( barrier ): if barrier is None : return ( \"missing\" ) elif barrier < 4 : return ( \"inside\" ) elif barrier < 9 : return ( \"mid_field\" ) else : return ( \"outside\" ) df [ 'distance_group' ] = pd . to_numeric ( df . race_distance , errors = \"coerce\" ) . apply ( distance_group ) df [ 'barrier_group' ] = pd . to_numeric ( df . barrier , errors = \"coerce\" ) . apply ( barrier_group )","title":"2.3.1 Chunking"},{"location":"historicData/automatedBettingAnglesTutorial/#232-in-sample-vs-out-of-sample","text":"The first thing I'm going to do is to split off a largish chunk of my data before even looking at it. I'll ultimately use it to paper trade some of my candidate angles but I want it to be as seperate from the idea generation process as possible. I'll use the model building nomenclature \"train\" and \"test\" even though I'm not really doing any \"training\". My data contains all AUS thoroughbred races from July 2020 until end of June 2021 so I'll cut off the period Apr-June 2021 as my \"test\" set. dfTrain = df . query ( 'date < \"2021-04-01\"' ) dfTest = df . query ( 'date >= \"2021-04-01\"' ) ' {} rows in the \"training\" set and {} rows in the \"test\" data' . format ( dfTrain . shape [ 0 ], dfTest . shape [ 0 ]) '119244 rows in the \"training\" set and 40783 rows in the \"test\" data'","title":"2.3.2 In Sample vs Out of Sample"},{"location":"historicData/automatedBettingAnglesTutorial/#233-statistically-measuring-profit","text":"Betting outcomes, and the randomness associated with them, at their core are the types of things the discipline of statistics was created to solve. Concepts like sample size, expected value, and variance are terms you might hear from sophisticated (and some novice) bettors and they are all drawn from the field of statistics. Though you don't need to become a PHD of statistics every little extra technique or concept you can glean from the field will help your betting if you want it to. To illustrate with an example, let's group by net backing profit on turnover for a horse to see which horses have the highest historical back POT: ( dfTrain . assign ( stake = 1 ) . groupby ( 'selection_name' , as_index = False ) . agg ({ 'back_npl' : 'sum' , 'stake' : 'sum' }) . assign ( pot = lambda x : x [ 'back_npl' ] / x [ 'stake' ]) . sort_values ( 'pot' , ascending = False ) . head ( 3 ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } selection_name back_npl stake pot 12247 Little Vulpine 274.550 1 274.550000 15384 Not Tonight Dear 130.701 1 130.701000 9987 Im Cheeky 617.307 7 88.186714 So back Little Vulpine whenever it races? We all know intuitively what's wrong with that betting angle - it's raced one time in our sample and happened to win at a bsp of 270. Sample size and variance are dominating this simple measure of historical POT. Instead what we can do is treat the historical betting outcomes as a random variable and apply some statistical tests of signifance to them. A more detailed discussion of this particular test can be found here as can an excel calculator you can input your stats into. I'll simply translate the test to python to enable it's use when formulating our betting angles. The TLDR version of this test is that; based on your bet sample size, your profit, and the average odds across that sample of bets, the calculation produces a p value which estimates the probability your profit (or loss) happened by pure chance (where chance would be an expectation of breakeven betting simply at fair odds). def pl_pValue ( number_bets , npl , stake , average_odds ): pot = npl / stake tStatistic = ( pot * np . sqrt ( number_bets )) / np . sqrt ( ( 1 + pot ) * ( average_odds - 1 - pot ) ) pValue = 2 * t . cdf ( - abs ( tStatistic ), number_bets - 1 ) return ( np . where ( np . logical_or ( np . isnan ( pValue ), pValue == 0 ), 1 , pValue )) That doesn't mean we can formulate our angles and use this metric (and this metric alone) to validate their profitability. You'll find that it will give you misleading results in some instances. As analysts we're also prone to finding infinite different ways to unintentionally overfit our analysis as you might have heard elsewhere described as the concept of p-hacking, but it does give us an extra filter to cast over our hypotheses before really validating them with out-of-sample testing.","title":"2.3.3 Statistically Measuring Profit"},{"location":"historicData/automatedBettingAnglesTutorial/#24-angle-1-track-distance-barrier","text":"The first thing I'll test is whether or not there are any combinations of track/distance/barrier where backing or laying could produce robust long term profit. This probably fits within the types of betting angles people before you have already sucked all the value out of long before you started reading this article. That's not to say that you shouldn't test them though, as people have made livings on betting angles as simple as these. # Calculate the profit (back and lay) and average odds across all track / distance / barrier group combos trackDistanceBarrier = ( dfTrain . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . groupby ([ 'track' , 'race_distance' , 'barrier_group' ], as_index = False ) . agg ({ 'back_npl' : 'sum' , 'lay_npl' : 'sum' , 'stake' : 'sum' , 'odds' : 'mean' }) ) trackDistanceBarrier .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track race_distance barrier_group back_npl lay_npl stake odds 0 Albany 1000 inside 11.2550 -11.95 2 15.450000 1 Albany 1000 mid_field -5.0000 4.75 5 101.136000 2 Albany 1000 outside -5.0000 4.75 5 88.374000 3 Albany 1100 inside -3.0525 2.70 6 29.430000 4 Albany 1100 mid_field -6.4040 5.92 9 37.483333 ... ... ... ... ... ... ... ... 6325 York 1500 inside 1.8995 -2.41 6 41.195000 6326 York 1500 mid_field -7.0000 6.65 7 32.472857 6327 York 1920 inside -3.0000 2.85 3 21.883333 6328 York 1920 mid_field -0.3520 -0.04 5 20.978000 6329 York 1920 outside -2.0000 1.90 2 21.450000 6330 rows \u00d7 7 columns So it looks like over 2 selections jumping from the inside 3 barriers at Albany 1000m you would have made a healthy profit if you'd decide to back them historically. Let's use our lense of statistical significance to view these profit figures trackDistanceBarrier = ( trackDistanceBarrier . assign ( backPL_pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'back_npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) . assign ( layPL_pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'lay_npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) ) trackDistanceBarrier /home/tmbish/.local/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in sqrt result = getattr(ufunc, method)(*inputs, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track race_distance barrier_group back_npl lay_npl stake odds backPL_pValue layPL_pValue 0 Albany 1000 inside 11.2550 -11.95 2 15.450000 0.487280 1.000000 1 Albany 1000 mid_field -5.0000 4.75 5 101.136000 1.000000 0.885995 2 Albany 1000 outside -5.0000 4.75 5 88.374000 1.000000 0.877954 3 Albany 1100 inside -3.0525 2.70 6 29.430000 0.754412 0.869397 4 Albany 1100 mid_field -6.4040 5.92 9 37.483333 0.532857 0.804366 ... ... ... ... ... ... ... ... ... ... 6325 York 1500 inside 1.8995 -2.41 6 41.195000 0.918934 0.849635 6326 York 1500 mid_field -7.0000 6.65 7 32.472857 1.000000 0.755643 6327 York 1920 inside -3.0000 2.85 3 21.883333 1.000000 0.816546 6328 York 1920 mid_field -0.3520 -0.04 5 20.978000 0.972659 0.996987 6329 York 1920 outside -2.0000 1.90 2 21.450000 1.000000 0.863432 6330 rows \u00d7 9 columns So as you can see, whilst having a back POT of nearly 500% because the results were generated over 2 runners at quite high odds the p value (50%) suggest that it's quite likely we could have seen these exact results due to randomness, which is very intuitive. Let's have a look to see if there's any statistically significant edge to be gained on the lay side # Top 5 lay combos Track | Distance | Barrier (TDB) TDB_bestLay = trackDistanceBarrier . query ( 'lay_npl>0' ) . sort_values ( 'layPL_pValue' ) . head ( 5 ) TDB_bestLay .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track race_distance barrier_group back_npl lay_npl stake odds backPL_pValue layPL_pValue 188 Ascot 1000 inside -83.6395 77.06 115 24.616870 0.003054 0.248157 3619 Moonee Valley 1200 inside -52.2195 48.81 64 17.725313 0.000565 0.254399 6299 Yeppoon 1400 inside -11.0000 10.45 11 6.022727 1.000000 0.289686 959 Caulfield 1400 mid_field -74.3150 67.45 114 24.828772 0.018780 0.301137 1366 Darwin 1200 mid_field -47.3980 43.94 64 19.354531 0.009844 0.318178 So despite high lay POT none of these angles suggest an irrefutablely profitable angle laying these combinations. However, that doesn't mean we shouldn't test them on our out of sample set of races. These are our statistically most promising examples, we'll just take the top 5 for now and see how we would have performed if we had of started betting them on April first 2021. Keep in mind this should give us a pretty good indication of what we could get over the next 3 months into the future if we started today because we haven't contaminated/leaked any data from the post April period into our angle formulation. # First let's test laying on the train set (by definition we know these will be profitable) train_TDB_bestLay = ( dfTrain . merge ( TDB_bestLay [[ 'track' , 'race_distance' ]]) . assign ( npl = lambda x : x [ 'lay_npl' ]) . assign ( stake = 1 ) . assign ( win = lambda x : np . where ( x [ 'lay_npl' ] > 0 , 1 , 0 )) ) # This is the key test (non of the races has been part of analysis to this point) test_TDB_bestLay = ( dfTest . merge ( TDB_bestLay [[ 'track' , 'race_distance' ]]) . assign ( npl = lambda x : x [ 'lay_npl' ]) . assign ( stake = 1 ) . assign ( win = lambda x : np . where ( x [ 'lay_npl' ] > 0 , 1 , 0 )) ) # Peaking at the bets in the test set test_TDB_bestLay [[ 'track' , 'race_distance' , 'barrier' , 'barrier_group' , 'bsp' , 'lay_npl' , 'win' , 'stake' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track race_distance barrier barrier_group bsp lay_npl win stake 0 Ascot 1000 4 mid_field 11.08 0.95 1 1 1 Ascot 1000 11 outside 5.41 0.95 1 1 2 Ascot 1000 1 inside 4.73 -3.73 0 1 3 Ascot 1000 5 mid_field 7.35 0.95 1 1 4 Ascot 1000 10 outside 4.97 0.95 1 1 ... ... ... ... ... ... ... ... ... 219 Darwin 1200 10 outside 18.31 0.95 1 1 220 Darwin 1200 8 mid_field 42.00 0.95 1 1 221 Darwin 1200 6 mid_field 29.91 0.95 1 1 222 Darwin 1200 11 outside 6.60 -5.60 0 1 223 Darwin 1200 7 mid_field 5.74 0.95 1 1 224 rows \u00d7 8 columns # Let's run our evaluation on the training set bet_eval_metrics ( train_TDB_bestLay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 98.1 1047.0 0.892073 0.093696 # And on the test set bet_eval_metrics ( test_TDB_bestLay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 18.44 224.0 0.870536 0.082321 That's promising results. Our test set shows similar betting performance as our training set and we're still seeing a profitble trend. These are lay strategies so they aren't as robust as backing strategies as your profit distribution is lots of small wins and some large losses, but this is potentially a profitble betting angle!","title":"2.4 Angle 1: Track | Distance | Barrier"},{"location":"historicData/automatedBettingAnglesTutorial/#25-angle-2-jockeys-market-opinion","text":"Moving up slightly in level of difficulty our angles could include different kinds of reference points. Jockeys seem to be a divisive form factor in thoroughbred racing, and their quality can be hard to isolate relative to the quality of the horse and its preperation etc. I'm going to look at isolating jockeys that are either favoured or unfavoured by the market to see if I can formulate a betting angle that could generate me expected profit. The metric I'm going to use to determine market favour will be the ratio between back price 5 minutes before the scheduled jump and 30 seconds before the scheduled jump. Plotting this ratio for jockeys in our training set we can see which jockeys tend to have high market support by a high ratio (horses they are riding tend to shorten before the off) ( dfTrain . assign ( market_support = lambda x : x [ 'wap_5m' ] / x [ 'wap_30s' ]) . assign ( races = 1 ) . groupby ( 'jockey' ) . agg ({ 'market_support' : 'mean' , 'races' : 'count' }) . query ( 'races > 10' ) . sort_values ( 'market_support' , ascending = False ) . head () ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_support races jockey Scott Sheargold 1.133095 192 Lorelle Crow 1.056582 106 Chris Mc Carthy 1.051022 26 Anthony Darmanin 1.048931 142 James Winks 1.048893 12 Bob El-Issa 1.046756 196 Elyce Smith 1.043593 164 Jessica Gray 1.043376 108 Paul Francis Hamblin 1.042248 61 Alana Livesey 1.042188 32 Next, let's split the sample of each jockey's races between two scenarios a) the market firmed for their horse b) their horse drifted in the market in the last 5 minutes of trading. We then calculate the same summary table of inputs (profit, average odds etc) for backing these jockeys at the BSP given some market move. We can then feed these metrics into our statistical significance test to get an idea of the profitability of each combination. # Group By Jockey and Market Support jockeys = ( dfTrain . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . groupby ([ 'jockey' , 'market_support' ], as_index = False ) . agg ({ 'odds' : 'mean' , 'stake' : 'sum' , 'npl' : 'sum' }) . assign ( pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) ) jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } jockey market_support odds stake npl pValue 624 K Jennings Y 18.106118 85 178.6955 0.005643 496 Jade Darose Y 87.343333 39 579.4265 0.008942 263 Clayton Gallagher Y 24.225338 148 226.7145 0.012994 906 Ms T Harrison Y 27.944125 160 241.3065 0.018095 615 Justin P Stanley N 13.084502 231 155.7305 0.019913 802 Michael Dee N 36.338213 263 299.7255 0.031634 753 Madeleine Wishart Y 25.249872 78 156.5065 0.033329 433 Hannah Fitzgerald Y 32.171944 72 170.1830 0.045334 937 Nick Heywood N 17.172857 98 111.8905 0.049176 745 M Pateman N 22.690808 260 189.2050 0.052283 You can think of each of these scenarios representing different cases. If profitable: - Under market support this could indicate the jockey is being correctly favoured to maximise their horse's chances of winning the race or perhaps even some kind of insider knowledge coming out of certain stables - Under market drift this could indicate some incorrect skepticism about the jockeys ability and thus their horse has been overlayed Either way we're interested to see how these combinations would perform paper trading in our out of sample set # First evaluate on our training set train_jockeyMarket = ( dfTrain . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . merge ( jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 )[[ 'jockey' , 'market_support' ]]) . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] > 0 , 1 , 0 )) ) # And on the test set test_jockeyMarket = ( dfTest . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . merge ( jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 )[[ 'jockey' , 'market_support' ]]) . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] > 0 , 1 , 0 )) ) bet_eval_metrics ( train_jockeyMarket ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 2309.384 1434.0 0.154812 1.610449 bet_eval_metrics ( test_jockeyMarket ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 36.329 375.0 0.109333 0.096877 You can see overfitting in full effect here with the train set performance. However, our out-of-sample performance is still decently profitable. We might have found another profitable betting angle! It's worth noting that implementing this strategy would be slightly more complex than implementing our first strategy. Our code (or third party tool) would need to be able to check whether the market had firmed between 2 distinct time points before the jump of the race and cross reference that with the jockey name. Trivial for someone who is comfortable with bet placement and the betfair API but a little more involved for the uninitiated. It's important to formulate angles that you would know how and are capable of implementing.","title":"2.5 Angle 2: Jockeys + Market Opinion"},{"location":"historicData/automatedBettingAnglesTutorial/#26-angle-3-backing-to-lay","text":"Now let's try to use some of our inplay price data we extracted from the stream files. I'm interested in testing some back-to-lay strategies where a horse is backed preplay with the intention to get some tradeout lay order filled during the race. The types of scenarios where this could be conceivably profitable would be on certain kinds of horses or jockeys that show promise or strength early in the race but generally fade late and might not convert those early advantages often. Things we could look at here are: - Horses that typically trade lower than their preplay odds but don't win often - Jockeys that typically trade lower than their preplay odds but don't win often - Certain combinations of jockey / trainer / horse / race distance that meet these criteria # First Investigate The Average Inplay Minimums And Loss Rates of Certain Jockeys tradeOutIndex = ( dfTrain . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . assign ( inplay_odds_ratio = lambda x : x [ 'inplay_min_lay' ] / x [ 'bsp' ]) . assign ( win = lambda x : np . where ( x [ 'place' ] == 1 , 1 , 0 )) . assign ( races = lambda x : 1 ) . groupby ([ 'jockey' ], as_index = False ) . agg ({ 'inplay_odds_ratio' : 'mean' , 'win' : 'mean' , 'races' : 'sum' }) . sort_values ( 'inplay_odds_ratio' ) . query ( 'races >= 5' ) ) tradeOutIndex .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } jockey inplay_odds_ratio win races 291 John Rudd 0.352796 0.000000 8 457 Natalie M Morton 0.357216 0.142857 7 451 Murray Henderson 0.455943 0.166667 6 92 Bridget Grylls 0.474635 0.000000 11 431 Ms Heather Poland 0.478529 0.000000 5 ... ... ... ... ... 438 Ms K Stanley 0.898819 0.000000 21 619 Yasuhiro Nishitani 0.902459 0.043478 23 99 Cameron Quilty 0.907503 0.000000 20 87 Brett Fliedner 0.923814 0.000000 20 169 Desiree Stra 0.949329 0.000000 5 558 rows \u00d7 4 columns Ok so what we have here is a list of all jockeys with over 5 races on long and mid-long race distance groups (over 1800m) ordered by their average ratio of inplay minimum traded price compared with their jump price. If this trend is predictive we could assume that these jockeys tend to have an agressive race style and like to get out and lead the race. We'd like to capitalise on that race style by backing the jockeys pre-play and putting in a lay order which we'll leave inplay hoping to get matched during the race. For simplicity let's just assume we're flat staking on both sides so that our payoff profile looks like this: - Horse never trades at <50% of it's BSP our lay bet never get's matched and we lose 1 unit - Horse trades at <50% of it's BSP but loses (our lay bet gets filled) we're breakeven for the market - Horse trades wins (our lay bet get's filled) and we profit on our back bet and lose our lay bet so our profit is: (BSP-1) - (0.5*BSP-1) Let's run this backtest on the top 20 jockeys in our tradeOutIndex dataframe to see how we'd perform on the train and test set. targetTradeoutFraction = 0.5 train_JockeyBackToLay = ( dfTrain . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . merge ( tradeOutIndex . head ( 20 )[ 'jockey' ]) . assign ( npl = lambda x : np . where ( x [ 'inplay_min_lay' ] <= targetTradeoutFraction * x [ 'bsp' ], np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'bsp' ] - 1 - ( 0.5 * x [ 'bsp' ] - 1 )), 0 ), - 1 )) . assign ( stake = lambda x : np . where ( x [ 'npl' ] != - 1 , 2 , 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] >= 0 , 1 , 0 )) ) bet_eval_metrics ( train_JockeyBackToLay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 23.797 671.0 0.5181 0.035465 test_JockeyBackToLay = ( dfTest . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . merge ( tradeOutIndex . head ( 20 )[ 'jockey' ]) . assign ( npl = lambda x : np . where ( x [ 'inplay_min_lay' ] <= targetTradeoutFraction * x [ 'bsp' ], np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'bsp' ] - 1 - ( 0.5 * x [ 'bsp' ] - 1 )), 0 ), - 1 )) . assign ( stake = lambda x : np . where ( x [ 'npl' ] != - 1 , 2 , 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] >= 0 , 1 , 0 )) ) bet_eval_metrics ( test_JockeyBackToLay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } npl stake win pot 0 45.62475 255.0 0.342105 0.178921 Not bad! Looks like we found another possibly promising lead. Again it's worth noting that this is probably another step up in implementation complexity again from previous angles. It's not very hard when you're familiar with betfair order types and placing them through the API but it requires some additional API savviness. But the documentation is quite good and there's plenty of resources available online to help you understand how to automate something like this.","title":"2.6 Angle 3: Backing To Lay"},{"location":"historicData/automatedBettingAnglesTutorial/#30-conclusion","text":"This analysis is just a sketch. Hopefully it helps inspire you to think about what kinds of betting angles you could test for a sport or racing code you're interested in. It should give you a framework for thinking about this kind of automated betting, and how it differs from fundamental modelling. It should also give you a few tricks for coming up with your own angles and testing them with the rigour needed to have any realistic expectations of profit. Most of the betting angles you're sold are faulty or have long evaporated from the market by people long before you even knew the rules of the sport. You'll need to be creative and scientific to create your own profitable betting angles, but it's certainly worth it to try.","title":"3.0 Conclusion"},{"location":"historicData/automatedBettingAnglesTutorial/#complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github import requests import pandas as pd from datetime import date , timedelta import numpy as np import os import re import tarfile import zipfile import bz2 import glob import logging import yaml from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) from scipy.stats import t import plotly.express as px # Utility Functions # + Stream Parsing # + Betfair Race Data Scraping # + Various utilities # _________________________________ def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None def slicePrice ( l , n ): try : x = l [ n ] . price except : x = np . nan return ( x ) def sliceSize ( l , n ): try : x = l [ n ] . size except : x = np . nan return ( x ) def wapPrice ( l , n ): try : x = round ( sum ( [ rung . price * rung . size for rung in l [ 0 :( n - 1 )] ] ) / sum ( [ rung . size for rung in l [ 0 :( n - 1 )] ]), 2 ) except : x = np . nan return ( x ) def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) # Core Execution Fucntions # _________________________________ def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): evaluate_market = None prev_market = None postplay = None preplay = None t5m = None t30s = None inplay_min_lay = None gen = s . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 30 seconds before scheduled off if t30s is None and seconds_to_start < 30 : t30s = market_book # Market at 5 mins before scheduled off if t5m is None and seconds_to_start < 5 * 60 : t5m = market_book # Manage Inplay Vectors if market_book . inplay : if inplay_min_lay is None : inplay_min_lay = [ slicePrice ( runner . ex . available_to_lay , 0 ) for runner in market_book . runners ] else : inplay_min_lay = np . fmin ( inplay_min_lay , [ slicePrice ( runner . ex . available_to_lay , 0 ) for runner in market_book . runners ]) # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay is not None and preplay is None : preplay = postplay inplay_min_lay = [ \"\" for runner in market_book . runners ] return ( t5m , t30s , preplay , postplay , inplay_min_lay , prev_market ) # Final market is last prev_market def parse_stream ( stream_files , output_file ): with open ( output_file , \"w+\" ) as output : output . write ( \"market_id,selection_id,selection_name,wap_5m,wap_30s,bsp,ltp,traded_vol,inplay_min_lay \\n \" ) for file_obj in load_markets ( stream_files ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) ( t5m , t30s , preplay , postplay , inplayMin , final ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay is None or final is None or t30s is None : continue ; # All runner removed if all ( runner . status == \"REMOVED\" for runner in final . runners ): continue runnerMeta = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final . runners ] ltp = [ runner . last_price_traded for runner in preplay . runners ] tradedVol = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay . runners ] wapBack30s = [ wapPrice ( runner . ex . available_to_back , 3 ) for runner in t30s . runners ] wapBack5m = [ wapPrice ( runner . ex . available_to_back , 3 ) for runner in t5m . runners ] # Writing To CSV # ______________________ for ( runnerMeta , ltp , tradedVol , inplayMin , wapBack5m , wapBack30s ) in zip ( runnerMeta , ltp , tradedVol , inplayMin , wapBack5m , wapBack30s ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final . market_id ), runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], wapBack5m , wapBack30s , runnerMeta [ 'sp' ], ltp , round ( tradedVol ), inplayMin ) ) def get_bf_markets ( dte ): url = 'https://apigateway.betfair.com.au/hub/racecard?date= {} ' . format ( dte ) responseJson = requests . get ( url ) . json () marketList = [] for meeting in responseJson [ 'MEETINGS' ]: for markets in meeting [ 'MARKETS' ]: marketList . append ( { 'date' : dte , 'track' : meeting [ 'VENUE_NAME' ], 'country' : meeting [ 'COUNTRY' ], 'race_type' : meeting [ 'RACE_TYPE' ], 'race_number' : markets [ 'RACE_NO' ], 'market_id' : str ( '1.' + markets [ 'MARKET_ID' ]), 'start_time' : markets [ 'START_TIME' ] } ) marketDf = pd . DataFrame ( marketList ) return ( marketDf ) def get_bf_race_meta ( market_id ): url = 'https://apigateway.betfair.com.au/hub/raceevent/ {} ' . format ( market_id ) responseJson = requests . get ( url ) . json () if 'error' in responseJson : return ( pd . DataFrame ()) raceList = [] for runner in responseJson [ 'runners' ]: if 'isScratched' in runner and runner [ 'isScratched' ]: continue # Jockey not always populated try : jockey = runner [ 'jockeyName' ] except : jockey = \"\" # Place not always populated try : placeResult = runner [ 'placedResult' ] except : placeResult = \"\" # Place not always populated try : trainer = runner [ 'trainerName' ] except : trainer = \"\" raceList . append ( { 'market_id' : market_id , 'weather' : responseJson [ 'weather' ], 'track_condition' : responseJson [ 'trackCondition' ], 'race_distance' : responseJson [ 'raceLength' ], 'selection_id' : runner [ 'selectionId' ], 'selection_name' : runner [ 'runnerName' ], 'barrier' : runner [ 'barrierNo' ], 'place' : placeResult , 'trainer' : trainer , 'jockey' : jockey , 'weight' : runner [ 'weight' ] } ) raceDf = pd . DataFrame ( raceList ) return ( raceDf ) def scrape_thoroughbred_bf_date ( dte ): markets = get_bf_markets ( dte ) if markets . shape [ 0 ] == 0 : return ( pd . DataFrame ()) thoMarkets = markets . query ( 'country == \"AUS\" and race_type == \"R\"' ) if thoMarkets . shape [ 0 ] == 0 : return ( pd . DataFrame ()) raceMetaList = [] for market in thoMarkets . market_id : raceMetaList . append ( get_bf_race_meta ( market )) raceMeta = pd . concat ( raceMetaList ) return ( markets . merge ( raceMeta , on = 'market_id' )) # Execute Data Pipeline # _________________________________ # Description: # Will loop through a set of dates (starting July 2020 in this instance) and return race metadata from betfair # Estimated Time: # ~60 mins # # if __name__ == '__main__': # dataList = [] # dateList = pd.date_range(date(2020,7,1),date.today()-timedelta(days=1),freq='d') # for dte in dateList: # dte = dte.date() # print(dte) # races = scrapeThoroughbredBfDate(dte) # dataList.append(races) # data = pd.concat(dataList) # data.to_csv(\"[LOCAL PATH SOMEWHERE]\", index=False) # Description: # Will loop through a set of stream data archive files and extract a few key pricing measures for each selection # Estimated Time: # ~6 hours # # trading = betfairlightweight.APIClient(\"username\", \"password\") # listener = StreamListener(max_latency=None) # stream_files = glob.glob(\"[PATH TO LOCAL FOLDER STORING ARCHIVE FILES]*.tar\") # output_file = \"[SOME OUTPUT DIRECTORY]/thoroughbred-odds-2021.csv\" # if __name__ == '__main__': # parse_stream(stream_files, output_file) # Analysis # _________________________________ # Functions ++++++++ def bet_eval_metrics ( d , side = False ): metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" , \"win\" : \"mean\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) def pl_pValue ( number_bets , npl , stake , average_odds ): pot = npl / stake tStatistic = ( pot * np . sqrt ( number_bets )) / np . sqrt ( ( 1 + pot ) * ( average_odds - 1 - pot ) ) pValue = 2 * t . cdf ( - abs ( tStatistic ), number_bets - 1 ) return ( np . where ( np . logical_or ( np . isnan ( pValue ), pValue == 0 ), 1 , pValue )) def distance_group ( distance ): if distance is None : return ( \"missing\" ) elif distance < 1100 : return ( \"sprint\" ) elif distance < 1400 : return ( \"mid_short\" ) elif distance < 1800 : return ( \"mid_long\" ) else : return ( \"long\" ) def barrier_group ( barrier ): if barrier is None : return ( \"missing\" ) elif barrier < 4 : return ( \"inside\" ) elif barrier < 9 : return ( \"mid_field\" ) else : return ( \"outside\" ) # Analysis ++++++++ # Local Paths (will be different on your machine) path_odds_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-odds-2021.csv\" path_race_local = \"[PATH TO YOUR LOCAL FILES]/thoroughbred-race-data.csv\" odds = pd . read_csv ( path_odds_local , dtype = { 'market_id' : object , 'selection_id' : object }) race = pd . read_csv ( path_race_local , dtype = { 'market_id' : object , 'selection_id' : object }) # Joining two datasets df = race . merge ( odds . loc [:, odds . columns != 'selection_name' ], how = \"inner\" , on = [ 'market_id' , 'selection_id' ]) # I'll also add columns for the net profit from backing and laying each selection to be picked up in subsequent sections df [ 'back_npl' ] = np . where ( df [ 'place' ] == 1 , 0.95 * ( df [ 'bsp' ] - 1 ), - 1 ) df [ 'lay_npl' ] = np . where ( df [ 'place' ] == 1 , - 1 * ( df [ 'bsp' ] - 1 ), 0.95 ) # Adding Variable Chunks df [ 'distance_group' ] = pd . to_numeric ( df . race_distance , errors = \"coerce\" ) . apply ( distance_group ) df [ 'barrier_group' ] = pd . to_numeric ( df . barrier , errors = \"coerce\" ) . apply ( barrier_group ) # Data Partitioning dfTrain = df . query ( 'date < \"2021-04-01\"' ) dfTest = df . query ( 'date >= \"2021-04-01\"' ) ' {} rows in the \"training\" set and {} rows in the \"test\" data' . format ( dfTrain . shape [ 0 ], dfTest . shape [ 0 ]) # Angle 1 ++++++++++++++++++++++++++++++++++++++++++++++ ( dfTrain . assign ( stake = 1 ) . groupby ( 'selection_name' , as_index = False ) . agg ({ 'back_npl' : 'sum' , 'stake' : 'sum' }) . assign ( pot = lambda x : x [ 'back_npl' ] / x [ 'stake' ]) . sort_values ( 'pot' , ascending = False ) . head ( 3 ) ) # Calculate the profit (back and lay) and average odds across all track / distance / barrier group combos trackDistanceBarrier = ( dfTrain . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . groupby ([ 'track' , 'race_distance' , 'barrier_group' ], as_index = False ) . agg ({ 'back_npl' : 'sum' , 'lay_npl' : 'sum' , 'stake' : 'sum' , 'odds' : 'mean' }) ) trackDistanceBarrier trackDistanceBarrier = ( trackDistanceBarrier . assign ( backPL_pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'back_npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) . assign ( layPL_pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'lay_npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) ) trackDistanceBarrier # Top 5 lay combos Track | Distance | Barrier (TDB) TDB_bestLay = trackDistanceBarrier . query ( 'lay_npl>0' ) . sort_values ( 'layPL_pValue' ) . head ( 5 ) TDB_bestLay # First let's test laying on the train set (by definition we know these will be profitable) train_TDB_bestLay = ( dfTrain . merge ( TDB_bestLay [[ 'track' , 'race_distance' ]]) . assign ( npl = lambda x : x [ 'lay_npl' ]) . assign ( stake = 1 ) . assign ( win = lambda x : np . where ( x [ 'lay_npl' ] > 0 , 1 , 0 )) ) # This is the key test (non of the races has been part of analysis to this point) test_TDB_bestLay = ( dfTest . merge ( TDB_bestLay [[ 'track' , 'race_distance' ]]) . assign ( npl = lambda x : x [ 'lay_npl' ]) . assign ( stake = 1 ) . assign ( win = lambda x : np . where ( x [ 'lay_npl' ] > 0 , 1 , 0 )) ) # Peaking at the bets in the test set test_TDB_bestLay [[ 'track' , 'race_distance' , 'barrier' , 'barrier_group' , 'bsp' , 'lay_npl' , 'win' , 'stake' ]] # Let's run our evaluation on the training set bet_eval_metrics ( train_TDB_bestLay ) # And on the test set bet_eval_metrics ( test_TDB_bestLay ) # Angle 2 ++++++++++++++++++++++++++++++++++++++++++++++ ( dfTrain . assign ( market_support = lambda x : x [ 'wap_5m' ] / x [ 'wap_30s' ]) . assign ( races = 1 ) . groupby ( 'jockey' ) . agg ({ 'market_support' : 'mean' , 'races' : 'count' }) . query ( 'races > 10' ) . sort_values ( 'market_support' , ascending = False ) . head () ) # Group By Jockey and Market Support jockeys = ( dfTrain . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . groupby ([ 'jockey' , 'market_support' ], as_index = False ) . agg ({ 'odds' : 'mean' , 'stake' : 'sum' , 'npl' : 'sum' }) . assign ( pValue = lambda x : pl_pValue ( number_bets = x [ 'stake' ], npl = x [ 'npl' ], stake = x [ 'stake' ], average_odds = x [ 'odds' ])) ) jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 ) # First evaluate on our training set train_jockeyMarket = ( dfTrain . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . merge ( jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 )[[ 'jockey' , 'market_support' ]]) . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] > 0 , 1 , 0 )) ) # And on the test set test_jockeyMarket = ( dfTest . assign ( market_support = lambda x : np . where ( x [ 'wap_5m' ] > x [ 'wap_30s' ], \"Y\" , \"N\" )) . merge ( jockeys . sort_values ( 'pValue' ) . query ( 'npl > 0' ) . head ( 10 )[[ 'jockey' , 'market_support' ]]) . assign ( stake = 1 ) . assign ( odds = lambda x : x [ 'bsp' ]) . assign ( npl = lambda x : np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'odds' ] - 1 ), - 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] > 0 , 1 , 0 )) ) bet_eval_metrics ( train_jockeyMarket ) bet_eval_metrics ( test_jockeyMarket ) # Angle 3 ++++++++++++++++++++++++++++++++++++++++++++++ # First Investigate The Average Inplay Minimums And Loss Rates of Certain Jockeys tradeOutIndex = ( dfTrain . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . assign ( inplay_odds_ratio = lambda x : x [ 'inplay_min_lay' ] / x [ 'bsp' ]) . assign ( win = lambda x : np . where ( x [ 'place' ] == 1 , 1 , 0 )) . assign ( races = lambda x : 1 ) . groupby ([ 'jockey' ], as_index = False ) . agg ({ 'inplay_odds_ratio' : 'mean' , 'win' : 'mean' , 'races' : 'sum' }) . sort_values ( 'inplay_odds_ratio' ) . query ( 'races >= 5' ) ) tradeOutIndex targetTradeoutFraction = 0.5 train_JockeyBackToLay = ( dfTrain . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . merge ( tradeOutIndex . head ( 20 )[ 'jockey' ]) . assign ( npl = lambda x : np . where ( x [ 'inplay_min_lay' ] <= targetTradeoutFraction * x [ 'bsp' ], np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'bsp' ] - 1 - ( 0.5 * x [ 'bsp' ] - 1 )), 0 ), - 1 )) . assign ( stake = lambda x : np . where ( x [ 'npl' ] != - 1 , 2 , 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] >= 0 , 1 , 0 )) ) bet_eval_metrics ( train_JockeyBackToLay ) test_JockeyBackToLay = ( dfTest . query ( 'distance_group in [\"long\", \"mid_long\"]' ) . merge ( tradeOutIndex . head ( 20 )[ 'jockey' ]) . assign ( npl = lambda x : np . where ( x [ 'inplay_min_lay' ] <= targetTradeoutFraction * x [ 'bsp' ], np . where ( x [ 'place' ] == 1 , 0.95 * ( x [ 'bsp' ] - 1 - ( 0.5 * x [ 'bsp' ] - 1 )), 0 ), - 1 )) . assign ( stake = lambda x : np . where ( x [ 'npl' ] != - 1 , 2 , 1 )) . assign ( win = lambda x : np . where ( x [ 'npl' ] >= 0 , 1 , 0 )) ) bet_eval_metrics ( test_JockeyBackToLay )","title":"Complete code"},{"location":"historicData/automatedBettingAnglesTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"historicData/backtestingRatingsTutorial/","text":"Backtesting wagering models with Betfair JSON stream data Workshop This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the JSON to CSV tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo . You can watch our workshop working through this tutorial on YouTube. Set up I'm going to be using a jupyter notebook for this investigation which is a special type of data analysis output that is used to combine code, outputs and explanatory text in a readable single document. It's mostly closely associated with python data analysis code which is the language I'll be using here also. The entire body of python code used will be repeated at the bottom of the article where you can copy it and repurpose it for yourself. If you're not familiar with python, don't worry neither am I really! I'm inexperienced with python so if you have experience with some other programming language you should be able to follow along with the logic here too. If you don't have experience using another programming language this all might appear intimidating but it's heavy on the explanatory text so you should get something out of it. We need a few non standard python libraries so make sure to install betfairlightweight and plotly before you get started with something like pip install betfairlightweight & pip install plotly . We'll load all our libraries and do some setup here. import pandas as pd import numpy as np import requests from datetime import date , timedelta import os import re import tarfile import zipfile import bz2 import glob import logging from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import plotly.express as px import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) % config IPCompleter . greedy = True Context Backtesting is the life-blood of most successful wagering systems. In short it attempts to answer a single question for you: \ud835\udf0f : How much money will I win or lose if I started using this system to place bets with real money? Without a rigorous and quantitative backtesting approach it's really quite hard to estimate the answer to this question $ \\tau $ that will be even reliably on the right side of zero. You could live test your system with real bets at small stakes, however, this isn't the panacea it seems. It will take time (more than you think) for your results to converge to their long term expectation. How long? Answering this question will require some expertise with probability and statistics you might not have. Even more than that though is that depending on where you're betting your results at small stakes could be very different than at larger stakes. You might not be able get a good answer to $ \\tau $ until betting at full stakes at which point finding the answer might coincide with blowing up your gambling bankroll. Backtesting is also very hard. To perfectly backtest your own predicted probability on a historical race or sporting match you need to produce 2 things: (1) What would my predicted chance have been exactly for this selection in this market on this day in the past? (2) What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction? The devil in the detail of backtesting tends to be in those exactlys. The aim of the backtesting game is answering (2) as accurately as possible because it tells you exactly how much you would have made over your backtesting period, from there you can confidently project that rate of profitability forward. It's easy to make mistakes and small errors in the quantitative reasoning can lead you to extremely misguided projections downstream. Question (1) won't be in the scope of this notebook but it's equally (and probably more) important that (2) but it is the key challenge of all predictive modelling exercises so there's plenty of discussion about it elsewhere. Backtesting on Betfair Answering question (2) for betting on the Betfair Exchange is difficult. The Exchange is a dynamic system that changes from one micro second to the next. What number should you use for odds? How much could you assume to get down at those odds? The conventional and easiest approach is to backtest at the BSP. The BSP is simple because it's a single number (to use for both back and lay bets) and is a taken price (there's no uncertainty about getting matched). Depending on the liquidity of the market a reasonably sized stake might also not move the BSP very much. For some markets you may be able to safely assume you could be $10s of dollars at the BSP without moving it an inch. However, that's definitely not true of all BSP markets and you need to be generally aware that your Betfair orders in the future will change the state of the exchange, and large bets will move the BSP in an unfavourable direction. Aside from uncertainty around the liquidity and resilience of the BSP, many many markets don't have a BSP. So what do we do then? Typically what a lot of people (who have a relationship with Betfair Australia) do at this point is request a data dump. They might request an odds file for all Australian harness race win markets since June 2018 with results and 4 different price points: the BSP, the last traded price, the weighted average price (WAP) traded in 3 minutes before the race starts, and the WAP for all bets matched prior to 3 mins before the race. However, you will likely need to be an existing VIP customer to get this file and it's not a perfect solution: it might take 2 weeks to get, you can't refresh it, you can't test more hypothetical price points after your initial analysis amongst many other problems. What if you could produce this valuable data file yourself? Betfair Stream Data Betfair's historical stream data is an extremely rich source of data. However, in it's raw form it's difficult to handle for the uninitiated. It also might not be immediately obvious how many different things this dataset could be used for without seeing some examples. These guides will hopefully demystify how to turn this raw data into a familiar and usable format whilst also hopefully providing some inspiration for the kinds of value that can be excavated from it. This example: backtesting Betfair Hub thoroughbred model To illustrate how you can use the stream files to backtest the outputs of a rating system we'll use the Australian Thoroughbred Rating model available on the Betfair Hub. The most recent model iteration only goes back till Feb 28th 2021 however as an illustrative example this is fine. We'd normally want to backtest with a lot more historical data than this, which just means in this case our estimation of future performance will be unreliable. I'm interested to see how we would have fared betting all selections rated by this model according to a few different staking schemes and also at a few different times / price points. Old ratings If you want to pull in ratings from before Feb 2021 to add to your database for more complete backtesting these are available in a data dump here . Scrape The Model Ratings If you travel to the Betfair hub ratings page you'll find that URL links behind the ratings download buttons have a consistent URL pattern that looks very scrape friendly. We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe. # Function to return Pandas DF of hub ratings for a particular date def getHubRatings ( dte ): # Substitute the date into the URL url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date= {} presenter=RatingsPresenter&json=true' . format ( dte ) # Convert the response into JSON responseJson = requests . get ( url ) . json () hubList = [] if not responseJson : return ( None ) # Want an normalised table (1 row per selection) # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields for meeting in responseJson [ 'meetings' ]: for race in meeting [ 'races' ]: for runner in race [ 'runners' ]: hubList . append ( { 'date' : dte , 'track' : meeting [ 'name' ], 'race_number' : race [ 'number' ], 'race_name' : race [ 'name' ], 'market_id' : race [ 'bfExchangeMarketId' ], 'selection_id' : str ( runner [ 'bfExchangeSelectionId' ]), 'selection_name' : runner [ 'name' ], 'model_odds' : runner [ 'ratedPrice' ] } ) out = pd . DataFrame ( hubList ) return ( out ) # See the response from a single day getHubRatings ( date ( 2021 , 3 , 1 )) . head ( 5 ) date track race_number race_name market_id selection_id selection_name model_odds 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620052 1. Military Affair 6.44 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 5889703 3. Proverbial 21.11 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38177688 4. A Real Wag 9.97 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620053 5. El Jay 44.12 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 37263264 6. Flying Honour 3.39 dateDFList = [] dateList = pd . date_range ( date ( 2021 , 2 , 18 ), date . today () - timedelta ( days = 1 ), freq = 'd' ) for dte in dateList : dateDFList . append ( getHubRatings ( dte )) # Concatenate (add rows to rows) all the dataframes within the list hubRatings = pd . concat ( dateDFList ) hubRatings . shape ( 32519 , 8 ) Assembling the odds file So part 1 was very painless. This is how we like data: served by some API or available in a nice tabular format on a webpage ready to be scraped with standard tools available in popular languages. Unfortunately, it won't be so painless to assemble our odds file. We'll find out why it's tricky as we go. The Data The data we'll be using is the historical Exchange data available from this website. The data available through this service is called streaming JSON data. There are a few options available relating to granularity (how many time points per second the data updates at) but we'll be using the most granular \"PRO\" set which has updates every 50 milliseconds. Essentially what the data allows us to do is, for a particular market, recreate the exact state of the Betfair Exchange at say: 150 milliseconds before the market closed. When people say the state of the Exchange they mean two things a) what are all the current open orders on all the selections b) what are the current traded volumes on each selection at each price point. We obviously don't have access to any information about which accounts are putting up which prices and other things Betfair has themselves. We're essentially getting a snapshot of everything you can see through the website by clicking on each selection manually and looking at the graphs, tables and ladders. However, with just these 2 sets of information we can build a rich view of the dynamics of exchange and also build out all of the summary metrics (WAP etc) we might have previously needed Betfair to help with. For our purposes 50 milli-second intervaled data is huge overkill. But you could imagine needing this kind of granularity for other kinds of wagering systems - eg a high frequency trading algorithm of some sort that needs to make many decisions and actions every second. Let's take a look at what the stream data looks like for a single market: So it looks pretty intractable. For this particular market there's 14,384 lines of data where each line consists of a single JSON packet of data. If you're not a data engineer (neither am I) your head might explode thinking about how you could read this into your computer and transform it into something usable. The data looks like this because it is saved from a special Betfair API called the Stream API which which is used by high end Betfair API users and which delivers fast speeds other performance improvements over the normal \"polling\" API. Now what's good about that, for the purposes of our exercise, is that the very nice python package betfairlightweight has the functionality built to not only parse the Stream API when connected live but also these historical saved versions of the stream data. Without it we'd be very far away from the finish line, with betfairlightweight we're pretty close. Unpacking / flattening the data Because these files are so large and unprocessed this process won't look the same as your normal data ETL in python: where you can read a raw data file (csv, JSON, text etc.) into memory and use python functions to transform into usable format. I personally had no idea how to use python and betfairlightweight to parse these data until I saw Betfair's very instructive overview which you should read for a more detailed look at some of the below code. By my count there were 4 key conceptual components that I had to get my head around to understand and be able to re-purpose that code. So if you're like me (a bit confused by some of the steps in that piece) this explanation might help. I'll assume you don't do any decompression and keep the monthly PRO files as the .tar archives as they are. Conceptually the process looks something like this: Load the \"archives\" into a \"generator\" Scan across the generator (market_ids) and the market states within those markets to extract useful objects Process those useful objects to pull out some metadata + useful summary numbers derived from the available orders and traded volumes snapshot data Write this useful summarised data to a file that can be read and understood with normal data analysis workflows First we'll run a bunch of setup code setting up my libraries and creating some utility functions that will be used throughout the main parsing component. It'll also point to the two stream files I'll be parsing for this exercise. # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = StreamListener ( max_latency = None ) ### Utility Functions # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input sample: R6 1400m Grp1 parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) # creating a flag that is True when markets are australian thoroughbreds def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) 1: .tar load This function I stole from Betfair's instructional article The stream files are downloaded as .tar archive files which are a special kind of file that we'll need to unpack Instead of loading each file into memory this function returns a \"generator\" which is a special python object that is to be iterated over This basically means it contains the instructions to unpack and scan over files on the fly This function also contains the logic to deal with if these files are zip archives or you've manually unpacked the archive and have the .bz2 zipped files # loading from tar and extracting files def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None 2: Scan across market states and extract useful objects So this function will take a special \"stream\" object which we'll create with betfairlightweight The function takes a stream object input and returns 4 instances of the market state The market state 3 mins before the scheduled off The market state immediately before it goes inplay The market state immediately before it closes for settlement The final market state with outcomes It basically just loops over all the market states and has a few checks to determine if it should save the current market state as key variables and then returns those # Extract Components From Generated Stream def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book evaluate_market = None prev_market = None postplay_market = None preplay_market = None t3m_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 3 mins before scheduled off if t3m_market is None and seconds_to_start < 3 * 60 : t3m_market = market_book # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay_market is not None and preplay_market is None : preplay_market = postplay_market return ( t3m_market , preplay_market , postplay_market , prev_market ) # Final market is last prev_market 3 & 4: Summarise those useful objects and write to .csv This next chunk contains a wrapper function that will do all the execution It will open a csv output file Use the load_markets utility to iterate over the .tar files Use betfairlightweight to instantiate the special stream object Pass that stream object to the extract_components_from_stream which will scan across the market states and pull out 4 key market books Convert those marketbooks into simple summary numbers or dictionaries that will be written to the output .csv file def run_stream_parsing (): # Run Pipeline with open ( \"outputs/tho-odds.csv\" , \"w+\" ) as output : # Write Column Headers To File output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m \\n \" ) for file_obj in load_markets ( data_path ): # Instantiate a \"stream\" object stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) # Extract key components according to the custom function above (outputs 4 objects) ( t3m_market , preplay_market , postplay_market , final_market ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay_market is None : continue ; # Runner metadata and key fields available from final market book runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final_market . runners ] # Last Traded Price # _____________________ # From the last marketbook before inplay or close ltp = [ runner . last_price_traded for runner in preplay_market . runners ] # Total Matched Volume # _____________________ # Calculates the traded volume across all traded price points for each selection def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) selection_traded_volume = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay_market . runners ] # Top 3 Ladder # ______________________ # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries def top_3_ladder ( availableLadder ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : 3 ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"price\" ] = price out [ \"volume\" ] = volume return ( out ) # Sometimes t-3 mins market book is empty try : atb_ladder_3m = [ top_3_ladder ( runner . ex . available_to_back ) for runner in t3m_market . runners ] atl_ladder_3m = [ top_3_ladder ( runner . ex . available_to_lay ) for runner in t3m_market . runners ] except : atb_ladder_3m = {} atl_ladder_3m = {} # Writing To CSV # ______________________ for ( runnerMeta , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ) in zip ( runner_data , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final_market . market_id ), final_market . market_definition . market_time , final_market . market_definition . country_code , final_market . market_definition . venue , final_market . market_definition . name , runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], runnerMeta [ 'selection_status' ], runnerMeta [ 'sp' ], ltp , selection_traded_volume , '\"' + str ( atb_ladder_3m ) + '\"' , # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas '\"' + str ( atl_ladder_3m ) + '\"' ) ) # This will execute the files (it took me ~2 hours for 2 months of data) #run_stream_parsing() Extending this code Because this process is very slow you might want to save much more information than you think you need For example I currently think I only want the best back and lay prices at t-3 mins before the off but I've saved the top 3 boxes in the available to back and lay ladders as dictionary strings From these ladders I can retroactively calculate not only just the best back and lay prices but also WAP prices and also sizes at those boxes which I could use for much more accurate backtesting if I wanted to later without having to scan across the entire stream files again I could easily save the entire open and traded orders ladders in the same way amongst many other ways of retaining more of the data for post-processing analysis Backtesting Analysis Let's take stock of where we are. We currently have model ratings (about 1.5 months worth) and Betfair Odds (2 months worth). Circling back to the original backtesting context we needed to solve for 2 key questions: What would my predicted chance have been exactly for this selection in this market on this day in the past? What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction? Backtesting with someone else's publicly available and historically logged ratings solves question 1. With these particular ratings we're fine but generally we should just be aware there are some sketchy services that might make retroactive adjustments to historical ratings to juice their performance which obviously violates 1. For the second part we now have several real Betfair odds values to combine with the ratings and some chosen staking formula to simulate actual bets. I won't dwell too much on the stake size component but it's important. Similarly we aren't out of the woods with the \"what odds exactly\" question either. I'll show performance of backtesting at the \"Last Traded Price\" however, there's literally no way of actually being the last bet matched order on every exchange market so there's some uncertainty in a few of these prices. Further, and from experience, if you placing bets at the BSP and you're using some form of proportional staking (like Kelly) then you're calculated stake size will need to include a quantity (the BSP) which you will literally never be 100% sure of. You'll need to estimate the BSP as close to market suspension as you can and place your BSP bets with a stake sized derived from that estimation. This imprecision in stake calculation WILL cost you some profit relative to your backtested expectation. These might seem like minor considerations but you should be aware of some of the gory details of the many ways becoming successful on Betfair is really difficult. To be reliably profitable on Betfair you don't just need a good model, you'll likely need to spend hours and hours thinking about these things: testing things, ironing out all these little kinks and trying to account for all your uncertainties. I'll just be running over the skeleton of what you should do. Setting up your master data # First we'll load and tidy our odds data # Load in odds file we created above bfOdds = pd . read_csv ( \"outputs/tho-odds.csv\" , dtype = { 'market_id' : object , 'selection_id' : object , 'atb_ladder_3m' : object , 'atl_ladder_3m' : object }) # Convert dictionary columns import ast bfOdds [ 'atb_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atb_ladder_3m' ]] bfOdds [ 'atl_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atl_ladder_3m' ]] # Convert LTP to Numeric bfOdds [ 'ltp' ] = pd . to_numeric ( bfOdds [ 'ltp' ], errors = 'coerce' ) # Filter after 18th Feb bfOdds = bfOdds . query ( 'event_date >= \"2021-02-18\"' ) bfOdds . head ( 5 ) market_id event_date country track market_name selection_id selection_name result bsp ltp matched_volume atb_ladder_3m atl_ladder_3m 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 31552374 2. Chubascos LOSER 5.84 5.9 7390.59 {'price': [6, 5.9, 5.8], 'volume': [30.99, 82.... {'price': [6.2, 6.4, 6.6], 'volume': [4.99, 22... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620171 3. Love You More LOSER 65.00 70.0 1297.27 {'price': [65, 60, 55], 'volume': [2, 2.9, 15.... {'price': [75, 80, 85], 'volume': [0.66, 3.24,... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620172 4. Splashing Rossa LOSER 10.98 10.5 2665.94 {'price': [9, 8.8, 8.6], 'volume': [21.92, 10.... {'price': [9.6, 9.8, 10], 'volume': [13.43, 7.... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620173 5. The Fairytale LOSER 54.56 50.0 221.13 {'price': [55, 50, 48], 'volume': [4.85, 2.85,... {'price': [65, 70, 75], 'volume': [2.1, 7.18, ... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620174 6. My Boy Dragon LOSER 166.90 160.0 199.00 {'price': [140, 120, 110], 'volume': [0.36, 1.... {'price': [260, 270, 340], 'volume': [1.29, 2.... When backtesting, and developing wagering systems more generally, I've found it really helpful to have a set of standard patterns or ways of representing common datasets. For a task like this it's really helpful to keep everything joined and together in a wide table. So we want a dataframe with everything we need to conduct the backtest: your model ratings, the odds you're betting at, the results on the bets, and ultimately betting logic will all become columns in a dataframe. It's helpful to have consistent column names so that the code for any new test you run looks much like previous tests and you can leverage custom functions that can be reused across tests and other projects. I like to have the following columns in my backtesting dataframe: date market_id (can be a surrogate id if dealing with fixed odds markets) selection_id (could be selection name) win (a binary win loss) model_odds model_prob market_odds market_prob bet_side stake gpl commission npl This analysis will be a little more complex as we're considering different price points so I'll leave out the market_odds and market_prob columns. # Joining the ratings data and odds data and combining rawDF = pd . merge ( hubRatings [ hubRatings [ 'market_id' ] . isin ( bfOdds . market_id . unique ())], bfOdds [[ 'market_name' , 'market_id' , 'selection_id' , 'result' , 'matched_volume' , 'bsp' , 'ltp' , 'atb_ladder_3m' , 'atl_ladder_3m' ]], on = [ 'market_id' , 'selection_id' ], how = 'inner' ) rawDF date track race_number race_name market_id selection_id selection_name model_odds market_name result matched_volume bsp ltp atb_ladder_3m atl_ladder_3m 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523320 11. Vast Kama 34.28 R1 1200m 3yo LOSER 1934.49 42.00 42.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523319 10. Triptonic 21.22 R1 1200m 3yo LOSER 1710.76 23.87 23.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 35773035 9. Right Reason 10.23 R1 1200m 3yo LOSER 5524.11 12.50 11.5 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523318 8. Off Road 40.75 R1 1200m 3yo LOSER 1506.51 35.31 34.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523317 7. More Than Value 77.49 R1 1200m 3yo LOSER 617.18 55.00 55.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28092381 11. Born A Warrior 10.67 R8 1300m Hcap LOSER 905.55 6.97 6.2 {'price': [6.2, 5.8, 5.1], 'volume': [7.98, 40... {'price': [6.8, 7, 7.8], 'volume': [6.26, 41.5... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38698010 12. Diva Bella 25.77 R8 1300m Hcap LOSER 11.06 23.60 18.5 {'price': [23, 22, 18], 'volume': [0.31, 24.91... {'price': [70, 75, 95], 'volume': [0.61, 3.5, ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28224034 13. Twice As Special 51.23 R8 1300m Hcap LOSER 52.49 36.37 26.0 {'price': [30, 29, 26], 'volume': [13.84, 5.92... {'price': [44, 50, 95], 'volume': [2.76, 1.66,... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38913296 15. Rosie Riveter 24.92 R8 1300m Hcap LOSER 58.65 9.72 11.0 {'price': [10.5, 10, 9.6], 'volume': [0.69, 28... {'price': [11, 12.5, 19], 'volume': [3.87, 2.7... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 4973624 8. Celer 26.23 R8 1300m Hcap LOSER 22.14 21.73 28.0 {'price': [24, 23, 20], 'volume': [1.55, 18.26... {'price': [30, 65, 70], 'volume': [0.55, 1.55,... df = ( rawDF # Extra Best Back + Lay 3 mins before of . assign ( best_back_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atb_ladder_3m' ]]) . assign ( best_lay_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atl_ladder_3m' ]]) # Coalesce LTP to BSP (about 60 rows) . assign ( ltp = lambda x : np . where ( x [ \"ltp\" ] . isnull (), x [ \"bsp\" ], x [ \"ltp\" ])) # Add a binary win / loss column . assign ( win = lambda x : np . where ( x [ 'result' ] == \"WINNER\" , 1 , 0 )) # Extra columns . assign ( model_prob = lambda x : 1 / x [ 'model_odds' ]) # Reorder Columns . reindex ( columns = [ 'date' , 'track' , 'race_number' , 'market_id' , 'selection_id' , 'bsp' , 'ltp' , 'best_back_3m' , 'best_lay_3m' , 'atb_ladder_3m' , 'atl_ladder_3m' , 'model_prob' , 'model_odds' , 'win' ]) ) df . head ( 5 ) date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0 Staking + Outcome Functions Now we can create a set of standard staking functions that a dataframe with an expected set of columns and add staking and bet outcome fields. We'll also add the ability of these functions to reference a different odds column so that we can backtest against our different price points. For simplicity we'll assume you're paying 5% commission on winnings however it could be higher or lower and depends on the MBR of the market. def bet_apply_commission ( df , com = 0.05 ): # Total Market GPL df [ 'market_gpl' ] = df . groupby ( 'market_id' )[ 'gpl' ] . transform ( sum ) # Apply 5% commission df [ 'market_commission' ] = np . where ( df [ 'market_gpl' ] <= 0 , 0 , 0.05 * df [ 'market_gpl' ]) # Sum of Market Winning Bets df [ 'floored_gpl' ] = np . where ( df [ 'gpl' ] <= 0 , 0 , df [ 'gpl' ]) df [ 'market_netwinnings' ] = df . groupby ( 'market_id' )[ 'floored_gpl' ] . transform ( sum ) # Partition Commission According to Selection GPL df [ 'commission' ] = np . where ( df [ 'market_netwinnings' ] == 0 , 0 , ( df [ 'market_commission' ] * df [ 'floored_gpl' ]) / ( df [ 'market_netwinnings' ])) # Calculate Selection NPL df [ 'npl' ] = df [ 'gpl' ] - df [ 'commission' ] # Drop excess columns df = df . drop ( columns = [ 'floored_gpl' , 'market_netwinnings' , 'market_commission' , 'market_gpl' ]) return ( df ) def bet_flat ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , # PUSH np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , stake ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) def bet_kelly ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , # PUSH np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , # PUSH 0 , np . where ( df [ 'bet_side' ] == \"B\" , ( ( 1 / df [ 'model_odds' ]) - ( 1 / df [ back_odds ]) ) / ( 1 - ( 1 / df [ back_odds ])), ( ( 1 / df [ lay_odds ]) - ( 1 / df [ 'model_odds' ]) ) / ( 1 - ( 1 / df [ lay_odds ])), ) ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) # Testing one of these functions flat_bets_bsp = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) flat_bets_bsp . head ( 5 ) date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win bet_side stake gpl commission npl 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 L 1 1.0 0.0 1.0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0 L 1 1.0 0.0 1.0 Evaluation Functions In my experience it's great to develop a suite of functions and analytical tools that really dig into every aspect of your simulated betting performance. You want to be as thorough and critical as possible, even when you're results are good. Another tip to guide this process is to have a reasonable benchmark. Essentially no one wins at 10% POT on thoroughbreds at the BSP so if your analysis suggests you can... there's a bug. Similarly you almost certainly won't lose at more than <-10%. Different sports and codes will have different realistic profitability ranges depending on the efficiency of the markets (will be roughly correlated to matched volume). Ruling out unreasonable results can save you a lot of time and delusion. I'm keeping it pretty simple here but you might also want to create functions to analyse: Track / distance based performance Performance across odds ranges Profit volatility (maybe using sharpe ratio to optimise volatility - adjusted profit) Date ranges (weeks / months etc) # Create simple PL and POT table def bet_eval_metrics ( d , side = False ): if side : metrics = ( d . groupby ( 'bet_side' , as_index = False ) . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) ) else : metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) # Cumulative PL by market to visually see trend and consistency def bet_eval_chart_cPl ( d ): d = ( d . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) d [ 'cNpl' ] = d . npl . cumsum () chart = px . line ( d , x = \"market_number\" , y = \"cNpl\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) return ( chart ) To illustrate these evaluation functions let's analyse flat staking at the BSP. bets = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) bet_eval_metrics ( bets , side = True ) bet_side npl stake pot B -749.493788 8356 -0.089695 L -268.499212 8592 -0.031250 bet_eval_chart_cPl ( bets ) So this isn't gonna build us an art gallery! This is to be expected though, it's not easy to make consistent profit certainly from free ratings sources available online. Testing different approaches We pulled those extra price points for a reason. Let's set up a little test harness that enables us to use different price points and bet using different staking functions. # We'll test a 2 different staking schemes on 3 different price points grid = { \"flat_bsp\" : ( bet_flat , \"bsp\" , \"bsp\" ), \"flat_ltp\" : ( bet_flat , \"ltp\" , \"ltp\" ), \"flat_3m\" : ( bet_flat , \"best_back_3m\" , \"best_lay_3m\" ), \"kelly_bsp\" : ( bet_kelly , \"bsp\" , \"bsp\" ), \"kelly_ltp\" : ( bet_kelly , \"ltp\" , \"ltp\" ), \"kelly_3m\" : ( bet_kelly , \"best_back_3m\" , \"best_lay_3m\" ) } metricSummary = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column # objects[0] is the staking function itself bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) # Calculate the metrics and tag with strategy label betMetrics = ( bet_eval_metrics ( bets ) . assign ( strategy = lambda x : strategy ) . reindex ( columns = [ 'strategy' , 'stake' , 'npl' , 'pot' ]) ) # Init the betMetrics df or append if already exists try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) strategy stake npl pot kelly_ltp 754.496453 -31.814150 -0.042166 kelly_bsp 732.165110 -34.613773 -0.047276 flat_bsp 16948.000000 -1017.993000 -0.060066 flat_ltp 16949.000000 -1184.546000 -0.069889 flat_3m 15712.000000 -1225.123000 -0.077974 kelly_3m 614.135601 -50.469295 -0.082179 # Compare Cumulative PL Charts cumulativePLs = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) d = ( bets . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' , 'stake' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) # Normalise to $10,000 stake for visual comparison d [ 'npl' ] = d [ 'npl' ] / ( d . stake . sum () / 10000 ) d [ 'cNpl' ] = d . npl . cumsum () d [ 'strategy' ] = strategy # Init the cumulativePLs df or append if already exists try : cumulativePLs = pd . concat ([ cumulativePLs , d ], ignore_index = True ) except : cumulativePLs = d px . line ( cumulativePLs , x = \"market_number\" , y = \"cNpl\" , color = \"strategy\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) Searching For Profit So this is often where you're going to arrive developing many wagering models: there's no indication of reliable long term profit. Where do you go from here? TBH I think most people give up here. Because you're not a quitter though you've got 3 main option categories: Make the underlying model better Search for better prices via detailed price analysis and clever bet placement Try to find a subset of these selections with these ratings and these price points that are sustainably profitable Obviously each situation is different but I think option 3 isn't a bad way to go initially because it will definitely help you understand your model better. For a racing model you might want to split your performance by: tracks or states track conditions or weather barriers race quality or grade odds ranges selection sample size (you likely perform worse on horses with little form for eg) perceived model value Finding a big enough slice across those dimensions that's either really profitable or really losing might reveal to you a bug in the data or workflow in your model development that you can go back and fix. As an example of a simple approach to selectiveness I'll quickly run through how being more selective about your perceived value might make a difference in final profitability. So our best performing strategy using our simple analysis above was Kelly staking at the last traded price. We'll start with that but be aware of that there's no way of implementing a LTP bet placement engine, you could imagine a proxy being placing limit bets \"just before\" the race jumps which is a whole other kettle of fish. Anyway, let's plot our profitability under this strategy at different perceived \"edges\". If we are more selective of only large overlays according to the hub's rated chance you can see we can increase the profitability. bets = bet_kelly ( df , back_odds = 'ltp' , lay_odds = 'ltp' ) metricSummary = None for bVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: for lVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: x = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( bVal , lVal )) betMetrics = bet_eval_metrics ( x , side = False ) betMetrics [ 'bVal' ] = bVal betMetrics [ 'lVal' ] = lVal try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) . head ( 4 ) npl stake pot bVal lVal -18.059813 574.944431 -0.031411 0.3 0.30 -22.887791 628.302349 -0.036428 0.3 0.20 -24.509182 669.482514 -0.036609 0.3 0.05 -22.997908 614.528386 -0.037424 0.2 0.30 betsFilters = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( 0.3 , 0.3 )) bet_eval_chart_cPl ( betsFilters ) We were doing ok till the last 200 market nightmare! Might be one to test with more data. So we still haven't found a clear profitable edge with these ratings, however we got a bit closer to break even which is positive. This step also indicates that this rating system performs better for large overlays which is a good model indicator (if you can't improve by selecting for larger overlays it's usually a sign you need to go back to the drawing board) You could imagine a few more iterations of analysis you might be able to eek out a slight edge However, be wary as these steps optimisation steps are very prone to overfitting so you need to be careful. Conclusion and Next Steps While using someone else's model is easy it's also not likely to end in personal riches. Developing your own model with your own tools and on a sport or racing code you know about is probably where you should start. However, hopefully this short guide helps you think about what to do when you finish the modelling component: How much money will I win or lose if I started using this system to place bets with real money? If you want to expand this backtesting analysis, here's a list (in no particular order) of things that I've omitted or angles I might look at next: Get more data -- more rating data and odds data is needed for draw a good conclusion about long term expectation Cross reference performance against race or selection metadata (track, # races run etc.) to improve performance with betting selectivity Extract more price points from the stream data to try to gain an pricing edge on these ratings Over to you We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space Complete code Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github import pandas as pd import numpy as np import requests from datetime import date , timedelta import os import re import tarfile import zipfile import bz2 import glob import logging from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import plotly.express as px import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) % config IPCompleter . greedy = True #### -------------------------- #### FUNCTIONS #### -------------------------- # Function to return Pandas DF of hub ratings for a particular date def getHubRatings ( dte ): # Substitute the date into the URL url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date= {} presenter=RatingsPresenter&json=true' . format ( dte ) # Convert the response into JSON responseJson = requests . get ( url ) . json () hubList = [] if not responseJson : return ( None ) # Want an normalised table (1 row per selection) # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields for meeting in responseJson [ 'meetings' ]: for race in meeting [ 'races' ]: for runner in race [ 'runners' ]: hubList . append ( { 'date' : dte , 'track' : meeting [ 'name' ], 'race_number' : race [ 'number' ], 'race_name' : race [ 'name' ], 'market_id' : race [ 'bfExchangeMarketId' ], 'selection_id' : str ( runner [ 'bfExchangeSelectionId' ]), 'selection_name' : runner [ 'name' ], 'model_odds' : runner [ 'ratedPrice' ] } ) out = pd . DataFrame ( hubList ) return ( out ) # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input sample: R6 1400m Grp1 parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) # creating a flag that is True when markets are australian thoroughbreds def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # loading from tar and extracting files def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # Extract Components From Generated Stream def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book evaluate_market = None prev_market = None postplay_market = None preplay_market = None t3m_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 3 mins before scheduled off if t3m_market is None and seconds_to_start < 3 * 60 : t3m_market = market_book # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay_market is not None and preplay_market is None : preplay_market = postplay_market return ( t3m_market , preplay_market , postplay_market , prev_market ) # Final market is last prev_market def run_stream_parsing (): # Run Pipeline with open ( \"outputs/tho-odds.csv\" , \"w+\" ) as output : # Write Column Headers To File output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m \\n \" ) for file_obj in load_markets ( data_path ): # Instantiate a \"stream\" object stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) # Extract key components according to the custom function above (outputs 4 objects) ( t3m_market , preplay_market , postplay_market , final_market ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay_market is None : continue ; # Runner metadata and key fields available from final market book runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final_market . runners ] # Last Traded Price # _____________________ # From the last marketbook before inplay or close ltp = [ runner . last_price_traded for runner in preplay_market . runners ] # Total Matched Volume # _____________________ # Calculates the traded volume across all traded price points for each selection def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) selection_traded_volume = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay_market . runners ] # Top 3 Ladder # ______________________ # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries def top_3_ladder ( availableLadder ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : 3 ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"price\" ] = price out [ \"volume\" ] = volume return ( out ) # Sometimes t-3 mins market book is empty try : atb_ladder_3m = [ top_3_ladder ( runner . ex . available_to_back ) for runner in t3m_market . runners ] atl_ladder_3m = [ top_3_ladder ( runner . ex . available_to_lay ) for runner in t3m_market . runners ] except : atb_ladder_3m = {} atl_ladder_3m = {} # Writing To CSV # ______________________ for ( runnerMeta , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ) in zip ( runner_data , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final_market . market_id ), final_market . market_definition . market_time , final_market . market_definition . country_code , final_market . market_definition . venue , final_market . market_definition . name , runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], runnerMeta [ 'selection_status' ], runnerMeta [ 'sp' ], ltp , selection_traded_volume , '\"' + str ( atb_ladder_3m ) + '\"' , # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas '\"' + str ( atl_ladder_3m ) + '\"' ) ) def bet_apply_commission ( df , com = 0.05 ): # Total Market GPL df [ 'market_gpl' ] = df . groupby ( 'market_id' )[ 'gpl' ] . transform ( sum ) # Apply 5% commission df [ 'market_commission' ] = np . where ( df [ 'market_gpl' ] <= 0 , 0 , 0.05 * df [ 'market_gpl' ]) # Sum of Market Winning Bets df [ 'floored_gpl' ] = np . where ( df [ 'gpl' ] <= 0 , 0 , df [ 'gpl' ]) df [ 'market_netwinnings' ] = df . groupby ( 'market_id' )[ 'floored_gpl' ] . transform ( sum ) # Partition Commission According to Selection GPL df [ 'commission' ] = np . where ( df [ 'market_netwinnings' ] == 0 , 0 , ( df [ 'market_commission' ] * df [ 'floored_gpl' ]) / ( df [ 'market_netwinnings' ])) # Calculate Selection NPL df [ 'npl' ] = df [ 'gpl' ] - df [ 'commission' ] # Drop excess columns df = df . drop ( columns = [ 'floored_gpl' , 'market_netwinnings' , 'market_commission' , 'market_gpl' ]) return ( df ) def bet_flat ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , stake ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) def bet_kelly ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , np . where ( df [ 'bet_side' ] == \"B\" , ( ( 1 / df [ 'model_odds' ]) - ( 1 / df [ back_odds ]) ) / ( 1 - ( 1 / df [ back_odds ])), ( ( 1 / df [ lay_odds ]) - ( 1 / df [ 'model_odds' ]) ) / ( 1 - ( 1 / df [ lay_odds ])), ) ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) # Create simple PL and POT table def bet_eval_metrics ( d , side = False ): if side : metrics = ( d . groupby ( 'bet_side' , as_index = False ) . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) ) else : metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) # Cumulative PL by market to visually see trend and consistency def bet_eval_chart_cPl ( d ): d = ( d . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) d [ 'cNpl' ] = d . npl . cumsum () chart = px . line ( d , x = \"market_number\" , y = \"cNpl\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) return ( chart ) #### -------------------------- #### EXECUTION #### -------------------------- # Loop through all recent history dateDFList = [] dateList = pd . date_range ( date ( 2021 , 2 , 18 ), date . today () - timedelta ( days = 1 ), freq = 'd' ) for dte in dateList : dateDFList . append ( getHubRatings ( dte )) # Concatenate (add rows to rows) all the dataframes within the list hubRatings = pd . concat ( dateDFList ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = StreamListener ( max_latency = None ) # This will execute the files (it took me ~2 hours for 2 months of data) #run_stream_parsing() # Load in odds file we created above bfOdds = pd . read_csv ( \"outputs/tho-odds.csv\" , dtype = { 'market_id' : object , 'selection_id' : object , 'atb_ladder_3m' : object , 'atl_ladder_3m' : object }) # Convert dictionary columns import ast bfOdds [ 'atb_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atb_ladder_3m' ]] bfOdds [ 'atl_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atl_ladder_3m' ]] # Convert LTP to Numeric bfOdds [ 'ltp' ] = pd . to_numeric ( bfOdds [ 'ltp' ], errors = 'coerce' ) # Filter after 18th Feb bfOdds = bfOdds . query ( 'event_date >= \"2021-02-18\"' ) # Joining the ratings data and odds data and combining rawDF = pd . merge ( hubRatings [ hubRatings [ 'market_id' ] . isin ( bfOdds . market_id . unique ())], bfOdds [[ 'market_name' , 'market_id' , 'selection_id' , 'result' , 'matched_volume' , 'bsp' , 'ltp' , 'atb_ladder_3m' , 'atl_ladder_3m' ]], on = [ 'market_id' , 'selection_id' ], how = 'inner' ) # Join and clean up columns df = ( rawDF # Extra Best Back + Lay 3 mins before of . assign ( best_back_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atb_ladder_3m' ]]) . assign ( best_lay_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atl_ladder_3m' ]]) # Coalesce LTP to BSP (about 60 rows) . assign ( ltp = lambda x : np . where ( x [ \"ltp\" ] . isnull (), x [ \"bsp\" ], x [ \"ltp\" ])) # Add a binary win / loss column . assign ( win = lambda x : np . where ( x [ 'result' ] == \"WINNER\" , 1 , 0 )) # Extra columns . assign ( model_prob = lambda x : 1 / x [ 'model_odds' ]) # Reorder Columns . reindex ( columns = [ 'date' , 'track' , 'race_number' , 'market_id' , 'selection_id' , 'bsp' , 'ltp' , 'best_back_3m' , 'best_lay_3m' , 'atb_ladder_3m' , 'atl_ladder_3m' , 'model_prob' , 'model_odds' , 'win' ]) ) bets = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) bet_eval_metrics ( bets , side = True ) bet_eval_chart_cPl ( bets ) # We'll test a 2 different staking schemes on 3 different price points grid = { \"flat_bsp\" : ( bet_flat , \"bsp\" , \"bsp\" ), \"flat_ltp\" : ( bet_flat , \"ltp\" , \"ltp\" ), \"flat_3m\" : ( bet_flat , \"best_back_3m\" , \"best_lay_3m\" ), \"kelly_bsp\" : ( bet_kelly , \"bsp\" , \"bsp\" ), \"kelly_ltp\" : ( bet_kelly , \"ltp\" , \"ltp\" ), \"kelly_3m\" : ( bet_kelly , \"best_back_3m\" , \"best_lay_3m\" ) } # Evaluate Metrics For Strategy Grid metricSummary = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column # objects[0] is the staking function itself bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) betMetrics = ( bet_eval_metrics ( bets ) . assign ( strategy = lambda x : strategy ) . reindex ( columns = [ 'strategy' , 'stake' , 'npl' , 'pot' ]) ) try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) # Compare Cumulative PL Charts cumulativePLs = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) d = ( bets . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' , 'stake' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) # Normalise to $10,000 stake for visual comparison d [ 'npl' ] = d [ 'npl' ] / ( d . stake . sum () / 10000 ) d [ 'cNpl' ] = d . npl . cumsum () d [ 'strategy' ] = strategy try : cumulativePLs = pd . concat ([ cumulativePLs , d ], ignore_index = True ) except : cumulativePLs = d px . line ( cumulativePLs , x = \"market_number\" , y = \"cNpl\" , color = \"strategy\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) bets = bet_kelly ( df , back_odds = 'ltp' , lay_odds = 'ltp' ) metricSummary = None for bVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: for lVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: x = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( bVal , lVal )) betMetrics = bet_eval_metrics ( x , side = False ) betMetrics [ 'bVal' ] = bVal betMetrics [ 'lVal' ] = lVal try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) . head ( 4 ) betsFilters = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( 0.3 , 0.3 )) bet_eval_chart_cPl ( betsFilters ) Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Back testing ratings in Python"},{"location":"historicData/backtestingRatingsTutorial/#backtesting-wagering-models-with-betfair-json-stream-data","text":"","title":"Backtesting wagering models with Betfair JSON stream data"},{"location":"historicData/backtestingRatingsTutorial/#workshop","text":"This tutorial was written by Tom Bishop and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the JSON to CSV tutorial we shared previously. If you're still new to working with the JSON data sets we suggest you take a look at that tutorial before diving into this one. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo . You can watch our workshop working through this tutorial on YouTube.","title":"Workshop"},{"location":"historicData/backtestingRatingsTutorial/#set-up","text":"I'm going to be using a jupyter notebook for this investigation which is a special type of data analysis output that is used to combine code, outputs and explanatory text in a readable single document. It's mostly closely associated with python data analysis code which is the language I'll be using here also. The entire body of python code used will be repeated at the bottom of the article where you can copy it and repurpose it for yourself. If you're not familiar with python, don't worry neither am I really! I'm inexperienced with python so if you have experience with some other programming language you should be able to follow along with the logic here too. If you don't have experience using another programming language this all might appear intimidating but it's heavy on the explanatory text so you should get something out of it. We need a few non standard python libraries so make sure to install betfairlightweight and plotly before you get started with something like pip install betfairlightweight & pip install plotly . We'll load all our libraries and do some setup here. import pandas as pd import numpy as np import requests from datetime import date , timedelta import os import re import tarfile import zipfile import bz2 import glob import logging from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import plotly.express as px import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) % config IPCompleter . greedy = True","title":"Set up"},{"location":"historicData/backtestingRatingsTutorial/#context","text":"Backtesting is the life-blood of most successful wagering systems. In short it attempts to answer a single question for you: \ud835\udf0f : How much money will I win or lose if I started using this system to place bets with real money? Without a rigorous and quantitative backtesting approach it's really quite hard to estimate the answer to this question $ \\tau $ that will be even reliably on the right side of zero. You could live test your system with real bets at small stakes, however, this isn't the panacea it seems. It will take time (more than you think) for your results to converge to their long term expectation. How long? Answering this question will require some expertise with probability and statistics you might not have. Even more than that though is that depending on where you're betting your results at small stakes could be very different than at larger stakes. You might not be able get a good answer to $ \\tau $ until betting at full stakes at which point finding the answer might coincide with blowing up your gambling bankroll. Backtesting is also very hard. To perfectly backtest your own predicted probability on a historical race or sporting match you need to produce 2 things: (1) What would my predicted chance have been exactly for this selection in this market on this day in the past? (2) What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction? The devil in the detail of backtesting tends to be in those exactlys. The aim of the backtesting game is answering (2) as accurately as possible because it tells you exactly how much you would have made over your backtesting period, from there you can confidently project that rate of profitability forward. It's easy to make mistakes and small errors in the quantitative reasoning can lead you to extremely misguided projections downstream. Question (1) won't be in the scope of this notebook but it's equally (and probably more) important that (2) but it is the key challenge of all predictive modelling exercises so there's plenty of discussion about it elsewhere.","title":"Context"},{"location":"historicData/backtestingRatingsTutorial/#backtesting-on-betfair","text":"Answering question (2) for betting on the Betfair Exchange is difficult. The Exchange is a dynamic system that changes from one micro second to the next. What number should you use for odds? How much could you assume to get down at those odds? The conventional and easiest approach is to backtest at the BSP. The BSP is simple because it's a single number (to use for both back and lay bets) and is a taken price (there's no uncertainty about getting matched). Depending on the liquidity of the market a reasonably sized stake might also not move the BSP very much. For some markets you may be able to safely assume you could be $10s of dollars at the BSP without moving it an inch. However, that's definitely not true of all BSP markets and you need to be generally aware that your Betfair orders in the future will change the state of the exchange, and large bets will move the BSP in an unfavourable direction. Aside from uncertainty around the liquidity and resilience of the BSP, many many markets don't have a BSP. So what do we do then? Typically what a lot of people (who have a relationship with Betfair Australia) do at this point is request a data dump. They might request an odds file for all Australian harness race win markets since June 2018 with results and 4 different price points: the BSP, the last traded price, the weighted average price (WAP) traded in 3 minutes before the race starts, and the WAP for all bets matched prior to 3 mins before the race. However, you will likely need to be an existing VIP customer to get this file and it's not a perfect solution: it might take 2 weeks to get, you can't refresh it, you can't test more hypothetical price points after your initial analysis amongst many other problems. What if you could produce this valuable data file yourself?","title":"Backtesting on Betfair"},{"location":"historicData/backtestingRatingsTutorial/#betfair-stream-data","text":"Betfair's historical stream data is an extremely rich source of data. However, in it's raw form it's difficult to handle for the uninitiated. It also might not be immediately obvious how many different things this dataset could be used for without seeing some examples. These guides will hopefully demystify how to turn this raw data into a familiar and usable format whilst also hopefully providing some inspiration for the kinds of value that can be excavated from it.","title":"Betfair Stream Data"},{"location":"historicData/backtestingRatingsTutorial/#this-example-backtesting-betfair-hub-thoroughbred-model","text":"To illustrate how you can use the stream files to backtest the outputs of a rating system we'll use the Australian Thoroughbred Rating model available on the Betfair Hub. The most recent model iteration only goes back till Feb 28th 2021 however as an illustrative example this is fine. We'd normally want to backtest with a lot more historical data than this, which just means in this case our estimation of future performance will be unreliable. I'm interested to see how we would have fared betting all selections rated by this model according to a few different staking schemes and also at a few different times / price points. Old ratings If you want to pull in ratings from before Feb 2021 to add to your database for more complete backtesting these are available in a data dump here .","title":"This example: backtesting Betfair Hub thoroughbred model"},{"location":"historicData/backtestingRatingsTutorial/#scrape-the-model-ratings","text":"If you travel to the Betfair hub ratings page you'll find that URL links behind the ratings download buttons have a consistent URL pattern that looks very scrape friendly. We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe. # Function to return Pandas DF of hub ratings for a particular date def getHubRatings ( dte ): # Substitute the date into the URL url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date= {} presenter=RatingsPresenter&json=true' . format ( dte ) # Convert the response into JSON responseJson = requests . get ( url ) . json () hubList = [] if not responseJson : return ( None ) # Want an normalised table (1 row per selection) # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields for meeting in responseJson [ 'meetings' ]: for race in meeting [ 'races' ]: for runner in race [ 'runners' ]: hubList . append ( { 'date' : dte , 'track' : meeting [ 'name' ], 'race_number' : race [ 'number' ], 'race_name' : race [ 'name' ], 'market_id' : race [ 'bfExchangeMarketId' ], 'selection_id' : str ( runner [ 'bfExchangeSelectionId' ]), 'selection_name' : runner [ 'name' ], 'model_odds' : runner [ 'ratedPrice' ] } ) out = pd . DataFrame ( hubList ) return ( out ) # See the response from a single day getHubRatings ( date ( 2021 , 3 , 1 )) . head ( 5 ) date track race_number race_name market_id selection_id selection_name model_odds 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620052 1. Military Affair 6.44 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 5889703 3. Proverbial 21.11 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38177688 4. A Real Wag 9.97 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 38620053 5. El Jay 44.12 2021-03-01 COWRA 1 R1 1375m Mdn 1.179845154 37263264 6. Flying Honour 3.39 dateDFList = [] dateList = pd . date_range ( date ( 2021 , 2 , 18 ), date . today () - timedelta ( days = 1 ), freq = 'd' ) for dte in dateList : dateDFList . append ( getHubRatings ( dte )) # Concatenate (add rows to rows) all the dataframes within the list hubRatings = pd . concat ( dateDFList ) hubRatings . shape ( 32519 , 8 )","title":"Scrape The Model Ratings"},{"location":"historicData/backtestingRatingsTutorial/#assembling-the-odds-file","text":"So part 1 was very painless. This is how we like data: served by some API or available in a nice tabular format on a webpage ready to be scraped with standard tools available in popular languages. Unfortunately, it won't be so painless to assemble our odds file. We'll find out why it's tricky as we go.","title":"Assembling the odds file"},{"location":"historicData/backtestingRatingsTutorial/#the-data","text":"The data we'll be using is the historical Exchange data available from this website. The data available through this service is called streaming JSON data. There are a few options available relating to granularity (how many time points per second the data updates at) but we'll be using the most granular \"PRO\" set which has updates every 50 milliseconds. Essentially what the data allows us to do is, for a particular market, recreate the exact state of the Betfair Exchange at say: 150 milliseconds before the market closed. When people say the state of the Exchange they mean two things a) what are all the current open orders on all the selections b) what are the current traded volumes on each selection at each price point. We obviously don't have access to any information about which accounts are putting up which prices and other things Betfair has themselves. We're essentially getting a snapshot of everything you can see through the website by clicking on each selection manually and looking at the graphs, tables and ladders. However, with just these 2 sets of information we can build a rich view of the dynamics of exchange and also build out all of the summary metrics (WAP etc) we might have previously needed Betfair to help with. For our purposes 50 milli-second intervaled data is huge overkill. But you could imagine needing this kind of granularity for other kinds of wagering systems - eg a high frequency trading algorithm of some sort that needs to make many decisions and actions every second. Let's take a look at what the stream data looks like for a single market: So it looks pretty intractable. For this particular market there's 14,384 lines of data where each line consists of a single JSON packet of data. If you're not a data engineer (neither am I) your head might explode thinking about how you could read this into your computer and transform it into something usable. The data looks like this because it is saved from a special Betfair API called the Stream API which which is used by high end Betfair API users and which delivers fast speeds other performance improvements over the normal \"polling\" API. Now what's good about that, for the purposes of our exercise, is that the very nice python package betfairlightweight has the functionality built to not only parse the Stream API when connected live but also these historical saved versions of the stream data. Without it we'd be very far away from the finish line, with betfairlightweight we're pretty close.","title":"The Data"},{"location":"historicData/backtestingRatingsTutorial/#unpacking-flattening-the-data","text":"Because these files are so large and unprocessed this process won't look the same as your normal data ETL in python: where you can read a raw data file (csv, JSON, text etc.) into memory and use python functions to transform into usable format. I personally had no idea how to use python and betfairlightweight to parse these data until I saw Betfair's very instructive overview which you should read for a more detailed look at some of the below code. By my count there were 4 key conceptual components that I had to get my head around to understand and be able to re-purpose that code. So if you're like me (a bit confused by some of the steps in that piece) this explanation might help. I'll assume you don't do any decompression and keep the monthly PRO files as the .tar archives as they are. Conceptually the process looks something like this: Load the \"archives\" into a \"generator\" Scan across the generator (market_ids) and the market states within those markets to extract useful objects Process those useful objects to pull out some metadata + useful summary numbers derived from the available orders and traded volumes snapshot data Write this useful summarised data to a file that can be read and understood with normal data analysis workflows First we'll run a bunch of setup code setting up my libraries and creating some utility functions that will be used throughout the main parsing component. It'll also point to the two stream files I'll be parsing for this exercise. # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = StreamListener ( max_latency = None ) ### Utility Functions # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input sample: R6 1400m Grp1 parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) # creating a flag that is True when markets are australian thoroughbreds def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' )","title":"Unpacking / flattening the data"},{"location":"historicData/backtestingRatingsTutorial/#1-tar-load","text":"This function I stole from Betfair's instructional article The stream files are downloaded as .tar archive files which are a special kind of file that we'll need to unpack Instead of loading each file into memory this function returns a \"generator\" which is a special python object that is to be iterated over This basically means it contains the instructions to unpack and scan over files on the fly This function also contains the logic to deal with if these files are zip archives or you've manually unpacked the archive and have the .bz2 zipped files # loading from tar and extracting files def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None","title":"1: .tar load"},{"location":"historicData/backtestingRatingsTutorial/#2-scan-across-market-states-and-extract-useful-objects","text":"So this function will take a special \"stream\" object which we'll create with betfairlightweight The function takes a stream object input and returns 4 instances of the market state The market state 3 mins before the scheduled off The market state immediately before it goes inplay The market state immediately before it closes for settlement The final market state with outcomes It basically just loops over all the market states and has a few checks to determine if it should save the current market state as key variables and then returns those # Extract Components From Generated Stream def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book evaluate_market = None prev_market = None postplay_market = None preplay_market = None t3m_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 3 mins before scheduled off if t3m_market is None and seconds_to_start < 3 * 60 : t3m_market = market_book # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay_market is not None and preplay_market is None : preplay_market = postplay_market return ( t3m_market , preplay_market , postplay_market , prev_market ) # Final market is last prev_market","title":"2: Scan across market states and extract useful objects"},{"location":"historicData/backtestingRatingsTutorial/#3-4-summarise-those-useful-objects-and-write-to-csv","text":"This next chunk contains a wrapper function that will do all the execution It will open a csv output file Use the load_markets utility to iterate over the .tar files Use betfairlightweight to instantiate the special stream object Pass that stream object to the extract_components_from_stream which will scan across the market states and pull out 4 key market books Convert those marketbooks into simple summary numbers or dictionaries that will be written to the output .csv file def run_stream_parsing (): # Run Pipeline with open ( \"outputs/tho-odds.csv\" , \"w+\" ) as output : # Write Column Headers To File output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m \\n \" ) for file_obj in load_markets ( data_path ): # Instantiate a \"stream\" object stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) # Extract key components according to the custom function above (outputs 4 objects) ( t3m_market , preplay_market , postplay_market , final_market ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay_market is None : continue ; # Runner metadata and key fields available from final market book runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final_market . runners ] # Last Traded Price # _____________________ # From the last marketbook before inplay or close ltp = [ runner . last_price_traded for runner in preplay_market . runners ] # Total Matched Volume # _____________________ # Calculates the traded volume across all traded price points for each selection def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) selection_traded_volume = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay_market . runners ] # Top 3 Ladder # ______________________ # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries def top_3_ladder ( availableLadder ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : 3 ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"price\" ] = price out [ \"volume\" ] = volume return ( out ) # Sometimes t-3 mins market book is empty try : atb_ladder_3m = [ top_3_ladder ( runner . ex . available_to_back ) for runner in t3m_market . runners ] atl_ladder_3m = [ top_3_ladder ( runner . ex . available_to_lay ) for runner in t3m_market . runners ] except : atb_ladder_3m = {} atl_ladder_3m = {} # Writing To CSV # ______________________ for ( runnerMeta , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ) in zip ( runner_data , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final_market . market_id ), final_market . market_definition . market_time , final_market . market_definition . country_code , final_market . market_definition . venue , final_market . market_definition . name , runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], runnerMeta [ 'selection_status' ], runnerMeta [ 'sp' ], ltp , selection_traded_volume , '\"' + str ( atb_ladder_3m ) + '\"' , # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas '\"' + str ( atl_ladder_3m ) + '\"' ) ) # This will execute the files (it took me ~2 hours for 2 months of data) #run_stream_parsing()","title":"3 &amp; 4: Summarise those useful objects and write to .csv"},{"location":"historicData/backtestingRatingsTutorial/#extending-this-code","text":"Because this process is very slow you might want to save much more information than you think you need For example I currently think I only want the best back and lay prices at t-3 mins before the off but I've saved the top 3 boxes in the available to back and lay ladders as dictionary strings From these ladders I can retroactively calculate not only just the best back and lay prices but also WAP prices and also sizes at those boxes which I could use for much more accurate backtesting if I wanted to later without having to scan across the entire stream files again I could easily save the entire open and traded orders ladders in the same way amongst many other ways of retaining more of the data for post-processing analysis","title":"Extending this code"},{"location":"historicData/backtestingRatingsTutorial/#backtesting-analysis","text":"Let's take stock of where we are. We currently have model ratings (about 1.5 months worth) and Betfair Odds (2 months worth). Circling back to the original backtesting context we needed to solve for 2 key questions: What would my predicted chance have been exactly for this selection in this market on this day in the past? What would have I decided to bet at what odds (exactly) and for how much stake (exactly) based on this prediction? Backtesting with someone else's publicly available and historically logged ratings solves question 1. With these particular ratings we're fine but generally we should just be aware there are some sketchy services that might make retroactive adjustments to historical ratings to juice their performance which obviously violates 1. For the second part we now have several real Betfair odds values to combine with the ratings and some chosen staking formula to simulate actual bets. I won't dwell too much on the stake size component but it's important. Similarly we aren't out of the woods with the \"what odds exactly\" question either. I'll show performance of backtesting at the \"Last Traded Price\" however, there's literally no way of actually being the last bet matched order on every exchange market so there's some uncertainty in a few of these prices. Further, and from experience, if you placing bets at the BSP and you're using some form of proportional staking (like Kelly) then you're calculated stake size will need to include a quantity (the BSP) which you will literally never be 100% sure of. You'll need to estimate the BSP as close to market suspension as you can and place your BSP bets with a stake sized derived from that estimation. This imprecision in stake calculation WILL cost you some profit relative to your backtested expectation. These might seem like minor considerations but you should be aware of some of the gory details of the many ways becoming successful on Betfair is really difficult. To be reliably profitable on Betfair you don't just need a good model, you'll likely need to spend hours and hours thinking about these things: testing things, ironing out all these little kinks and trying to account for all your uncertainties. I'll just be running over the skeleton of what you should do.","title":"Backtesting Analysis"},{"location":"historicData/backtestingRatingsTutorial/#setting-up-your-master-data","text":"# First we'll load and tidy our odds data # Load in odds file we created above bfOdds = pd . read_csv ( \"outputs/tho-odds.csv\" , dtype = { 'market_id' : object , 'selection_id' : object , 'atb_ladder_3m' : object , 'atl_ladder_3m' : object }) # Convert dictionary columns import ast bfOdds [ 'atb_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atb_ladder_3m' ]] bfOdds [ 'atl_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atl_ladder_3m' ]] # Convert LTP to Numeric bfOdds [ 'ltp' ] = pd . to_numeric ( bfOdds [ 'ltp' ], errors = 'coerce' ) # Filter after 18th Feb bfOdds = bfOdds . query ( 'event_date >= \"2021-02-18\"' ) bfOdds . head ( 5 ) market_id event_date country track market_name selection_id selection_name result bsp ltp matched_volume atb_ladder_3m atl_ladder_3m 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 31552374 2. Chubascos LOSER 5.84 5.9 7390.59 {'price': [6, 5.9, 5.8], 'volume': [30.99, 82.... {'price': [6.2, 6.4, 6.6], 'volume': [4.99, 22... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620171 3. Love You More LOSER 65.00 70.0 1297.27 {'price': [65, 60, 55], 'volume': [2, 2.9, 15.... {'price': [75, 80, 85], 'volume': [0.66, 3.24,... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620172 4. Splashing Rossa LOSER 10.98 10.5 2665.94 {'price': [9, 8.8, 8.6], 'volume': [21.92, 10.... {'price': [9.6, 9.8, 10], 'volume': [13.43, 7.... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620173 5. The Fairytale LOSER 54.56 50.0 221.13 {'price': [55, 50, 48], 'volume': [4.85, 2.85,... {'price': [65, 70, 75], 'volume': [2.1, 7.18, ... 1.179776179 2021-02-27 10:48:00 AU Toowoomba R7 1000m Mdn 38620174 6. My Boy Dragon LOSER 166.90 160.0 199.00 {'price': [140, 120, 110], 'volume': [0.36, 1.... {'price': [260, 270, 340], 'volume': [1.29, 2.... When backtesting, and developing wagering systems more generally, I've found it really helpful to have a set of standard patterns or ways of representing common datasets. For a task like this it's really helpful to keep everything joined and together in a wide table. So we want a dataframe with everything we need to conduct the backtest: your model ratings, the odds you're betting at, the results on the bets, and ultimately betting logic will all become columns in a dataframe. It's helpful to have consistent column names so that the code for any new test you run looks much like previous tests and you can leverage custom functions that can be reused across tests and other projects. I like to have the following columns in my backtesting dataframe: date market_id (can be a surrogate id if dealing with fixed odds markets) selection_id (could be selection name) win (a binary win loss) model_odds model_prob market_odds market_prob bet_side stake gpl commission npl This analysis will be a little more complex as we're considering different price points so I'll leave out the market_odds and market_prob columns. # Joining the ratings data and odds data and combining rawDF = pd . merge ( hubRatings [ hubRatings [ 'market_id' ] . isin ( bfOdds . market_id . unique ())], bfOdds [[ 'market_name' , 'market_id' , 'selection_id' , 'result' , 'matched_volume' , 'bsp' , 'ltp' , 'atb_ladder_3m' , 'atl_ladder_3m' ]], on = [ 'market_id' , 'selection_id' ], how = 'inner' ) rawDF date track race_number race_name market_id selection_id selection_name model_odds market_name result matched_volume bsp ltp atb_ladder_3m atl_ladder_3m 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523320 11. Vast Kama 34.28 R1 1200m 3yo LOSER 1934.49 42.00 42.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523319 10. Triptonic 21.22 R1 1200m 3yo LOSER 1710.76 23.87 23.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 35773035 9. Right Reason 10.23 R1 1200m 3yo LOSER 5524.11 12.50 11.5 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523318 8. Off Road 40.75 R1 1200m 3yo LOSER 1506.51 35.31 34.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 2021-02-18 DOOMBEN 1 R1 1200m 3yo 1.179418181 38523317 7. More Than Value 77.49 R1 1200m 3yo LOSER 617.18 55.00 55.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28092381 11. Born A Warrior 10.67 R8 1300m Hcap LOSER 905.55 6.97 6.2 {'price': [6.2, 5.8, 5.1], 'volume': [7.98, 40... {'price': [6.8, 7, 7.8], 'volume': [6.26, 41.5... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38698010 12. Diva Bella 25.77 R8 1300m Hcap LOSER 11.06 23.60 18.5 {'price': [23, 22, 18], 'volume': [0.31, 24.91... {'price': [70, 75, 95], 'volume': [0.61, 3.5, ... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 28224034 13. Twice As Special 51.23 R8 1300m Hcap LOSER 52.49 36.37 26.0 {'price': [30, 29, 26], 'volume': [13.84, 5.92... {'price': [44, 50, 95], 'volume': [2.76, 1.66,... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 38913296 15. Rosie Riveter 24.92 R8 1300m Hcap LOSER 58.65 9.72 11.0 {'price': [10.5, 10, 9.6], 'volume': [0.69, 28... {'price': [11, 12.5, 19], 'volume': [3.87, 2.7... 2021-03-31 WARWICK FARM 8 R8 1300m Hcap 1.181250426 4973624 8. Celer 26.23 R8 1300m Hcap LOSER 22.14 21.73 28.0 {'price': [24, 23, 20], 'volume': [1.55, 18.26... {'price': [30, 65, 70], 'volume': [0.55, 1.55,... df = ( rawDF # Extra Best Back + Lay 3 mins before of . assign ( best_back_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atb_ladder_3m' ]]) . assign ( best_lay_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atl_ladder_3m' ]]) # Coalesce LTP to BSP (about 60 rows) . assign ( ltp = lambda x : np . where ( x [ \"ltp\" ] . isnull (), x [ \"bsp\" ], x [ \"ltp\" ])) # Add a binary win / loss column . assign ( win = lambda x : np . where ( x [ 'result' ] == \"WINNER\" , 1 , 0 )) # Extra columns . assign ( model_prob = lambda x : 1 / x [ 'model_odds' ]) # Reorder Columns . reindex ( columns = [ 'date' , 'track' , 'race_number' , 'market_id' , 'selection_id' , 'bsp' , 'ltp' , 'best_back_3m' , 'best_lay_3m' , 'atb_ladder_3m' , 'atl_ladder_3m' , 'model_prob' , 'model_odds' , 'win' ]) ) df . head ( 5 ) date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0","title":"Setting up your master data"},{"location":"historicData/backtestingRatingsTutorial/#staking-outcome-functions","text":"Now we can create a set of standard staking functions that a dataframe with an expected set of columns and add staking and bet outcome fields. We'll also add the ability of these functions to reference a different odds column so that we can backtest against our different price points. For simplicity we'll assume you're paying 5% commission on winnings however it could be higher or lower and depends on the MBR of the market. def bet_apply_commission ( df , com = 0.05 ): # Total Market GPL df [ 'market_gpl' ] = df . groupby ( 'market_id' )[ 'gpl' ] . transform ( sum ) # Apply 5% commission df [ 'market_commission' ] = np . where ( df [ 'market_gpl' ] <= 0 , 0 , 0.05 * df [ 'market_gpl' ]) # Sum of Market Winning Bets df [ 'floored_gpl' ] = np . where ( df [ 'gpl' ] <= 0 , 0 , df [ 'gpl' ]) df [ 'market_netwinnings' ] = df . groupby ( 'market_id' )[ 'floored_gpl' ] . transform ( sum ) # Partition Commission According to Selection GPL df [ 'commission' ] = np . where ( df [ 'market_netwinnings' ] == 0 , 0 , ( df [ 'market_commission' ] * df [ 'floored_gpl' ]) / ( df [ 'market_netwinnings' ])) # Calculate Selection NPL df [ 'npl' ] = df [ 'gpl' ] - df [ 'commission' ] # Drop excess columns df = df . drop ( columns = [ 'floored_gpl' , 'market_netwinnings' , 'market_commission' , 'market_gpl' ]) return ( df ) def bet_flat ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , # PUSH np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , stake ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) def bet_kelly ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , # PUSH np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , # PUSH 0 , np . where ( df [ 'bet_side' ] == \"B\" , ( ( 1 / df [ 'model_odds' ]) - ( 1 / df [ back_odds ]) ) / ( 1 - ( 1 / df [ back_odds ])), ( ( 1 / df [ lay_odds ]) - ( 1 / df [ 'model_odds' ]) ) / ( 1 - ( 1 / df [ lay_odds ])), ) ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) # Testing one of these functions flat_bets_bsp = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) flat_bets_bsp . head ( 5 ) date track race_number market_id selection_id bsp ltp best_back_3m best_lay_3m atb_ladder_3m atl_ladder_3m model_prob model_odds win bet_side stake gpl commission npl 2021-02-18 DOOMBEN 1 1.179418181 38523320 42.00 42.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [4.01, 6.15,... {'price': [32, 34, 36], 'volume': [1.76, 27.55... 0.029172 34.28 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523319 23.87 23.0 30.0 32.0 {'price': [30, 29, 28], 'volume': [24.72, 2.99... {'price': [32, 34, 36], 'volume': [0.4, 5.87, ... 0.047125 21.22 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 35773035 12.50 11.5 9.4 9.6 {'price': [9.4, 9.2, 9], 'volume': [6.22, 15.1... {'price': [9.6, 9.8, 10], 'volume': [11.19, 0.... 0.097752 10.23 0 B 1 -1.0 0.0 -1.0 2021-02-18 DOOMBEN 1 1.179418181 38523318 35.31 34.0 30.0 36.0 {'price': [30, 29, 28], 'volume': [5.52, 3.61,... {'price': [36, 38, 40], 'volume': [6.37, 3.87,... 0.024540 40.75 0 L 1 1.0 0.0 1.0 2021-02-18 DOOMBEN 1 1.179418181 38523317 55.00 55.0 65.0 70.0 {'price': [65, 60, 55], 'volume': [0.26, 4, 8.... {'price': [70, 75, 80], 'volume': [0.67, 3.68,... 0.012905 77.49 0 L 1 1.0 0.0 1.0","title":"Staking + Outcome Functions"},{"location":"historicData/backtestingRatingsTutorial/#evaluation-functions","text":"In my experience it's great to develop a suite of functions and analytical tools that really dig into every aspect of your simulated betting performance. You want to be as thorough and critical as possible, even when you're results are good. Another tip to guide this process is to have a reasonable benchmark. Essentially no one wins at 10% POT on thoroughbreds at the BSP so if your analysis suggests you can... there's a bug. Similarly you almost certainly won't lose at more than <-10%. Different sports and codes will have different realistic profitability ranges depending on the efficiency of the markets (will be roughly correlated to matched volume). Ruling out unreasonable results can save you a lot of time and delusion. I'm keeping it pretty simple here but you might also want to create functions to analyse: Track / distance based performance Performance across odds ranges Profit volatility (maybe using sharpe ratio to optimise volatility - adjusted profit) Date ranges (weeks / months etc) # Create simple PL and POT table def bet_eval_metrics ( d , side = False ): if side : metrics = ( d . groupby ( 'bet_side' , as_index = False ) . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) ) else : metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) # Cumulative PL by market to visually see trend and consistency def bet_eval_chart_cPl ( d ): d = ( d . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) d [ 'cNpl' ] = d . npl . cumsum () chart = px . line ( d , x = \"market_number\" , y = \"cNpl\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) return ( chart ) To illustrate these evaluation functions let's analyse flat staking at the BSP. bets = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) bet_eval_metrics ( bets , side = True ) bet_side npl stake pot B -749.493788 8356 -0.089695 L -268.499212 8592 -0.031250 bet_eval_chart_cPl ( bets ) So this isn't gonna build us an art gallery! This is to be expected though, it's not easy to make consistent profit certainly from free ratings sources available online.","title":"Evaluation Functions"},{"location":"historicData/backtestingRatingsTutorial/#testing-different-approaches","text":"We pulled those extra price points for a reason. Let's set up a little test harness that enables us to use different price points and bet using different staking functions. # We'll test a 2 different staking schemes on 3 different price points grid = { \"flat_bsp\" : ( bet_flat , \"bsp\" , \"bsp\" ), \"flat_ltp\" : ( bet_flat , \"ltp\" , \"ltp\" ), \"flat_3m\" : ( bet_flat , \"best_back_3m\" , \"best_lay_3m\" ), \"kelly_bsp\" : ( bet_kelly , \"bsp\" , \"bsp\" ), \"kelly_ltp\" : ( bet_kelly , \"ltp\" , \"ltp\" ), \"kelly_3m\" : ( bet_kelly , \"best_back_3m\" , \"best_lay_3m\" ) } metricSummary = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column # objects[0] is the staking function itself bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) # Calculate the metrics and tag with strategy label betMetrics = ( bet_eval_metrics ( bets ) . assign ( strategy = lambda x : strategy ) . reindex ( columns = [ 'strategy' , 'stake' , 'npl' , 'pot' ]) ) # Init the betMetrics df or append if already exists try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) strategy stake npl pot kelly_ltp 754.496453 -31.814150 -0.042166 kelly_bsp 732.165110 -34.613773 -0.047276 flat_bsp 16948.000000 -1017.993000 -0.060066 flat_ltp 16949.000000 -1184.546000 -0.069889 flat_3m 15712.000000 -1225.123000 -0.077974 kelly_3m 614.135601 -50.469295 -0.082179 # Compare Cumulative PL Charts cumulativePLs = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) d = ( bets . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' , 'stake' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) # Normalise to $10,000 stake for visual comparison d [ 'npl' ] = d [ 'npl' ] / ( d . stake . sum () / 10000 ) d [ 'cNpl' ] = d . npl . cumsum () d [ 'strategy' ] = strategy # Init the cumulativePLs df or append if already exists try : cumulativePLs = pd . concat ([ cumulativePLs , d ], ignore_index = True ) except : cumulativePLs = d px . line ( cumulativePLs , x = \"market_number\" , y = \"cNpl\" , color = \"strategy\" , title = 'Cumulative Net Profit' , template = 'simple_white' )","title":"Testing different approaches"},{"location":"historicData/backtestingRatingsTutorial/#searching-for-profit","text":"So this is often where you're going to arrive developing many wagering models: there's no indication of reliable long term profit. Where do you go from here? TBH I think most people give up here. Because you're not a quitter though you've got 3 main option categories: Make the underlying model better Search for better prices via detailed price analysis and clever bet placement Try to find a subset of these selections with these ratings and these price points that are sustainably profitable Obviously each situation is different but I think option 3 isn't a bad way to go initially because it will definitely help you understand your model better. For a racing model you might want to split your performance by: tracks or states track conditions or weather barriers race quality or grade odds ranges selection sample size (you likely perform worse on horses with little form for eg) perceived model value Finding a big enough slice across those dimensions that's either really profitable or really losing might reveal to you a bug in the data or workflow in your model development that you can go back and fix. As an example of a simple approach to selectiveness I'll quickly run through how being more selective about your perceived value might make a difference in final profitability. So our best performing strategy using our simple analysis above was Kelly staking at the last traded price. We'll start with that but be aware of that there's no way of implementing a LTP bet placement engine, you could imagine a proxy being placing limit bets \"just before\" the race jumps which is a whole other kettle of fish. Anyway, let's plot our profitability under this strategy at different perceived \"edges\". If we are more selective of only large overlays according to the hub's rated chance you can see we can increase the profitability. bets = bet_kelly ( df , back_odds = 'ltp' , lay_odds = 'ltp' ) metricSummary = None for bVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: for lVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: x = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( bVal , lVal )) betMetrics = bet_eval_metrics ( x , side = False ) betMetrics [ 'bVal' ] = bVal betMetrics [ 'lVal' ] = lVal try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) . head ( 4 ) npl stake pot bVal lVal -18.059813 574.944431 -0.031411 0.3 0.30 -22.887791 628.302349 -0.036428 0.3 0.20 -24.509182 669.482514 -0.036609 0.3 0.05 -22.997908 614.528386 -0.037424 0.2 0.30 betsFilters = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( 0.3 , 0.3 )) bet_eval_chart_cPl ( betsFilters ) We were doing ok till the last 200 market nightmare! Might be one to test with more data. So we still haven't found a clear profitable edge with these ratings, however we got a bit closer to break even which is positive. This step also indicates that this rating system performs better for large overlays which is a good model indicator (if you can't improve by selecting for larger overlays it's usually a sign you need to go back to the drawing board) You could imagine a few more iterations of analysis you might be able to eek out a slight edge However, be wary as these steps optimisation steps are very prone to overfitting so you need to be careful.","title":"Searching For Profit"},{"location":"historicData/backtestingRatingsTutorial/#conclusion-and-next-steps","text":"While using someone else's model is easy it's also not likely to end in personal riches. Developing your own model with your own tools and on a sport or racing code you know about is probably where you should start. However, hopefully this short guide helps you think about what to do when you finish the modelling component: How much money will I win or lose if I started using this system to place bets with real money? If you want to expand this backtesting analysis, here's a list (in no particular order) of things that I've omitted or angles I might look at next: Get more data -- more rating data and odds data is needed for draw a good conclusion about long term expectation Cross reference performance against race or selection metadata (track, # races run etc.) to improve performance with betting selectivity Extract more price points from the stream data to try to gain an pricing edge on these ratings","title":"Conclusion and Next Steps"},{"location":"historicData/backtestingRatingsTutorial/#over-to-you","text":"We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space","title":"Over to you"},{"location":"historicData/backtestingRatingsTutorial/#complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github import pandas as pd import numpy as np import requests from datetime import date , timedelta import os import re import tarfile import zipfile import bz2 import glob import logging from unittest.mock import patch from typing import List , Set , Dict , Tuple , Optional from itertools import zip_longest import plotly.express as px import betfairlightweight from betfairlightweight import StreamListener from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) % config IPCompleter . greedy = True #### -------------------------- #### FUNCTIONS #### -------------------------- # Function to return Pandas DF of hub ratings for a particular date def getHubRatings ( dte ): # Substitute the date into the URL url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date= {} presenter=RatingsPresenter&json=true' . format ( dte ) # Convert the response into JSON responseJson = requests . get ( url ) . json () hubList = [] if not responseJson : return ( None ) # Want an normalised table (1 row per selection) # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields for meeting in responseJson [ 'meetings' ]: for race in meeting [ 'races' ]: for runner in race [ 'runners' ]: hubList . append ( { 'date' : dte , 'track' : meeting [ 'name' ], 'race_number' : race [ 'number' ], 'race_name' : race [ 'name' ], 'market_id' : race [ 'bfExchangeMarketId' ], 'selection_id' : str ( runner [ 'bfExchangeSelectionId' ]), 'selection_name' : runner [ 'name' ], 'model_odds' : runner [ 'ratedPrice' ] } ) out = pd . DataFrame ( hubList ) return ( out ) # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if type ( v ) is float else v if type ( v ) is str else '' # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input sample: R6 1400m Grp1 parts = market_name . split ( ' ' ) race_no = parts [ 0 ] # return example R6 race_len = parts [ 1 ] # return example 1400m race_type = parts [ 2 ] . lower () # return example grp1, trot, pace return ( race_no , race_len , race_type ) # creating a flag that is True when markets are australian thoroughbreds def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # loading from tar and extracting files def load_markets ( file_paths ): for file_path in file_paths : print ( file_path ) if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # Extract Components From Generated Stream def extract_components_from_stream ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): # Will return 3 market books t-3mins marketbook, the last preplay marketbook and the final market book evaluate_market = None prev_market = None postplay_market = None preplay_market = None t3m_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # If markets don't meet filter return None's if evaluate_market is None and (( evaluate_market := filter_market ( market_book )) == False ): return ( None , None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = market_book # final market view before market goes is closed for settlement if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # Calculate Seconds Till Scheduled Market Start Time seconds_to_start = ( market_book . market_definition . market_time - market_book . publish_time ) . total_seconds () # Market at 3 mins before scheduled off if t3m_market is None and seconds_to_start < 3 * 60 : t3m_market = market_book # update reference to previous market prev_market = market_book # If market didn't go inplay if postplay_market is not None and preplay_market is None : preplay_market = postplay_market return ( t3m_market , preplay_market , postplay_market , prev_market ) # Final market is last prev_market def run_stream_parsing (): # Run Pipeline with open ( \"outputs/tho-odds.csv\" , \"w+\" ) as output : # Write Column Headers To File output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,ltp,matched_volume,atb_ladder_3m,atl_ladder_3m \\n \" ) for file_obj in load_markets ( data_path ): # Instantiate a \"stream\" object stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) # Extract key components according to the custom function above (outputs 4 objects) ( t3m_market , preplay_market , postplay_market , final_market ) = extract_components_from_stream ( stream ) # If no price data for market don't write to file if postplay_market is None : continue ; # Runner metadata and key fields available from final market book runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : r . sp . actual_sp } for r in final_market . runners ] # Last Traded Price # _____________________ # From the last marketbook before inplay or close ltp = [ runner . last_price_traded for runner in preplay_market . runners ] # Total Matched Volume # _____________________ # Calculates the traded volume across all traded price points for each selection def ladder_traded_volume ( ladder ): return ( sum ([ rung . size for rung in ladder ])) selection_traded_volume = [ ladder_traded_volume ( runner . ex . traded_volume ) for runner in postplay_market . runners ] # Top 3 Ladder # ______________________ # Extracts the top 3 price / stakes in available orders on both back and lay sides. Returns python dictionaries def top_3_ladder ( availableLadder ): out = {} price = [] volume = [] if len ( availableLadder ) == 0 : return ( out ) else : for rung in availableLadder [ 0 : 3 ]: price . append ( rung . price ) volume . append ( rung . size ) out [ \"price\" ] = price out [ \"volume\" ] = volume return ( out ) # Sometimes t-3 mins market book is empty try : atb_ladder_3m = [ top_3_ladder ( runner . ex . available_to_back ) for runner in t3m_market . runners ] atl_ladder_3m = [ top_3_ladder ( runner . ex . available_to_lay ) for runner in t3m_market . runners ] except : atb_ladder_3m = {} atl_ladder_3m = {} # Writing To CSV # ______________________ for ( runnerMeta , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ) in zip ( runner_data , ltp , selection_traded_volume , atb_ladder_3m , atl_ladder_3m ): if runnerMeta [ 'selection_status' ] != 'REMOVED' : output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( str ( final_market . market_id ), final_market . market_definition . market_time , final_market . market_definition . country_code , final_market . market_definition . venue , final_market . market_definition . name , runnerMeta [ 'selection_id' ], runnerMeta [ 'selection_name' ], runnerMeta [ 'selection_status' ], runnerMeta [ 'sp' ], ltp , selection_traded_volume , '\"' + str ( atb_ladder_3m ) + '\"' , # Forcing the dictionaries to strings so we don't run into issues loading the csvs with the dictionary commas '\"' + str ( atl_ladder_3m ) + '\"' ) ) def bet_apply_commission ( df , com = 0.05 ): # Total Market GPL df [ 'market_gpl' ] = df . groupby ( 'market_id' )[ 'gpl' ] . transform ( sum ) # Apply 5% commission df [ 'market_commission' ] = np . where ( df [ 'market_gpl' ] <= 0 , 0 , 0.05 * df [ 'market_gpl' ]) # Sum of Market Winning Bets df [ 'floored_gpl' ] = np . where ( df [ 'gpl' ] <= 0 , 0 , df [ 'gpl' ]) df [ 'market_netwinnings' ] = df . groupby ( 'market_id' )[ 'floored_gpl' ] . transform ( sum ) # Partition Commission According to Selection GPL df [ 'commission' ] = np . where ( df [ 'market_netwinnings' ] == 0 , 0 , ( df [ 'market_commission' ] * df [ 'floored_gpl' ]) / ( df [ 'market_netwinnings' ])) # Calculate Selection NPL df [ 'npl' ] = df [ 'gpl' ] - df [ 'commission' ] # Drop excess columns df = df . drop ( columns = [ 'floored_gpl' , 'market_netwinnings' , 'market_commission' , 'market_gpl' ]) return ( df ) def bet_flat ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , stake ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) def bet_kelly ( df , stake = 1 , back_odds = 'market_odds' , lay_odds = 'market_odds' ): \"\"\" Betting DF should always contain: model_odds, and win (binary encoded), and the specified odds column columns \"\"\" df [ 'bet_side' ] = np . where (( df [ \"model_odds\" ] >= df [ back_odds ]) & ( df [ \"model_odds\" ] <= df [ lay_odds ]), \"P\" , np . where ( df [ \"model_odds\" ] < df [ back_odds ], \"B\" , \"L\" ) ) df [ 'stake' ] = np . where ( df [ 'bet_side' ] == \"P\" , 0 , np . where ( df [ 'bet_side' ] == \"B\" , ( ( 1 / df [ 'model_odds' ]) - ( 1 / df [ back_odds ]) ) / ( 1 - ( 1 / df [ back_odds ])), ( ( 1 / df [ lay_odds ]) - ( 1 / df [ 'model_odds' ]) ) / ( 1 - ( 1 / df [ lay_odds ])), ) ) df [ 'gpl' ] = np . where ( df [ 'bet_side' ] == \"B\" , np . where ( df [ 'win' ] == 1 , df [ 'stake' ] * ( df [ back_odds ] - 1 ), - df [ 'stake' ]), # PL for back bets np . where ( df [ 'win' ] == 1 , - df [ 'stake' ] * ( df [ lay_odds ] - 1 ), df [ 'stake' ]) # PL for lay bets ) # Apply commission and NPL df = bet_apply_commission ( df ) return ( df ) # Create simple PL and POT table def bet_eval_metrics ( d , side = False ): if side : metrics = ( d . groupby ( 'bet_side' , as_index = False ) . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) ) else : metrics = pd . DataFrame ( d . agg ({ \"npl\" : \"sum\" , \"stake\" : \"sum\" }) ) . transpose () . assign ( pot = lambda x : x [ 'npl' ] / x [ 'stake' ]) return ( metrics [ metrics [ 'stake' ] != 0 ]) # Cumulative PL by market to visually see trend and consistency def bet_eval_chart_cPl ( d ): d = ( d . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) d [ 'cNpl' ] = d . npl . cumsum () chart = px . line ( d , x = \"market_number\" , y = \"cNpl\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) return ( chart ) #### -------------------------- #### EXECUTION #### -------------------------- # Loop through all recent history dateDFList = [] dateList = pd . date_range ( date ( 2021 , 2 , 18 ), date . today () - timedelta ( days = 1 ), freq = 'd' ) for dte in dateList : dateDFList . append ( getHubRatings ( dte )) # Concatenate (add rows to rows) all the dataframes within the list hubRatings = pd . concat ( dateDFList ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" ) # create listener listener = StreamListener ( max_latency = None ) # This will execute the files (it took me ~2 hours for 2 months of data) #run_stream_parsing() # Load in odds file we created above bfOdds = pd . read_csv ( \"outputs/tho-odds.csv\" , dtype = { 'market_id' : object , 'selection_id' : object , 'atb_ladder_3m' : object , 'atl_ladder_3m' : object }) # Convert dictionary columns import ast bfOdds [ 'atb_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atb_ladder_3m' ]] bfOdds [ 'atl_ladder_3m' ] = [ ast . literal_eval ( x ) for x in bfOdds [ 'atl_ladder_3m' ]] # Convert LTP to Numeric bfOdds [ 'ltp' ] = pd . to_numeric ( bfOdds [ 'ltp' ], errors = 'coerce' ) # Filter after 18th Feb bfOdds = bfOdds . query ( 'event_date >= \"2021-02-18\"' ) # Joining the ratings data and odds data and combining rawDF = pd . merge ( hubRatings [ hubRatings [ 'market_id' ] . isin ( bfOdds . market_id . unique ())], bfOdds [[ 'market_name' , 'market_id' , 'selection_id' , 'result' , 'matched_volume' , 'bsp' , 'ltp' , 'atb_ladder_3m' , 'atl_ladder_3m' ]], on = [ 'market_id' , 'selection_id' ], how = 'inner' ) # Join and clean up columns df = ( rawDF # Extra Best Back + Lay 3 mins before of . assign ( best_back_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atb_ladder_3m' ]]) . assign ( best_lay_3m = lambda x : [ np . nan if d . get ( 'price' ) is None else d . get ( 'price' )[ 0 ] for d in x [ 'atl_ladder_3m' ]]) # Coalesce LTP to BSP (about 60 rows) . assign ( ltp = lambda x : np . where ( x [ \"ltp\" ] . isnull (), x [ \"bsp\" ], x [ \"ltp\" ])) # Add a binary win / loss column . assign ( win = lambda x : np . where ( x [ 'result' ] == \"WINNER\" , 1 , 0 )) # Extra columns . assign ( model_prob = lambda x : 1 / x [ 'model_odds' ]) # Reorder Columns . reindex ( columns = [ 'date' , 'track' , 'race_number' , 'market_id' , 'selection_id' , 'bsp' , 'ltp' , 'best_back_3m' , 'best_lay_3m' , 'atb_ladder_3m' , 'atl_ladder_3m' , 'model_prob' , 'model_odds' , 'win' ]) ) bets = bet_flat ( df , stake = 1 , back_odds = 'bsp' , lay_odds = 'bsp' ) bet_eval_metrics ( bets , side = True ) bet_eval_chart_cPl ( bets ) # We'll test a 2 different staking schemes on 3 different price points grid = { \"flat_bsp\" : ( bet_flat , \"bsp\" , \"bsp\" ), \"flat_ltp\" : ( bet_flat , \"ltp\" , \"ltp\" ), \"flat_3m\" : ( bet_flat , \"best_back_3m\" , \"best_lay_3m\" ), \"kelly_bsp\" : ( bet_kelly , \"bsp\" , \"bsp\" ), \"kelly_ltp\" : ( bet_kelly , \"ltp\" , \"ltp\" ), \"kelly_3m\" : ( bet_kelly , \"best_back_3m\" , \"best_lay_3m\" ) } # Evaluate Metrics For Strategy Grid metricSummary = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column # objects[0] is the staking function itself bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) betMetrics = ( bet_eval_metrics ( bets ) . assign ( strategy = lambda x : strategy ) . reindex ( columns = [ 'strategy' , 'stake' , 'npl' , 'pot' ]) ) try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) # Compare Cumulative PL Charts cumulativePLs = None for strategy , objects in grid . items (): # Assemble bets based on staking function and odds column bets = objects [ 0 ]( df , back_odds = objects [ 1 ], lay_odds = objects [ 2 ]) d = ( bets . groupby ( 'market_id' ) . agg ({ 'npl' : 'sum' , 'stake' : 'sum' }) ) d [ 'market_number' ] = np . arange ( len ( d )) # Normalise to $10,000 stake for visual comparison d [ 'npl' ] = d [ 'npl' ] / ( d . stake . sum () / 10000 ) d [ 'cNpl' ] = d . npl . cumsum () d [ 'strategy' ] = strategy try : cumulativePLs = pd . concat ([ cumulativePLs , d ], ignore_index = True ) except : cumulativePLs = d px . line ( cumulativePLs , x = \"market_number\" , y = \"cNpl\" , color = \"strategy\" , title = 'Cumulative Net Profit' , template = 'simple_white' ) bets = bet_kelly ( df , back_odds = 'ltp' , lay_odds = 'ltp' ) metricSummary = None for bVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: for lVal in [ 0.05 , 0.1 , 0.15 , 0.2 , 0.3 ]: x = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( bVal , lVal )) betMetrics = bet_eval_metrics ( x , side = False ) betMetrics [ 'bVal' ] = bVal betMetrics [ 'lVal' ] = lVal try : metricSummary = pd . concat ([ metricSummary , betMetrics ], ignore_index = True ) except : metricSummary = betMetrics metricSummary . sort_values ( by = [ 'pot' ], ascending = False ) . head ( 4 ) betsFilters = bets . query ( '((ltp-model_odds) / ltp) > {} | ((model_odds-ltp) / ltp) > {} ' . format ( 0.3 , 0.3 )) bet_eval_chart_cPl ( betsFilters )","title":"Complete code"},{"location":"historicData/backtestingRatingsTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"historicData/dataSources/","text":"Historical Data Sources We know that your automated strategies and models are only as good as your data. We work hard to make sure you have access to the data you need to allow you to achieve what you're setting out to in your automation and modelling projects. There\u2019s a huge variety of historic pricing data available, and hopefully this page shows you how to access what you're looking for. For more information on how to use this data to make your own predictive model, take a look at our modelling section . Historical Stream API data Betfair UK give access to all the historical Stream API data since 2016. It is excellent to use in building models and back testing strategies, however isn't necessarily in an easily accessible format for everyone. What you need to know about this data source: JSON format, downloads as TAR files (zipped) Australian and overseas racing, plus soccer, tennis, cricket, golf and \u2018other sport\u2019 data All Exchange markets included since the Stream API was introduced in 2016 Time-stamped odds and volume data Able to filter by Event ID, market type and other parameters 3 tiers of access: Basic free tier \u2013 1 minute intervals for odds, no volume (free) Advanced tier \u2013 1 second intervals for odds, volume included (cost associated) Pro tier \u2013 50 millisecond intervals for odds, volume included (cost associated) Includes a Historic Data API endpoint for download management Supporting resources to help you access this data: Historic Data FAQs & sample data Historic Data Specifications Sample code for using the historic data download API Tutorials for working with this data JSON to CSV in Python JSON to CSV | Revisited - where we make it 30 times faster Backtesting ratings using historic data in Python Automated betting angles: no modelling required Historical racing data This is an excellent resource if you are interested in racing and like to see market level data in a CSV format. What you need to know about this data source: CSV format Free to download All Australian and overseas races, dating back to the beginning of the Exchange Available as a single file per day, per country, win or place market Market snapshot by runner, including Max and min matched prices and volume, pre-play and in-play Weighted average price, pre-play and in-play BSP Winner If none of these options suit your needs please contact us at data@betfair.com.au to discuss other potential options.","title":"Pricing Data Sources"},{"location":"historicData/dataSources/#historical-data-sources","text":"We know that your automated strategies and models are only as good as your data. We work hard to make sure you have access to the data you need to allow you to achieve what you're setting out to in your automation and modelling projects. There\u2019s a huge variety of historic pricing data available, and hopefully this page shows you how to access what you're looking for. For more information on how to use this data to make your own predictive model, take a look at our modelling section .","title":"Historical Data Sources"},{"location":"historicData/dataSources/#historical-stream-api-data","text":"Betfair UK give access to all the historical Stream API data since 2016. It is excellent to use in building models and back testing strategies, however isn't necessarily in an easily accessible format for everyone.","title":"Historical Stream API data"},{"location":"historicData/dataSources/#what-you-need-to-know-about-this-data-source","text":"JSON format, downloads as TAR files (zipped) Australian and overseas racing, plus soccer, tennis, cricket, golf and \u2018other sport\u2019 data All Exchange markets included since the Stream API was introduced in 2016 Time-stamped odds and volume data Able to filter by Event ID, market type and other parameters 3 tiers of access: Basic free tier \u2013 1 minute intervals for odds, no volume (free) Advanced tier \u2013 1 second intervals for odds, volume included (cost associated) Pro tier \u2013 50 millisecond intervals for odds, volume included (cost associated) Includes a Historic Data API endpoint for download management","title":"What you need to know about this data source:"},{"location":"historicData/dataSources/#supporting-resources-to-help-you-access-this-data","text":"Historic Data FAQs & sample data Historic Data Specifications Sample code for using the historic data download API","title":"Supporting resources to help you access this data:"},{"location":"historicData/dataSources/#tutorials-for-working-with-this-data","text":"JSON to CSV in Python JSON to CSV | Revisited - where we make it 30 times faster Backtesting ratings using historic data in Python Automated betting angles: no modelling required","title":"Tutorials for working with this data"},{"location":"historicData/dataSources/#historical-racing-data","text":"This is an excellent resource if you are interested in racing and like to see market level data in a CSV format.","title":"Historical racing data"},{"location":"historicData/dataSources/#what-you-need-to-know-about-this-data-source_1","text":"CSV format Free to download All Australian and overseas races, dating back to the beginning of the Exchange Available as a single file per day, per country, win or place market Market snapshot by runner, including Max and min matched prices and volume, pre-play and in-play Weighted average price, pre-play and in-play BSP Winner If none of these options suit your needs please contact us at data@betfair.com.au to discuss other potential options.","title":"What you need to know about this data source:"},{"location":"historicData/jsonToCsvTutorial/","text":"JSON to CSV tutorial: making a market summary Before you start This tutorial was original shared in 2021. Since then a new library has been created that allows you to run the same logic included here with about a 97% reduction in run time, which makes a significant difference in usability. To learn about these changes and how to implement them to speed up your code take a look at our JSON to CSV revisited article . The historic pricing data available on the Betfair Historic Data site is an excellent resource, including almost every market offered on the Exchange back to 2016. We do appreciate though that the JSON format of the data sets can make it challenging to find value in the data, especially if you're not confident in working with large data sets. In this tutorial we're going to step through the process of using the Python betfairlightweight library to take in a compressed tar folder, process the historic JSON files, and convert the data into a simple csv output, including basic market summary data for each runner split into pre play and in play values. We're also going to include a filter function, to allow us to filter out markets we're not interested in. The idea of this tutorial is to share a way of using existing libraries to make working with the JSON data sets easier, and hopefully the provide a foundation that you can build your own code base and data sets from. We'll be focusing on horse racing data; what we want to produce is a csv output that includes one row per runner for each market we're interested in, along with summary pre-play and in-play data for the runner. We'll step through the issues we encountered and how we went about solving the various challenges, including sharing relevant code snips along the way. We're not Python natives and acknowledge that there are probably more efficient and neater ways of achieving the same end goal! As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. The script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! Make sure you amend your data path to point to your data file (instructions below). We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo . Setting up your environment You're going to need to make sure you have Python and pip installed to get this code to run. If you're just starting out with Python, you may have to add Python to your environment variables . The is generally easiest to do by checking the box when you're installing Python choosing to 'add to PATH'. The alternative approach to the above is to use a Jupyter notebook which has the environment already set up - this might be the easier option for people new to programming. We're using some pretty new Python features, so it might be worth checking your version and updating if you're keen to follow along. To install betfairlightweight open a command prompt, or a terminal in your text editor of choice and input pip install betfairlightweight then return. Data input We started with the historic data parsing example from liampauling 's Github repo. Our first issue was that the example provided was expecting to take in an individual market file. We wanted to be able to accept data in a tar archive, a zipped folder, or a directory of individual bz2 files. Here's the code we used for handling the different file formats. # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None and then used it like this: # the path directories to the data sets # accepts tar files, zipped files or # directory with bz2 file(s) market_paths = [ './2020_12_DecRacingPro.zip' , './PRO' , './2021_01_JanRacingPro.tar' ] ... for file_obj in load_markets ( market_paths ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): This means we can pass in the tar and/or zipped file in its compressed form and/or directory with individual bz2 files in it and not worry about extracting the file contents, or having to handle the logic of iterating over the inner nested file structure. File paths The program will look at the file path you pass in relative to the location of the script you're running. So it will start by looking in the same folder it's saved in and then follow your navigation instructions from there, using / to indicate a folder and ../ to navigate up a level in the folder structure. If our example the data files sit in the same folder as the script ( ./PRO ). If it were in a folder at the same level as the folder that our script is in then we'd need to navigate 'up' a level (using ../ ) and then into the folder housing the data, i.e. '../dataFolder/PRO' and if the data were in a different folder within the same folder as our script file we'd use './dataFolder/PRO' etc. Type definitions If you're used to working in strongly typed languages, especially those with type definitions, you might find it a bit frustrating to try and figure out where you can access the different data types, for example market name or runner BSP. There are some things you can do to make this a bit easier, other than digging into the betfairlightweight source code, which was where we started. If you want to look at the definitions from the source code: MarketBook, RunnerBook MarketDefinitionRunner, MarketDefinition There are some Python extensions you can use in your ide that go some way to helping here. # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) Market summary data The raw files show the data at 50ms (PRO) or 1 second (ADVANCED) intervals. Too produce our csv we will need to look at the state of the market before the market goes in play, and then the state at the end of the market, and calculate from that what the pre play and in play figures are. This is the data we're going to include in our output csv. Column Definition market_id unique market identifier event_date scheduled start date/time (UTC) country event country code track track name market_name market name selection_id unique runner identifier selection_name runner name result win/loss/removed bsp Betfair starting price pp_min pre play min price traded pp_max pre play max price traded pp_wap pre play weighted average price pp_ltp pre play last traded price pp_volume pre play matched volume ip_min in play min price traded ip_max in play max price traded ip_wap in play weighted average price ip_ltp in play last traded price ip_volume in play matched volume betfairlightweight exposes snapshots of the market that include all the price data we need. To allow us to compute pre play and in play figures there are three market snapshots we need to find. These are the final view before the market turns in play, the market at the end of the race once it's no longer open but the price ladder hasn't yet been cleared, and the final closed snapshot that shows winner/loser status etc. We can then use the deltas between these market views to calculate the pre play and in play summary statistics. We iterate over these market snapshots and when we find the first market showing as in play we go back to the previous update, and use this as our pre play view. After this we keep iterating until we find the last time that the market status shows as 'open' and then use the data from the following update for the final pricing data (i.e. the first market view once the market was suspended at the end of the race). The winner/loser statuses come from the final market view. def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) We needed to write a function to parse the price data (pre play and in play) and pull out the values we're interested in. We used a reduce function to go over each matched price point, and calculate the four necessary values. To calculate weighted average price we multiplied price by size for each price point, and added them together. Once they're summed, we divided that figure by the total matched value. The matched volume is simply the sum of all matched stakes. The min price and max price are the lowest and highest values where money has matched on the runner. Reduce functions I gather from some actual Python gurus in our community that while reduce functions are very common in other languages (i.e. the ones I normally work in!), apparently they're not very Pythonic... if you're super keen, feel free to rewrite this section into a list/dict comprehension or another more Pythonic solution! # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> ( float , float , float , float ): if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) Our volume figures don't include BSP bets yet, so to account for that we're looking at the back_stake_taken and lay_liability_taken values on the SP object from the post play market snapshot, then finding whichever the smaller of those two values is and saving it that so we can add it to the traded_volume field in a later step. We use the smaller value of back_stake_taken or ( lay_liability_taken /(BSP - 1)) (i.e. backer's stake for SP lay bets) as any difference between the two values will have matched against non-BSP money and therefore is already accounted for in our matched volume. preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) ) for r in postplay_market . runners ] For our csv, we have columns for runner id, runner name, winning status and BSP, so we'll store these values too. The runner name is a bit harder to get, as we need to match up the runner definition with the same selection_id as the market_book object we're currently looking at. # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] Not all markets go in play, and therefore won't have any values for the in play portion of the csv, so we need to make sure we can handle this case. We don't have in play figures separate to pre play; we have a snapshot before the market went in play, and then the view at the end of the market, so we need to use the difference between these two sets of figures to figure out what happened in play. We have two ladders, one post play and one pre play. We go through every price point in the post play ladder, and remove any volume that's showing in the pre play ladder at the corresponding price point. This leaves us with the volumes matched while the market was in play. One corner case we had to catch is that our resulting list might have prices with 0 volume, which trip up our min and max values, which doesn't use volume in its calculations. To catch this we filter out any items from the ladder with a volume of 0. Note: there are some markets included in the data files that are effective empty and don't contain any price data. We're disregarding these markets and printing out an error message to the log ( market has no price data ). # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] Writing to CSV We defined the columns we want for our csv pretty early in the code. # record prices to a file with open ( \"output.csv\" , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) We then assign the values for each column. # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Filtering markets Currently we're going through every file provided in the raw data folders, which in our case included markets from different countries, all different market types and both gallops and harness races. To save filtering these markets manually later in Excel, and also to avoid processing additional data we don't need and slowing the process down further, we decided to add a market filter so we only kept the markets we were interested in. We filtered on three things: event country code (i.e. AU, NZ, GB etc) market type (i.e. win, place etc) race type (i.e. gallops or harness) Using this logic, we are only keeping Australian win markets for gallops races. # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) Filtering out harness markets was the trickiest part of the process, as there's no neat way of separating harness meetings from gallops. To do this we had to parse the market name and look for the words 'trot' and 'pace', and treat the market as harness if we found either. To make it a little tidier we wrote a function to split the market name into its component parts. # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) We declare an evaluate_market flag and set it to none, and then in our loop the first time we evaluate the market we run the filter and skip any markets that don't meet our criteria. eval_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) Helper functions There are a couple of helper functions we wrote along the way to make the rest of the code easier to handle. As string Takes in a number and returns a text representation of it, rounding to two decimal places. # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' Min value greater than 0 Returns the smaller of two numbers, where the smaller isn't 0. # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) Final thoughts betfairlightweight provides a ready made package that makes it easier to work with the JSON data and a pretty easy way to convert the data into a csv format, allowing you to then do your data wrangling in Excel if that's where you're more comfortable. Our intention is that you don't need a heap of Python experience to be able to work through this tutorial; as long as you're prepared to get the Python environment set up and learn some basic programming skills, the hope is that you'll be able to customise your own csv file and maybe even extend on what we've covered and produced here. We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space Complete code Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Please note: the script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! You'll also see errors logged to the out file or terminal screen depending on your set up. Download from Github import logging from typing import List , Tuple from unittest.mock import patch from itertools import zip_longest import functools import os import tarfile import zipfile import bz2 import glob # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) file_output = \"output_bflw.csv\" market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] # setup logging logging . basicConfig ( level = logging . FATAL ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" , \"appkey\" ) # create listener listener = betfairlightweight . StreamListener ( max_latency = None , # ignore latency errors output_queue = None , # use generator rather than a queue (faster) lightweight = False , # lightweight mode is faster update_clk = False , # do not update clk on updates (not required when backtesting) cumulative_runner_tv = True , calculate_market_tv = True ) # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> Tuple [ float , float , float , float ]: if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> Tuple [ str , str , str ]: # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d != None and d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # record prices to a file with open ( file_output , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for i , file_obj in enumerate ( load_markets ( market_paths )): print ( \"Market {} \" . format ( i ), end = ' \\r ' ) stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) # no price data for market if postplay_market is None : continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) if r . sp . actual_sp is not None else 0 , ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"JSON to CSV in Python"},{"location":"historicData/jsonToCsvTutorial/#json-to-csv-tutorial-making-a-market-summary","text":"Before you start This tutorial was original shared in 2021. Since then a new library has been created that allows you to run the same logic included here with about a 97% reduction in run time, which makes a significant difference in usability. To learn about these changes and how to implement them to speed up your code take a look at our JSON to CSV revisited article . The historic pricing data available on the Betfair Historic Data site is an excellent resource, including almost every market offered on the Exchange back to 2016. We do appreciate though that the JSON format of the data sets can make it challenging to find value in the data, especially if you're not confident in working with large data sets. In this tutorial we're going to step through the process of using the Python betfairlightweight library to take in a compressed tar folder, process the historic JSON files, and convert the data into a simple csv output, including basic market summary data for each runner split into pre play and in play values. We're also going to include a filter function, to allow us to filter out markets we're not interested in. The idea of this tutorial is to share a way of using existing libraries to make working with the JSON data sets easier, and hopefully the provide a foundation that you can build your own code base and data sets from. We'll be focusing on horse racing data; what we want to produce is a csv output that includes one row per runner for each market we're interested in, along with summary pre-play and in-play data for the runner. We'll step through the issues we encountered and how we went about solving the various challenges, including sharing relevant code snips along the way. We're not Python natives and acknowledge that there are probably more efficient and neater ways of achieving the same end goal! As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. The script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! Make sure you amend your data path to point to your data file (instructions below). We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfairlightweight package to do the heavy lifting We've also posted the completed code logic on the betfair-downunder Github repo .","title":"JSON to CSV tutorial: making a market summary"},{"location":"historicData/jsonToCsvTutorial/#setting-up-your-environment","text":"You're going to need to make sure you have Python and pip installed to get this code to run. If you're just starting out with Python, you may have to add Python to your environment variables . The is generally easiest to do by checking the box when you're installing Python choosing to 'add to PATH'. The alternative approach to the above is to use a Jupyter notebook which has the environment already set up - this might be the easier option for people new to programming. We're using some pretty new Python features, so it might be worth checking your version and updating if you're keen to follow along. To install betfairlightweight open a command prompt, or a terminal in your text editor of choice and input pip install betfairlightweight then return.","title":"Setting up your environment"},{"location":"historicData/jsonToCsvTutorial/#data-input","text":"We started with the historic data parsing example from liampauling 's Github repo. Our first issue was that the example provided was expecting to take in an individual market file. We wanted to be able to accept data in a tar archive, a zipped folder, or a directory of individual bz2 files. Here's the code we used for handling the different file formats. # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None and then used it like this: # the path directories to the data sets # accepts tar files, zipped files or # directory with bz2 file(s) market_paths = [ './2020_12_DecRacingPro.zip' , './PRO' , './2021_01_JanRacingPro.tar' ] ... for file_obj in load_markets ( market_paths ): stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): This means we can pass in the tar and/or zipped file in its compressed form and/or directory with individual bz2 files in it and not worry about extracting the file contents, or having to handle the logic of iterating over the inner nested file structure. File paths The program will look at the file path you pass in relative to the location of the script you're running. So it will start by looking in the same folder it's saved in and then follow your navigation instructions from there, using / to indicate a folder and ../ to navigate up a level in the folder structure. If our example the data files sit in the same folder as the script ( ./PRO ). If it were in a folder at the same level as the folder that our script is in then we'd need to navigate 'up' a level (using ../ ) and then into the folder housing the data, i.e. '../dataFolder/PRO' and if the data were in a different folder within the same folder as our script file we'd use './dataFolder/PRO' etc.","title":"Data input"},{"location":"historicData/jsonToCsvTutorial/#type-definitions","text":"If you're used to working in strongly typed languages, especially those with type definitions, you might find it a bit frustrating to try and figure out where you can access the different data types, for example market name or runner BSP. There are some things you can do to make this a bit easier, other than digging into the betfairlightweight source code, which was where we started. If you want to look at the definitions from the source code: MarketBook, RunnerBook MarketDefinitionRunner, MarketDefinition There are some Python extensions you can use in your ide that go some way to helping here. # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook )","title":"Type definitions"},{"location":"historicData/jsonToCsvTutorial/#market-summary-data","text":"The raw files show the data at 50ms (PRO) or 1 second (ADVANCED) intervals. Too produce our csv we will need to look at the state of the market before the market goes in play, and then the state at the end of the market, and calculate from that what the pre play and in play figures are. This is the data we're going to include in our output csv. Column Definition market_id unique market identifier event_date scheduled start date/time (UTC) country event country code track track name market_name market name selection_id unique runner identifier selection_name runner name result win/loss/removed bsp Betfair starting price pp_min pre play min price traded pp_max pre play max price traded pp_wap pre play weighted average price pp_ltp pre play last traded price pp_volume pre play matched volume ip_min in play min price traded ip_max in play max price traded ip_wap in play weighted average price ip_ltp in play last traded price ip_volume in play matched volume betfairlightweight exposes snapshots of the market that include all the price data we need. To allow us to compute pre play and in play figures there are three market snapshots we need to find. These are the final view before the market turns in play, the market at the end of the race once it's no longer open but the price ladder hasn't yet been cleared, and the final closed snapshot that shows winner/loser status etc. We can then use the deltas between these market views to calculate the pre play and in play summary statistics. We iterate over these market snapshots and when we find the first market showing as in play we go back to the previous update, and use this as our pre play view. After this we keep iterating until we find the last time that the market status shows as 'open' and then use the data from the following update for the final pricing data (i.e. the first market view once the market was suspended at the end of the race). The winner/loser statuses come from the final market view. def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) We needed to write a function to parse the price data (pre play and in play) and pull out the values we're interested in. We used a reduce function to go over each matched price point, and calculate the four necessary values. To calculate weighted average price we multiplied price by size for each price point, and added them together. Once they're summed, we divided that figure by the total matched value. The matched volume is simply the sum of all matched stakes. The min price and max price are the lowest and highest values where money has matched on the runner. Reduce functions I gather from some actual Python gurus in our community that while reduce functions are very common in other languages (i.e. the ones I normally work in!), apparently they're not very Pythonic... if you're super keen, feel free to rewrite this section into a list/dict comprehension or another more Pythonic solution! # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> ( float , float , float , float ): if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) Our volume figures don't include BSP bets yet, so to account for that we're looking at the back_stake_taken and lay_liability_taken values on the SP object from the post play market snapshot, then finding whichever the smaller of those two values is and saving it that so we can add it to the traded_volume field in a later step. We use the smaller value of back_stake_taken or ( lay_liability_taken /(BSP - 1)) (i.e. backer's stake for SP lay bets) as any difference between the two values will have matched against non-BSP money and therefore is already accounted for in our matched volume. preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) ) for r in postplay_market . runners ] For our csv, we have columns for runner id, runner name, winning status and BSP, so we'll store these values too. The runner name is a bit harder to get, as we need to match up the runner definition with the same selection_id as the market_book object we're currently looking at. # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] Not all markets go in play, and therefore won't have any values for the in play portion of the csv, so we need to make sure we can handle this case. We don't have in play figures separate to pre play; we have a snapshot before the market went in play, and then the view at the end of the market, so we need to use the difference between these two sets of figures to figure out what happened in play. We have two ladders, one post play and one pre play. We go through every price point in the post play ladder, and remove any volume that's showing in the pre play ladder at the corresponding price point. This leaves us with the volumes matched while the market was in play. One corner case we had to catch is that our resulting list might have prices with 0 volume, which trip up our min and max values, which doesn't use volume in its calculations. To catch this we filter out any items from the ladder with a volume of 0. Note: there are some markets included in the data files that are effective empty and don't contain any price data. We're disregarding these markets and printing out an error message to the log ( market has no price data ). # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ]","title":"Market summary data"},{"location":"historicData/jsonToCsvTutorial/#writing-to-csv","text":"We defined the columns we want for our csv pretty early in the code. # record prices to a file with open ( \"output.csv\" , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) We then assign the values for each column. # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) )","title":"Writing to CSV"},{"location":"historicData/jsonToCsvTutorial/#filtering-markets","text":"Currently we're going through every file provided in the raw data folders, which in our case included markets from different countries, all different market types and both gallops and harness races. To save filtering these markets manually later in Excel, and also to avoid processing additional data we don't need and slowing the process down further, we decided to add a market filter so we only kept the markets we were interested in. We filtered on three things: event country code (i.e. AU, NZ, GB etc) market type (i.e. win, place etc) race type (i.e. gallops or harness) Using this logic, we are only keeping Australian win markets for gallops races. # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) Filtering out harness markets was the trickiest part of the process, as there's no neat way of separating harness meetings from gallops. To do this we had to parse the market name and look for the words 'trot' and 'pace', and treat the market as harness if we found either. To make it a little tidier we wrote a function to split the market name into its component parts. # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> ( str , str , str ): # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) We declare an evaluate_market flag and set it to none, and then in our loop the first time we evaluate the market we run the filter and skip any markets that don't meet our criteria. eval_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None )","title":"Filtering markets"},{"location":"historicData/jsonToCsvTutorial/#helper-functions","text":"There are a couple of helper functions we wrote along the way to make the rest of the code easier to handle. As string Takes in a number and returns a text representation of it, rounding to two decimal places. # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' Min value greater than 0 Returns the smaller of two numbers, where the smaller isn't 0. # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b )","title":"Helper functions"},{"location":"historicData/jsonToCsvTutorial/#final-thoughts","text":"betfairlightweight provides a ready made package that makes it easier to work with the JSON data and a pretty easy way to convert the data into a csv format, allowing you to then do your data wrangling in Excel if that's where you're more comfortable. Our intention is that you don't need a heap of Python experience to be able to work through this tutorial; as long as you're prepared to get the Python environment set up and learn some basic programming skills, the hope is that you'll be able to customise your own csv file and maybe even extend on what we've covered and produced here. We're planning on writing some more tutorials to help make it easier to work with the JSON data sets. If there are particular examples or data sets you'd like to see us walk through please reach out . Community support There's a really active betfairlightweight Slack community that's a great place to go to ask questions about the library and get support from other people who are also working in the space","title":"Final thoughts"},{"location":"historicData/jsonToCsvTutorial/#complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Please note: the script will take some time before it starts outputting to the output.csv file, so let it run for a few minutes before getting worried that it's not working! You'll also see errors logged to the out file or terminal screen depending on your set up. Download from Github import logging from typing import List , Tuple from unittest.mock import patch from itertools import zip_longest import functools import os import tarfile import zipfile import bz2 import glob # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) file_output = \"output_bflw.csv\" market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] # setup logging logging . basicConfig ( level = logging . FATAL ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" , \"appkey\" ) # create listener listener = betfairlightweight . StreamListener ( max_latency = None , # ignore latency errors output_queue = None , # use generator rather than a queue (faster) lightweight = False , # lightweight mode is faster update_clk = False , # do not update clk on updates (not required when backtesting) cumulative_runner_tv = True , calculate_market_tv = True ) # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> Tuple [ float , float , float , float ]: if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> Tuple [ str , str , str ]: # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d != None and d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # record prices to a file with open ( file_output , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for i , file_obj in enumerate ( load_markets ( market_paths )): print ( \"Market {} \" . format ( i ), end = ' \\r ' ) stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) # no price data for market if postplay_market is None : continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) if r . sp . actual_sp is not None else 0 , ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) )","title":"Complete code"},{"location":"historicData/jsonToCsvTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"historicData/usingHistoricDataSite/","text":"Using the Historic Data site The Betfair Historic Data site includes complete historic data for nearly all markets offered on the Exchange since 2016, when the new APING was launched. The data available includes prices, volume traded, winning status, average weighted price, BSP, and a variety of other details that are valuable for modelling and strategy development. We know that the process of downloading and extracting these data files can be a bit intimidating the first time round, so here's a walk through of one way to go about it to help make it more accessible. Data tiers There are three tiers of historic data available on this site. You can download samples of each tier of data here . The biggest differece is between the free and paid data. The free data includes a lot of information about the market, but no volume, and only last traded price per minute, not a full price ladder. The two paid tiers include the same data, just at different frequencies. If your strategy isn't particularly price sensitive and doesn't need volume as a variable then you'll probably be fine wtih the free tier, however if you need to see a more granular view of the market then you should probably consider the paid advanced or pro tiers. A full catalogue of the values included in each data tier is available here . Basic Advanced Pro 1 minute intervals last traded price no volume 1 second intervals price ladder (top 3) volume API tick intervals (50ms) price ladder (full) volume Purchasing the data Start by going to the Betfair Historic Data site and log in using your Betfair account. Note: if you have less than 100 Betfair points you may have problems downloading data. On the Home page select the data set you want to download. Free data You need to 'purchase' the data set you want to download, even if it's from the free tier You can only 'purchase' each time period of data once. For example, if you had previously 'purchased' all Greyhound data for January 2018, then tried to download Greyhound data for January to March 2018 you would receive an error, and would need to purchase the data for February to March instead. Once you 'purchase' your choice of data it's recommended that you go to the My Data page, and choose the subset of data to then download. Downloading the data On the My Data page you can filter the purchased data to the actual markets you're interested in. You can filter by Sport, Date range, EventId, Event Name, Market Type, Country & File Type (M = market, E = Event), which will cut down the size of the data you need to download. For example, if you wanted the win market for Australian and New Zealand greyhound races you'd use these filters. File type The file type filter has two options that you can choose from: E = Event - includes event level data, i.e. Geelong greyhounds on x date M = Market - includes market level data, i.e. the win market for Geelong greyhounds race 3 on x date The site can be pretty slow to download from, and you'll generally have a better experience if you download the data a bit at a time, say month by month. Alternatively if you're going to download a lot of data it might be worth having a look at the historic data API, that can automate the download process and speed it up significantly. There's a guide available here , and some sample code the help get you started. Unzipping the files You'll need to download a program to unzip the TAR files. Here we'll be using 7Zip , which is free, open source and generally well respected. Once you've downloaded it make sure you also install it onto the computer you'll be using to open the data files. Locate the data.tar file in your computer's file explorer program. Right click on the file, select '7-Zip' from the menu then choose 'Extract files...'. In the model that pops up change the path mode to 'No pathnames'. You can also change the name and/or path of the folder you want the files extracted to if you want to. You now have a collection of .bz2 files. The final step is to select all the files, right click, select '7-Zip' from the menu then choose 'Extract here'. This will then extract all the individual zipped files which you can then either open in a text editor - you can use something basic like Notepad (installed on basically all computers by default) or a more complete program like Visual Studio Code (my go to), Vim or Notepad++ - or you can parse over the using a program to do the work for you. We'll explore how to parse the data another time. If you're opening the files with a text editor you might need to right click, choose 'open with' and select your preferred program. What's it for? The data available on the Historic Data site is extensive, and can be a really valuable tool or input. For example, you can include some of the columns as variables in a predictive model, compare BSP odds against win rates, or determine the average length of time it takes for 2 year old horses to run 1200m at Geelong. Quality data underpins the vast majority of successful betting strategies, so becoming comfortable working with the data available to you is a really important part of both the modelling and automation processes. Extra resources Here are some other useful resources that can help you work with this Historic Data: Historic Data FAQs Data Specification API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API","title":"Using the Historic Data site"},{"location":"historicData/usingHistoricDataSite/#using-the-historic-data-site","text":"The Betfair Historic Data site includes complete historic data for nearly all markets offered on the Exchange since 2016, when the new APING was launched. The data available includes prices, volume traded, winning status, average weighted price, BSP, and a variety of other details that are valuable for modelling and strategy development. We know that the process of downloading and extracting these data files can be a bit intimidating the first time round, so here's a walk through of one way to go about it to help make it more accessible. Data tiers There are three tiers of historic data available on this site. You can download samples of each tier of data here . The biggest differece is between the free and paid data. The free data includes a lot of information about the market, but no volume, and only last traded price per minute, not a full price ladder. The two paid tiers include the same data, just at different frequencies. If your strategy isn't particularly price sensitive and doesn't need volume as a variable then you'll probably be fine wtih the free tier, however if you need to see a more granular view of the market then you should probably consider the paid advanced or pro tiers. A full catalogue of the values included in each data tier is available here . Basic Advanced Pro 1 minute intervals last traded price no volume 1 second intervals price ladder (top 3) volume API tick intervals (50ms) price ladder (full) volume","title":"Using the Historic Data site"},{"location":"historicData/usingHistoricDataSite/#purchasing-the-data","text":"Start by going to the Betfair Historic Data site and log in using your Betfair account. Note: if you have less than 100 Betfair points you may have problems downloading data. On the Home page select the data set you want to download. Free data You need to 'purchase' the data set you want to download, even if it's from the free tier You can only 'purchase' each time period of data once. For example, if you had previously 'purchased' all Greyhound data for January 2018, then tried to download Greyhound data for January to March 2018 you would receive an error, and would need to purchase the data for February to March instead. Once you 'purchase' your choice of data it's recommended that you go to the My Data page, and choose the subset of data to then download.","title":"Purchasing the data"},{"location":"historicData/usingHistoricDataSite/#downloading-the-data","text":"On the My Data page you can filter the purchased data to the actual markets you're interested in. You can filter by Sport, Date range, EventId, Event Name, Market Type, Country & File Type (M = market, E = Event), which will cut down the size of the data you need to download. For example, if you wanted the win market for Australian and New Zealand greyhound races you'd use these filters. File type The file type filter has two options that you can choose from: E = Event - includes event level data, i.e. Geelong greyhounds on x date M = Market - includes market level data, i.e. the win market for Geelong greyhounds race 3 on x date The site can be pretty slow to download from, and you'll generally have a better experience if you download the data a bit at a time, say month by month. Alternatively if you're going to download a lot of data it might be worth having a look at the historic data API, that can automate the download process and speed it up significantly. There's a guide available here , and some sample code the help get you started.","title":"Downloading the data"},{"location":"historicData/usingHistoricDataSite/#unzipping-the-files","text":"You'll need to download a program to unzip the TAR files. Here we'll be using 7Zip , which is free, open source and generally well respected. Once you've downloaded it make sure you also install it onto the computer you'll be using to open the data files. Locate the data.tar file in your computer's file explorer program. Right click on the file, select '7-Zip' from the menu then choose 'Extract files...'. In the model that pops up change the path mode to 'No pathnames'. You can also change the name and/or path of the folder you want the files extracted to if you want to. You now have a collection of .bz2 files. The final step is to select all the files, right click, select '7-Zip' from the menu then choose 'Extract here'. This will then extract all the individual zipped files which you can then either open in a text editor - you can use something basic like Notepad (installed on basically all computers by default) or a more complete program like Visual Studio Code (my go to), Vim or Notepad++ - or you can parse over the using a program to do the work for you. We'll explore how to parse the data another time. If you're opening the files with a text editor you might need to right click, choose 'open with' and select your preferred program.","title":"Unzipping the files"},{"location":"historicData/usingHistoricDataSite/#whats-it-for","text":"The data available on the Historic Data site is extensive, and can be a really valuable tool or input. For example, you can include some of the columns as variables in a predictive model, compare BSP odds against win rates, or determine the average length of time it takes for 2 year old horses to run 1200m at Geelong. Quality data underpins the vast majority of successful betting strategies, so becoming comfortable working with the data available to you is a really important part of both the modelling and automation processes.","title":"What's it for?"},{"location":"historicData/usingHistoricDataSite/#extra-resources","text":"Here are some other useful resources that can help you work with this Historic Data: Historic Data FAQs Data Specification API for downloading historic data files (quicker than manually downloading) Sample code for using the historic data download API","title":"Extra resources"},{"location":"historicData/jsonToCsvRevisited/","text":"JSON to CSV | Revisited Before you start This tutorial follows on from our previous JSON to CSV tutorial where we stepped through how to work with the historic recordings of Betfair's price data. You should make sure you read that first before continuing here! So we're nearly a year on from our original JSON to CSV tutorial , and while it was generally well received a very common (and fair!) complaint was how long it took the script to run. Many people said that it could take all day, or they had to leave their PC on over night to finish, which obviously makes the data processing and back testing process a lot less accessible, which was the whole point of the original article. So we're circling back around to see what we can do to decrease the running time and to make working with larger samples of data less of an all day affair. You might want to stick around, because the final results are really something. Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfair_data package to do the heavy lifting; it's a Python library with a Rust backend, which makes it incredibly fast (spoiler alert!) Let's get a baseline So how slow are we talking? Let's pick some data and run our script as it is from our last tutorial and see how long it takes. We'll be using 3 months worth of PRO Australian Racing data, from October to December 2021, as the basis for this benchmark and our other tests going forward. Files used through this article market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] Now let's run the OG script and time how long it takes with the terminal command below: Terminal gtime python json2csv.py 9665.79user 18.27system 2:41:26elapsed 99%CPU (0avgtext+0avgdata 96624maxresident)k 0inputs+0outputs (405major+10862minor)pagefaults 0swaps ... oof. This took a while: 2h 41m 26s to be precise. This is less 'run to the kitchen and grab a cup of coffee' and more 'head to the pub and settle in for the night' territory, and thats on a new, very fast, MacBook Pro... if you have an older pc you should expect this to take significantly longer again. But is this actually slow? Let's count up how many files there are in our TAR archives, and then how many lines in each of the files, with each line being an update to a market, to give us an ideal of the scale of data we're dealing with: Markets: 22,001 | Updates: 166,677,569 Each update can effect any or all of the runners in a market, and, to be fair, we are keeping track of a sizeable amount of data for each runner, including all the prices and volumes available in both back and lay ladders, all the money matched at every price point, the money in the BSP pools at each limit, plus a lot more... how much faster can we really expect this to run? RTFM (Read the *** manual) The betfairlightweight library is doing all the computationally heavy lifting in our script, and speeding it up even slightly could save us a lot of total run time, so we figured we should probably take a look online to see if there were known ways of speeding it up. Whoops. There is a page addressing performance on the betfairlightweight docs discussing how to speed up parsing historic files. Although we were pretty upfront in our initial tutorial about our lack of Python coding ability, that's not an excuse for us to not read the instructions... Let's implement these changes and see if that's enough to let us keep this article to just a couple of paragraphs! Firstly, let's install the speed version of betfairlightweight; it says we should have this by default on Mac but there's no harm in checking anyway. Terminal pip install 'betfairlightweight[speed]' There's some quick and easy code changes to throw in as well while we're at it: Performance parameters # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" , \"appkey\" ) # create listener listener = betfairlightweight . StreamListener ( max_latency = None , # ignore latency errors output_queue = None , # use generator rather than a queue (faster) lightweight = True , # lightweight mode is faster update_clk = False , # do not update clk on updates (not required when backtesting) # We need these as were using historic files cumulative_runner_tv = True , calculate_market_tv = True ) Okay, so let's give it a go with these changes: Terminal python json2csv.py Traceback (most recent call last): File \"xxx/json2csv.py\", line 162, in <module> (preplay_market, postplay_market, final_market) = get_pre_post_final(stream) File \"xxx/json2csv.py\", line 146, in get_pre_post_final if eval_market is None and ((eval_market := filter_market(market_book)) == False): File \"xxx/json2csv.py\", line 104, in filter_market d = market.market_definition AttributeError: 'dict' object has no attribute 'market_definition' ... and now it's broken. Taking another look at the docs reveals the culprit to be lightweight=True which makes the library skip parsing into objects and returns the raw JSON in dicts instead. This reportedly has a decent speed increase, but will come with some pretty significant usability constraints which undermines the point of the tutorial, and we would also need to rewrite our whole script. Let's just set that back to false lightweight=False , and see how much performance we gain from the other changes. Terminal gtime python json2csv.py 9716.23user 39.08system 2:42:44elapsed 99%CPU (0avgtext+0avgdata 106784maxresident)k 0inputs+0outputs (80major+11965minor)pagefaults 0swaps ... so that's basically the same. Without switching to the slightly faster, but significantly harder to use lightweight mode, this may be the best we are going to get from betfairlightweight in this context. It's worth noting here that betfairlightweight is a Python library, written predominantly in Python (more on this in a minute). Looking at this execution through a profiler, we can see the vast majority of our time (> 90%) is spent in parsing and creating objects. The only other significant time was a relatively small (~4%) amount spent both reading the file from disk and decompressing the data. Betfairlightweight Python sits in a peculiar space as both a very fast and simlutaneously very slow programming language. When your program is spending most of its time running slow interpreted Python code it can be one of the slowest languages around - much slower then many other popular languages. Having said that though, there is a good chance if you're working in Python that a lot of your program isn't actually written in Python, but is instead written in a very fast systems language like C, C++ or Rust; much of the Python standard library, and many popular libraries like numpy, scipy and tensorflow are all examples of this. In these cases when Python is just acting as glue between code written in lower, faster languages a Python program can actually be really fast. This leaves us with two paths: stay with betfairlightweight and try and optimise the performance up to a sufficiently faster level, or rewrite the portion of betfairlightweight that handles parsing the JSON stream files in a much faster language and expose that back to Python as a new library. It is worth noting that betfairlightweight is not a new library - it's been around for a good period of time and has been actively developed enough that the chance of us finding some ground breaking optimisations that could dramatically speed up performance is low, and we would ideally like to see a very large performance increase to move the dial on our original numbers. So we took the path less travelled. And that made all the difference. RIIR (Rewrite it in Rust) So we've chosen path two: write a new, faster library. I've been playing round in Rust for the last few years - it's a great, modern, and extremely fast language, with a well-loved mascot (hi Ferris!), but most importantly we know it has really great support for writing fast Python libraries. If this was a movie this is where they would show a montage of us staying up late, chugging coffee, and tapping away at our keyboards, but I'll save you that and instead just give you a link to our new library, betfair_data . Quite a bit of effort has gone into making this library as fast as we can, and if there's interest we might write a follow up article that discusses the techniques we used to increase it's speed - let us know if you're keen for that - but for the purpose of this discussion the internals of the library don't really matter. To use our library we'll only need to make a few small changes to our script. You can jump ahead and see the completed file for this stage, but we'll walk through the main changes step by step below. We need to start by importing the new library - it's available in pypi and can be easily installed with pip. Terminal pip install betfair_data Import the new library from betfair_data import PriceSize from betfair_data import bflw We'll also need to update our load-markets function to return the bytes of the file, instead of the file object itself. This saves us the complexity of having to make our library know how to read Python files. Change load_markets function # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): with bz2 . BZ2File ( path , 'rb' ) as f : bytes = f . read () yield bflw . File ( path , bytes ) elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : name = file . name bytes = bz2 . open ( archive . extractfile ( file )) . read () yield bflw . File ( name , bytes ) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for name in archive . namelist (): bytes = bz2 . open ( archive . open ( name )) . read () yield bflw . File ( name , bytes ) return None Then we just need to use these values, passing them to our new library in a similar way to how we previously passed the data to betfairlightweight . iterate over bflw.File for file in load_markets ( market_paths ): def get_pre_post_final (): eval_market = None prev_market = None preplay_market = None postplay_market = None for market_books in file : for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) Testing it out Terminal gtime python json2csv_bfd.py 618.00user 20.53system 10:38.74elapsed 99%CPU (0avgtext+0avgdata 767712maxresident)k 0inputs+0outputs (0major+51396minor)pagefaults 0swaps Not bad! 10m 38s - that's roughly a 93% reduction in run time, and nicely in the range we hoped to achieve. We could stop here, having happily achieved our goals. But can we go even faster? Loading up the profiler again shows us some interesting results. Rust Library Betfairlightweight The work seems to be divided into two main areas. On the left you can see load_markets making up about ~70% of the computation time, and the parsing ( get_pre_post_final ) comprising the majority of the remaining. Interestingly, we've now sped up our parsing and object creation so much that it no longer comprises the majority of the workload of our script. Looking back at the betfairlightweight example from our original implementation we can see that the parsing took up a touch over 90% of the run time, while reading and decompressing the files only took only ~4%. Now because our parsing and object creation is so much faster, that same decompressing work that was previously only a small fraction is the vast majority of our workload! This split work loads highlights another opportunity. Unlike the JSON parsing, which needs to interact closely with the Python environment to create new objects and merge data, the process of reading and decompressing the files can be completed completely isolated and can therefor take place concurrently in another thread. A simple visualisation of our current workload can be seen in the chart below, where between each parsing workload we need to load and decompress the file. However if we were to move the file loading and decompression process to another thread we could free up the main Python thread to just spend its time on the JSON parsing, object creation, merging values and execution of our script. Essentially our worker thread would load and decompress the file and free up the main Python thread to complete the tasks that can only run on there. Gotta go fast This version only has a few additional code changes to the previous betfair_data example - we'll be removing the load_markets generator function from the previous script, and will replace it with a call to a new Rust function that we have added to our library. Now the betfair_data.Files function is doing all the heavy lifting. It creates its own thread, loads the files, decompresses them (if needed) and buffers them for the main Python thread to take as needed. Loading the files in a seperate thread for file in betfair_data . Files ( market_paths ) . bflw (): def get_pre_post_final (): eval_market = None prev_market = None preplay_market = None postplay_market = None for market_books in file : for market_book in market_books : Now we can run it again as usual. Terminal gtime python json2csv_bfd_Rustsrc.py 673.28user 18.31system 4:25.71elapsed 260%CPU (0avgtext+0avgdata 878720maxresident)k 0inputs+0outputs (0major+72667minor)pagefaults 0swaps Down to 4m 25s , an over 100% improvement again! Realistically though it's probably better to think of this change as a reduction of a fixed amount of work, as opposed to a doubling of performance. We are removing the loading and decompressing workload from the main thread, and in all fairness, we could take this exact same approach to the original betfairlightweight solution and expect to save roughly the same amount of time, 5mins (from 2h 41m to 2h 36m ). When this workload is the majority of the computation time, this optimisation feels necessary, but much less so when it represents only a small fraction of the total run time. Loading up the profiler, we can see that our flamegraph has gotten pretty bare. There's not much Python code left anymore, and what is left is truly just acting as glue between fast systems libraries to join them together and retrieve our result. Future optimisations will probably need to happen inside our new library, but whilst there are probably some performance gains to be found, we'll be unlikely to achieve anywhere near the scale of the speed increase we've already made. Rust Library, threaded loading Rust Library, Python loading Betfairlightweight Validating our results All of this effort measuring speed, and we haven't actually stopped to look at our output! Getting an answer quickly is only useful if we actually get the right answer, so let's run some comarisons between our original output and the output of the new library. All of the outputted CSV files are 52,483 lines long, which is definitely a good start. We do need to go deeper though to make sure the values in every row are the same - we have essentially rewritten all the complex data handling logic so small differences in the output could be expected. Luckily as CSV files are just plain text we can check the outputs of each line using diff : Terminal diff output_bfd.csv output_bflw.csv <no output> diff found no differences at all between the generated files! This confirms we have produced the exact same CSV. Conclusion These comparisons leave us confident that we're getting the same outputs from our new library as we were getting from our original implementation, but now we can run a year's worth of data in ~20 minutes, instead of close to 11 hours... Also, with our new library being able to be a drop in replacement for the betfairlightweights object structure, we only needed to make a small number of changes to our script, and most of those changes were deleting lines (always a good feeling). We'll clock that up as a win. Completed code Rust Library, threaded loading Rust Library, Python loading Betfairlightweight implementation Download from Github import logging import functools from typing import List , Tuple from itertools import zip_longest from betfair_data import PriceSize from betfair_data import bflw import betfair_data file_output = \"output_rust_source.csv\" market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] # setup logging logging . basicConfig ( level = logging . FATAL ) # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> Tuple [ float , float , float , float ]: if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> Tuple [ str , str , str ]: # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : bflw . MarketBook ) -> bool : d = market . market_definition return ( d != None and d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # record prices to a file with open ( file_output , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for i , g in enumerate ( betfair_data . Files ( market_paths ) . bflw ()): print ( \"Market {} \" . format ( i ), end = ' \\r ' ) def get_pre_post_final (): eval_market = None prev_market = None preplay_market = None postplay_market = None for market_books in g : for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final () # no price data for market if postplay_market is None : continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) if r . sp . actual_sp is not None else 0 , ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Download from Github import logging from typing import List , Tuple from itertools import zip_longest import functools import os import tarfile import zipfile import bz2 import glob from betfair_data import PriceSize from betfair_data import bflw file_output = \"output_py_source.csv\" market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] # setup logging logging . basicConfig ( level = logging . FATAL ) # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): with bz2 . BZ2File ( path , 'rb' ) as f : bytes = f . read () yield bflw . File ( path , bytes ) elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : name = file . name bytes = bz2 . open ( archive . extractfile ( file )) . read () yield bflw . File ( name , bytes ) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for name in archive . namelist (): bytes = bz2 . open ( archive . open ( name )) . read () yield bflw . File ( name , bytes ) return None # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> Tuple [ float , float , float , float ]: if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> Tuple [ str , str , str ]: # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : bflw . MarketBook ) -> bool : d = market . market_definition return ( d != None and d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # record prices to a file with open ( file_output , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for i , file in enumerate ( load_markets ( market_paths )): print ( \"Market {} \" . format ( i ), end = ' \\r ' ) def get_pre_post_final (): eval_market = None prev_market = None preplay_market = None postplay_market = None for market_books in file : for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final () # no price data for market if postplay_market is None : continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) if r . sp . actual_sp is not None else 0 , ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Download from Github import logging from typing import List , Tuple from unittest.mock import patch from itertools import zip_longest import functools import os import tarfile import zipfile import bz2 import glob # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) file_output = \"output_bflw.csv\" market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] # setup logging logging . basicConfig ( level = logging . FATAL ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" , \"appkey\" ) # create listener listener = betfairlightweight . StreamListener ( max_latency = None , # ignore latency errors output_queue = None , # use generator rather than a queue (faster) lightweight = False , # lightweight mode is faster update_clk = False , # do not update clk on updates (not required when backtesting) cumulative_runner_tv = True , calculate_market_tv = True ) # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> Tuple [ float , float , float , float ]: if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> Tuple [ str , str , str ]: # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d != None and d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # record prices to a file with open ( file_output , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for i , file_obj in enumerate ( load_markets ( market_paths )): print ( \"Market {} \" . format ( i ), end = ' \\r ' ) stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) # no price data for market if postplay_market is None : continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) if r . sp . actual_sp is not None else 0 , ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"JSON to CSV | Revisited"},{"location":"historicData/jsonToCsvRevisited/#json-to-csv-revisited","text":"Before you start This tutorial follows on from our previous JSON to CSV tutorial where we stepped through how to work with the historic recordings of Betfair's price data. You should make sure you read that first before continuing here! So we're nearly a year on from our original JSON to CSV tutorial , and while it was generally well received a very common (and fair!) complaint was how long it took the script to run. Many people said that it could take all day, or they had to leave their PC on over night to finish, which obviously makes the data processing and back testing process a lot less accessible, which was the whole point of the original article. So we're circling back around to see what we can do to decrease the running time and to make working with larger samples of data less of an all day affair. You might want to stick around, because the final results are really something. Cheat sheet If you're looking for the complete code head to the bottom of the page or download the script from Github . To run the code, save it to your machine, open a command prompt, or a terminal in your text editor of choice (we're using VS code ), make sure you've navigated in the terminal to the folder you've saved the script in and then type py main.py (or whatever you've called your script file if not main) then hit enter. To stop the code running use Ctrl C. Make sure you amend your data path to point to your data file. We'll be taking in an input of a historical tar file downloaded from the Betfair historic data site . We're using a PRO version, though the code should work on ADVANCED too. This approach won't work with the BASIC data tier. We're using the betfair_data package to do the heavy lifting; it's a Python library with a Rust backend, which makes it incredibly fast (spoiler alert!)","title":"JSON to CSV | Revisited"},{"location":"historicData/jsonToCsvRevisited/#lets-get-a-baseline","text":"So how slow are we talking? Let's pick some data and run our script as it is from our last tutorial and see how long it takes. We'll be using 3 months worth of PRO Australian Racing data, from October to December 2021, as the basis for this benchmark and our other tests going forward. Files used through this article market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] Now let's run the OG script and time how long it takes with the terminal command below: Terminal gtime python json2csv.py 9665.79user 18.27system 2:41:26elapsed 99%CPU (0avgtext+0avgdata 96624maxresident)k 0inputs+0outputs (405major+10862minor)pagefaults 0swaps ... oof. This took a while: 2h 41m 26s to be precise. This is less 'run to the kitchen and grab a cup of coffee' and more 'head to the pub and settle in for the night' territory, and thats on a new, very fast, MacBook Pro... if you have an older pc you should expect this to take significantly longer again. But is this actually slow? Let's count up how many files there are in our TAR archives, and then how many lines in each of the files, with each line being an update to a market, to give us an ideal of the scale of data we're dealing with: Markets: 22,001 | Updates: 166,677,569 Each update can effect any or all of the runners in a market, and, to be fair, we are keeping track of a sizeable amount of data for each runner, including all the prices and volumes available in both back and lay ladders, all the money matched at every price point, the money in the BSP pools at each limit, plus a lot more... how much faster can we really expect this to run?","title":"Let's get a baseline"},{"location":"historicData/jsonToCsvRevisited/#rtfm-read-the-manual","text":"The betfairlightweight library is doing all the computationally heavy lifting in our script, and speeding it up even slightly could save us a lot of total run time, so we figured we should probably take a look online to see if there were known ways of speeding it up. Whoops. There is a page addressing performance on the betfairlightweight docs discussing how to speed up parsing historic files. Although we were pretty upfront in our initial tutorial about our lack of Python coding ability, that's not an excuse for us to not read the instructions... Let's implement these changes and see if that's enough to let us keep this article to just a couple of paragraphs! Firstly, let's install the speed version of betfairlightweight; it says we should have this by default on Mac but there's no harm in checking anyway. Terminal pip install 'betfairlightweight[speed]' There's some quick and easy code changes to throw in as well while we're at it: Performance parameters # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" , \"appkey\" ) # create listener listener = betfairlightweight . StreamListener ( max_latency = None , # ignore latency errors output_queue = None , # use generator rather than a queue (faster) lightweight = True , # lightweight mode is faster update_clk = False , # do not update clk on updates (not required when backtesting) # We need these as were using historic files cumulative_runner_tv = True , calculate_market_tv = True ) Okay, so let's give it a go with these changes: Terminal python json2csv.py Traceback (most recent call last): File \"xxx/json2csv.py\", line 162, in <module> (preplay_market, postplay_market, final_market) = get_pre_post_final(stream) File \"xxx/json2csv.py\", line 146, in get_pre_post_final if eval_market is None and ((eval_market := filter_market(market_book)) == False): File \"xxx/json2csv.py\", line 104, in filter_market d = market.market_definition AttributeError: 'dict' object has no attribute 'market_definition' ... and now it's broken. Taking another look at the docs reveals the culprit to be lightweight=True which makes the library skip parsing into objects and returns the raw JSON in dicts instead. This reportedly has a decent speed increase, but will come with some pretty significant usability constraints which undermines the point of the tutorial, and we would also need to rewrite our whole script. Let's just set that back to false lightweight=False , and see how much performance we gain from the other changes. Terminal gtime python json2csv.py 9716.23user 39.08system 2:42:44elapsed 99%CPU (0avgtext+0avgdata 106784maxresident)k 0inputs+0outputs (80major+11965minor)pagefaults 0swaps ... so that's basically the same. Without switching to the slightly faster, but significantly harder to use lightweight mode, this may be the best we are going to get from betfairlightweight in this context. It's worth noting here that betfairlightweight is a Python library, written predominantly in Python (more on this in a minute). Looking at this execution through a profiler, we can see the vast majority of our time (> 90%) is spent in parsing and creating objects. The only other significant time was a relatively small (~4%) amount spent both reading the file from disk and decompressing the data. Betfairlightweight Python sits in a peculiar space as both a very fast and simlutaneously very slow programming language. When your program is spending most of its time running slow interpreted Python code it can be one of the slowest languages around - much slower then many other popular languages. Having said that though, there is a good chance if you're working in Python that a lot of your program isn't actually written in Python, but is instead written in a very fast systems language like C, C++ or Rust; much of the Python standard library, and many popular libraries like numpy, scipy and tensorflow are all examples of this. In these cases when Python is just acting as glue between code written in lower, faster languages a Python program can actually be really fast. This leaves us with two paths: stay with betfairlightweight and try and optimise the performance up to a sufficiently faster level, or rewrite the portion of betfairlightweight that handles parsing the JSON stream files in a much faster language and expose that back to Python as a new library. It is worth noting that betfairlightweight is not a new library - it's been around for a good period of time and has been actively developed enough that the chance of us finding some ground breaking optimisations that could dramatically speed up performance is low, and we would ideally like to see a very large performance increase to move the dial on our original numbers. So we took the path less travelled. And that made all the difference.","title":"RTFM (Read the *** manual)"},{"location":"historicData/jsonToCsvRevisited/#riir-rewrite-it-in-rust","text":"So we've chosen path two: write a new, faster library. I've been playing round in Rust for the last few years - it's a great, modern, and extremely fast language, with a well-loved mascot (hi Ferris!), but most importantly we know it has really great support for writing fast Python libraries. If this was a movie this is where they would show a montage of us staying up late, chugging coffee, and tapping away at our keyboards, but I'll save you that and instead just give you a link to our new library, betfair_data . Quite a bit of effort has gone into making this library as fast as we can, and if there's interest we might write a follow up article that discusses the techniques we used to increase it's speed - let us know if you're keen for that - but for the purpose of this discussion the internals of the library don't really matter. To use our library we'll only need to make a few small changes to our script. You can jump ahead and see the completed file for this stage, but we'll walk through the main changes step by step below. We need to start by importing the new library - it's available in pypi and can be easily installed with pip. Terminal pip install betfair_data Import the new library from betfair_data import PriceSize from betfair_data import bflw We'll also need to update our load-markets function to return the bytes of the file, instead of the file object itself. This saves us the complexity of having to make our library know how to read Python files. Change load_markets function # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): with bz2 . BZ2File ( path , 'rb' ) as f : bytes = f . read () yield bflw . File ( path , bytes ) elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : name = file . name bytes = bz2 . open ( archive . extractfile ( file )) . read () yield bflw . File ( name , bytes ) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for name in archive . namelist (): bytes = bz2 . open ( archive . open ( name )) . read () yield bflw . File ( name , bytes ) return None Then we just need to use these values, passing them to our new library in a similar way to how we previously passed the data to betfairlightweight . iterate over bflw.File for file in load_markets ( market_paths ): def get_pre_post_final (): eval_market = None prev_market = None preplay_market = None postplay_market = None for market_books in file : for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market )","title":"RIIR (Rewrite it in Rust)"},{"location":"historicData/jsonToCsvRevisited/#testing-it-out","text":"Terminal gtime python json2csv_bfd.py 618.00user 20.53system 10:38.74elapsed 99%CPU (0avgtext+0avgdata 767712maxresident)k 0inputs+0outputs (0major+51396minor)pagefaults 0swaps Not bad! 10m 38s - that's roughly a 93% reduction in run time, and nicely in the range we hoped to achieve. We could stop here, having happily achieved our goals. But can we go even faster? Loading up the profiler again shows us some interesting results. Rust Library Betfairlightweight The work seems to be divided into two main areas. On the left you can see load_markets making up about ~70% of the computation time, and the parsing ( get_pre_post_final ) comprising the majority of the remaining. Interestingly, we've now sped up our parsing and object creation so much that it no longer comprises the majority of the workload of our script. Looking back at the betfairlightweight example from our original implementation we can see that the parsing took up a touch over 90% of the run time, while reading and decompressing the files only took only ~4%. Now because our parsing and object creation is so much faster, that same decompressing work that was previously only a small fraction is the vast majority of our workload! This split work loads highlights another opportunity. Unlike the JSON parsing, which needs to interact closely with the Python environment to create new objects and merge data, the process of reading and decompressing the files can be completed completely isolated and can therefor take place concurrently in another thread. A simple visualisation of our current workload can be seen in the chart below, where between each parsing workload we need to load and decompress the file. However if we were to move the file loading and decompression process to another thread we could free up the main Python thread to just spend its time on the JSON parsing, object creation, merging values and execution of our script. Essentially our worker thread would load and decompress the file and free up the main Python thread to complete the tasks that can only run on there.","title":"Testing it out"},{"location":"historicData/jsonToCsvRevisited/#gotta-go-fast","text":"This version only has a few additional code changes to the previous betfair_data example - we'll be removing the load_markets generator function from the previous script, and will replace it with a call to a new Rust function that we have added to our library. Now the betfair_data.Files function is doing all the heavy lifting. It creates its own thread, loads the files, decompresses them (if needed) and buffers them for the main Python thread to take as needed. Loading the files in a seperate thread for file in betfair_data . Files ( market_paths ) . bflw (): def get_pre_post_final (): eval_market = None prev_market = None preplay_market = None postplay_market = None for market_books in file : for market_book in market_books : Now we can run it again as usual. Terminal gtime python json2csv_bfd_Rustsrc.py 673.28user 18.31system 4:25.71elapsed 260%CPU (0avgtext+0avgdata 878720maxresident)k 0inputs+0outputs (0major+72667minor)pagefaults 0swaps Down to 4m 25s , an over 100% improvement again! Realistically though it's probably better to think of this change as a reduction of a fixed amount of work, as opposed to a doubling of performance. We are removing the loading and decompressing workload from the main thread, and in all fairness, we could take this exact same approach to the original betfairlightweight solution and expect to save roughly the same amount of time, 5mins (from 2h 41m to 2h 36m ). When this workload is the majority of the computation time, this optimisation feels necessary, but much less so when it represents only a small fraction of the total run time. Loading up the profiler, we can see that our flamegraph has gotten pretty bare. There's not much Python code left anymore, and what is left is truly just acting as glue between fast systems libraries to join them together and retrieve our result. Future optimisations will probably need to happen inside our new library, but whilst there are probably some performance gains to be found, we'll be unlikely to achieve anywhere near the scale of the speed increase we've already made. Rust Library, threaded loading Rust Library, Python loading Betfairlightweight","title":"Gotta go fast"},{"location":"historicData/jsonToCsvRevisited/#validating-our-results","text":"All of this effort measuring speed, and we haven't actually stopped to look at our output! Getting an answer quickly is only useful if we actually get the right answer, so let's run some comarisons between our original output and the output of the new library. All of the outputted CSV files are 52,483 lines long, which is definitely a good start. We do need to go deeper though to make sure the values in every row are the same - we have essentially rewritten all the complex data handling logic so small differences in the output could be expected. Luckily as CSV files are just plain text we can check the outputs of each line using diff : Terminal diff output_bfd.csv output_bflw.csv <no output> diff found no differences at all between the generated files! This confirms we have produced the exact same CSV.","title":"Validating our results"},{"location":"historicData/jsonToCsvRevisited/#conclusion","text":"These comparisons leave us confident that we're getting the same outputs from our new library as we were getting from our original implementation, but now we can run a year's worth of data in ~20 minutes, instead of close to 11 hours... Also, with our new library being able to be a drop in replacement for the betfairlightweights object structure, we only needed to make a small number of changes to our script, and most of those changes were deleting lines (always a good feeling). We'll clock that up as a win.","title":"Conclusion"},{"location":"historicData/jsonToCsvRevisited/#completed-code","text":"Rust Library, threaded loading Rust Library, Python loading Betfairlightweight implementation Download from Github import logging import functools from typing import List , Tuple from itertools import zip_longest from betfair_data import PriceSize from betfair_data import bflw import betfair_data file_output = \"output_rust_source.csv\" market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] # setup logging logging . basicConfig ( level = logging . FATAL ) # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> Tuple [ float , float , float , float ]: if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> Tuple [ str , str , str ]: # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : bflw . MarketBook ) -> bool : d = market . market_definition return ( d != None and d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # record prices to a file with open ( file_output , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for i , g in enumerate ( betfair_data . Files ( market_paths ) . bflw ()): print ( \"Market {} \" . format ( i ), end = ' \\r ' ) def get_pre_post_final (): eval_market = None prev_market = None preplay_market = None postplay_market = None for market_books in g : for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final () # no price data for market if postplay_market is None : continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) if r . sp . actual_sp is not None else 0 , ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Download from Github import logging from typing import List , Tuple from itertools import zip_longest import functools import os import tarfile import zipfile import bz2 import glob from betfair_data import PriceSize from betfair_data import bflw file_output = \"output_py_source.csv\" market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] # setup logging logging . basicConfig ( level = logging . FATAL ) # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): with bz2 . BZ2File ( path , 'rb' ) as f : bytes = f . read () yield bflw . File ( path , bytes ) elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : name = file . name bytes = bz2 . open ( archive . extractfile ( file )) . read () yield bflw . File ( name , bytes ) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for name in archive . namelist (): bytes = bz2 . open ( archive . open ( name )) . read () yield bflw . File ( name , bytes ) return None # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> Tuple [ float , float , float , float ]: if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> Tuple [ str , str , str ]: # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : bflw . MarketBook ) -> bool : d = market . market_definition return ( d != None and d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # record prices to a file with open ( file_output , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for i , file in enumerate ( load_markets ( market_paths )): print ( \"Market {} \" . format ( i ), end = ' \\r ' ) def get_pre_post_final (): eval_market = None prev_market = None preplay_market = None postplay_market = None for market_books in file : for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final () # no price data for market if postplay_market is None : continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) if r . sp . actual_sp is not None else 0 , ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) ) Download from Github import logging from typing import List , Tuple from unittest.mock import patch from itertools import zip_longest import functools import os import tarfile import zipfile import bz2 import glob # importing data types import betfairlightweight from betfairlightweight.resources.bettingresources import ( PriceSize , MarketBook ) file_output = \"output_bflw.csv\" market_paths = [ \"data/2021_10_OctRacingAUPro.tar\" , \"data/2021_11_NovRacingAUPro.tar\" , \"data/2021_12_DecRacingAUPro.tar\" , ] # setup logging logging . basicConfig ( level = logging . FATAL ) # create trading instance (don't need username/password) trading = betfairlightweight . APIClient ( \"username\" , \"password\" , \"appkey\" ) # create listener listener = betfairlightweight . StreamListener ( max_latency = None , # ignore latency errors output_queue = None , # use generator rather than a queue (faster) lightweight = False , # lightweight mode is faster update_clk = False , # do not update clk on updates (not required when backtesting) cumulative_runner_tv = True , calculate_market_tv = True ) # loading from tar and extracting files def load_markets ( file_paths : List [ str ]): for file_path in file_paths : if os . path . isdir ( file_path ): for path in glob . iglob ( file_path + '**/**/*.bz2' , recursive = True ): f = bz2 . BZ2File ( path , 'rb' ) yield f f . close () elif os . path . isfile ( file_path ): ext = os . path . splitext ( file_path )[ 1 ] # iterate through a tar archive if ext == '.tar' : with tarfile . TarFile ( file_path ) as archive : for file in archive : yield bz2 . open ( archive . extractfile ( file )) # or a zip archive elif ext == '.zip' : with zipfile . ZipFile ( file_path ) as archive : for file in archive . namelist (): yield bz2 . open ( archive . open ( file )) return None # rounding to 2 decimal places or returning '' if blank def as_str ( v ) -> str : return ' %.2f ' % v if ( type ( v ) is float ) or ( type ( v ) is int ) else v if type ( v ) is str else '' # returning smaller of two numbers where min not 0 def min_gr0 ( a : float , b : float ) -> float : if a <= 0 : return b if b <= 0 : return a return min ( a , b ) # parsing price data and pulling out weighted avg price, matched, min price and max price def parse_traded ( traded : List [ PriceSize ]) -> Tuple [ float , float , float , float ]: if len ( traded ) == 0 : return ( None , None , None , None ) ( wavg_sum , matched , min_price , max_price ) = functools . reduce ( lambda total , ps : ( total [ 0 ] + ( ps . price * ps . size ), # wavg_sum before we divide by total matched total [ 1 ] + ps . size , # total matched min ( total [ 2 ], ps . price ), # min price matched max ( total [ 3 ], ps . price ), # max price matched ), traded , ( 0 , 0 , 1001 , 0 ) # starting default values ) wavg_sum = ( wavg_sum / matched ) if matched > 0 else None # dividing sum of wavg by total matched matched = matched if matched > 0 else None min_price = min_price if min_price != 1001 else None max_price = max_price if max_price != 0 else None return ( wavg_sum , matched , min_price , max_price ) # splitting race name and returning the parts def split_anz_horse_market_name ( market_name : str ) -> Tuple [ str , str , str ]: # return race no, length, race type # input samples: # 'R6 1400m Grp1' -> ('R6','1400m','grp1') # 'R1 1609m Trot M' -> ('R1', '1609m', 'trot') # 'R4 1660m Pace M' -> ('R4', '1660m', 'pace') parts = market_name . split ( ' ' ) race_no = parts [ 0 ] race_len = parts [ 1 ] race_type = parts [ 2 ] . lower () return ( race_no , race_len , race_type ) # filtering markets to those that fit the following criteria def filter_market ( market : MarketBook ) -> bool : d = market . market_definition return ( d != None and d . country_code == 'AU' and d . market_type == 'WIN' and ( c := split_anz_horse_market_name ( d . name )[ 2 ]) != 'trot' and c != 'pace' ) # record prices to a file with open ( file_output , \"w\" ) as output : # defining column headers output . write ( \"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,pp_min,pp_max,pp_wap,pp_ltp,pp_volume,ip_min,ip_max,ip_wap,ip_ltp,ip_volume \\n \" ) for i , file_obj in enumerate ( load_markets ( market_paths )): print ( \"Market {} \" . format ( i ), end = ' \\r ' ) stream = trading . streaming . create_historical_generator_stream ( file_path = file_obj , listener = listener , ) def get_pre_post_final ( s ): with patch ( \"builtins.open\" , lambda f , _ : f ): eval_market = None prev_market = None preplay_market = None postplay_market = None gen = stream . get_generator () for market_books in gen (): for market_book in market_books : # if market doesn't meet filter return out if eval_market is None and (( eval_market := filter_market ( market_book )) == False ): return ( None , None , None ) # final market view before market goes in play if prev_market is not None and prev_market . inplay != market_book . inplay : preplay_market = prev_market # final market view at the conclusion of the market if prev_market is not None and prev_market . status == \"OPEN\" and market_book . status != prev_market . status : postplay_market = market_book # update reference to previous market prev_market = market_book return ( preplay_market , postplay_market , prev_market ) # prev is now final ( preplay_market , postplay_market , final_market ) = get_pre_post_final ( stream ) # no price data for market if postplay_market is None : continue ; preplay_traded = [ ( r . last_price_traded , r . ex . traded_volume ) for r in preplay_market . runners ] if preplay_market is not None else None postplay_traded = [ ( r . last_price_traded , r . ex . traded_volume , # calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1)) min_gr0 ( next (( pv . size for pv in r . sp . back_stake_taken if pv . size > 0 ), 0 ), next (( pv . size for pv in r . sp . lay_liability_taken if pv . size > 0 ), 0 ) / (( r . sp . actual_sp if ( type ( r . sp . actual_sp ) is float ) or ( type ( r . sp . actual_sp ) is int ) else 0 ) - 1 ) ) if r . sp . actual_sp is not None else 0 , ) for r in postplay_market . runners ] # generic runner data runner_data = [ { 'selection_id' : r . selection_id , 'selection_name' : next (( rd . name for rd in final_market . market_definition . runners if rd . selection_id == r . selection_id ), None ), 'selection_status' : r . status , 'sp' : as_str ( r . sp . actual_sp ), } for r in final_market . runners ] # runner price data for markets that go in play if preplay_traded is not None : def runner_vals ( r ): ( pre_ltp , pre_traded ), ( post_ltp , post_traded , sp_traded ) = r inplay_only = list ( filter ( lambda ps : ps . size > 0 , [ PriceSize ( price = post_ps . price , size = post_ps . size - next (( pre_ps . size for pre_ps in pre_traded if pre_ps . price == post_ps . price ), 0 ) ) for post_ps in post_traded ])) ( ip_wavg , ip_matched , ip_min , ip_max ) = parse_traded ( inplay_only ) ( pre_wavg , pre_matched , pre_min , pre_max ) = parse_traded ( pre_traded ) return { 'preplay_ltp' : as_str ( pre_ltp ), 'preplay_min' : as_str ( pre_min ), 'preplay_max' : as_str ( pre_max ), 'preplay_wavg' : as_str ( pre_wavg ), 'preplay_matched' : as_str (( pre_matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : as_str ( post_ltp ), 'inplay_min' : as_str ( ip_min ), 'inplay_max' : as_str ( ip_max ), 'inplay_wavg' : as_str ( ip_wavg ), 'inplay_matched' : as_str ( ip_matched ), } runner_traded = [ runner_vals ( r ) for r in zip_longest ( preplay_traded , postplay_traded , fillvalue = PriceSize ( 0 , 0 )) ] # runner price data for markets that don't go in play else : def runner_vals ( r ): ( ltp , traded , sp_traded ) = r ( wavg , matched , min_price , max_price ) = parse_traded ( traded ) return { 'preplay_ltp' : as_str ( ltp ), 'preplay_min' : as_str ( min_price ), 'preplay_max' : as_str ( max_price ), 'preplay_wavg' : as_str ( wavg ), 'preplay_matched' : as_str (( matched or 0 ) + ( sp_traded or 0 )), 'inplay_ltp' : '' , 'inplay_min' : '' , 'inplay_max' : '' , 'inplay_wavg' : '' , 'inplay_matched' : '' , } runner_traded = [ runner_vals ( r ) for r in postplay_traded ] # printing to csv for each runner for ( rdata , rprices ) in zip ( runner_data , runner_traded ): # defining data to go in each column output . write ( \" {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} , {} \\n \" . format ( postplay_market . market_id , postplay_market . market_definition . market_time , postplay_market . market_definition . country_code , postplay_market . market_definition . venue , postplay_market . market_definition . name , rdata [ 'selection_id' ], rdata [ 'selection_name' ], rdata [ 'selection_status' ], rdata [ 'sp' ], rprices [ 'preplay_min' ], rprices [ 'preplay_max' ], rprices [ 'preplay_wavg' ], rprices [ 'preplay_ltp' ], rprices [ 'preplay_matched' ], rprices [ 'inplay_min' ], rprices [ 'inplay_max' ], rprices [ 'inplay_wavg' ], rprices [ 'inplay_ltp' ], rprices [ 'inplay_matched' ], ) )","title":"Completed code"},{"location":"historicData/jsonToCsvRevisited/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/AFLmodellingPython/","text":"AFL Modelling Walkthrough 01. Data Cleaning These tutorials will walk you through how to construct your own basic AFL model, using publicly available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through the basics of cleaning this dataset and how we have done it. If you want to get straight to feature creation or modelling, feel free to jump ahead! # Import libraries import pandas as pd import numpy as np import re pd . set_option ( 'display.max_columns' , None ) We will first explore the DataFrames, and then create functions to wrangle them and clean them into more consistent sets of data. # Read/clean each DataFrame match_results = pd . read_csv ( \"data/afl_match_results.csv\" ) odds = pd . read_csv ( \"data/afl_odds.csv\" ) player_stats = pd . read_csv ( \"data/afl_player_stats.csv\" ) odds . tail ( 3 ) trunc event_name path selection_name odds 4179 2018-09-01 Match Odds VFL/Richmond Reserves v Williamstown Williamstown 2.3878 4180 2018-09-01 Match Odds WAFL/South Fremantle v West Perth South Fremantle 1.5024 4181 2018-09-01 Match Odds WAFL/South Fremantle v West Perth West Perth 2.7382 match_results . tail ( 3 ) Game Date Round Home.Team Home.Goals Home.Behinds Home.Points Away.Team Away.Goals Away.Behinds Away.Points Venue Margin Season Round.Type Round.Number 15395 15396 2018-08-26 R23 Brisbane Lions 11 6 72 West Coast 14 14 98 Gabba -26 2018 Regular 23 15396 15397 2018-08-26 R23 Melbourne 15 12 102 GWS 8 9 57 M.C.G. 45 2018 Regular 23 15397 15398 2018-08-26 R23 St Kilda 14 10 94 North Melbourne 17 15 117 Docklands -23 2018 Regular 23 player_stats . tail ( 3 ) AF B BO CCL CG CL CM CP D DE Date ED FA FF G GA HB HO I50 ITC K M MG MI5 Match_id One.Percenters Opposition Player R50 Round SC SCL SI Season Status T T5 TO TOG Team UP Venue 89317 38 1 0 0.0 0 0 1 2 9 55.6 25/08/2018 5 0 0 0 0 3 0 0 2.0 6 3 132.0 2 9711 0 Fremantle Christopher Mayne 1 Round 23 35 0.0 2.0 2018 Away 1 0.0 1.0 57 Collingwood 7 Optus Stadium 89318 38 0 0 0.0 3 0 0 3 9 55.6 25/08/2018 5 0 1 0 0 3 0 0 4.0 6 3 172.0 0 9711 2 Fremantle Nathan Murphy 5 Round 23 29 0.0 0.0 2018 Away 1 0.0 3.0 70 Collingwood 6 Optus Stadium 89319 56 1 0 0.0 1 0 0 3 8 62.5 25/08/2018 5 0 0 2 0 2 0 0 2.0 6 3 180.0 3 9711 2 Fremantle Jaidyn Stephenson 0 Round 23 56 0.0 4.0 2018 Away 3 1.0 2.0 87 Collingwood 5 Optus Stadium Have a look at the structure of the DataFrames. Notice that for the odds DataFrame, each game is split between two rows, whilst for the match_results each game is on one row. We will have to get around this by splitting the games up onto two rows, as this will allow our feature transformation functions to be applied more easily later on. For the player_stats DataFrame we will aggregate these stats into each game on separate rows. First, we will write functions to make the odds data look a bit nicer, with only a team column, a date column and a 'home_game' column which takes the values 0 or 1 depending on if it was a home game for that team. To do this we will use the regex module to extract the team names from the path column, as well as the to_datetime function from pandas. We will also replace all the inconsistent team names with consistent team names. def get_cleaned_odds ( df = None ): # If a df hasn't been specified as a parameter, read the odds df if df is None : df = pd . read_csv ( \"data/afl_odds.csv\" ) # Get a dictionary of team names we want to change and their new values team_name_mapping = { 'Adelaide Crows' : 'Adelaide' , 'Brisbane Lions' : 'Brisbane' , 'Carlton Blues' : 'Carlton' , 'Collingwood Magpies' : 'Collingwood' , 'Essendon Bombers' : 'Essendon' , 'Fremantle Dockers' : 'Fremantle' , 'GWS Giants' : 'GWS' , 'Geelong Cats' : 'Geelong' , 'Gold Coast Suns' : 'Gold Coast' , 'Greater Western Sydney' : 'GWS' , 'Greater Western Sydney Giants' : 'GWS' , 'Hawthorn Hawks' : 'Hawthorn' , 'Melbourne Demons' : 'Melbourne' , 'North Melbourne Kangaroos' : 'North Melbourne' , 'Port Adelaide Magpies' : 'Port Adelaide' , 'Port Adelaide Power' : 'Port Adelaide' , 'P Adelaide' : 'Port Adelaide' , 'Richmond Tigers' : 'Richmond' , 'St Kilda Saints' : 'St Kilda' , 'Sydney Swans' : 'Sydney' , 'West Coast Eagles' : 'West Coast' , 'Wetsern Bulldogs' : 'Western Bulldogs' , 'Western Bullbogs' : 'Western Bulldogs' } # Add columns df = ( df . assign ( date = lambda df : pd . to_datetime ( df . trunc ), # Create a datetime column home_team = lambda df : df . path . str . extract ( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 1 ] . str . strip (), away_team = lambda df : df . path . str . extract ( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 2 ] . str . strip ()) . drop ( columns = [ 'path' , 'trunc' , 'event_name' ]) # Drop irrelevant columns . rename ( columns = { 'selection_name' : 'team' }) # Rename columns . replace ( team_name_mapping ) . sort_values ( by = 'date' ) . reset_index ( drop = True ) . assign ( home_game = lambda df : df . apply ( lambda row : 1 if row . home_team == row . team else 0 , axis = 'columns' )) . drop ( columns = [ 'home_team' , 'away_team' ])) return df # Apply the wrangling and cleaning function odds = get_cleaned_odds ( odds ) odds . tail () team odds date home_game 4177 South Fremantle 1.5024 2018-09-01 1 4178 Port Melbourne 2.8000 2018-09-01 0 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 We now have a DataFrame that looks nice and easy to join with our other DataFrames. Now let's lean up the match_details DataFrame. # Define a function which cleans the match results df, and separates each teams' stats onto individual rows def get_cleaned_match_results ( df = None ): # If a df hasn't been specified as a parameter, read the match_results df if df is None : df = pd . read_csv ( \"data/afl_match_results.csv\" ) # Create column lists to loop through - these are the columns we want in home and away dfs home_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' , 'Margin' , 'Venue' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' ] away_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' , 'Margin' , 'Venue' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' ] mapping = [ 'game' , 'date' , 'round' , 'team' , 'goals' , 'behinds' , 'points' , 'margin' , 'venue' , 'opponent' , 'opponent_goals' , 'opponent_behinds' , 'opponent_points' ] team_name_mapping = { 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' } # Create a df with only home games df_home = ( df [ home_columns ] . rename ( columns = { old_col : new_col for old_col , new_col in zip ( home_columns , mapping )}) . assign ( home_game = 1 )) # Create a df with only away games df_away = ( df [ away_columns ] . rename ( columns = { old_col : new_col for old_col , new_col in zip ( away_columns , mapping )}) . assign ( home_game = 0 , margin = lambda df : df . margin * - 1 )) # Append these dfs together new_df = ( df_home . append ( df_away ) . sort_values ( by = 'game' ) # Sort by game ID . reset_index ( drop = True ) # Reset index . assign ( date = lambda df : pd . to_datetime ( df . date )) # Create a datetime column . replace ( team_name_mapping )) # Rename team names to be consistent with other dfs return new_df match_results = get_cleaned_match_results ( match_results ) match_results . head () game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 3 2 1897-05-08 1 St Kilda 2 4 16 -25 Victoria Park Collingwood 5 11 41 0 4 3 1897-05-08 1 Geelong 3 6 24 -23 Corio Oval Essendon 7 5 47 1 Now we have both the odds DataFrame and match_results DataFrame ready for feature creation! Finally, we will aggregate the player_stats DataFrame stats for each game rather than individual player stats. For this DataFrame we have regular stats, such as disposals, marks etc. and Advanced Stats, such as Tackles Inside 50 and Metres Gained. However these advanced stats are only available from 2015, so we will not be using them in this tutorial - as there isn't enough data from 2015 to train our models. Let's now aggregate the player_stats DataFrame. def get_cleaned_aggregate_player_stats ( df = None ): # If a df hasn't been specified as a parameter, read the player_stats df if df is None : df = pd . read_csv ( \"data/afl_player_stats.csv\" ) agg_stats = ( df . rename ( columns = { # Rename columns to lowercase 'Season' : 'season' , 'Round' : 'round' , 'Team' : 'team' , 'Opposition' : 'opponent' , 'Date' : 'date' }) . groupby ( by = [ 'date' , 'season' , 'team' , 'opponent' ], as_index = False ) # Groupby to aggregate the stats for each game . sum () . drop ( columns = [ 'DE' , 'TOG' , 'Match_id' ]) # Drop columns . assign ( date = lambda df : pd . to_datetime ( df . date , format = \" %d /%m/%Y\" )) # Create a datetime object . sort_values ( by = 'date' ) . reset_index ( drop = True )) return agg_stats agg_stats = get_cleaned_aggregate_player_stats ( player_stats ) agg_stats . tail () date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3621 2018-08-26 2018 Brisbane West Coast 1652 5 0 14.0 49 37 8 132 394 302 20 18 11 9 167 48 49 59.0 227 104 5571.0 6 48 39 1645 23.0 86.0 62 13.0 69.0 256 3622 2018-08-26 2018 West Coast Brisbane 1548 11 5 13.0 49 42 9 141 360 262 18 20 14 8 137 39 56 70.0 223 95 5809.0 12 39 34 1655 29.0 94.0 55 6.0 59.0 217 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 We now have a three fully prepared DataFrames which are almost ready to be analysed and for a model to be built on! Let's have a look at how they look and then merge them together into our final DataFrame. odds . tail ( 3 ) team odds date home_game 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 match_results . tail ( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 30793 15397 2018-08-26 23 Melbourne 15 12 102 45 M.C.G. GWS 8 9 57 1 30794 15398 2018-08-26 23 St Kilda 14 10 94 -23 Docklands North Melbourne 17 15 117 1 30795 15398 2018-08-26 23 North Melbourne 17 15 117 23 Docklands St Kilda 14 10 94 0 agg_stats . tail ( 3 ) date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 merged_df = ( odds [ odds . team . isin ( agg_stats . team . unique ())] . pipe ( pd . merge , match_results , on = [ 'date' , 'team' , 'home_game' ]) . pipe ( pd . merge , agg_stats , on = [ 'date' , 'team' , 'opponent' ]) . sort_values ( by = [ 'game' ])) merged_df . tail ( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3199 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3195 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3200 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Great! We now have a clean looking datset with each row representing one team in a game. Let's now eliminate the outliers from a dataset. We know that Essendon had a doping scandal which resulted in their entire team being banned for a year in 2016, so let's remove all of their 2016 games. To do this we will filter based on the team and season, and then invert this with ~. # Define a function which eliminates outliers def outlier_eliminator ( df ): # Eliminate Essendon 2016 games essendon_filter_criteria = ~ ((( df [ 'team' ] == 'Essendon' ) & ( df [ 'season' ] == 2016 )) | (( df [ 'opponent' ] == 'Essendon' ) & ( df [ 'season' ] == 2016 ))) df = df [ essendon_filter_criteria ] . reset_index ( drop = True ) return df afl_data = outlier_eliminator ( merged_df ) afl_data . tail ( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Finally, let's mark all of the columns that we are going to use in feature creation with the string 'f_' at the start of their column name so that we can easily filter for these columns. non_feature_cols = [ 'team' , 'date' , 'home_game' , 'game' , 'round' , 'venue' , 'opponent' , 'season' ] afl_data = afl_data . rename ( columns = { col : 'f_' + col for col in afl_data if col not in non_feature_cols }) afl_data . tail ( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Our data is now fully ready to be explored and for features to be created. 02. Feature Creation These tutorials will walk you through how to construct your own basic AFL model. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through creating features from our dataset, which was cleaned in the first tutorial. Feature engineering is an integral part of the Data Science process. Creative and smart features can be the difference between an average performing model and a model profitable which beats the market odds. Grabbing Our Dataset First, we will import our required modules, as well as the prepare_afl_data function which we created in our afl_data_cleaning script. This essentially cleans all the data for us so that we're ready to explore the data and make some features. # Import modules from afl_data_cleaning_v2 import * import afl_data_cleaning_v2 import pandas as pd pd . set_option ( 'display.max_columns' , None ) import warnings warnings . filterwarnings ( 'ignore' ) import numpy as np # Use the prepare_afl_data function to prepare the data for us; this function condenses what we walked through in the previous tutorial afl_data = prepare_afl_data () afl_data . tail ( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Creating A Feature DataFrame Let's create a feature DataFrame and merge all of our features into this DataFrame as we go. features = afl_data [[ 'date' , 'game' , 'team' , 'opponent' , 'venue' , 'home_game' ]] . copy () What Each Column Refers To Below is a DataFrame which outlines what each column refers to. column_abbreviations = pd . read_csv ( \"data/afl_data_columns_mapping.csv\" ) column_abbreviations Feature Abbreviated Feature 0 GA Goal Assists 1 CP Contested Possessions 2 UP Uncontested Possessions 3 ED Effective Disposals 4 CM Contested Marks 5 MI5 Marks Inside 50 6 One.Percenters One Percenters 7 BO Bounces 8 K Kicks 9 HB Handballs 10 D Disposals 11 M Marks 12 G Goals 13 B Behinds 14 T Tackles 15 HO Hitouts 16 I50 Inside 50s 17 CL Clearances 18 CG Clangers 19 R50 Rebound 50s 20 FF Frees For 21 FA Frees Against 22 AF AFL Fantasy Points 23 SC Supercoach Points 24 CCL Centre Clearances 25 SCL Stoppage Clearances 26 SI Score Involvements 27 MG Metres Gained 28 TO Turnovers 29 ITC Intercepts 30 T5 Tackles Inside 50 Feature Creation Now let's think about what features we can create. We have a enormous amount of stats to sift through. To start, let's create some simple features based on our domain knowledge of Aussie Rules. Creating Expontentially Weighted Rolling Averages as Features Next, we will create rolling averages of statistics such as Tackles, which we will use as features. It is fair to assume that a team's performance in a certain stat may have predictive power to the overall result. And in general, if a team consistently performs well in this stat, this may have predictive power to the result of their future games. We can't simply train a model on stats from the game which we are trying to predict (i.e. data that we don't have before the game begins), as this will leak the result. We need to train our model on past data. One way of doing this is to train our model on average stats over a certain amount of games. If a team is averaging high in this stat, this may give insight into if they are a strong team. Similarly, if the team is averaging poorly in this stat (relative to the team they are playing), this may have predictive power and give rise to a predicted loss. To do this we will create a function which calculates the rolling averages, known as create_exp_weighted_avgs, which takes our cleaned DataFrame as an input, as well as the alpha which, when higher, weights recent performances more than old performances. To read more about expontentially weighted moving averages, please read the documentation here . First, we will grab all the columns which we want to create EMAs for, and then use our function to create the average for that column. We will create a new DataFrame and add these columns to this new DataFrame. # Define a function which returns a DataFrame with the expontential moving average for each numeric stat def create_exp_weighted_avgs ( df , span ): # Create a copy of the df with only the game id and the team - we will add cols to this df ema_features = df [[ 'game' , 'team' ]] . copy () feature_names = [ col for col in df . columns if col . startswith ( 'f_' )] # Get a list of columns we will iterate over for feature_name in feature_names : feature_ema = ( df . groupby ( 'team' )[ feature_name ] . transform ( lambda row : ( row . ewm ( span = span ) . mean () . shift ( 1 )))) ema_features [ feature_name ] = feature_ema return ema_features features_rolling_averages = create_exp_weighted_avgs ( afl_data , span = 10 ) features_rolling_averages . tail () game team f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3152 15396 West Coast 2.094236 12.809630 10.047145 86.904928 8.888770 11.435452 9.403444 78.016158 3193.612782 16.472115 11.958482 23.379562 100.095244 68.252001 27.688669 284.463270 719.884644 525.878017 36.762440 44.867118 25.618202 17.522871 270.478779 88.139376 105.698031 148.005305 449.405865 201.198907 11581.929999 20.048124 95.018480 74.180967 3314.157893 44.872398 177.894442 126.985101 20.565549 138.876613 438.848376 3153 15397 GWS 1.805565 13.100372 13.179329 91.781563 18.527618 10.371198 11.026754 73.253945 3165.127358 19.875913 12.947209 25.114002 105.856671 80.609640 23.374884 303.160047 741.439198 534.520295 42.597317 38.160889 26.208715 18.688880 300.188301 81.540693 106.989070 143.032506 441.250897 173.050118 12091.630837 21.106142 103.077097 80.201059 3419.245919 55.495610 219.879895 138.202470 25.313148 135.966798 438.466439 3154 15397 Melbourne 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.787800 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 3155 15398 North Melbourne 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.541130 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3156 15398 St Kilda 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.498760 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 As you can see our function worked perfectly! Now we have a full DataFrame of exponentially weighted moving averages. Note that as these rolling averages have been shifted by 1 to ensure no data leakage, the first round of the data will have all NA values. We can drop these later. Let's add these averages to our features DataFrame features = pd . merge ( features , features_rolling_averages , on = [ 'game' , 'team' ]) Creating a 'Form Between the Teams' Feature It is well known in Aussie Rules that often some teams perform better against certain teams than others. If we isolate our features to pure stats based on previous games not between the teams playing, or elo ratings, we won't account for any relationships between certain teams. An example is the Kennett Curse , where Geelong won 11 consecutive games against Hawthorn, despite being similarly matched teams. Let's create a feature which calculates how many games a team has won against their opposition over a given window of games. To do this, we will need to use historical data that dates back well before our current DataFrame starts at. Otherwise we will be using a lot of our games to calculate form, meaning we will have to drop these rows before feeding it into an algorithm. So let's use our prepare_match_results function which we defined in the afl_data_cleaning tutorial to grab a clean DataFrame of all match results since 1897. We can then calculate the form and join this to our current DataFrame. match_results = afl_data_cleaning_v2 . get_cleaned_match_results () match_results . head ( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 form_btwn_teams = match_results [[ 'game' , 'team' , 'opponent' , 'margin' ]] . copy () form_btwn_teams [ 'f_form_margin_btwn_teams' ] = ( match_results . groupby ([ 'team' , 'opponent' ])[ 'margin' ] . transform ( lambda row : row . rolling ( 5 ) . mean () . shift ()) . fillna ( 0 )) form_btwn_teams [ 'f_form_past_5_btwn_teams' ] = \\ ( match_results . assign ( win = lambda df : df . apply ( lambda row : 1 if row . margin > 0 else 0 , axis = 'columns' )) . groupby ([ 'team' , 'opponent' ])[ 'win' ] . transform ( lambda row : row . rolling ( 5 ) . mean () . shift () * 5 ) . fillna ( 0 )) form_btwn_teams . tail ( 3 ) game team opponent margin f_form_margin_btwn_teams f_form_past_5_btwn_teams 30793 15397 Melbourne GWS 45 -23.2 2.0 30794 15398 St Kilda North Melbourne -23 -3.2 2.0 30795 15398 North Melbourne St Kilda 23 3.2 3.0 # Merge to our features df features = pd . merge ( features , form_btwn_teams . drop ( columns = [ 'margin' ]), on = [ 'game' , 'team' , 'opponent' ]) Creating Efficiency Features Disposal Efficiency Disposal efficiency is pivotal in Aussie Rules football. If you are dispose of the ball effectively you are much more likely to score and much less likely to concede goals than if you dispose of it ineffectively. Let's create a disposal efficiency feature by dividing Effective Disposals by Disposals. Inside 50/Rebound 50 Efficiency Similarly, one could hypothesise that teams who keep the footy in their Inside 50 regularly will be more likely to score, whilst teams who are effective at getting the ball out of their Defensive 50 will be less likely to concede. Let's use this logic to create Inside 50 Efficiency and Rebound 50 Efficiency features. The formula used will be: Inside 50 Efficiency = R50_Opponents / I50 (lower is better). Rebound 50 Efficiency = R50 / I50_Opponents (higher is better). Using these formulas, I50 Efficiency = R50 Efficiency_Opponent. So we will just need to create the formulas for I50 efficiency. To create these features we will need the opposition's Inside 50s/Rebound 50s. So we will split out data into two DataFrames, create a new DataFrame by joining these two DataFrames on the Game, calculate our efficiency features, then join our features with our main features DataFrame. # Get each match on single rows single_row_df = ( afl_data [[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' , ]] . query ( 'home_game == 1' ) . rename ( columns = { 'team' : 'home_team' , 'f_I50' : 'f_I50_home' , 'f_R50' : 'f_R50_home' , 'f_D' : 'f_D_home' , 'f_ED' : 'f_ED_home' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , afl_data [[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' ]] . query ( 'home_game == 0' ) . rename ( columns = { 'team' : 'away_team' , 'f_I50' : 'f_I50_away' , 'f_R50' : 'f_R50_away' , 'f_D' : 'f_D_away' , 'f_ED' : 'f_ED_away' }) . drop ( columns = 'home_game' ), on = 'game' )) single_row_df . head () game home_team f_I50_home f_R50_home f_D_home f_ED_home away_team f_I50_away f_R50_away f_D_away f_ED_away 0 13764 Carlton 69 21 373 268 Richmond 37 50 316 226 1 13765 Geelong 54 40 428 310 St Kilda 52 45 334 246 2 13766 Collingwood 70 38 398 289 Port Adelaide 50 44 331 232 3 13767 Adelaide 59 38 366 264 Hawthorn 54 38 372 264 4 13768 Brisbane 50 39 343 227 Fremantle 57 30 351 250 single_row_df = single_row_df . assign ( f_I50_efficiency_home = lambda df : df . f_R50_away / df . f_I50_home , f_I50_efficiency_away = lambda df : df . f_R50_home / df . f_I50_away ) feature_efficiency_cols = [ 'f_I50_efficiency_home' , 'f_I50_efficiency_away' ] # Now let's create an Expontentially Weighted Moving Average for these features - we will need to reshape our DataFrame to do this efficiency_features_multi_row = ( single_row_df [[ 'game' , 'home_team' ] + feature_efficiency_cols ] . rename ( columns = { 'home_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency' , 'f_I50_efficiency_away' : 'f_I50_efficiency_opponent' , }) . append (( single_row_df [[ 'game' , 'away_team' ] + feature_efficiency_cols ] . rename ( columns = { 'away_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency_opponent' , 'f_I50_efficiency_away' : 'f_I50_efficiency' , })), sort = True ) . sort_values ( by = 'game' ) . reset_index ( drop = True )) efficiency_features = efficiency_features_multi_row [[ 'game' , 'team' ]] . copy () feature_efficiency_cols = [ 'f_I50_efficiency' , 'f_I50_efficiency_opponent' ] for feature in feature_efficiency_cols : efficiency_features [ feature ] = ( efficiency_features_multi_row . groupby ( 'team' )[ feature ] . transform ( lambda row : row . ewm ( span = 10 ) . mean () . shift ( 1 ))) # Get feature efficiency df back onto single rows efficiency_features = pd . merge ( efficiency_features , afl_data [[ 'game' , 'team' , 'home_game' ]], on = [ 'game' , 'team' ]) efficiency_features_single_row = ( efficiency_features . query ( 'home_game == 1' ) . rename ( columns = { 'team' : 'home_team' , 'f_I50_efficiency' : 'f_I50_efficiency_home' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_home' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , ( efficiency_features . query ( 'home_game == 0' ) . rename ( columns = { 'team' : 'away_team' , 'f_I50_efficiency' : 'f_I50_efficiency_away' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_away' }) . drop ( columns = 'home_game' )), on = 'game' )) efficiency_features_single_row . tail ( 5 ) game home_team f_I50_efficiency_home f_R50_efficiency_home away_team f_I50_efficiency_away f_R50_efficiency_away 1580 15394 Carlton 0.730668 0.675002 Adelaide 0.691614 0.677128 1581 15395 Sydney 0.699994 0.778280 Hawthorn 0.699158 0.673409 1582 15396 Brisbane 0.683604 0.691730 West Coast 0.696822 0.709605 1583 15397 Melbourne 0.667240 0.692632 GWS 0.684525 0.753783 1584 15398 St Kilda 0.730843 0.635819 North Melbourne 0.697018 0.654991 We will merge these features back to our features df later, when the features data frame is on a single row as well. Creating an Elo Feature Another feature which we could create is an Elo feature. If you don't know what Elo is, go ahead and read our article on it here . We have also written a guide on using elo to model the 2018 FIFA World Cup here . Essentially, Elo ratings increase if you win. The amount the rating increases is based on how strong the opponent is relative to the team who won. Weak teams get more points for beating stronger teams than they do for beating weaker teams, and vice versa for losses (teams lose points for losses). Mathematically, Elo ratings can also assign a probability for winning or losing based on the two Elo Ratings of the teams playing. So let's get into it. We will first define a function which calculates the elo for each team and applies these elos to our DataFrame. # Define a function which finds the elo for each team in each game and returns a dictionary with the game ID as a key and the # elos as the key's value, in a list. It also outputs the probabilities and a dictionary of the final elos for each team def elo_applier ( df , k_factor ): # Initialise a dictionary with default elos for each team elo_dict = { team : 1500 for team in df [ 'team' ] . unique ()} elos , elo_probs = {}, {} # Get a home and away dataframe so that we can get the teams on the same row home_df = df . loc [ df . home_game == 1 , [ 'team' , 'game' , 'f_margin' , 'home_game' ]] . rename ( columns = { 'team' : 'home_team' }) away_df = df . loc [ df . home_game == 0 , [ 'team' , 'game' ]] . rename ( columns = { 'team' : 'away_team' }) df = ( pd . merge ( home_df , away_df , on = 'game' ) . sort_values ( by = 'game' ) . drop_duplicates ( subset = 'game' , keep = 'first' ) . reset_index ( drop = True )) # Loop over the rows in the DataFrame for index , row in df . iterrows (): # Get the Game ID game_id = row [ 'game' ] # Get the margin margin = row [ 'f_margin' ] # If the game already has the elos for the home and away team in the elos dictionary, go to the next game if game_id in elos . keys (): continue # Get the team and opposition home_team = row [ 'home_team' ] away_team = row [ 'away_team' ] # Get the team and opposition elo score home_team_elo = elo_dict [ home_team ] away_team_elo = elo_dict [ away_team ] # Calculated the probability of winning for the team and opposition prob_win_home = 1 / ( 1 + 10 ** (( away_team_elo - home_team_elo ) / 400 )) prob_win_away = 1 - prob_win_home # Add the elos and probabilities our elos dictionary and elo_probs dictionary based on the Game ID elos [ game_id ] = [ home_team_elo , away_team_elo ] elo_probs [ game_id ] = [ prob_win_home , prob_win_away ] # Calculate the new elos of each team if margin > 0 : # Home team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 1 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 0 - prob_win_away ) elif margin < 0 : # Away team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 1 - prob_win_away ) elif margin == 0 : # Drawn game' update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0.5 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 0.5 - prob_win_away ) # Update elos in elo dictionary elo_dict [ home_team ] = new_home_team_elo elo_dict [ away_team ] = new_away_team_elo return elos , elo_probs , elo_dict # Use the elo applier function to get the elos and elo probabilities for each game - we will map these later elos , probs , elo_dict = elo_applier ( afl_data , 30 ) Great! now we have both rolling averages for stats as a feature, and the elo of the teams! Let's have a quick look at the current elo standings with a k-factor of 30, out of curiosity. for team in sorted ( elo_dict , key = elo_dict . get )[:: - 1 ]: print ( team , elo_dict [ team ]) Richmond 1695.2241513840117 Sydney 1645.548990879842 Hawthorn 1632.5266709780622 West Coast 1625.871701773721 Geelong 1625.423154644809 GWS 1597.4158602131877 Adelaide 1591.1704934545442 Collingwood 1560.370309216614 Melbourne 1558.5666572771509 Essendon 1529.0198398117086 Port Adelaide 1524.8882517820093 North Melbourne 1465.5637511922569 Western Bulldogs 1452.2110697845148 Fremantle 1393.142087030804 St Kilda 1360.9120149937303 Brisbane 1276.2923772139352 Gold Coast 1239.174528704772 Carlton 1226.6780896643265 This looks extremely similar to the currently AFL ladder, so this is a good sign for elo being an effective predictor of winning. Merging Our Features Into One Features DataFrame Now we need to reshape our features df so that we have all of the statistics for both teams in a game on a single row. We can then merge our elo and efficiency features to this df. # Look at our current features df features . tail ( 3 ) date game team opponent venue home_game f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP f_form_margin_btwn_teams f_form_past_5_btwn_teams 3156 2018-08-26 15397 Melbourne GWS M.C.G. 1 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.78780 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 -23.2 2.0 3157 2018-08-26 15398 North Melbourne St Kilda Docklands 0 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.54113 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3.2 3.0 3158 2018-08-26 15398 St Kilda North Melbourne Docklands 1 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.49876 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 -3.2 2.0 one_line_cols = [ 'game' , 'team' , 'home_game' ] + [ col for col in features if col . startswith ( 'f_' )] # Get all features onto individual rows for each match features_one_line = ( features . loc [ features . home_game == 1 , one_line_cols ] . rename ( columns = { 'team' : 'home_team' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , ( features . loc [ features . home_game == 0 , one_line_cols ] . drop ( columns = 'home_game' ) . rename ( columns = { 'team' : 'away_team' }) . rename ( columns = { col : col + '_away' for col in features . columns if col . startswith ( 'f_' )})), on = 'game' ) . drop ( columns = [ 'f_form_margin_btwn_teams_away' , 'f_form_past_5_btwn_teams_away' ])) # Add our created features - elo, efficiency etc. features_one_line = ( features_one_line . assign ( f_elo_home = lambda df : df . game . map ( elos ) . apply ( lambda x : x [ 0 ]), f_elo_away = lambda df : df . game . map ( elos ) . apply ( lambda x : x [ 1 ])) . pipe ( pd . merge , efficiency_features_single_row , on = [ 'game' , 'home_team' , 'away_team' ]) . pipe ( pd . merge , afl_data . loc [ afl_data . home_game == 1 , [ 'game' , 'date' , 'round' , 'venue' ]], on = [ 'game' ]) . dropna () . reset_index ( drop = True ) . assign ( season = lambda df : df . date . apply ( lambda row : row . year ))) ordered_cols = [ col for col in features_one_line if col [: 2 ] != 'f_' ] + [ col for col in features_one_line if col . startswith ( 'f_' )] feature_df = features_one_line [ ordered_cols ] Finally, let's reduce the dimensionality of the features df by subtracting the home features from the away features. This will reduce the huge amount of columns we have and make our data more manageable. To do this, we will need a list of columns which we are subtracting from each other. We will then loop over each of these columns to create our new differential columns. We will then add in the implied probability from the odds of the home and away team, as our current odds feature is simply an exponential moving average over the past n games. # Create differential df - this df is the home features - the away features diff_cols = [ col for col in feature_df . columns if col + '_away' in feature_df . columns and col != 'f_odds' and col . startswith ( 'f_' )] non_diff_cols = [ col for col in feature_df . columns if col not in diff_cols and col [: - 5 ] not in diff_cols ] diff_df = feature_df [ non_diff_cols ] . copy () for col in diff_cols : diff_df [ col + '_diff' ] = feature_df [ col ] - feature_df [ col + '_away' ] # Add current odds in to diff_df odds = get_cleaned_odds () home_odds = ( odds [ odds . home_game == 1 ] . assign ( f_current_odds_prob = lambda df : 1 / df . odds ) . rename ( columns = { 'team' : 'home_team' }) . drop ( columns = [ 'home_game' , 'odds' ])) away_odds = ( odds [ odds . home_game == 0 ] . assign ( f_current_odds_prob_away = lambda df : 1 / df . odds ) . rename ( columns = { 'team' : 'away_team' }) . drop ( columns = [ 'home_game' , 'odds' ])) diff_df = ( diff_df . pipe ( pd . merge , home_odds , on = [ 'date' , 'home_team' ]) . pipe ( pd . merge , away_odds , on = [ 'date' , 'away_team' ])) diff_df . tail () game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1626 15394 Carlton Adelaide 2018-08-25 23 Docklands 2018 6.467328 -26.2 1.0 2.066016 1230.072138 1587.776445 0.730668 0.675002 0.691614 0.677128 -3.498547 -5.527193 -26.518474 -34.473769 1.289715 0.217006 7.955295 -341.342677 -9.317269 3.088569 -2.600593 15.192839 -12.518345 -4.136673 -41.855717 -72.258378 -51.998775 9.499447 8.670917 -6.973088 -4.740623 -26.964945 -13.147675 -23.928700 -28.940883 -45.293433 -15.183406 -1900.784014 -0.362402 -1.314627 4.116133 -294.813511 -9.917793 -34.724925 -5.462844 -9.367141 -19.623785 -38.188082 0.187709 0.816860 1627 15395 Sydney Hawthorn 2018-08-25 23 S.C.G. 2018 2.128611 1.0 2.0 1.777290 1662.568452 1615.507209 0.699994 0.778280 0.699158 0.673409 -1.756730 -0.874690 -11.415069 -15.575319 0.014390 4.073909 4.160250 -174.005092 -0.942357 -4.078635 -4.192916 7.814496 -2.225780 6.215760 15.042979 -34.894261 -50.615255 4.214158 0.683548 -3.535594 -3.168608 -12.068691 -30.493980 -9.867332 2.588103 -22.825570 -5.604199 253.086090 -2.697132 -22.612327 25.340623 -90.812188 1.967104 -31.047879 0.007606 -6.880120 11.415593 -49.957313 0.440180 0.561924 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 Wrapping it Up We now have a fairly decent amount of features. Some other features which could be added include whether the game is in a major Capital city outisde of Mebourne (i.e. Sydney, Adelaide or Peth), how many 'Elite' players are playing (which could be judged by average SuperCoach scores over 110, for example), as well as your own metrics for attacking and defending. Note that all of our features have columns starting with 'f_' so in the section, we will grab this feature dataframe and use these features to sport predicting the matches. 03. Modelling These tutorials will walk you through how to construct your own basic AFL model, using publically available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through modelling our AFL data to create predictions. We will train a variety of quick and easy models to get a feel of what works and what doesn't. We will then tune our hyperparameters so that we are ready to make week by week predictions. Grabbing Our Dataset First, we will import our required modules, as well as the prepare_afl_features function which we created in our afl_feature_creation script. This essentially creates some basic features for us so that we can get started on the modelling component. # Import libraries from afl_data_cleaning_v2 import * import datetime import pandas as pd import numpy as np from sklearn import svm , tree , linear_model , neighbors , naive_bayes , ensemble , discriminant_analysis , gaussian_process # from xgboost import XGBClassifier from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV , train_test_split from sklearn.linear_model import LogisticRegressionCV from sklearn.feature_selection import RFECV import seaborn as sns from sklearn.preprocessing import OneHotEncoder , LabelEncoder , StandardScaler from sklearn import feature_selection from sklearn import metrics from sklearn.linear_model import LogisticRegression , RidgeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB import warnings warnings . filterwarnings ( 'ignore' ) import afl_feature_creation_v2 import afl_data_cleaning_v2 # Grab our feature DataFrame which we created in the previous tutorial feature_df = afl_feature_creation_v2 . prepare_afl_features () afl_data = afl_data_cleaning_v2 . prepare_afl_data () feature_df . tail ( 3 ) game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 # Get the result and merge to the feature_df match_results = ( pd . read_csv ( \"data/afl_match_results.csv\" ) . rename ( columns = { 'Game' : 'game' }) . assign ( result = lambda df : df . apply ( lambda row : 1 if row [ 'Home.Points' ] > row [ 'Away.Points' ] else 0 , axis = 1 ))) # Merge result column to feature_df feature_df = pd . merge ( feature_df , match_results [[ 'game' , 'result' ]], on = 'game' ) Creating a Training and Testing Set So that we don't train our data on the data that we will later test our model on, we will create separate train and test sets. For this exercise we will use the 2018 season to test how our model performs, whilst the rest of the data can be used to train the model. # Create our test and train sets from our afl DataFrame; drop the columns which leak the result, duplicates, and the advanced # stats which don't have data until 2015 feature_columns = [ col for col in feature_df if col . startswith ( 'f_' )] # Create our test set test_x = feature_df . loc [ feature_df . season == 2018 , [ 'game' ] + feature_columns ] test_y = feature_df . loc [ feature_df . season == 2018 , 'result' ] # Create our train set X = feature_df . loc [ feature_df . season != 2018 , [ 'game' ] + feature_columns ] y = feature_df . loc [ feature_df . season != 2018 , 'result' ] # Scale features scaler = StandardScaler () X [ feature_columns ] = scaler . fit_transform ( X [ feature_columns ]) test_x [ feature_columns ] = scaler . transform ( test_x [ feature_columns ]) Using Cross Validation to Find The Best Algorithms Now that we have our training set, we can run through a list of popular classifiers to determine which classifier is best for modelling our data. To do this we will create a function which uses Kfold cross-validation to find the 'best' algorithms, based on how accurate the algorithms' predictions are. This function will take in a list of classifiers, which we will define below, as well as the training set and it's outcome, and output a DataFrame with the mean and std of the accuracy of each algorithm. Let's jump into it! # Create a list of standard classifiers classifiers = [ #Ensemble Methods ensemble . AdaBoostClassifier (), ensemble . BaggingClassifier (), ensemble . ExtraTreesClassifier (), ensemble . GradientBoostingClassifier (), ensemble . RandomForestClassifier (), #Gaussian Processes gaussian_process . GaussianProcessClassifier (), #GLM linear_model . LogisticRegressionCV (), #Navies Bayes naive_bayes . BernoulliNB (), naive_bayes . GaussianNB (), #SVM svm . SVC ( probability = True ), svm . NuSVC ( probability = True ), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis (), discriminant_analysis . QuadraticDiscriminantAnalysis (), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # XGBClassifier() ] # Define a functiom which finds the best algorithms for our modelling task def find_best_algorithms ( classifier_list , X , y ): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold ( n_splits = 5 ) # Grab the cross validation scores for each algorithm cv_results = [ cross_val_score ( classifier , X , y , scoring = \"neg_log_loss\" , cv = kfold ) for classifier in classifier_list ] cv_means = [ cv_result . mean () * - 1 for cv_result in cv_results ] cv_std = [ cv_result . std () for cv_result in cv_results ] algorithm_names = [ alg . __class__ . __name__ for alg in classifiers ] # Create a DataFrame of all the CV results cv_results = pd . DataFrame ({ \"Mean Log Loss\" : cv_means , \"Log Loss Std\" : cv_std , \"Algorithm\" : algorithm_names }) return cv_results . sort_values ( by = 'Mean Log Loss' ) . reset_index ( drop = True ) best_algos = find_best_algorithms ( classifiers , X , y ) best_algos Mean Log Loss Log Loss Std Algorithm 0 0.539131 3.640578e-02 LogisticRegressionCV 1 0.551241 5.775685e-02 LinearDiscriminantAnalysis 2 0.630994 8.257481e-02 GradientBoostingClassifier 3 0.670041 9.205780e-03 AdaBoostClassifier 4 0.693147 2.360121e-08 GaussianProcessClassifier 5 0.712537 2.770864e-02 SVC 6 0.712896 2.440755e-02 NuSVC 7 0.836191 2.094224e-01 ExtraTreesClassifier 8 0.874307 1.558144e-01 RandomForestClassifier 9 1.288174 3.953037e-01 BaggingClassifier 10 1.884019 4.769589e-01 QuadraticDiscriminantAnalysis 11 2.652161 6.886897e-01 BernoulliNB 12 3.299651 6.427551e-01 GaussianNB # Try a logistic regression model and see how it performs in terms of accuracy kfold = StratifiedKFold ( n_splits = 5 ) cv_scores = cross_val_score ( linear_model . LogisticRegressionCV (), X , y , scoring = 'accuracy' , cv = kfold ) cv_scores . mean () 0.7452268937025035 Choosing Our Algorithms As we can see from above, there are some pretty poor algorithms for predicting the winner. On the other hand, whilst attaining an accuracy of 74.5% (at the time of writing) may seem like a decent result; we must first establish a baseline to judge our performance on. In this case, we will have two baselines; the proportion of games won by the home team and what the odds predict. If we can beat the odds we have created a very powerful model. Note that a baseline for the log loss can also be both the odds log loss and randomly guessing. Randomly guessing between two teams attains a log loss of log(2) = 0.69, so we have beaten this result. Once we establish our baseline, we will choose the top algorithms from above and tune their hyperparameters, as well as automatically selecting the best features to be used in our model. Defining Our Baseline As stated above, we must define our baseline so that we have a measure to beat. We will use the proportion of games won by the home team, as well as the proportion of favourites who won, based off the odds. To establish this baseline we will use our feature_df, as this has no dropped rows. # Find the percentage chance of winning at home in each season. afl_data = afl_data_cleaning_v2 . prepare_afl_data () afl_data [ 'home_win' ] = afl_data . apply ( lambda x : 1 if x [ 'f_margin' ] > 0 else 0 , axis = 1 ) home_games = afl_data [ afl_data [ 'home_game' ] == 1 ] home_games [[ \"home_win\" , 'season' ]] . groupby ([ 'season' ]) . mean () season home_win 2011 0.561856 2012 0.563725 2013 0.561576 2014 0.574257 2015 0.539604 2016 0.606742 2017 0.604061 2018 0.540404 # Find the proportion of favourites who have won # Define a function which finds if the odds correctly guessed the response def find_odds_prediction ( a_row ): if a_row [ 'f_odds' ] <= a_row [ 'f_odds_away' ] and a_row [ 'home_win' ] == 1 : return 1 elif a_row [ 'f_odds_away' ] < a_row [ 'f_odds' ] and a_row [ 'home_win' ] == 0 : return 1 else : return 0 # Define a function which splits our DataFrame so each game is on one row instead of two def get_df_on_one_line ( df ): cols_to_drop = [ 'date' , 'home_game' , 'opponent' , 'f_opponent_behinds' , 'f_opponent_goals' , 'f_opponent_points' , 'f_points' , 'round' , 'venue' , 'season' ] home_df = df [ df [ 'home_game' ] == 1 ] . rename ( columns = { 'team' : 'home_team' }) away_df = df [ df [ 'home_game' ] == 0 ] . rename ( columns = { 'team' : 'away_team' }) away_df = away_df . drop ( columns = cols_to_drop ) # Rename away_df columns away_df_renamed = away_df . rename ( columns = { col : col + '_away' for col in away_df . columns if col != 'game' }) merged_df = pd . merge ( home_df , away_df_renamed , on = 'game' ) merged_df [ 'home_win' ] = merged_df . f_margin . apply ( lambda x : 1 if x > 0 else 0 ) return merged_df afl_data_one_line = get_df_on_one_line ( afl_data ) afl_data_one_line [ 'odds_prediction' ] = afl_data_one_line . apply ( find_odds_prediction , axis = 1 ) print ( 'The overall mean accuracy of choosing the favourite based on the odds is {} %' . format ( round ( afl_data_one_line [ 'odds_prediction' ] . mean () * 100 , 2 ))) afl_data_one_line [[ \"odds_prediction\" , 'season' ]] . groupby ([ 'season' ]) . mean () The overall mean accuracy of choosing the favourite based on the odds is 73.15% season odds_prediction 2011 0.784615 2012 0.774510 2013 0.748768 2014 0.727723 2015 0.727723 2016 0.713483 2017 0.659898 2018 0.712121 ## Get a baseline log loss score from the odds afl_data_one_line [ 'odds_home_prob' ] = 1 / afl_data_one_line . f_odds afl_data_one_line [ 'odds_away_prob' ] = 1 / afl_data_one_line . f_odds_away metrics . log_loss ( afl_data_one_line . home_win , afl_data_one_line [[ 'odds_away_prob' , 'odds_home_prob' ]]) 0.5375306549682837 We can see that the odds are MUCH more accurate than just choosing the home team to win. We can also see that the mean accuracy of choosing the favourite is around 73%. That means that this is the score we need to beat. Similarly, the log loss of the odds is around 0.5385, whilst our model scores around 0.539 (at the time of writing), without hyperparamter optimisation. Let's choose only the algorithms with log losses below 0.67 chosen_algorithms = best_algos . loc [ best_algos [ 'Mean Log Loss' ] < 0.67 , 'Algorithm' ] . tolist () chosen_algorithms [ 'LogisticRegressionCV' , 'LinearDiscriminantAnalysis' , 'GradientBoostingClassifier' ] Using Grid Search To Tune Hyperparameters Now that we have our best models, we can use Grid Search to optimise our hyperparameters. Grid search basically involves searching through a range of different algorithm hyperparameters, and choosing those which result in the best score from some metrics, which in our case is accuracy. Let's do this for the algorithms which have hyperparameters which can be tuned. Note that if you are running this on your own computer it may take up to 10 minutes. # Define a function which optimises the hyperparameters of our chosen algorithms def optimise_hyperparameters ( train_x , train_y , algorithms , parameters ): kfold = StratifiedKFold ( n_splits = 5 ) best_estimators = [] for alg , params in zip ( algorithms , parameters ): gs = GridSearchCV ( alg , param_grid = params , cv = kfold , scoring = 'neg_log_loss' , verbose = 1 ) gs . fit ( train_x , train_y ) best_estimators . append ( gs . best_estimator_ ) return best_estimators # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.001 , 0.01 , 0.05 , 0.2 , 0.5 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } # Add our algorithms and parameters to lists to be used in our function alg_list = [ LogisticRegression ()] param_list = [ lr_grid ] # Find the best estimators, then add our other estimators which don't need optimisation best_estimators = optimise_hyperparameters ( X , y , alg_list , param_list ) Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=1)]: Done 90 out of 90 | elapsed: 5.2s finished lr_best_params = best_estimators [ 0 ] . get_params () lr_best_params { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } kfold = StratifiedKFold ( n_splits = 10 ) cv_scores = cross_val_score ( linear_model . LogisticRegression ( ** lr_best_params ), X , y , scoring = 'neg_log_loss' , cv = kfold ) cv_scores . mean () - 0.528741673153639 In the next iteration of this tutorial we will also optimise an XGB model and hopefully outperform our logistic regression model. Creating Predictions for the 2018 Season Now that we have an optimised logistic regression model, let's see how it performs on predicting the 2018 season. lr = LogisticRegression ( ** lr_best_params ) lr . fit ( X , y ) final_predictions = lr . predict ( test_x ) accuracy = ( final_predictions == test_y ) . mean () * 100 print ( \"Our accuracy in predicting the 2018 season is: {:.2f} %\" . format ( accuracy )) Our accuracy in predicting the 2018 season is: 67.68% Now let's have a look at all the games which we incorrectly predicted. game_ids = test_x [( final_predictions != test_y )] . game afl_data_one_line . loc [ afl_data_one_line . game . isin ( game_ids ), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] date home_team opponent f_odds f_odds_away f_margin 1386 2018-03-24 Gold Coast North Melbourne 2.0161 1.9784 16 1388 2018-03-25 Melbourne Geelong 1.7737 2.2755 -3 1391 2018-03-30 North Melbourne St Kilda 3.5769 1.3867 52 1392 2018-03-31 Carlton Gold Coast 1.5992 2.6620 -34 1396 2018-04-01 Western Bulldogs West Coast 1.8044 2.2445 -51 1397 2018-04-01 Sydney Port Adelaide 1.4949 3.0060 -23 1398 2018-04-02 Geelong Hawthorn 1.7597 2.3024 -1 1406 2018-04-08 Western Bulldogs Essendon 3.8560 1.3538 21 1408 2018-04-13 Adelaide Collingwood 1.2048 5.9197 -48 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1415 2018-04-15 Hawthorn Melbourne 2.2855 1.7772 67 1417 2018-04-20 Sydney Adelaide 1.2640 4.6929 -10 1420 2018-04-21 Port Adelaide Geelong 1.5053 2.9515 -34 1422 2018-04-22 North Melbourne Hawthorn 2.6170 1.6132 28 1423 2018-04-22 Brisbane Gold Coast 1.7464 2.3277 -5 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1427 2018-04-28 Geelong Sydney 1.5019 2.9833 -17 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 1439 2018-05-05 Sydney North Melbourne 1.2777 4.5690 -2 1444 2018-05-11 Hawthorn Sydney 1.6283 2.5818 -8 1445 2018-05-12 GWS West Coast 1.5425 2.8292 -25 1446 2018-05-12 Carlton Essendon 3.1742 1.4570 13 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1456 2018-05-19 Essendon Geelong 5.6530 1.2104 34 1460 2018-05-20 Brisbane Hawthorn 3.2891 1.4318 56 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1466 2018-05-26 GWS Essendon 1.4364 3.2652 -35 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 ... ... ... ... ... ... ... 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 1485 2018-06-11 Melbourne Collingwood 1.6034 2.6450 -42 1492 2018-06-21 West Coast Essendon 1.3694 3.6843 -28 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1499 2018-06-29 Western Bulldogs Geelong 6.2067 1.1889 2 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1504 2018-07-01 Melbourne St Kilda 1.1405 7.7934 -2 1505 2018-07-01 Essendon North Melbourne 2.0993 1.9022 17 1506 2018-07-01 Fremantle Brisbane 1.2914 4.3743 -55 1507 2018-07-05 Sydney Geelong 1.7807 2.2675 -12 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1516 2018-07-12 Adelaide Geelong 2.0517 1.9444 15 1518 2018-07-14 Hawthorn Brisbane 1.2281 5.4105 -33 1521 2018-07-14 GWS Richmond 2.7257 1.5765 2 1522 2018-07-15 Collingwood West Coast 1.5600 2.7815 -35 1523 2018-07-15 North Melbourne Sydney 1.9263 2.0647 -6 1524 2018-07-15 Fremantle Port Adelaide 5.9110 1.2047 9 1527 2018-07-21 Sydney Gold Coast 1.0342 27.8520 -24 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 1533 2018-07-22 Port Adelaide GWS 1.6480 2.5452 -22 1538 2018-07-28 Gold Coast Carlton 1.3933 3.5296 -35 1546 2018-08-04 Adelaide Port Adelaide 2.0950 1.9135 3 1548 2018-08-04 St Kilda Western Bulldogs 1.6120 2.6368 -35 1555 2018-08-11 Port Adelaide West Coast 1.4187 3.3505 -4 1558 2018-08-12 North Melbourne Western Bulldogs 1.3175 4.1239 -7 1559 2018-08-12 Melbourne Sydney 1.3627 3.7445 -9 1564 2018-08-18 GWS Sydney 1.8478 2.1672 -20 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 Very interesting! Most of the games we got wrong were upsets. Let's have a look at the games we incorrectly predicted that weren't upsets. ( afl_data_one_line . loc [ afl_data_one_line . game . isin ( game_ids ), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] . assign ( home_favourite = lambda df : df . apply ( lambda row : 1 if row . f_odds < row . f_odds_away else 0 , axis = 1 )) . assign ( upset = lambda df : df . apply ( lambda row : 1 if row . home_favourite == 1 and row . f_margin < 0 else ( 1 if row . home_favourite == 0 and row . f_margin > 0 else 0 ), axis = 1 )) . query ( 'upset == 0' )) date home_team opponent f_odds f_odds_away f_margin home_favourite upset 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1 0 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1 0 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 0 0 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 0 0 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 0 0 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1 0 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1 0 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 0 0 1479 2018-06-08 Port Adelaide Richmond 1.7422 2.3420 14 1 0 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 0 0 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1 0 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1 0 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 0 0 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1 0 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 0 0 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 0 0 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 0 0 Let's now look at our model's log loss for the 2018 season compared to the odds. predictions_probs = lr . predict_proba ( test_x ) metrics . log_loss ( test_y , predictions_probs ) 0.584824211055384 test_x_unscaled = feature_df . loc [ feature_df . season == 2018 , [ 'game' ] + feature_columns ] metrics . log_loss ( test_y , test_x_unscaled [[ 'f_current_odds_prob_away' , 'f_current_odds_prob' ]]) 0.5545776633924343 So whilst our model performs decently, it doesn't beat the odds in terms of log loss. That's okay, it's still a decent start. In future iterations we can implement other algorithms and create new features which may improve performance. Next Steps Now that we have a model up and running, the next steps are to implement the model on a week to week basis. 04. Weekly Predictions Now that we have explored different algorithms for modelling, we can implement our chosen model and predict this week's AFL games! All you need to do is run the afl_modelling script each Thursday or Friday to predict the following week's games. # Import Modules from afl_feature_creation_v2 import prepare_afl_features import afl_data_cleaning_v2 import afl_feature_creation_v2 import afl_modelling_v2 import datetime import pandas as pd import numpy as np pd . set_option ( 'display.max_columns' , None ) from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler import warnings warnings . filterwarnings ( 'ignore' ) Creating The Features For This Weekend's Games To actually predict this weekend's games, we need to create the same features that we have created in the previous tutorials for the games that will be played this weekend. This includes all the rolling averages, efficiency features, elo features etc. So the majority of this tutorial will be using previously defined functions to create features for the following weekend's games. Create Next Week's DataFrame Let's first get our cleaned afl_data dataset, as well as the odds for next weekend and the 2018 fixture. # Grab the cleaned AFL dataset and the column order afl_data = afl_data_cleaning_v2 . prepare_afl_data () ordered_cols = afl_data . columns # Define a function which grabs the odds for each game for the following weekend def get_next_week_odds ( path ): # Get next week's odds next_week_odds = pd . read_csv ( path ) next_week_odds = next_week_odds . rename ( columns = { \"team_1\" : \"home_team\" , \"team_2\" : \"away_team\" , \"team_1_odds\" : \"odds\" , \"team_2_odds\" : \"odds_away\" }) return next_week_odds # Import the fixture # Define a function which gets the fixture and cleans it up def get_fixture ( path ): # Get the afl fixture fixture = pd . read_csv ( path ) # Replace team names and reformat fixture = fixture . replace ({ 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' }) fixture [ 'Date' ] = pd . to_datetime ( fixture [ 'Date' ]) . dt . date . astype ( str ) fixture = fixture . rename ( columns = { \"Home.Team\" : \"home_team\" , \"Away.Team\" : \"away_team\" }) return fixture next_week_odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) fixture = get_fixture ( \"data/afl_fixture_2018.csv\" ) fixture . tail () Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG next_week_odds home_team away_team odds odds_away 0 West Coast Collingwood 2.34 1.75 Now that we have these DataFrames, we will define a function which combines the fixture and next week's odds to create a single DataFrame for the games over the next 7 days. To use this function we will need Game IDs for next week. So we will create another function which creates Game IDs by using the Game ID from the last game played and adding 1 to it. # Define a function which creates game IDs for this week's footy games def create_next_weeks_game_ids ( afl_data ): odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) # Get last week's Game ID last_afl_data_game = afl_data [ 'game' ] . iloc [ - 1 ] # Create Game IDs for next week game_ids = [( i + 1 ) + last_afl_data_game for i in range ( odds . shape [ 0 ])] return game_ids # Define a function which creates this week's footy game DataFrame def get_next_week_df ( afl_data ): # Get the fixture and the odds for next week's footy games fixture = get_fixture ( \"data/afl_fixture_2018.csv\" ) next_week_odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) next_week_odds [ 'game' ] = create_next_weeks_game_ids ( afl_data ) # Get today's date and next week's date and create a DataFrame for next week's games # todays_date = datetime.datetime.today().strftime('%Y-%m-%d') # date_in_7_days = (datetime.datetime.today() + datetime.timedelta(days=7)).strftime('%Y-%m-%d') todays_date = '2018-09-27' date_in_7_days = '2018-10-04' fixture = fixture [( fixture [ 'Date' ] >= todays_date ) & ( fixture [ 'Date' ] < date_in_7_days )] . drop ( columns = [ 'Season.Game' ]) next_week_df = pd . merge ( fixture , next_week_odds , on = [ 'home_team' , 'away_team' ]) # Split the DataFrame onto two rows for each game h_df = ( next_week_df [[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds' , 'Season' , 'Round' , 'Venue' ]] . rename ( columns = { 'home_team' : 'team' , 'away_team' : 'opponent' }) . assign ( home_game = 1 )) a_df = ( next_week_df [[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds_away' , 'Season' , 'Round' , 'Venue' ]] . rename ( columns = { 'odds_away' : 'odds' , 'home_team' : 'opponent' , 'away_team' : 'team' }) . assign ( home_game = 0 )) next_week = a_df . append ( h_df ) . sort_values ( by = 'game' ) . rename ( columns = { 'Date' : 'date' , 'Season' : 'season' , 'Round' : 'round' , 'Venue' : 'venue' }) next_week [ 'date' ] = pd . to_datetime ( next_week . date ) next_week [ 'round' ] = afl_data [ 'round' ] . iloc [ - 1 ] + 1 return next_week next_week_df = get_next_week_df ( afl_data ) game_ids_next_round = create_next_weeks_game_ids ( afl_data ) next_week_df date round season venue game home_game odds opponent team 0 2018-09-29 27 2018 MCG 15407 0 1.75 West Coast Collingwood 0 2018-09-29 27 2018 MCG 15407 1 2.34 Collingwood West Coast fixture . tail () Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG Create Each Feature Now let's append next week's DataFrame to our afl_data, match_results and odds DataFrames and then create all the features we used in the AFL Feature Creation Tutorial . We need to append the games and then feed them into our function so that we can create features for upcoming games. # Append next week's games to our afl_data DataFrame afl_data = afl_data . append ( next_week_df ) . reset_index ( drop = True ) # Append next week's games to match results (we need to do this for our feature creation to run) match_results = afl_data_cleaning_v2 . get_cleaned_match_results () . append ( next_week_df ) # Append next week's games to odds odds = ( afl_data_cleaning_v2 . get_cleaned_odds () . pipe ( lambda df : df . append ( next_week_df [ df . columns ])) . reset_index ( drop = True )) features_df = afl_feature_creation_v2 . prepare_afl_features ( afl_data = afl_data , match_results = match_results , odds = odds ) features_df . tail () game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_GA1_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_Unnamed: 0_diff f_behinds_diff f_goals_diff f_margin_diff f_opponent_behinds_diff f_opponent_goals_diff f_opponent_points_diff f_points_diff f_current_odds_prob f_current_odds_prob_away 1065 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.966936 -23.2 2.0 1.813998 1523.456734 1609.444874 0.653525 0.680168 0.704767 0.749812 140.535514 0.605144 -9.771981 5.892176 7.172376 6.614609 -1.365211 30.766262 21.998618 0.067228 -1.404730 -3.166732 6.933998 6.675576 0.000000 38.708158 24.587333 12.008987 10.482382 -16.709540 -15.415060 289.188486 6.350287 -2.263536 -20.966818 50.388632 0.723637 15.537783 22.912269 2.065039 10.215523 -6.689429 3259.163465 -0.136383 3.553795 16.563721 -2.353514 1.162696 4.622664 21.186385 0.661551 0.340379 1066 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.089084 -3.2 2.0 2.577161 1397.237139 1499.366007 0.725980 0.655749 0.723949 0.677174 51.799992 3.399035 6.067393 -2.189489 -10.475859 1.154766 -8.883840 -21.810962 33.058382 40.618410 2.286314 -0.345734 -3.778445 -2.182673 0.000000 19.816372 -21.562916 2.678384 -14.777698 13.242010 12.065594 -82.381996 -2.176564 2.335825 -4.952336 45.719406 3.344217 -2.095613 -3.929084 -3.182381 -12.832197 57.226776 -20221.371526 1.968709 -1.897958 -15.177001 1.067099 0.781811 5.757963 -9.419038 0.284269 0.717566 1067 15404 Collingwood GWS 2018-09-15 25 M.C.G. 2018 1.882301 12.6 3.0 2.018344 1546.000498 1590.806454 0.693185 0.706222 0.718446 0.727961 205.916671 -1.642954 -2.980828 -0.266023 8.547225 -3.751909 -0.664977 10.563513 48.175985 43.531908 -5.836979 5.388668 4.395675 2.555152 0.000000 51.588962 11.558254 4.276481 11.284445 -3.412977 -2.206815 -234.577304 2.637758 -10.537765 -11.127876 125.607377 -3.485896 3.532031 15.102292 -2.500685 8.187543 38.053445 12500.525732 -1.006173 2.520135 18.634835 -2.159882 -0.393386 -4.520198 14.114637 0.608495 0.393856 1068 15406 West Coast Melbourne 2018-09-22 26 Perth Stadium 2018 2.013572 21.2 3.0 1.884148 1577.888606 1542.095154 0.688877 0.708941 0.649180 0.698319 -118.135184 -3.005709 2.453190 -5.103869 -14.368949 -12.245458 2.771411 -45.364271 -60.210182 -24.049523 -2.791277 6.115918 -5.041030 -5.335746 0.000000 -78.816902 -18.784547 -13.957754 -5.527613 18.606721 25.366778 -910.988860 -5.515812 -9.483590 8.914093 -131.380758 -7.142529 -49.484957 -13.718798 -4.862994 -9.834616 -23.673638 -3178.282073 -1.785349 -2.569957 -20.008787 0.476202 0.387915 2.803694 -17.205093 0.543774 0.457875 1069 15407 West Coast Collingwood 2018-09-29 27 MCG 2018 1.981832 17.2 3.0 1.838864 1591.348723 1562.924273 0.679011 0.724125 0.711352 0.709346 159.522670 0.893421 -0.475725 3.391070 -5.088751 5.875388 5.352234 7.729063 -7.358202 -4.719968 6.113565 4.822252 2.871241 2.690270 3.636364 -64.238180 -0.631102 2.078832 6.005613 56.879978 34.373271 1016.491933 1.199751 2.454685 12.197047 219.666562 2.484363 0.379162 2.566991 0.639666 2.258377 -23.841529 -368920.360240 -0.646160 0.892051 3.040850 1.589568 0.012622 1.665299 4.706148 0.427350 0.571429 Create Predictions For the Upcoming Round Now that we have our features, we can use our model that we created in part 3 to predict the next round. First we need to filter our features_df into a training df and a df with next round's features/matches. Then we can use the model created in the last tutorial to create predictions. For simplicity, I have hardcoded the parameters we used in the last tutorial. # Get the train df by only taking the games IDs which aren't in the next week df train_df = features_df [ ~ features_df . game . isin ( next_week_df . game )] # Get the result and merge to the feature_df match_results = ( pd . read_csv ( \"data/afl_match_results.csv\" ) . rename ( columns = { 'Game' : 'game' }) . assign ( result = lambda df : df . apply ( lambda row : 1 if row [ 'Home.Points' ] > row [ 'Away.Points' ] else 0 , axis = 1 ))) train_df = pd . merge ( train_df , match_results [[ 'game' , 'result' ]], on = 'game' ) train_x = train_df . drop ( columns = [ 'result' ]) train_y = train_df . result next_round_x = features_df [ features_df . game . isin ( next_week_df . game )] # Fit out logistic regression model - note that our predictions come out in the order of [away_team_prob, home_team_prob] lr_best_params = { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } feature_cols = [ col for col in train_df if col . startswith ( 'f_' )] # Scale features scaler = StandardScaler () train_x [ feature_cols ] = scaler . fit_transform ( train_x [ feature_cols ]) next_round_x [ feature_cols ] = scaler . transform ( next_round_x [ feature_cols ]) lr = LogisticRegression ( ** lr_best_params ) lr . fit ( train_x [ feature_cols ], train_y ) prediction_probs = lr . predict_proba ( next_round_x [ feature_cols ]) modelled_home_odds = [ 1 / i [ 1 ] for i in prediction_probs ] modelled_away_odds = [ 1 / i [ 0 ] for i in prediction_probs ] # Create a predictions df preds_df = ( next_round_x [[ 'date' , 'home_team' , 'away_team' , 'venue' , 'game' ]] . copy () . assign ( modelled_home_odds = modelled_home_odds , modelled_away_odds = modelled_away_odds ) . pipe ( pd . merge , next_week_odds , on = [ 'home_team' , 'away_team' ]) . pipe ( pd . merge , features_df [[ 'game' , 'f_elo_home' , 'f_elo_away' ]], on = 'game' ) . drop ( columns = 'game' ) ) preds_df date home_team away_team venue modelled_home_odds modelled_away_odds odds odds_away f_elo_home f_elo_away 0 2018-09-29 West Coast Collingwood MCG 2.326826 1.753679 2.34 1.75 1591.348723 1562.924273 Alternatively, if you want to generate predictions using a script which uses all the above code, just run the following: print ( afl_modelling_v2 . create_predictions ()) date home_team away_team venue modelled_home_odds \\ 0 2018-09-29 West Coast Collingwood MCG 2.326826 modelled_away_odds odds odds_away f_elo_home f_elo_away 0 1.753679 2.34 1.75 1591.348723 1562.924273 Conclusion Congratulations! You have created AFL predictions for this week. If you are beginner to this, don't be overwhelmed. The process gets easier each time you do it. And it is super rewarding. In future iterations we will update this tutorial to predict actual odds, and then integrate this with Betfair's API so that you can create an automated betting strategy using Machine Learning to create your predictions! Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"AFL modelling walk through in Python"},{"location":"modelling/AFLmodellingPython/#afl-modelling-walkthrough","text":"","title":"AFL Modelling Walkthrough"},{"location":"modelling/AFLmodellingPython/#01-data-cleaning","text":"These tutorials will walk you through how to construct your own basic AFL model, using publicly available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through the basics of cleaning this dataset and how we have done it. If you want to get straight to feature creation or modelling, feel free to jump ahead! # Import libraries import pandas as pd import numpy as np import re pd . set_option ( 'display.max_columns' , None ) We will first explore the DataFrames, and then create functions to wrangle them and clean them into more consistent sets of data. # Read/clean each DataFrame match_results = pd . read_csv ( \"data/afl_match_results.csv\" ) odds = pd . read_csv ( \"data/afl_odds.csv\" ) player_stats = pd . read_csv ( \"data/afl_player_stats.csv\" ) odds . tail ( 3 ) trunc event_name path selection_name odds 4179 2018-09-01 Match Odds VFL/Richmond Reserves v Williamstown Williamstown 2.3878 4180 2018-09-01 Match Odds WAFL/South Fremantle v West Perth South Fremantle 1.5024 4181 2018-09-01 Match Odds WAFL/South Fremantle v West Perth West Perth 2.7382 match_results . tail ( 3 ) Game Date Round Home.Team Home.Goals Home.Behinds Home.Points Away.Team Away.Goals Away.Behinds Away.Points Venue Margin Season Round.Type Round.Number 15395 15396 2018-08-26 R23 Brisbane Lions 11 6 72 West Coast 14 14 98 Gabba -26 2018 Regular 23 15396 15397 2018-08-26 R23 Melbourne 15 12 102 GWS 8 9 57 M.C.G. 45 2018 Regular 23 15397 15398 2018-08-26 R23 St Kilda 14 10 94 North Melbourne 17 15 117 Docklands -23 2018 Regular 23 player_stats . tail ( 3 ) AF B BO CCL CG CL CM CP D DE Date ED FA FF G GA HB HO I50 ITC K M MG MI5 Match_id One.Percenters Opposition Player R50 Round SC SCL SI Season Status T T5 TO TOG Team UP Venue 89317 38 1 0 0.0 0 0 1 2 9 55.6 25/08/2018 5 0 0 0 0 3 0 0 2.0 6 3 132.0 2 9711 0 Fremantle Christopher Mayne 1 Round 23 35 0.0 2.0 2018 Away 1 0.0 1.0 57 Collingwood 7 Optus Stadium 89318 38 0 0 0.0 3 0 0 3 9 55.6 25/08/2018 5 0 1 0 0 3 0 0 4.0 6 3 172.0 0 9711 2 Fremantle Nathan Murphy 5 Round 23 29 0.0 0.0 2018 Away 1 0.0 3.0 70 Collingwood 6 Optus Stadium 89319 56 1 0 0.0 1 0 0 3 8 62.5 25/08/2018 5 0 0 2 0 2 0 0 2.0 6 3 180.0 3 9711 2 Fremantle Jaidyn Stephenson 0 Round 23 56 0.0 4.0 2018 Away 3 1.0 2.0 87 Collingwood 5 Optus Stadium Have a look at the structure of the DataFrames. Notice that for the odds DataFrame, each game is split between two rows, whilst for the match_results each game is on one row. We will have to get around this by splitting the games up onto two rows, as this will allow our feature transformation functions to be applied more easily later on. For the player_stats DataFrame we will aggregate these stats into each game on separate rows. First, we will write functions to make the odds data look a bit nicer, with only a team column, a date column and a 'home_game' column which takes the values 0 or 1 depending on if it was a home game for that team. To do this we will use the regex module to extract the team names from the path column, as well as the to_datetime function from pandas. We will also replace all the inconsistent team names with consistent team names. def get_cleaned_odds ( df = None ): # If a df hasn't been specified as a parameter, read the odds df if df is None : df = pd . read_csv ( \"data/afl_odds.csv\" ) # Get a dictionary of team names we want to change and their new values team_name_mapping = { 'Adelaide Crows' : 'Adelaide' , 'Brisbane Lions' : 'Brisbane' , 'Carlton Blues' : 'Carlton' , 'Collingwood Magpies' : 'Collingwood' , 'Essendon Bombers' : 'Essendon' , 'Fremantle Dockers' : 'Fremantle' , 'GWS Giants' : 'GWS' , 'Geelong Cats' : 'Geelong' , 'Gold Coast Suns' : 'Gold Coast' , 'Greater Western Sydney' : 'GWS' , 'Greater Western Sydney Giants' : 'GWS' , 'Hawthorn Hawks' : 'Hawthorn' , 'Melbourne Demons' : 'Melbourne' , 'North Melbourne Kangaroos' : 'North Melbourne' , 'Port Adelaide Magpies' : 'Port Adelaide' , 'Port Adelaide Power' : 'Port Adelaide' , 'P Adelaide' : 'Port Adelaide' , 'Richmond Tigers' : 'Richmond' , 'St Kilda Saints' : 'St Kilda' , 'Sydney Swans' : 'Sydney' , 'West Coast Eagles' : 'West Coast' , 'Wetsern Bulldogs' : 'Western Bulldogs' , 'Western Bullbogs' : 'Western Bulldogs' } # Add columns df = ( df . assign ( date = lambda df : pd . to_datetime ( df . trunc ), # Create a datetime column home_team = lambda df : df . path . str . extract ( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 1 ] . str . strip (), away_team = lambda df : df . path . str . extract ( '(([\\w\\s]+) v ([\\w\\s]+))' , expand = True )[ 2 ] . str . strip ()) . drop ( columns = [ 'path' , 'trunc' , 'event_name' ]) # Drop irrelevant columns . rename ( columns = { 'selection_name' : 'team' }) # Rename columns . replace ( team_name_mapping ) . sort_values ( by = 'date' ) . reset_index ( drop = True ) . assign ( home_game = lambda df : df . apply ( lambda row : 1 if row . home_team == row . team else 0 , axis = 'columns' )) . drop ( columns = [ 'home_team' , 'away_team' ])) return df # Apply the wrangling and cleaning function odds = get_cleaned_odds ( odds ) odds . tail () team odds date home_game 4177 South Fremantle 1.5024 2018-09-01 1 4178 Port Melbourne 2.8000 2018-09-01 0 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 We now have a DataFrame that looks nice and easy to join with our other DataFrames. Now let's lean up the match_details DataFrame. # Define a function which cleans the match results df, and separates each teams' stats onto individual rows def get_cleaned_match_results ( df = None ): # If a df hasn't been specified as a parameter, read the match_results df if df is None : df = pd . read_csv ( \"data/afl_match_results.csv\" ) # Create column lists to loop through - these are the columns we want in home and away dfs home_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' , 'Margin' , 'Venue' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' ] away_columns = [ 'Game' , 'Date' , 'Round.Number' , 'Away.Team' , 'Away.Goals' , 'Away.Behinds' , 'Away.Points' , 'Margin' , 'Venue' , 'Home.Team' , 'Home.Goals' , 'Home.Behinds' , 'Home.Points' ] mapping = [ 'game' , 'date' , 'round' , 'team' , 'goals' , 'behinds' , 'points' , 'margin' , 'venue' , 'opponent' , 'opponent_goals' , 'opponent_behinds' , 'opponent_points' ] team_name_mapping = { 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' } # Create a df with only home games df_home = ( df [ home_columns ] . rename ( columns = { old_col : new_col for old_col , new_col in zip ( home_columns , mapping )}) . assign ( home_game = 1 )) # Create a df with only away games df_away = ( df [ away_columns ] . rename ( columns = { old_col : new_col for old_col , new_col in zip ( away_columns , mapping )}) . assign ( home_game = 0 , margin = lambda df : df . margin * - 1 )) # Append these dfs together new_df = ( df_home . append ( df_away ) . sort_values ( by = 'game' ) # Sort by game ID . reset_index ( drop = True ) # Reset index . assign ( date = lambda df : pd . to_datetime ( df . date )) # Create a datetime column . replace ( team_name_mapping )) # Rename team names to be consistent with other dfs return new_df match_results = get_cleaned_match_results ( match_results ) match_results . head () game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 3 2 1897-05-08 1 St Kilda 2 4 16 -25 Victoria Park Collingwood 5 11 41 0 4 3 1897-05-08 1 Geelong 3 6 24 -23 Corio Oval Essendon 7 5 47 1 Now we have both the odds DataFrame and match_results DataFrame ready for feature creation! Finally, we will aggregate the player_stats DataFrame stats for each game rather than individual player stats. For this DataFrame we have regular stats, such as disposals, marks etc. and Advanced Stats, such as Tackles Inside 50 and Metres Gained. However these advanced stats are only available from 2015, so we will not be using them in this tutorial - as there isn't enough data from 2015 to train our models. Let's now aggregate the player_stats DataFrame. def get_cleaned_aggregate_player_stats ( df = None ): # If a df hasn't been specified as a parameter, read the player_stats df if df is None : df = pd . read_csv ( \"data/afl_player_stats.csv\" ) agg_stats = ( df . rename ( columns = { # Rename columns to lowercase 'Season' : 'season' , 'Round' : 'round' , 'Team' : 'team' , 'Opposition' : 'opponent' , 'Date' : 'date' }) . groupby ( by = [ 'date' , 'season' , 'team' , 'opponent' ], as_index = False ) # Groupby to aggregate the stats for each game . sum () . drop ( columns = [ 'DE' , 'TOG' , 'Match_id' ]) # Drop columns . assign ( date = lambda df : pd . to_datetime ( df . date , format = \" %d /%m/%Y\" )) # Create a datetime object . sort_values ( by = 'date' ) . reset_index ( drop = True )) return agg_stats agg_stats = get_cleaned_aggregate_player_stats ( player_stats ) agg_stats . tail () date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3621 2018-08-26 2018 Brisbane West Coast 1652 5 0 14.0 49 37 8 132 394 302 20 18 11 9 167 48 49 59.0 227 104 5571.0 6 48 39 1645 23.0 86.0 62 13.0 69.0 256 3622 2018-08-26 2018 West Coast Brisbane 1548 11 5 13.0 49 42 9 141 360 262 18 20 14 8 137 39 56 70.0 223 95 5809.0 12 39 34 1655 29.0 94.0 55 6.0 59.0 217 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 We now have a three fully prepared DataFrames which are almost ready to be analysed and for a model to be built on! Let's have a look at how they look and then merge them together into our final DataFrame. odds . tail ( 3 ) team odds date home_game 4179 Box Hill Hawks 1.4300 2018-09-01 1 4180 Casey Demons 1.9000 2018-09-01 1 4181 West Perth 2.7382 2018-09-01 0 match_results . tail ( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 30793 15397 2018-08-26 23 Melbourne 15 12 102 45 M.C.G. GWS 8 9 57 1 30794 15398 2018-08-26 23 St Kilda 14 10 94 -23 Docklands North Melbourne 17 15 117 1 30795 15398 2018-08-26 23 North Melbourne 17 15 117 23 Docklands St Kilda 14 10 94 0 agg_stats . tail ( 3 ) date season team opponent AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3623 2018-08-26 2018 St Kilda North Melbourne 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 3624 2018-08-26 2018 GWS Melbourne 1449 7 17 14.0 42 31 12 111 355 274 19 13 8 7 159 18 50 54.0 196 110 5416.0 10 62 34 1532 17.0 78.0 46 5.0 58.0 254 3625 2018-08-26 2018 Melbourne GWS 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 merged_df = ( odds [ odds . team . isin ( agg_stats . team . unique ())] . pipe ( pd . merge , match_results , on = [ 'date' , 'team' , 'home_game' ]) . pipe ( pd . merge , agg_stats , on = [ 'date' , 'team' , 'opponent' ]) . sort_values ( by = [ 'game' ])) merged_df . tail ( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3199 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3195 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3200 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Great! We now have a clean looking datset with each row representing one team in a game. Let's now eliminate the outliers from a dataset. We know that Essendon had a doping scandal which resulted in their entire team being banned for a year in 2016, so let's remove all of their 2016 games. To do this we will filter based on the team and season, and then invert this with ~. # Define a function which eliminates outliers def outlier_eliminator ( df ): # Eliminate Essendon 2016 games essendon_filter_criteria = ~ ((( df [ 'team' ] == 'Essendon' ) & ( df [ 'season' ] == 2016 )) | (( df [ 'opponent' ] == 'Essendon' ) & ( df [ 'season' ] == 2016 ))) df = df [ essendon_filter_criteria ] . reset_index ( drop = True ) return df afl_data = outlier_eliminator ( merged_df ) afl_data . tail ( 3 ) team odds date home_game game round goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points season AF B BO CCL CG CL CM CP D ED FA FF G GA HB HO I50 ITC K M MG MI5 One.Percenters R50 SC SCL SI T T5 TO UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Finally, let's mark all of the columns that we are going to use in feature creation with the string 'f_' at the start of their column name so that we can easily filter for these columns. non_feature_cols = [ 'team' , 'date' , 'home_game' , 'game' , 'round' , 'venue' , 'opponent' , 'season' ] afl_data = afl_data . rename ( columns = { col : 'f_' + col for col in afl_data if col not in non_feature_cols }) afl_data . tail ( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269 Our data is now fully ready to be explored and for features to be created.","title":"01. Data Cleaning"},{"location":"modelling/AFLmodellingPython/#02-feature-creation","text":"These tutorials will walk you through how to construct your own basic AFL model. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through creating features from our dataset, which was cleaned in the first tutorial. Feature engineering is an integral part of the Data Science process. Creative and smart features can be the difference between an average performing model and a model profitable which beats the market odds.","title":"02. Feature Creation"},{"location":"modelling/AFLmodellingPython/#grabbing-our-dataset","text":"First, we will import our required modules, as well as the prepare_afl_data function which we created in our afl_data_cleaning script. This essentially cleans all the data for us so that we're ready to explore the data and make some features. # Import modules from afl_data_cleaning_v2 import * import afl_data_cleaning_v2 import pandas as pd pd . set_option ( 'display.max_columns' , None ) import warnings warnings . filterwarnings ( 'ignore' ) import numpy as np # Use the prepare_afl_data function to prepare the data for us; this function condenses what we walked through in the previous tutorial afl_data = prepare_afl_data () afl_data . tail ( 3 ) team f_odds date home_game game round f_goals f_behinds f_points f_margin venue opponent f_opponent_goals f_opponent_behinds f_opponent_points season f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3154 Melbourne 1.5116 2018-08-26 1 15397 23 15 12 102 45 M.C.G. GWS 8 9 57 2018 1712 8 12 10.0 38 30 12 139 403 302 13 19 15 14 181 48 54 59.0 222 106 6198.0 16 34 39 1768 20.0 147.0 60 2.0 53.0 269 3155 North Melbourne 1.3936 2018-08-26 0 15398 23 17 15 117 23 Docklands St Kilda 14 10 94 2018 1707 13 7 15.0 50 24 6 131 425 322 14 18 17 13 201 32 59 77.0 224 106 5833.0 23 33 29 1735 9.0 154.0 48 9.0 66.0 300 3156 St Kilda 3.5178 2018-08-26 1 15398 23 14 10 94 -23 Docklands North Melbourne 17 15 117 2018 1587 8 11 19.0 48 33 7 125 383 299 18 14 14 13 173 23 48 68.0 210 112 5522.0 14 46 35 1568 14.0 95.0 50 7.0 77.0 269","title":"Grabbing Our Dataset"},{"location":"modelling/AFLmodellingPython/#creating-a-feature-dataframe","text":"Let's create a feature DataFrame and merge all of our features into this DataFrame as we go. features = afl_data [[ 'date' , 'game' , 'team' , 'opponent' , 'venue' , 'home_game' ]] . copy ()","title":"Creating A Feature DataFrame"},{"location":"modelling/AFLmodellingPython/#what-each-column-refers-to","text":"Below is a DataFrame which outlines what each column refers to. column_abbreviations = pd . read_csv ( \"data/afl_data_columns_mapping.csv\" ) column_abbreviations Feature Abbreviated Feature 0 GA Goal Assists 1 CP Contested Possessions 2 UP Uncontested Possessions 3 ED Effective Disposals 4 CM Contested Marks 5 MI5 Marks Inside 50 6 One.Percenters One Percenters 7 BO Bounces 8 K Kicks 9 HB Handballs 10 D Disposals 11 M Marks 12 G Goals 13 B Behinds 14 T Tackles 15 HO Hitouts 16 I50 Inside 50s 17 CL Clearances 18 CG Clangers 19 R50 Rebound 50s 20 FF Frees For 21 FA Frees Against 22 AF AFL Fantasy Points 23 SC Supercoach Points 24 CCL Centre Clearances 25 SCL Stoppage Clearances 26 SI Score Involvements 27 MG Metres Gained 28 TO Turnovers 29 ITC Intercepts 30 T5 Tackles Inside 50","title":"What Each Column Refers To"},{"location":"modelling/AFLmodellingPython/#feature-creation","text":"Now let's think about what features we can create. We have a enormous amount of stats to sift through. To start, let's create some simple features based on our domain knowledge of Aussie Rules.","title":"Feature Creation"},{"location":"modelling/AFLmodellingPython/#creating-expontentially-weighted-rolling-averages-as-features","text":"Next, we will create rolling averages of statistics such as Tackles, which we will use as features. It is fair to assume that a team's performance in a certain stat may have predictive power to the overall result. And in general, if a team consistently performs well in this stat, this may have predictive power to the result of their future games. We can't simply train a model on stats from the game which we are trying to predict (i.e. data that we don't have before the game begins), as this will leak the result. We need to train our model on past data. One way of doing this is to train our model on average stats over a certain amount of games. If a team is averaging high in this stat, this may give insight into if they are a strong team. Similarly, if the team is averaging poorly in this stat (relative to the team they are playing), this may have predictive power and give rise to a predicted loss. To do this we will create a function which calculates the rolling averages, known as create_exp_weighted_avgs, which takes our cleaned DataFrame as an input, as well as the alpha which, when higher, weights recent performances more than old performances. To read more about expontentially weighted moving averages, please read the documentation here . First, we will grab all the columns which we want to create EMAs for, and then use our function to create the average for that column. We will create a new DataFrame and add these columns to this new DataFrame. # Define a function which returns a DataFrame with the expontential moving average for each numeric stat def create_exp_weighted_avgs ( df , span ): # Create a copy of the df with only the game id and the team - we will add cols to this df ema_features = df [[ 'game' , 'team' ]] . copy () feature_names = [ col for col in df . columns if col . startswith ( 'f_' )] # Get a list of columns we will iterate over for feature_name in feature_names : feature_ema = ( df . groupby ( 'team' )[ feature_name ] . transform ( lambda row : ( row . ewm ( span = span ) . mean () . shift ( 1 )))) ema_features [ feature_name ] = feature_ema return ema_features features_rolling_averages = create_exp_weighted_avgs ( afl_data , span = 10 ) features_rolling_averages . tail () game team f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP 3152 15396 West Coast 2.094236 12.809630 10.047145 86.904928 8.888770 11.435452 9.403444 78.016158 3193.612782 16.472115 11.958482 23.379562 100.095244 68.252001 27.688669 284.463270 719.884644 525.878017 36.762440 44.867118 25.618202 17.522871 270.478779 88.139376 105.698031 148.005305 449.405865 201.198907 11581.929999 20.048124 95.018480 74.180967 3314.157893 44.872398 177.894442 126.985101 20.565549 138.876613 438.848376 3153 15397 GWS 1.805565 13.100372 13.179329 91.781563 18.527618 10.371198 11.026754 73.253945 3165.127358 19.875913 12.947209 25.114002 105.856671 80.609640 23.374884 303.160047 741.439198 534.520295 42.597317 38.160889 26.208715 18.688880 300.188301 81.540693 106.989070 143.032506 441.250897 173.050118 12091.630837 21.106142 103.077097 80.201059 3419.245919 55.495610 219.879895 138.202470 25.313148 135.966798 438.466439 3154 15397 Melbourne 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.787800 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 3155 15398 North Melbourne 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.541130 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3156 15398 St Kilda 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.498760 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 As you can see our function worked perfectly! Now we have a full DataFrame of exponentially weighted moving averages. Note that as these rolling averages have been shifted by 1 to ensure no data leakage, the first round of the data will have all NA values. We can drop these later. Let's add these averages to our features DataFrame features = pd . merge ( features , features_rolling_averages , on = [ 'game' , 'team' ])","title":"Creating Expontentially Weighted Rolling Averages as Features"},{"location":"modelling/AFLmodellingPython/#creating-a-form-between-the-teams-feature","text":"It is well known in Aussie Rules that often some teams perform better against certain teams than others. If we isolate our features to pure stats based on previous games not between the teams playing, or elo ratings, we won't account for any relationships between certain teams. An example is the Kennett Curse , where Geelong won 11 consecutive games against Hawthorn, despite being similarly matched teams. Let's create a feature which calculates how many games a team has won against their opposition over a given window of games. To do this, we will need to use historical data that dates back well before our current DataFrame starts at. Otherwise we will be using a lot of our games to calculate form, meaning we will have to drop these rows before feeding it into an algorithm. So let's use our prepare_match_results function which we defined in the afl_data_cleaning tutorial to grab a clean DataFrame of all match results since 1897. We can then calculate the form and join this to our current DataFrame. match_results = afl_data_cleaning_v2 . get_cleaned_match_results () match_results . head ( 3 ) game date round team goals behinds points margin venue opponent opponent_goals opponent_behinds opponent_points home_game 0 1 1897-05-08 1 Fitzroy 6 13 49 33 Brunswick St Carlton 2 4 16 1 1 1 1897-05-08 1 Carlton 2 4 16 -33 Brunswick St Fitzroy 6 13 49 0 2 2 1897-05-08 1 Collingwood 5 11 41 25 Victoria Park St Kilda 2 4 16 1 form_btwn_teams = match_results [[ 'game' , 'team' , 'opponent' , 'margin' ]] . copy () form_btwn_teams [ 'f_form_margin_btwn_teams' ] = ( match_results . groupby ([ 'team' , 'opponent' ])[ 'margin' ] . transform ( lambda row : row . rolling ( 5 ) . mean () . shift ()) . fillna ( 0 )) form_btwn_teams [ 'f_form_past_5_btwn_teams' ] = \\ ( match_results . assign ( win = lambda df : df . apply ( lambda row : 1 if row . margin > 0 else 0 , axis = 'columns' )) . groupby ([ 'team' , 'opponent' ])[ 'win' ] . transform ( lambda row : row . rolling ( 5 ) . mean () . shift () * 5 ) . fillna ( 0 )) form_btwn_teams . tail ( 3 ) game team opponent margin f_form_margin_btwn_teams f_form_past_5_btwn_teams 30793 15397 Melbourne GWS 45 -23.2 2.0 30794 15398 St Kilda North Melbourne -23 -3.2 2.0 30795 15398 North Melbourne St Kilda 23 3.2 3.0 # Merge to our features df features = pd . merge ( features , form_btwn_teams . drop ( columns = [ 'margin' ]), on = [ 'game' , 'team' , 'opponent' ])","title":"Creating a 'Form Between the Teams' Feature"},{"location":"modelling/AFLmodellingPython/#creating-efficiency-features","text":"","title":"Creating Efficiency Features"},{"location":"modelling/AFLmodellingPython/#disposal-efficiency","text":"Disposal efficiency is pivotal in Aussie Rules football. If you are dispose of the ball effectively you are much more likely to score and much less likely to concede goals than if you dispose of it ineffectively. Let's create a disposal efficiency feature by dividing Effective Disposals by Disposals.","title":"Disposal Efficiency"},{"location":"modelling/AFLmodellingPython/#inside-50rebound-50-efficiency","text":"Similarly, one could hypothesise that teams who keep the footy in their Inside 50 regularly will be more likely to score, whilst teams who are effective at getting the ball out of their Defensive 50 will be less likely to concede. Let's use this logic to create Inside 50 Efficiency and Rebound 50 Efficiency features. The formula used will be: Inside 50 Efficiency = R50_Opponents / I50 (lower is better). Rebound 50 Efficiency = R50 / I50_Opponents (higher is better). Using these formulas, I50 Efficiency = R50 Efficiency_Opponent. So we will just need to create the formulas for I50 efficiency. To create these features we will need the opposition's Inside 50s/Rebound 50s. So we will split out data into two DataFrames, create a new DataFrame by joining these two DataFrames on the Game, calculate our efficiency features, then join our features with our main features DataFrame. # Get each match on single rows single_row_df = ( afl_data [[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' , ]] . query ( 'home_game == 1' ) . rename ( columns = { 'team' : 'home_team' , 'f_I50' : 'f_I50_home' , 'f_R50' : 'f_R50_home' , 'f_D' : 'f_D_home' , 'f_ED' : 'f_ED_home' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , afl_data [[ 'game' , 'team' , 'f_I50' , 'f_R50' , 'f_D' , 'f_ED' , 'home_game' ]] . query ( 'home_game == 0' ) . rename ( columns = { 'team' : 'away_team' , 'f_I50' : 'f_I50_away' , 'f_R50' : 'f_R50_away' , 'f_D' : 'f_D_away' , 'f_ED' : 'f_ED_away' }) . drop ( columns = 'home_game' ), on = 'game' )) single_row_df . head () game home_team f_I50_home f_R50_home f_D_home f_ED_home away_team f_I50_away f_R50_away f_D_away f_ED_away 0 13764 Carlton 69 21 373 268 Richmond 37 50 316 226 1 13765 Geelong 54 40 428 310 St Kilda 52 45 334 246 2 13766 Collingwood 70 38 398 289 Port Adelaide 50 44 331 232 3 13767 Adelaide 59 38 366 264 Hawthorn 54 38 372 264 4 13768 Brisbane 50 39 343 227 Fremantle 57 30 351 250 single_row_df = single_row_df . assign ( f_I50_efficiency_home = lambda df : df . f_R50_away / df . f_I50_home , f_I50_efficiency_away = lambda df : df . f_R50_home / df . f_I50_away ) feature_efficiency_cols = [ 'f_I50_efficiency_home' , 'f_I50_efficiency_away' ] # Now let's create an Expontentially Weighted Moving Average for these features - we will need to reshape our DataFrame to do this efficiency_features_multi_row = ( single_row_df [[ 'game' , 'home_team' ] + feature_efficiency_cols ] . rename ( columns = { 'home_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency' , 'f_I50_efficiency_away' : 'f_I50_efficiency_opponent' , }) . append (( single_row_df [[ 'game' , 'away_team' ] + feature_efficiency_cols ] . rename ( columns = { 'away_team' : 'team' , 'f_I50_efficiency_home' : 'f_I50_efficiency_opponent' , 'f_I50_efficiency_away' : 'f_I50_efficiency' , })), sort = True ) . sort_values ( by = 'game' ) . reset_index ( drop = True )) efficiency_features = efficiency_features_multi_row [[ 'game' , 'team' ]] . copy () feature_efficiency_cols = [ 'f_I50_efficiency' , 'f_I50_efficiency_opponent' ] for feature in feature_efficiency_cols : efficiency_features [ feature ] = ( efficiency_features_multi_row . groupby ( 'team' )[ feature ] . transform ( lambda row : row . ewm ( span = 10 ) . mean () . shift ( 1 ))) # Get feature efficiency df back onto single rows efficiency_features = pd . merge ( efficiency_features , afl_data [[ 'game' , 'team' , 'home_game' ]], on = [ 'game' , 'team' ]) efficiency_features_single_row = ( efficiency_features . query ( 'home_game == 1' ) . rename ( columns = { 'team' : 'home_team' , 'f_I50_efficiency' : 'f_I50_efficiency_home' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_home' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , ( efficiency_features . query ( 'home_game == 0' ) . rename ( columns = { 'team' : 'away_team' , 'f_I50_efficiency' : 'f_I50_efficiency_away' , 'f_I50_efficiency_opponent' : 'f_R50_efficiency_away' }) . drop ( columns = 'home_game' )), on = 'game' )) efficiency_features_single_row . tail ( 5 ) game home_team f_I50_efficiency_home f_R50_efficiency_home away_team f_I50_efficiency_away f_R50_efficiency_away 1580 15394 Carlton 0.730668 0.675002 Adelaide 0.691614 0.677128 1581 15395 Sydney 0.699994 0.778280 Hawthorn 0.699158 0.673409 1582 15396 Brisbane 0.683604 0.691730 West Coast 0.696822 0.709605 1583 15397 Melbourne 0.667240 0.692632 GWS 0.684525 0.753783 1584 15398 St Kilda 0.730843 0.635819 North Melbourne 0.697018 0.654991 We will merge these features back to our features df later, when the features data frame is on a single row as well.","title":"Inside 50/Rebound 50 Efficiency"},{"location":"modelling/AFLmodellingPython/#creating-an-elo-feature","text":"Another feature which we could create is an Elo feature. If you don't know what Elo is, go ahead and read our article on it here . We have also written a guide on using elo to model the 2018 FIFA World Cup here . Essentially, Elo ratings increase if you win. The amount the rating increases is based on how strong the opponent is relative to the team who won. Weak teams get more points for beating stronger teams than they do for beating weaker teams, and vice versa for losses (teams lose points for losses). Mathematically, Elo ratings can also assign a probability for winning or losing based on the two Elo Ratings of the teams playing. So let's get into it. We will first define a function which calculates the elo for each team and applies these elos to our DataFrame. # Define a function which finds the elo for each team in each game and returns a dictionary with the game ID as a key and the # elos as the key's value, in a list. It also outputs the probabilities and a dictionary of the final elos for each team def elo_applier ( df , k_factor ): # Initialise a dictionary with default elos for each team elo_dict = { team : 1500 for team in df [ 'team' ] . unique ()} elos , elo_probs = {}, {} # Get a home and away dataframe so that we can get the teams on the same row home_df = df . loc [ df . home_game == 1 , [ 'team' , 'game' , 'f_margin' , 'home_game' ]] . rename ( columns = { 'team' : 'home_team' }) away_df = df . loc [ df . home_game == 0 , [ 'team' , 'game' ]] . rename ( columns = { 'team' : 'away_team' }) df = ( pd . merge ( home_df , away_df , on = 'game' ) . sort_values ( by = 'game' ) . drop_duplicates ( subset = 'game' , keep = 'first' ) . reset_index ( drop = True )) # Loop over the rows in the DataFrame for index , row in df . iterrows (): # Get the Game ID game_id = row [ 'game' ] # Get the margin margin = row [ 'f_margin' ] # If the game already has the elos for the home and away team in the elos dictionary, go to the next game if game_id in elos . keys (): continue # Get the team and opposition home_team = row [ 'home_team' ] away_team = row [ 'away_team' ] # Get the team and opposition elo score home_team_elo = elo_dict [ home_team ] away_team_elo = elo_dict [ away_team ] # Calculated the probability of winning for the team and opposition prob_win_home = 1 / ( 1 + 10 ** (( away_team_elo - home_team_elo ) / 400 )) prob_win_away = 1 - prob_win_home # Add the elos and probabilities our elos dictionary and elo_probs dictionary based on the Game ID elos [ game_id ] = [ home_team_elo , away_team_elo ] elo_probs [ game_id ] = [ prob_win_home , prob_win_away ] # Calculate the new elos of each team if margin > 0 : # Home team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 1 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 0 - prob_win_away ) elif margin < 0 : # Away team wins; update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 1 - prob_win_away ) elif margin == 0 : # Drawn game' update both teams' elo new_home_team_elo = home_team_elo + k_factor * ( 0.5 - prob_win_home ) new_away_team_elo = away_team_elo + k_factor * ( 0.5 - prob_win_away ) # Update elos in elo dictionary elo_dict [ home_team ] = new_home_team_elo elo_dict [ away_team ] = new_away_team_elo return elos , elo_probs , elo_dict # Use the elo applier function to get the elos and elo probabilities for each game - we will map these later elos , probs , elo_dict = elo_applier ( afl_data , 30 ) Great! now we have both rolling averages for stats as a feature, and the elo of the teams! Let's have a quick look at the current elo standings with a k-factor of 30, out of curiosity. for team in sorted ( elo_dict , key = elo_dict . get )[:: - 1 ]: print ( team , elo_dict [ team ]) Richmond 1695.2241513840117 Sydney 1645.548990879842 Hawthorn 1632.5266709780622 West Coast 1625.871701773721 Geelong 1625.423154644809 GWS 1597.4158602131877 Adelaide 1591.1704934545442 Collingwood 1560.370309216614 Melbourne 1558.5666572771509 Essendon 1529.0198398117086 Port Adelaide 1524.8882517820093 North Melbourne 1465.5637511922569 Western Bulldogs 1452.2110697845148 Fremantle 1393.142087030804 St Kilda 1360.9120149937303 Brisbane 1276.2923772139352 Gold Coast 1239.174528704772 Carlton 1226.6780896643265 This looks extremely similar to the currently AFL ladder, so this is a good sign for elo being an effective predictor of winning.","title":"Creating an Elo Feature"},{"location":"modelling/AFLmodellingPython/#merging-our-features-into-one-features-dataframe","text":"Now we need to reshape our features df so that we have all of the statistics for both teams in a game on a single row. We can then merge our elo and efficiency features to this df. # Look at our current features df features . tail ( 3 ) date game team opponent venue home_game f_odds f_goals f_behinds f_points f_margin f_opponent_goals f_opponent_behinds f_opponent_points f_AF f_B f_BO f_CCL f_CG f_CL f_CM f_CP f_D f_ED f_FA f_FF f_G f_GA f_HB f_HO f_I50 f_ITC f_K f_M f_MG f_MI5 f_One.Percenters f_R50 f_SC f_SCL f_SI f_T f_T5 f_TO f_UP f_form_margin_btwn_teams f_form_past_5_btwn_teams 3156 2018-08-26 15397 Melbourne GWS M.C.G. 1 1.706488 15.157271 13.815113 104.758740 25.170429 11.814319 8.702396 79.588311 3312.408470 22.077317 7.724955 28.364418 114.399147 78.406069 26.934677 324.352577 775.176933 547.385948 39.353251 36.025646 30.308918 22.461080 348.613592 99.78780 120.339062 154.417642 426.563341 178.102118 12395.717925 32.168752 96.390688 63.786515 3427.596843 50.041649 232.287556 144.875098 23.789233 149.042149 456.988552 -23.2 2.0 3157 2018-08-26 15398 North Melbourne St Kilda Docklands 0 2.272313 12.721783 10.733785 87.064486 -1.214246 12.915796 10.783958 88.278732 3066.272143 17.322710 9.815243 26.015421 106.465181 67.504286 26.064079 291.259574 736.279779 534.154748 34.301603 40.908551 25.386136 17.816570 341.210547 81.54113 102.589427 145.265493 395.069232 173.089408 10875.002463 21.802751 82.347511 70.416194 3171.120023 41.488865 197.620152 122.547684 22.286256 142.780474 450.374058 3.2 3.0 3158 2018-08-26 15398 St Kilda North Melbourne Docklands 1 5.516150 10.464266 11.957047 74.742643 -21.138101 14.105551 11.247440 95.880745 3094.163405 20.523847 14.569589 24.134276 102.540441 66.976211 18.018350 270.674857 773.086015 573.769838 41.319843 36.198820 20.850476 14.443658 364.405251 63.49876 103.803779 130.494307 408.680763 184.780054 10765.717942 21.572806 94.731555 65.790561 3228.278599 42.841935 196.086493 115.901425 18.796764 127.364334 508.844514 -3.2 2.0 one_line_cols = [ 'game' , 'team' , 'home_game' ] + [ col for col in features if col . startswith ( 'f_' )] # Get all features onto individual rows for each match features_one_line = ( features . loc [ features . home_game == 1 , one_line_cols ] . rename ( columns = { 'team' : 'home_team' }) . drop ( columns = 'home_game' ) . pipe ( pd . merge , ( features . loc [ features . home_game == 0 , one_line_cols ] . drop ( columns = 'home_game' ) . rename ( columns = { 'team' : 'away_team' }) . rename ( columns = { col : col + '_away' for col in features . columns if col . startswith ( 'f_' )})), on = 'game' ) . drop ( columns = [ 'f_form_margin_btwn_teams_away' , 'f_form_past_5_btwn_teams_away' ])) # Add our created features - elo, efficiency etc. features_one_line = ( features_one_line . assign ( f_elo_home = lambda df : df . game . map ( elos ) . apply ( lambda x : x [ 0 ]), f_elo_away = lambda df : df . game . map ( elos ) . apply ( lambda x : x [ 1 ])) . pipe ( pd . merge , efficiency_features_single_row , on = [ 'game' , 'home_team' , 'away_team' ]) . pipe ( pd . merge , afl_data . loc [ afl_data . home_game == 1 , [ 'game' , 'date' , 'round' , 'venue' ]], on = [ 'game' ]) . dropna () . reset_index ( drop = True ) . assign ( season = lambda df : df . date . apply ( lambda row : row . year ))) ordered_cols = [ col for col in features_one_line if col [: 2 ] != 'f_' ] + [ col for col in features_one_line if col . startswith ( 'f_' )] feature_df = features_one_line [ ordered_cols ] Finally, let's reduce the dimensionality of the features df by subtracting the home features from the away features. This will reduce the huge amount of columns we have and make our data more manageable. To do this, we will need a list of columns which we are subtracting from each other. We will then loop over each of these columns to create our new differential columns. We will then add in the implied probability from the odds of the home and away team, as our current odds feature is simply an exponential moving average over the past n games. # Create differential df - this df is the home features - the away features diff_cols = [ col for col in feature_df . columns if col + '_away' in feature_df . columns and col != 'f_odds' and col . startswith ( 'f_' )] non_diff_cols = [ col for col in feature_df . columns if col not in diff_cols and col [: - 5 ] not in diff_cols ] diff_df = feature_df [ non_diff_cols ] . copy () for col in diff_cols : diff_df [ col + '_diff' ] = feature_df [ col ] - feature_df [ col + '_away' ] # Add current odds in to diff_df odds = get_cleaned_odds () home_odds = ( odds [ odds . home_game == 1 ] . assign ( f_current_odds_prob = lambda df : 1 / df . odds ) . rename ( columns = { 'team' : 'home_team' }) . drop ( columns = [ 'home_game' , 'odds' ])) away_odds = ( odds [ odds . home_game == 0 ] . assign ( f_current_odds_prob_away = lambda df : 1 / df . odds ) . rename ( columns = { 'team' : 'away_team' }) . drop ( columns = [ 'home_game' , 'odds' ])) diff_df = ( diff_df . pipe ( pd . merge , home_odds , on = [ 'date' , 'home_team' ]) . pipe ( pd . merge , away_odds , on = [ 'date' , 'away_team' ])) diff_df . tail () game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1626 15394 Carlton Adelaide 2018-08-25 23 Docklands 2018 6.467328 -26.2 1.0 2.066016 1230.072138 1587.776445 0.730668 0.675002 0.691614 0.677128 -3.498547 -5.527193 -26.518474 -34.473769 1.289715 0.217006 7.955295 -341.342677 -9.317269 3.088569 -2.600593 15.192839 -12.518345 -4.136673 -41.855717 -72.258378 -51.998775 9.499447 8.670917 -6.973088 -4.740623 -26.964945 -13.147675 -23.928700 -28.940883 -45.293433 -15.183406 -1900.784014 -0.362402 -1.314627 4.116133 -294.813511 -9.917793 -34.724925 -5.462844 -9.367141 -19.623785 -38.188082 0.187709 0.816860 1627 15395 Sydney Hawthorn 2018-08-25 23 S.C.G. 2018 2.128611 1.0 2.0 1.777290 1662.568452 1615.507209 0.699994 0.778280 0.699158 0.673409 -1.756730 -0.874690 -11.415069 -15.575319 0.014390 4.073909 4.160250 -174.005092 -0.942357 -4.078635 -4.192916 7.814496 -2.225780 6.215760 15.042979 -34.894261 -50.615255 4.214158 0.683548 -3.535594 -3.168608 -12.068691 -30.493980 -9.867332 2.588103 -22.825570 -5.604199 253.086090 -2.697132 -22.612327 25.340623 -90.812188 1.967104 -31.047879 0.007606 -6.880120 11.415593 -49.957313 0.440180 0.561924 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566","title":"Merging Our Features Into One Features DataFrame"},{"location":"modelling/AFLmodellingPython/#wrapping-it-up","text":"We now have a fairly decent amount of features. Some other features which could be added include whether the game is in a major Capital city outisde of Mebourne (i.e. Sydney, Adelaide or Peth), how many 'Elite' players are playing (which could be judged by average SuperCoach scores over 110, for example), as well as your own metrics for attacking and defending. Note that all of our features have columns starting with 'f_' so in the section, we will grab this feature dataframe and use these features to sport predicting the matches.","title":"Wrapping it Up"},{"location":"modelling/AFLmodellingPython/#03-modelling","text":"These tutorials will walk you through how to construct your own basic AFL model, using publically available data. The output will be odds for each team to win, which will be shown on The Hub . In this notebook we will walk you through modelling our AFL data to create predictions. We will train a variety of quick and easy models to get a feel of what works and what doesn't. We will then tune our hyperparameters so that we are ready to make week by week predictions.","title":"03. Modelling"},{"location":"modelling/AFLmodellingPython/#grabbing-our-dataset_1","text":"First, we will import our required modules, as well as the prepare_afl_features function which we created in our afl_feature_creation script. This essentially creates some basic features for us so that we can get started on the modelling component. # Import libraries from afl_data_cleaning_v2 import * import datetime import pandas as pd import numpy as np from sklearn import svm , tree , linear_model , neighbors , naive_bayes , ensemble , discriminant_analysis , gaussian_process # from xgboost import XGBClassifier from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV , train_test_split from sklearn.linear_model import LogisticRegressionCV from sklearn.feature_selection import RFECV import seaborn as sns from sklearn.preprocessing import OneHotEncoder , LabelEncoder , StandardScaler from sklearn import feature_selection from sklearn import metrics from sklearn.linear_model import LogisticRegression , RidgeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB import warnings warnings . filterwarnings ( 'ignore' ) import afl_feature_creation_v2 import afl_data_cleaning_v2 # Grab our feature DataFrame which we created in the previous tutorial feature_df = afl_feature_creation_v2 . prepare_afl_features () afl_data = afl_data_cleaning_v2 . prepare_afl_data () feature_df . tail ( 3 ) game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_goals_diff f_behinds_diff f_points_diff f_margin_diff f_opponent_goals_diff f_opponent_behinds_diff f_opponent_points_diff f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_current_odds_prob f_current_odds_prob_away 1628 15396 Brisbane West Coast 2018-08-26 23 Gabba 2018 3.442757 -49.2 0.0 2.094236 1279.963814 1622.200265 0.683604 0.691730 0.696822 0.709605 -0.190413 1.182699 0.040221 -13.621456 1.772577 3.026217 13.661677 -22.709485 2.424261 -4.848054 1.800473 5.051157 6.440524 -5.549630 -17.041838 27.543023 33.983159 4.459181 -3.213885 -0.428455 1.514474 42.646138 -7.141638 1.457375 -17.472537 -15.103115 8.001966 -383.083539 6.458915 7.275716 0.942863 44.461590 4.640136 13.180967 -15.704694 2.366444 -5.985843 38.195255 0.433501 0.569866 1629 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.706488 -23.2 2.0 1.805565 1540.367850 1615.614668 0.667240 0.692632 0.684525 0.753783 2.056899 0.635785 12.977177 6.642811 1.443121 -2.324358 6.334366 147.281112 2.201404 -5.222254 3.250416 8.542475 -2.203571 3.559792 21.192530 33.737734 12.865653 -3.244066 -2.135243 4.100203 3.772200 48.425291 18.247107 13.349992 11.385136 -14.687556 5.052000 304.087088 11.062610 -6.686409 -16.414544 8.350924 -5.453961 12.407662 6.672628 -1.523915 13.075351 18.522113 0.661551 0.340379 1630 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.516150 -3.2 2.0 2.272313 1372.453734 1454.022032 0.730843 0.635819 0.697018 0.654991 -2.257517 1.223261 -12.321842 -19.923855 1.189755 0.463481 7.602012 27.891262 3.201137 4.754346 -1.881145 -3.924740 -0.528075 -8.045729 -20.584717 36.806235 39.615090 7.018240 -4.709732 -4.535660 -3.372912 23.194704 -18.042370 1.214353 -14.771187 13.611531 11.690647 -109.284521 -0.229945 12.384044 -4.625633 57.158576 1.353070 -1.533659 -6.646259 -3.489492 -15.416140 58.470456 0.284269 0.717566 # Get the result and merge to the feature_df match_results = ( pd . read_csv ( \"data/afl_match_results.csv\" ) . rename ( columns = { 'Game' : 'game' }) . assign ( result = lambda df : df . apply ( lambda row : 1 if row [ 'Home.Points' ] > row [ 'Away.Points' ] else 0 , axis = 1 ))) # Merge result column to feature_df feature_df = pd . merge ( feature_df , match_results [[ 'game' , 'result' ]], on = 'game' )","title":"Grabbing Our Dataset"},{"location":"modelling/AFLmodellingPython/#creating-a-training-and-testing-set","text":"So that we don't train our data on the data that we will later test our model on, we will create separate train and test sets. For this exercise we will use the 2018 season to test how our model performs, whilst the rest of the data can be used to train the model. # Create our test and train sets from our afl DataFrame; drop the columns which leak the result, duplicates, and the advanced # stats which don't have data until 2015 feature_columns = [ col for col in feature_df if col . startswith ( 'f_' )] # Create our test set test_x = feature_df . loc [ feature_df . season == 2018 , [ 'game' ] + feature_columns ] test_y = feature_df . loc [ feature_df . season == 2018 , 'result' ] # Create our train set X = feature_df . loc [ feature_df . season != 2018 , [ 'game' ] + feature_columns ] y = feature_df . loc [ feature_df . season != 2018 , 'result' ] # Scale features scaler = StandardScaler () X [ feature_columns ] = scaler . fit_transform ( X [ feature_columns ]) test_x [ feature_columns ] = scaler . transform ( test_x [ feature_columns ])","title":"Creating a Training and Testing Set"},{"location":"modelling/AFLmodellingPython/#using-cross-validation-to-find-the-best-algorithms","text":"Now that we have our training set, we can run through a list of popular classifiers to determine which classifier is best for modelling our data. To do this we will create a function which uses Kfold cross-validation to find the 'best' algorithms, based on how accurate the algorithms' predictions are. This function will take in a list of classifiers, which we will define below, as well as the training set and it's outcome, and output a DataFrame with the mean and std of the accuracy of each algorithm. Let's jump into it! # Create a list of standard classifiers classifiers = [ #Ensemble Methods ensemble . AdaBoostClassifier (), ensemble . BaggingClassifier (), ensemble . ExtraTreesClassifier (), ensemble . GradientBoostingClassifier (), ensemble . RandomForestClassifier (), #Gaussian Processes gaussian_process . GaussianProcessClassifier (), #GLM linear_model . LogisticRegressionCV (), #Navies Bayes naive_bayes . BernoulliNB (), naive_bayes . GaussianNB (), #SVM svm . SVC ( probability = True ), svm . NuSVC ( probability = True ), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis (), discriminant_analysis . QuadraticDiscriminantAnalysis (), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # XGBClassifier() ] # Define a functiom which finds the best algorithms for our modelling task def find_best_algorithms ( classifier_list , X , y ): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold ( n_splits = 5 ) # Grab the cross validation scores for each algorithm cv_results = [ cross_val_score ( classifier , X , y , scoring = \"neg_log_loss\" , cv = kfold ) for classifier in classifier_list ] cv_means = [ cv_result . mean () * - 1 for cv_result in cv_results ] cv_std = [ cv_result . std () for cv_result in cv_results ] algorithm_names = [ alg . __class__ . __name__ for alg in classifiers ] # Create a DataFrame of all the CV results cv_results = pd . DataFrame ({ \"Mean Log Loss\" : cv_means , \"Log Loss Std\" : cv_std , \"Algorithm\" : algorithm_names }) return cv_results . sort_values ( by = 'Mean Log Loss' ) . reset_index ( drop = True ) best_algos = find_best_algorithms ( classifiers , X , y ) best_algos Mean Log Loss Log Loss Std Algorithm 0 0.539131 3.640578e-02 LogisticRegressionCV 1 0.551241 5.775685e-02 LinearDiscriminantAnalysis 2 0.630994 8.257481e-02 GradientBoostingClassifier 3 0.670041 9.205780e-03 AdaBoostClassifier 4 0.693147 2.360121e-08 GaussianProcessClassifier 5 0.712537 2.770864e-02 SVC 6 0.712896 2.440755e-02 NuSVC 7 0.836191 2.094224e-01 ExtraTreesClassifier 8 0.874307 1.558144e-01 RandomForestClassifier 9 1.288174 3.953037e-01 BaggingClassifier 10 1.884019 4.769589e-01 QuadraticDiscriminantAnalysis 11 2.652161 6.886897e-01 BernoulliNB 12 3.299651 6.427551e-01 GaussianNB # Try a logistic regression model and see how it performs in terms of accuracy kfold = StratifiedKFold ( n_splits = 5 ) cv_scores = cross_val_score ( linear_model . LogisticRegressionCV (), X , y , scoring = 'accuracy' , cv = kfold ) cv_scores . mean () 0.7452268937025035","title":"Using Cross Validation to Find The Best Algorithms"},{"location":"modelling/AFLmodellingPython/#choosing-our-algorithms","text":"As we can see from above, there are some pretty poor algorithms for predicting the winner. On the other hand, whilst attaining an accuracy of 74.5% (at the time of writing) may seem like a decent result; we must first establish a baseline to judge our performance on. In this case, we will have two baselines; the proportion of games won by the home team and what the odds predict. If we can beat the odds we have created a very powerful model. Note that a baseline for the log loss can also be both the odds log loss and randomly guessing. Randomly guessing between two teams attains a log loss of log(2) = 0.69, so we have beaten this result. Once we establish our baseline, we will choose the top algorithms from above and tune their hyperparameters, as well as automatically selecting the best features to be used in our model.","title":"Choosing Our Algorithms"},{"location":"modelling/AFLmodellingPython/#defining-our-baseline","text":"As stated above, we must define our baseline so that we have a measure to beat. We will use the proportion of games won by the home team, as well as the proportion of favourites who won, based off the odds. To establish this baseline we will use our feature_df, as this has no dropped rows. # Find the percentage chance of winning at home in each season. afl_data = afl_data_cleaning_v2 . prepare_afl_data () afl_data [ 'home_win' ] = afl_data . apply ( lambda x : 1 if x [ 'f_margin' ] > 0 else 0 , axis = 1 ) home_games = afl_data [ afl_data [ 'home_game' ] == 1 ] home_games [[ \"home_win\" , 'season' ]] . groupby ([ 'season' ]) . mean () season home_win 2011 0.561856 2012 0.563725 2013 0.561576 2014 0.574257 2015 0.539604 2016 0.606742 2017 0.604061 2018 0.540404 # Find the proportion of favourites who have won # Define a function which finds if the odds correctly guessed the response def find_odds_prediction ( a_row ): if a_row [ 'f_odds' ] <= a_row [ 'f_odds_away' ] and a_row [ 'home_win' ] == 1 : return 1 elif a_row [ 'f_odds_away' ] < a_row [ 'f_odds' ] and a_row [ 'home_win' ] == 0 : return 1 else : return 0 # Define a function which splits our DataFrame so each game is on one row instead of two def get_df_on_one_line ( df ): cols_to_drop = [ 'date' , 'home_game' , 'opponent' , 'f_opponent_behinds' , 'f_opponent_goals' , 'f_opponent_points' , 'f_points' , 'round' , 'venue' , 'season' ] home_df = df [ df [ 'home_game' ] == 1 ] . rename ( columns = { 'team' : 'home_team' }) away_df = df [ df [ 'home_game' ] == 0 ] . rename ( columns = { 'team' : 'away_team' }) away_df = away_df . drop ( columns = cols_to_drop ) # Rename away_df columns away_df_renamed = away_df . rename ( columns = { col : col + '_away' for col in away_df . columns if col != 'game' }) merged_df = pd . merge ( home_df , away_df_renamed , on = 'game' ) merged_df [ 'home_win' ] = merged_df . f_margin . apply ( lambda x : 1 if x > 0 else 0 ) return merged_df afl_data_one_line = get_df_on_one_line ( afl_data ) afl_data_one_line [ 'odds_prediction' ] = afl_data_one_line . apply ( find_odds_prediction , axis = 1 ) print ( 'The overall mean accuracy of choosing the favourite based on the odds is {} %' . format ( round ( afl_data_one_line [ 'odds_prediction' ] . mean () * 100 , 2 ))) afl_data_one_line [[ \"odds_prediction\" , 'season' ]] . groupby ([ 'season' ]) . mean () The overall mean accuracy of choosing the favourite based on the odds is 73.15% season odds_prediction 2011 0.784615 2012 0.774510 2013 0.748768 2014 0.727723 2015 0.727723 2016 0.713483 2017 0.659898 2018 0.712121 ## Get a baseline log loss score from the odds afl_data_one_line [ 'odds_home_prob' ] = 1 / afl_data_one_line . f_odds afl_data_one_line [ 'odds_away_prob' ] = 1 / afl_data_one_line . f_odds_away metrics . log_loss ( afl_data_one_line . home_win , afl_data_one_line [[ 'odds_away_prob' , 'odds_home_prob' ]]) 0.5375306549682837 We can see that the odds are MUCH more accurate than just choosing the home team to win. We can also see that the mean accuracy of choosing the favourite is around 73%. That means that this is the score we need to beat. Similarly, the log loss of the odds is around 0.5385, whilst our model scores around 0.539 (at the time of writing), without hyperparamter optimisation. Let's choose only the algorithms with log losses below 0.67 chosen_algorithms = best_algos . loc [ best_algos [ 'Mean Log Loss' ] < 0.67 , 'Algorithm' ] . tolist () chosen_algorithms [ 'LogisticRegressionCV' , 'LinearDiscriminantAnalysis' , 'GradientBoostingClassifier' ]","title":"Defining Our Baseline"},{"location":"modelling/AFLmodellingPython/#using-grid-search-to-tune-hyperparameters","text":"Now that we have our best models, we can use Grid Search to optimise our hyperparameters. Grid search basically involves searching through a range of different algorithm hyperparameters, and choosing those which result in the best score from some metrics, which in our case is accuracy. Let's do this for the algorithms which have hyperparameters which can be tuned. Note that if you are running this on your own computer it may take up to 10 minutes. # Define a function which optimises the hyperparameters of our chosen algorithms def optimise_hyperparameters ( train_x , train_y , algorithms , parameters ): kfold = StratifiedKFold ( n_splits = 5 ) best_estimators = [] for alg , params in zip ( algorithms , parameters ): gs = GridSearchCV ( alg , param_grid = params , cv = kfold , scoring = 'neg_log_loss' , verbose = 1 ) gs . fit ( train_x , train_y ) best_estimators . append ( gs . best_estimator_ ) return best_estimators # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.001 , 0.01 , 0.05 , 0.2 , 0.5 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } # Add our algorithms and parameters to lists to be used in our function alg_list = [ LogisticRegression ()] param_list = [ lr_grid ] # Find the best estimators, then add our other estimators which don't need optimisation best_estimators = optimise_hyperparameters ( X , y , alg_list , param_list ) Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=1)]: Done 90 out of 90 | elapsed: 5.2s finished lr_best_params = best_estimators [ 0 ] . get_params () lr_best_params { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } kfold = StratifiedKFold ( n_splits = 10 ) cv_scores = cross_val_score ( linear_model . LogisticRegression ( ** lr_best_params ), X , y , scoring = 'neg_log_loss' , cv = kfold ) cv_scores . mean () - 0.528741673153639 In the next iteration of this tutorial we will also optimise an XGB model and hopefully outperform our logistic regression model.","title":"Using Grid Search To Tune Hyperparameters"},{"location":"modelling/AFLmodellingPython/#creating-predictions-for-the-2018-season","text":"Now that we have an optimised logistic regression model, let's see how it performs on predicting the 2018 season. lr = LogisticRegression ( ** lr_best_params ) lr . fit ( X , y ) final_predictions = lr . predict ( test_x ) accuracy = ( final_predictions == test_y ) . mean () * 100 print ( \"Our accuracy in predicting the 2018 season is: {:.2f} %\" . format ( accuracy )) Our accuracy in predicting the 2018 season is: 67.68% Now let's have a look at all the games which we incorrectly predicted. game_ids = test_x [( final_predictions != test_y )] . game afl_data_one_line . loc [ afl_data_one_line . game . isin ( game_ids ), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] date home_team opponent f_odds f_odds_away f_margin 1386 2018-03-24 Gold Coast North Melbourne 2.0161 1.9784 16 1388 2018-03-25 Melbourne Geelong 1.7737 2.2755 -3 1391 2018-03-30 North Melbourne St Kilda 3.5769 1.3867 52 1392 2018-03-31 Carlton Gold Coast 1.5992 2.6620 -34 1396 2018-04-01 Western Bulldogs West Coast 1.8044 2.2445 -51 1397 2018-04-01 Sydney Port Adelaide 1.4949 3.0060 -23 1398 2018-04-02 Geelong Hawthorn 1.7597 2.3024 -1 1406 2018-04-08 Western Bulldogs Essendon 3.8560 1.3538 21 1408 2018-04-13 Adelaide Collingwood 1.2048 5.9197 -48 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1415 2018-04-15 Hawthorn Melbourne 2.2855 1.7772 67 1417 2018-04-20 Sydney Adelaide 1.2640 4.6929 -10 1420 2018-04-21 Port Adelaide Geelong 1.5053 2.9515 -34 1422 2018-04-22 North Melbourne Hawthorn 2.6170 1.6132 28 1423 2018-04-22 Brisbane Gold Coast 1.7464 2.3277 -5 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1427 2018-04-28 Geelong Sydney 1.5019 2.9833 -17 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 1439 2018-05-05 Sydney North Melbourne 1.2777 4.5690 -2 1444 2018-05-11 Hawthorn Sydney 1.6283 2.5818 -8 1445 2018-05-12 GWS West Coast 1.5425 2.8292 -25 1446 2018-05-12 Carlton Essendon 3.1742 1.4570 13 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1456 2018-05-19 Essendon Geelong 5.6530 1.2104 34 1460 2018-05-20 Brisbane Hawthorn 3.2891 1.4318 56 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1466 2018-05-26 GWS Essendon 1.4364 3.2652 -35 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 ... ... ... ... ... ... ... 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 1485 2018-06-11 Melbourne Collingwood 1.6034 2.6450 -42 1492 2018-06-21 West Coast Essendon 1.3694 3.6843 -28 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1499 2018-06-29 Western Bulldogs Geelong 6.2067 1.1889 2 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1504 2018-07-01 Melbourne St Kilda 1.1405 7.7934 -2 1505 2018-07-01 Essendon North Melbourne 2.0993 1.9022 17 1506 2018-07-01 Fremantle Brisbane 1.2914 4.3743 -55 1507 2018-07-05 Sydney Geelong 1.7807 2.2675 -12 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1516 2018-07-12 Adelaide Geelong 2.0517 1.9444 15 1518 2018-07-14 Hawthorn Brisbane 1.2281 5.4105 -33 1521 2018-07-14 GWS Richmond 2.7257 1.5765 2 1522 2018-07-15 Collingwood West Coast 1.5600 2.7815 -35 1523 2018-07-15 North Melbourne Sydney 1.9263 2.0647 -6 1524 2018-07-15 Fremantle Port Adelaide 5.9110 1.2047 9 1527 2018-07-21 Sydney Gold Coast 1.0342 27.8520 -24 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 1533 2018-07-22 Port Adelaide GWS 1.6480 2.5452 -22 1538 2018-07-28 Gold Coast Carlton 1.3933 3.5296 -35 1546 2018-08-04 Adelaide Port Adelaide 2.0950 1.9135 3 1548 2018-08-04 St Kilda Western Bulldogs 1.6120 2.6368 -35 1555 2018-08-11 Port Adelaide West Coast 1.4187 3.3505 -4 1558 2018-08-12 North Melbourne Western Bulldogs 1.3175 4.1239 -7 1559 2018-08-12 Melbourne Sydney 1.3627 3.7445 -9 1564 2018-08-18 GWS Sydney 1.8478 2.1672 -20 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 Very interesting! Most of the games we got wrong were upsets. Let's have a look at the games we incorrectly predicted that weren't upsets. ( afl_data_one_line . loc [ afl_data_one_line . game . isin ( game_ids ), [ 'date' , 'home_team' , 'opponent' , 'f_odds' , 'f_odds_away' , 'f_margin' ]] . assign ( home_favourite = lambda df : df . apply ( lambda row : 1 if row . f_odds < row . f_odds_away else 0 , axis = 1 )) . assign ( upset = lambda df : df . apply ( lambda row : 1 if row . home_favourite == 1 and row . f_margin < 0 else ( 1 if row . home_favourite == 0 and row . f_margin > 0 else 0 ), axis = 1 )) . query ( 'upset == 0' )) date home_team opponent f_odds f_odds_away f_margin home_favourite upset 1412 2018-04-14 North Melbourne Carlton 1.5799 2.7228 86 1 0 1425 2018-04-25 Collingwood Essendon 1.8372 2.1754 49 1 0 1434 2018-04-29 Fremantle West Coast 2.4926 1.6531 -8 0 0 1437 2018-05-05 Essendon Hawthorn 2.8430 1.5393 -23 0 0 1452 2018-05-13 Collingwood Geelong 2.4127 1.7040 -21 0 0 1455 2018-05-19 North Melbourne GWS 1.5049 2.9752 43 1 0 1461 2018-05-20 West Coast Richmond 1.9755 2.0154 47 1 0 1467 2018-05-27 Hawthorn West Coast 2.2123 1.8133 -15 0 0 1479 2018-06-08 Port Adelaide Richmond 1.7422 2.3420 14 1 0 1483 2018-06-10 Brisbane Essendon 2.3018 1.7543 -22 0 0 1493 2018-06-22 Port Adelaide Melbourne 1.7391 2.3426 10 1 0 1501 2018-06-30 Adelaide West Coast 1.4989 2.9756 10 1 0 1514 2018-07-08 Essendon Collingwood 2.5442 1.6473 -16 0 0 1515 2018-07-08 West Coast GWS 1.6790 2.4754 11 1 0 1529 2018-07-21 Brisbane Adelaide 2.4614 1.6730 -5 0 0 1576 2018-08-26 Brisbane West Coast 2.3068 1.7548 -26 0 0 1578 2018-08-26 St Kilda North Melbourne 3.5178 1.3936 -23 0 0 Let's now look at our model's log loss for the 2018 season compared to the odds. predictions_probs = lr . predict_proba ( test_x ) metrics . log_loss ( test_y , predictions_probs ) 0.584824211055384 test_x_unscaled = feature_df . loc [ feature_df . season == 2018 , [ 'game' ] + feature_columns ] metrics . log_loss ( test_y , test_x_unscaled [[ 'f_current_odds_prob_away' , 'f_current_odds_prob' ]]) 0.5545776633924343 So whilst our model performs decently, it doesn't beat the odds in terms of log loss. That's okay, it's still a decent start. In future iterations we can implement other algorithms and create new features which may improve performance.","title":"Creating Predictions for the 2018 Season"},{"location":"modelling/AFLmodellingPython/#next-steps","text":"Now that we have a model up and running, the next steps are to implement the model on a week to week basis.","title":"Next Steps"},{"location":"modelling/AFLmodellingPython/#04-weekly-predictions","text":"Now that we have explored different algorithms for modelling, we can implement our chosen model and predict this week's AFL games! All you need to do is run the afl_modelling script each Thursday or Friday to predict the following week's games. # Import Modules from afl_feature_creation_v2 import prepare_afl_features import afl_data_cleaning_v2 import afl_feature_creation_v2 import afl_modelling_v2 import datetime import pandas as pd import numpy as np pd . set_option ( 'display.max_columns' , None ) from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler import warnings warnings . filterwarnings ( 'ignore' )","title":"04. Weekly Predictions"},{"location":"modelling/AFLmodellingPython/#creating-the-features-for-this-weekends-games","text":"To actually predict this weekend's games, we need to create the same features that we have created in the previous tutorials for the games that will be played this weekend. This includes all the rolling averages, efficiency features, elo features etc. So the majority of this tutorial will be using previously defined functions to create features for the following weekend's games.","title":"Creating The Features For This Weekend's Games"},{"location":"modelling/AFLmodellingPython/#create-next-weeks-dataframe","text":"Let's first get our cleaned afl_data dataset, as well as the odds for next weekend and the 2018 fixture. # Grab the cleaned AFL dataset and the column order afl_data = afl_data_cleaning_v2 . prepare_afl_data () ordered_cols = afl_data . columns # Define a function which grabs the odds for each game for the following weekend def get_next_week_odds ( path ): # Get next week's odds next_week_odds = pd . read_csv ( path ) next_week_odds = next_week_odds . rename ( columns = { \"team_1\" : \"home_team\" , \"team_2\" : \"away_team\" , \"team_1_odds\" : \"odds\" , \"team_2_odds\" : \"odds_away\" }) return next_week_odds # Import the fixture # Define a function which gets the fixture and cleans it up def get_fixture ( path ): # Get the afl fixture fixture = pd . read_csv ( path ) # Replace team names and reformat fixture = fixture . replace ({ 'Brisbane Lions' : 'Brisbane' , 'Footscray' : 'Western Bulldogs' }) fixture [ 'Date' ] = pd . to_datetime ( fixture [ 'Date' ]) . dt . date . astype ( str ) fixture = fixture . rename ( columns = { \"Home.Team\" : \"home_team\" , \"Away.Team\" : \"away_team\" }) return fixture next_week_odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) fixture = get_fixture ( \"data/afl_fixture_2018.csv\" ) fixture . tail () Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG next_week_odds home_team away_team odds odds_away 0 West Coast Collingwood 2.34 1.75 Now that we have these DataFrames, we will define a function which combines the fixture and next week's odds to create a single DataFrame for the games over the next 7 days. To use this function we will need Game IDs for next week. So we will create another function which creates Game IDs by using the Game ID from the last game played and adding 1 to it. # Define a function which creates game IDs for this week's footy games def create_next_weeks_game_ids ( afl_data ): odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) # Get last week's Game ID last_afl_data_game = afl_data [ 'game' ] . iloc [ - 1 ] # Create Game IDs for next week game_ids = [( i + 1 ) + last_afl_data_game for i in range ( odds . shape [ 0 ])] return game_ids # Define a function which creates this week's footy game DataFrame def get_next_week_df ( afl_data ): # Get the fixture and the odds for next week's footy games fixture = get_fixture ( \"data/afl_fixture_2018.csv\" ) next_week_odds = get_next_week_odds ( \"data/weekly_odds.csv\" ) next_week_odds [ 'game' ] = create_next_weeks_game_ids ( afl_data ) # Get today's date and next week's date and create a DataFrame for next week's games # todays_date = datetime.datetime.today().strftime('%Y-%m-%d') # date_in_7_days = (datetime.datetime.today() + datetime.timedelta(days=7)).strftime('%Y-%m-%d') todays_date = '2018-09-27' date_in_7_days = '2018-10-04' fixture = fixture [( fixture [ 'Date' ] >= todays_date ) & ( fixture [ 'Date' ] < date_in_7_days )] . drop ( columns = [ 'Season.Game' ]) next_week_df = pd . merge ( fixture , next_week_odds , on = [ 'home_team' , 'away_team' ]) # Split the DataFrame onto two rows for each game h_df = ( next_week_df [[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds' , 'Season' , 'Round' , 'Venue' ]] . rename ( columns = { 'home_team' : 'team' , 'away_team' : 'opponent' }) . assign ( home_game = 1 )) a_df = ( next_week_df [[ 'Date' , 'game' , 'home_team' , 'away_team' , 'odds_away' , 'Season' , 'Round' , 'Venue' ]] . rename ( columns = { 'odds_away' : 'odds' , 'home_team' : 'opponent' , 'away_team' : 'team' }) . assign ( home_game = 0 )) next_week = a_df . append ( h_df ) . sort_values ( by = 'game' ) . rename ( columns = { 'Date' : 'date' , 'Season' : 'season' , 'Round' : 'round' , 'Venue' : 'venue' }) next_week [ 'date' ] = pd . to_datetime ( next_week . date ) next_week [ 'round' ] = afl_data [ 'round' ] . iloc [ - 1 ] + 1 return next_week next_week_df = get_next_week_df ( afl_data ) game_ids_next_round = create_next_weeks_game_ids ( afl_data ) next_week_df date round season venue game home_game odds opponent team 0 2018-09-29 27 2018 MCG 15407 0 1.75 West Coast Collingwood 0 2018-09-29 27 2018 MCG 15407 1 2.34 Collingwood West Coast fixture . tail () Date Season Season.Game Round home_team away_team Venue 202 2018-09-14 2018 1 26 Hawthorn Melbourne MCG 203 2018-09-15 2018 1 26 Collingwood GWS MCG 204 2018-09-21 2018 1 27 Richmond Collingwood MCG 205 2018-09-22 2018 1 27 West Coast Melbourne Optus Stadium 206 2018-09-29 2018 1 28 West Coast Collingwood MCG","title":"Create Next Week's DataFrame"},{"location":"modelling/AFLmodellingPython/#create-each-feature","text":"Now let's append next week's DataFrame to our afl_data, match_results and odds DataFrames and then create all the features we used in the AFL Feature Creation Tutorial . We need to append the games and then feed them into our function so that we can create features for upcoming games. # Append next week's games to our afl_data DataFrame afl_data = afl_data . append ( next_week_df ) . reset_index ( drop = True ) # Append next week's games to match results (we need to do this for our feature creation to run) match_results = afl_data_cleaning_v2 . get_cleaned_match_results () . append ( next_week_df ) # Append next week's games to odds odds = ( afl_data_cleaning_v2 . get_cleaned_odds () . pipe ( lambda df : df . append ( next_week_df [ df . columns ])) . reset_index ( drop = True )) features_df = afl_feature_creation_v2 . prepare_afl_features ( afl_data = afl_data , match_results = match_results , odds = odds ) features_df . tail () game home_team away_team date round venue season f_odds f_form_margin_btwn_teams f_form_past_5_btwn_teams f_odds_away f_elo_home f_elo_away f_I50_efficiency_home f_R50_efficiency_home f_I50_efficiency_away f_R50_efficiency_away f_AF_diff f_B_diff f_BO_diff f_CCL_diff f_CG_diff f_CL_diff f_CM_diff f_CP_diff f_D_diff f_ED_diff f_FA_diff f_FF_diff f_G_diff f_GA_diff f_GA1_diff f_HB_diff f_HO_diff f_I50_diff f_ITC_diff f_K_diff f_M_diff f_MG_diff f_MI5_diff f_One.Percenters_diff f_R50_diff f_SC_diff f_SCL_diff f_SI_diff f_T_diff f_T5_diff f_TO_diff f_UP_diff f_Unnamed: 0_diff f_behinds_diff f_goals_diff f_margin_diff f_opponent_behinds_diff f_opponent_goals_diff f_opponent_points_diff f_points_diff f_current_odds_prob f_current_odds_prob_away 1065 15397 Melbourne GWS 2018-08-26 23 M.C.G. 2018 1.966936 -23.2 2.0 1.813998 1523.456734 1609.444874 0.653525 0.680168 0.704767 0.749812 140.535514 0.605144 -9.771981 5.892176 7.172376 6.614609 -1.365211 30.766262 21.998618 0.067228 -1.404730 -3.166732 6.933998 6.675576 0.000000 38.708158 24.587333 12.008987 10.482382 -16.709540 -15.415060 289.188486 6.350287 -2.263536 -20.966818 50.388632 0.723637 15.537783 22.912269 2.065039 10.215523 -6.689429 3259.163465 -0.136383 3.553795 16.563721 -2.353514 1.162696 4.622664 21.186385 0.661551 0.340379 1066 15398 St Kilda North Melbourne 2018-08-26 23 Docklands 2018 5.089084 -3.2 2.0 2.577161 1397.237139 1499.366007 0.725980 0.655749 0.723949 0.677174 51.799992 3.399035 6.067393 -2.189489 -10.475859 1.154766 -8.883840 -21.810962 33.058382 40.618410 2.286314 -0.345734 -3.778445 -2.182673 0.000000 19.816372 -21.562916 2.678384 -14.777698 13.242010 12.065594 -82.381996 -2.176564 2.335825 -4.952336 45.719406 3.344217 -2.095613 -3.929084 -3.182381 -12.832197 57.226776 -20221.371526 1.968709 -1.897958 -15.177001 1.067099 0.781811 5.757963 -9.419038 0.284269 0.717566 1067 15404 Collingwood GWS 2018-09-15 25 M.C.G. 2018 1.882301 12.6 3.0 2.018344 1546.000498 1590.806454 0.693185 0.706222 0.718446 0.727961 205.916671 -1.642954 -2.980828 -0.266023 8.547225 -3.751909 -0.664977 10.563513 48.175985 43.531908 -5.836979 5.388668 4.395675 2.555152 0.000000 51.588962 11.558254 4.276481 11.284445 -3.412977 -2.206815 -234.577304 2.637758 -10.537765 -11.127876 125.607377 -3.485896 3.532031 15.102292 -2.500685 8.187543 38.053445 12500.525732 -1.006173 2.520135 18.634835 -2.159882 -0.393386 -4.520198 14.114637 0.608495 0.393856 1068 15406 West Coast Melbourne 2018-09-22 26 Perth Stadium 2018 2.013572 21.2 3.0 1.884148 1577.888606 1542.095154 0.688877 0.708941 0.649180 0.698319 -118.135184 -3.005709 2.453190 -5.103869 -14.368949 -12.245458 2.771411 -45.364271 -60.210182 -24.049523 -2.791277 6.115918 -5.041030 -5.335746 0.000000 -78.816902 -18.784547 -13.957754 -5.527613 18.606721 25.366778 -910.988860 -5.515812 -9.483590 8.914093 -131.380758 -7.142529 -49.484957 -13.718798 -4.862994 -9.834616 -23.673638 -3178.282073 -1.785349 -2.569957 -20.008787 0.476202 0.387915 2.803694 -17.205093 0.543774 0.457875 1069 15407 West Coast Collingwood 2018-09-29 27 MCG 2018 1.981832 17.2 3.0 1.838864 1591.348723 1562.924273 0.679011 0.724125 0.711352 0.709346 159.522670 0.893421 -0.475725 3.391070 -5.088751 5.875388 5.352234 7.729063 -7.358202 -4.719968 6.113565 4.822252 2.871241 2.690270 3.636364 -64.238180 -0.631102 2.078832 6.005613 56.879978 34.373271 1016.491933 1.199751 2.454685 12.197047 219.666562 2.484363 0.379162 2.566991 0.639666 2.258377 -23.841529 -368920.360240 -0.646160 0.892051 3.040850 1.589568 0.012622 1.665299 4.706148 0.427350 0.571429","title":"Create Each Feature"},{"location":"modelling/AFLmodellingPython/#create-predictions-for-the-upcoming-round","text":"Now that we have our features, we can use our model that we created in part 3 to predict the next round. First we need to filter our features_df into a training df and a df with next round's features/matches. Then we can use the model created in the last tutorial to create predictions. For simplicity, I have hardcoded the parameters we used in the last tutorial. # Get the train df by only taking the games IDs which aren't in the next week df train_df = features_df [ ~ features_df . game . isin ( next_week_df . game )] # Get the result and merge to the feature_df match_results = ( pd . read_csv ( \"data/afl_match_results.csv\" ) . rename ( columns = { 'Game' : 'game' }) . assign ( result = lambda df : df . apply ( lambda row : 1 if row [ 'Home.Points' ] > row [ 'Away.Points' ] else 0 , axis = 1 ))) train_df = pd . merge ( train_df , match_results [[ 'game' , 'result' ]], on = 'game' ) train_x = train_df . drop ( columns = [ 'result' ]) train_y = train_df . result next_round_x = features_df [ features_df . game . isin ( next_week_df . game )] # Fit out logistic regression model - note that our predictions come out in the order of [away_team_prob, home_team_prob] lr_best_params = { 'C' : 0.01 , 'class_weight' : None , 'dual' : False , 'fit_intercept' : True , 'intercept_scaling' : 1 , 'max_iter' : 100 , 'multi_class' : 'ovr' , 'n_jobs' : 1 , 'penalty' : 'l2' , 'random_state' : None , 'solver' : 'newton-cg' , 'tol' : 0.0001 , 'verbose' : 0 , 'warm_start' : False } feature_cols = [ col for col in train_df if col . startswith ( 'f_' )] # Scale features scaler = StandardScaler () train_x [ feature_cols ] = scaler . fit_transform ( train_x [ feature_cols ]) next_round_x [ feature_cols ] = scaler . transform ( next_round_x [ feature_cols ]) lr = LogisticRegression ( ** lr_best_params ) lr . fit ( train_x [ feature_cols ], train_y ) prediction_probs = lr . predict_proba ( next_round_x [ feature_cols ]) modelled_home_odds = [ 1 / i [ 1 ] for i in prediction_probs ] modelled_away_odds = [ 1 / i [ 0 ] for i in prediction_probs ] # Create a predictions df preds_df = ( next_round_x [[ 'date' , 'home_team' , 'away_team' , 'venue' , 'game' ]] . copy () . assign ( modelled_home_odds = modelled_home_odds , modelled_away_odds = modelled_away_odds ) . pipe ( pd . merge , next_week_odds , on = [ 'home_team' , 'away_team' ]) . pipe ( pd . merge , features_df [[ 'game' , 'f_elo_home' , 'f_elo_away' ]], on = 'game' ) . drop ( columns = 'game' ) ) preds_df date home_team away_team venue modelled_home_odds modelled_away_odds odds odds_away f_elo_home f_elo_away 0 2018-09-29 West Coast Collingwood MCG 2.326826 1.753679 2.34 1.75 1591.348723 1562.924273 Alternatively, if you want to generate predictions using a script which uses all the above code, just run the following: print ( afl_modelling_v2 . create_predictions ()) date home_team away_team venue modelled_home_odds \\ 0 2018-09-29 West Coast Collingwood MCG 2.326826 modelled_away_odds odds odds_away f_elo_home f_elo_away 0 1.753679 2.34 1.75 1591.348723 1562.924273","title":"Create Predictions For the Upcoming Round"},{"location":"modelling/AFLmodellingPython/#conclusion","text":"Congratulations! You have created AFL predictions for this week. If you are beginner to this, don't be overwhelmed. The process gets easier each time you do it. And it is super rewarding. In future iterations we will update this tutorial to predict actual odds, and then integrate this with Betfair's API so that you can create an automated betting strategy using Machine Learning to create your predictions!","title":"Conclusion"},{"location":"modelling/AFLmodellingPython/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/AusOpenPythonTutorial/","text":"Australian Open Datathon Python Tutorial Overview The Task This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodology and thought process, read this article. Intention This notebook will demonstrate how to: Process the raw data sets Produce simple features Run a predictive model on H2O Outputs the final predictions for the submissions Load the data and required packages import numpy as np import pandas as pd import os import gc import sys import warnings warnings.filterwarnings ( 'ignore' ) import h2o from h2o.automl import H2OAutoML pd.options.display.max_columns = 999 # We are loading both the mens and womens match csvs df_atp = pd.read_csv ( \"data/ATP_matches.csv\" ) df_wta = pd.read_csv ( \"data/WTA_matches.csv\" ) Data pre-processing Filter the matches to hard and indoor hard only due to the fact that Australian Open is on hard surface and we want the models to train specifically for hard surfaces matches Convert the columns in both datasets to the correct types. For example, we want to make sure the date columns are in the datetime format and numerical columns are either integer or floats. This will help reduce the memory in use and make the feature engineering process easier ### Include hard and indoor hard only df_atp = df_atp.loc [ df_atp.Court_Surface.isin ([ 'Hard' , 'Indoor Hard' ])] df_wta = df_wta.loc [ df_wta.Court_Surface.isin ([ 'Hard' , 'Indoor Hard' ])] ### Exclude qualifying rounds df_atp = df_atp.loc [ df_atp.Round_Description != 'Qualifying' ] df_wta = df_wta.loc [ df_wta.Round_Description != 'Qualifying' ] # Store the shape of the data for reference check later atp_shape = df_atp.shape wta_shape = df_wta.shape numeric_columns = [ 'Winner_Rank' , 'Loser_Rank' , 'Retirement_Ind' , 'Winner_Sets_Won' , 'Winner_Games_Won' , 'Winner_Aces' , 'Winner_DoubleFaults' , 'Winner_FirstServes_Won' , 'Winner_FirstServes_In' , 'Winner_SecondServes_Won' , 'Winner_SecondServes_In' , 'Winner_BreakPoints_Won' , 'Winner_BreakPoints' , 'Winner_ReturnPoints_Won' , 'Winner_ReturnPoints_Faced' , 'Winner_TotalPoints_Won' , 'Loser_Sets_Won' , 'Loser_Games_Won' , 'Loser_Aces' , 'Loser_DoubleFaults' , 'Loser_FirstServes_Won' , 'Loser_FirstServes_In' , 'Loser_SecondServes_Won' , 'Loser_SecondServes_In' , 'Loser_BreakPoints_Won' , 'Loser_BreakPoints' , 'Loser_ReturnPoints_Won' , 'Loser_ReturnPoints_Faced' , 'Loser_TotalPoints_Won' ] text_columns = [ 'Winner' , 'Loser' , 'Tournament' , 'Court_Surface' , 'Round_Description' ] date_columns = [ 'Tournament_Date' ] # we set the **erros** to coerce so any non-numerical values (text,special characters) will return an NA df_atp [ numeric_columns ] = df_atp [ numeric_columns ] .apply ( pd.to_numeric , errors = 'coerce' ) df_wta [ numeric_columns ] = df_wta [ numeric_columns ] .apply ( pd.to_numeric , errors = 'coerce' ) df_atp [ date_columns ] = df_atp [ date_columns ] .apply ( pd.to_datetime ) df_wta [ date_columns ] = df_wta [ date_columns ] .apply ( pd.to_datetime ) Feature Engineering The raw datasets are constructed in a way that each row will have the seperate stats for both the winner and loser of that match. However, we want to reshape the data so that each row we will only have one player randomly selected from the winner/loser columns and the features are the difference between opponents statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. In addition, for the features, we will take the rolling average of the player's most recent 15 matches before the particular tournament starts. For example, if the match is the second round of the Australian Open 2018, the features will be the last 15 matches before the first round of Australian Open 2018. The reason of not including the stats in the first round is that we would not have known the player's stats in the first round for the final submissions A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, difference in ELO rating etc. The target variable will be whether or not Player A wins (1=Player A wins and 0=lose). The steps we take are: Convert the raw data frames into long format: Create some new features Take the rolling average for each player and each match Since we will be only training our models on US Open and Australian Open, we will only be creating features for those matches. However, the rolling average will take into account any hard surface matches before those tournaments Calculate the difference of averages for each match in the data frames Convert the raw data frames into long format: # Before we split the data frame into winner and loser, we want to create a feature that captures the total number of games the match takes. # We have to do it before the split or we will lose this information df_atp [ 'Total_Games' ] = df_atp.Winner_Games_Won + df_atp.Loser_Games_Won df_wta [ 'Total_Games' ] = df_wta.Winner_Games_Won + df_wta.Loser_Games_Won # Get the column names for the winner and loser stats winner_cols = [ col for col in df_atp.columns if col.startswith ( 'Winner' )] loser_cols = [ col for col in df_atp.columns if col.startswith ( 'Loser' )] # create a winner data frame to store the winner stats and a loser data frame for the losers # In addition to the winner and loser columns, we are adding common columns as well (e.g. tournamnt dates) common_cols = [ 'Total_Games' , 'Tournament' , 'Tournament_Date' , 'Court_Surface' , 'Round_Description' ] df_winner_atp = df_atp [ winner_cols + common_cols ] df_loser_atp = df_atp [ loser_cols + common_cols ] df_winner_wta = df_wta [ winner_cols + common_cols ] df_loser_wta = df_wta [ loser_cols + common_cols ] # Create a new column to show whether the player has won or not. df_winner_atp [ \"won\" ] = 1 df_loser_atp [ \"won\" ] = 0 df_winner_wta [ \"won\" ] = 1 df_loser_wta [ \"won\" ] = 0 # Rename the columns for the winner and loser data frames so we can append them later on. # We will rename the Winner_ / Loser_ columns to Player_ new_column_names = [ col.replace ( 'Winner' , 'Player' ) for col in winner_cols ] df_winner_atp.columns = new_column_names + common_cols + [ 'won' ] # They all should be the same df_loser_atp.columns = df_winner_atp.columns df_winner_wta.columns = df_winner_atp.columns df_loser_wta.columns = df_winner_atp.columns # append the winner and loser data frames df_long_atp = df_winner_atp.append ( df_loser_atp ) df_long_wta = df_winner_wta.append ( df_loser_wta ) So now our data frames are in long format and should looks like this df_long_atp.head () Player Player_Rank Player_Sets_Won Player_Games_Won Player_Aces Player_DoubleFaults Player_FirstServes_Won Player_FirstServes_In Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Total_Games Tournament Tournament_Date Court_Surface Round_Description won Edouard Roger-Vasselin 106.0 2.0 12 5.0 2.0 22 30 12 19 4.0 7.0 25.0 59.0 59 19 Chennai 2012-01-02 Hard First Round 1 Dudi Sela 83.0 2.0 12 2.0 0.0 14 17 11 16 6.0 14.0 36.0 58.0 61 13 Chennai 2012-01-02 Hard First Round 1 Go Soeda 120.0 2.0 19 6.0 1.0 48 64 19 39 5.0 11.0 42.0 105.0 109 33 Chennai 2012-01-02 Hard First Round 1 Yuki Bhambri 345.0 2.0 12 1.0 2.0 22 29 9 17 5.0 13.0 34.0 62.0 65 17 Chennai 2012-01-02 Hard First Round 1 Yuichi Sugita 235.0 2.0 12 3.0 1.0 37 51 11 27 3.0 7.0 22.0 54.0 70 19 Chennai 2012-01-02 Hard First Round 1 Create some new features Thinking about the dynamics of tennis, we know that players often will matches by \u201cbreaking\u201d the opponent\u2019s serve (i.e. winning a game when the opponent is serving). This is especially important in tennis. Let\u2019s create a feature called Player_BreakPoints_Per_Game, which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let\u2019s also create a feature called Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \u201cholding\u201d serve is important (i.e. winning a game when you are serving). Let\u2019s create a feature called Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let\u2019s create a feature called Player_Game_Win_Percentage which is the propotion of games that a player wins. So the four new features we will create are: Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage # Here, we will define a function so we can apply it to both atp and wta data frames def get_new_features ( df ) : # Input: # df: data frame to get the data from # Return: the df with the new features # Point Win ratio when serving df [ 'Player_Serve_Win_Ratio' ] = ( df.Player_FirstServes_Won + df.Player_SecondServes_Won - df.Player_DoubleFaults ) \\ / ( df.Player_FirstServes_In + df.Player_SecondServes_In + df.Player_DoubleFaults ) # Point win ratio when returning df [ 'Player_Return_Win_Ratio' ] = df.Player_ReturnPoints_Won / df.Player_ReturnPoints_Faced # Breakpoints per receiving game df [ 'Player_BreakPoints_Per_Return_Game' ] = df.Player_BreakPoints / df.Total_Games df [ 'Player_Game_Win_Percentage' ] = df.Player_Games_Won / df.Total_Games return df # Apply the function we just created to the long data frames df_long_atp = get_new_features ( df_long_atp ) df_long_wta = get_new_features ( df_long_wta ) # The long table should have exactly twice of the rows of the original data assert df_long_atp.shape [ 0 ] == atp_shape [ 0 ] * 2 assert df_long_wta.shape [ 0 ] == wta_shape [ 0 ] * 2 Take the rolling average for each player and each match To train our models, we cannot simply use the player stats for that current match. In fact, we wont be able to use any stats from the same tournament. The logic behind this is that when we try to predict the results in 2019, we would not know the stats of any of the matches in the Australian Open 2019 tournament. As a result, we will use the players' past performance. Here, we will do a rolling average of the most recent 15 matches before the tournament. To do the above, we will follow the steps below: List all the tournament dates for US and Australian Opens Loop through the dates from point 1, for each date, we filter the data to only include matches before that date and take the most recent 15 games Take the average of those 15 games # the two tournaments we will be using for training and thus the feature generation tournaments = [ 'U.S. Open, New York' , 'Australian Open, Melbourne' ] # Store the dates for the loops tournament_dates_atp = df_atp.loc [ df_atp.Tournament.isin ( tournaments )] .groupby ([ 'Tournament' , 'Tournament_Date' ]) \\ .size () .reset_index ()[[ 'Tournament' , 'Tournament_Date' ]] tournament_dates_wta = df_wta.loc [ df_wta.Tournament.isin ( tournaments )] .groupby ([ 'Tournament' , 'Tournament_Date' ]) \\ .size () .reset_index ()[[ 'Tournament' , 'Tournament_Date' ]] # We are adding one more date for the final prediction tournament_dates_atp.loc [ -1 ] = [ 'Australian Open, Melbourne' , pd.to_datetime ( '2019-01-15' )] tournament_dates_wta.loc [ -1 ] = [ 'Australian Open, Melbourne' , pd.to_datetime ( '2019-01-15' )] Following are the dates for each tournament tournament_dates_atp Tournament Tournament_Date Australian Open, Melbourne 2012-01-16 Australian Open, Melbourne 2013-01-1 Australian Open, Melbourne 2014-01-1 Australian Open, Melbourne 2015-01-1 Australian Open, Melbourne 2016-01-1 Australian Open, Melbourne 2017-01-1 Australian Open, Melbourne 2018-01-1 U.S. Open, New York 2012-08-2 U.S. Open, New York 2013-08-2 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 tournament_dates_wta Tournament Tournament_Date Australian Open, Melbourne 2014-01-13 Australian Open, Melbourne 2015-01-19 Australian Open, Melbourne 2016-01-18 Australian Open, Melbourne 2017-01-16 Australian Open, Melbourne 2018-01-15 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 They look fine but it is interesting that for men's, we have two more years of data from 2012 to 2013 # Let's define a function to calculate the rolling averages def get_rolling_features ( df , date_df = None , rolling_cols = None , last_cols = None ) : # Input: # df: data frame to get the data from # date_df: data frame that has the start dates for each tournament # rolling_cols: columns to get the rolling averages # last_cols: columns to get the last value (most recent) # Return: the df with the new features # Sort the data by player and dates so the most recent matches are at the bottom df = df.sort_values ([ 'Player' , 'Tournament_Date' , 'Tournament' ], ascending = True ) # For each tournament, get the rolling averages of that player's past matches before the tournament start date for index , tournament_date in enumerate ( date_df.Tournament_Date ) : # create a temp df to store the interim results df_temp = df.loc [ df.Tournament_Date < tournament_date ] # for ranks, we only take the last one. (comment this out if want to take avg of rank) df_temp_last = df_temp.groupby ( 'Player' )[ last_cols ] .last () .reset_index () # take the most recent 15 matches for the rolling average df_temp = df_temp.groupby ( 'Player' )[ rolling_cols ] .rolling ( 15 , min_periods = 1 ) .mean () .reset_index () df_temp = df_temp.groupby ( 'Player' ) .tail ( 1 ) # take the last row of the above df_temp = df_temp.merge ( df_temp_last , on = 'Player' , how = 'left' ) if index == 0 : df_result = df_temp df_result [ 'tournament_date_index' ] = tournament_date # so we know which tournament this feature is for else : df_temp [ 'tournament_date_index' ] = tournament_date df_result = df_result.append ( df_temp ) df_result.drop ( 'level_1' , axis = 1 , inplace = True ) return df_result # columns we are applying the rolling averages on rolling_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' ] # columns we are taking the most recent values on # For the player rank, we think we can just use the latest rank (before the tournament starts) # as it should refect the most recent performance of the player last_cols = [ 'Player_Rank' ] # Apply the rolling average function to the long data frames (it will take a few mins to run) df_rolling_atp = get_rolling_features ( df_long_atp , tournament_dates_atp , rolling_cols , last_cols = last_cols ) df_rolling_wta = get_rolling_features ( df_long_wta , tournament_dates_wta , rolling_cols , last_cols = last_cols ) df_rolling_atp.head ( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Adrian Mannarino 0.623408 0.353397 0.257859 0.447246 87.0 2012-01-16 Albert Montanes 0.507246 0.195652 0.000000 0.294118 50.0 2012-01-16 df_rolling_wta.head ( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Agnieszka Radwanska 0.413333 0.475410 0.350000 0.350000 5.0 2014-01-13 Ajla Tomljanovic 0.468457 0.407319 0.242253 0.462634 75.0 2014-01-13 Calculate the difference of averages for each match in the data frames In the original data frames, the first column is always the winner and followed by the loser. Same for the player stats. Thus, we cannot simply calculate the difference between winner and loser and create a target variable indicating player 1 will win or not because it will always be the winner in this case (target always = 1). As a result, we need to pick a player randomly so the player might or might not be the winner In addition, instead of using both the features for player 1 and 2, we will take the difference of averages between the randomised player 1 and 2. The main benefit is that it will reduce the number of features to half Steps: We will create a random number for each player which only return 0 or 1 If it is zero, we will assign the winner to player 1 and loser to player 2 We will join the features to the player 1 and 2. The join will be on the player names and the tournament date (tournament_index in the feature data frames) For players who do not have any history, we will fill the stats by zeros and rank by 999 # Randomise the match_wide dataset so the first player is not always the winner # set a seed so the random number is reproducable np.random.seed ( 2 ) # randomise a number 0/1 with 50% chance each # if 0 then take the winner, 1 then take loser df_atp [ 'random_number' ] = np.random.randint ( 2 , size = len ( df_atp )) df_atp [ 'randomised_player_1' ] = np.where ( df_atp [ 'random_number' ] == 0 , df_atp [ 'Winner' ], df_atp [ 'Loser' ]) df_atp [ 'randomised_player_2' ] = np.where ( df_atp [ 'random_number' ] == 0 , df_atp [ 'Loser' ], df_atp [ 'Winner' ]) df_wta [ 'random_number' ] = np.random.randint ( 2 , size = len ( df_wta )) df_wta [ 'randomised_player_1' ] = np.where ( df_wta [ 'random_number' ] == 0 , df_wta [ 'Winner' ], df_wta [ 'Loser' ]) df_wta [ 'randomised_player_2' ] = np.where ( df_wta [ 'random_number' ] == 0 , df_wta [ 'Loser' ], df_wta [ 'Winner' ]) # set the target (win/loss) based on the new randomise number df_atp [ 'player_1_win' ] = np.where ( df_atp [ 'random_number' ] == 0 , 1 , 0 ) df_wta [ 'player_1_win' ] = np.where ( df_wta [ 'random_number' ] == 0 , 1 , 0 ) print ( 'After shuffling, the win rate for player 1 for the mens is {}%' .format ( df_atp [ 'player_1_win' ] .mean () * 100 )) print ( 'After shuffling, the win rate for player 1 for the womens is {}%' .format ( df_wta [ 'player_1_win' ] .mean () * 100 )) After shuffling, the win rate for player 1 for the mens is 49.64798919857267% After shuffling, the win rate for player 1 for the womens is 49.697671426733564% The win rates are close enough to 50%. So we are good to go # To get our data frames ready for model training, we will exclude other tournaments from the data now because we have gotten the rolling averages from them and # for training, we only need US and Australian Open matches df_atp = df_atp.loc [ df_atp.Tournament.isin ( tournaments )] df_wta = df_wta.loc [ df_wta.Tournament.isin ( tournaments )] # now we can remove other stats columns because we will be using the differences cols_to_keep = [ 'Winner' , 'Loser' , 'Tournament' , 'Tournament_Date' , 'player_1_win' , 'randomised_player_1' , 'randomised_player_2' ] df_atp = df_atp [ cols_to_keep ] df_wta = df_wta [ cols_to_keep ] # Here, we are joining the rolling average data frames to the individual matches. # We need to do it twice. One for player 1 and one for player 2 # Get the rolling features for player 1 df_atp = df_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_wta = df_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # we will use '_p1' to denote player 1 and '_p2' for player 2 df_atp = df_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_wta = df_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' .format ( df_atp.loc [ df_atp.Player_p1.isna (), 'randomised_player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament' .format ( df_atp.loc [ df_atp.Player_p2.isna (), 'randomised_player_2' ] .nunique ())) 59 player_1s do Not have previous match history before the tournament 56 player_2s do Not have previous match history before the tournament # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' .format ( df_wta.loc [ df_wta.Player_p1.isna (), 'randomised_player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament' .format ( df_wta.loc [ df_wta.Player_p2.isna (), 'randomised_player_2' ] .nunique ())) 41 player_1s do Not have previous match history before the tournament 37 player_2s do Not have previous match history before the tournament # Most of the missing are for the early years which makes sense as we dont have enough history for them df_wta.loc [ df_wta.Player_p1.isna (), 'Tournament_Date' ] .value_counts () 2014-01-13 29 2014-08-25 7 2015-08-31 5 2015-01-19 3 2017-08-28 3 2018-01-15 3 2018-08-27 3 Name: Tournament_Date, dtype: int64 df_atp.loc [ df_atp.Player_p1.isna (), 'Tournament_Date' ] .value_counts () 2012-01-16 29 2012-08-27 9 2014-01-13 5 2013-08-26 5 2016-01-18 5 2013-01-14 4 2014-08-25 3 2018-01-15 3 2017-08-28 3 2018-08-27 2 2016-08-29 2 2015-01-19 1 Name: Tournament_Date, dtype: int64 Now we have gotten the rolling averages for both player 1 and 2. What we need to do next is to simply calculate their difference. To calculate the difference, we need to: Split the data frames into two new data frames: Player 1 and Player 2 Take the difference between the two data frames def get_player_difference ( df , diff_cols = None ) : # Input: # df: data frame to get the data from # diff_cols: columns we take the difference on. For example is diff_cols = win rate. This function will calculate the # difference of the win rates between player 1 and player 2 # Return: the df with the new features p1_cols = [ i + '_p1' for i in diff_cols ] # column names for player 1 stats p2_cols = [ i + '_p2' for i in diff_cols ] # column names for player 2 stats # For any missing values, we will fill them by zeros except the ranking where we will use 999 df [ 'Player_Rank_p1' ] = df [ 'Player_Rank_p1' ] .fillna ( 999 ) df [ p1_cols ] = df [ p1_cols ] .fillna ( 0 ) df [ 'Player_Rank_p2' ] = df [ 'Player_Rank_p2' ] .fillna ( 999 ) df [ p2_cols ] = df [ p2_cols ] .fillna ( 0 ) new_column_name = [ i + '_diff' for i in diff_cols ] # Take the difference df_p1 = df [ p1_cols ] df_p2 = df [ p2_cols ] df_p1.columns = new_column_name df_p2.columns = new_column_name df_diff = df_p1 - df_p2 df_diff.columns = new_column_name # drop the p1 and p2 columns because We have the differences now df.drop ( p1_cols + p2_cols , axis = 1 , inplace = True ) # Concat the df_diff and raw_df df = pd.concat ([ df , df_diff ], axis = 1 ) return df , new_column_name diff_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' , 'Player_Rank' ] # Apply the function and get the difference between player 1 and 2 df_atp , _ = get_player_difference ( df_atp , diff_cols = diff_cols ) df_wta , _ = get_player_difference ( df_wta , diff_cols = diff_cols ) # Make a copy of the data frames in case we need to come back to check the values df_atp_final = df_atp.copy () df_wta_final = df_wta.copy () df_atp_final.head () Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Juan Martin del Potro Adrian Mannarino Australian Open, Melbourne 2012-01-16 1 Juan Martin del Potro Adrian Mannarino Juan Martin del Potro 2012-01-16 Adrian Mannarino 2012-01-16 0.035030 -0.021271 -0.025975 0.103479 -76.0 Pere Riba Albert Montanes Australian Open, Melbourne 2012-01-16 1 Pere Riba Albert Montanes Pere Riba 2012-01-16 Albert Montanes 2012-01-16 -0.156369 0.008893 0.066667 -0.094118 39.0 Tomas Berdych Albert Ramos-Vinolas Australian Open, Melbourne 2012-01-16 0 Albert Ramos-Vinolas Tomas Berdych Albert Ramos-Vinolas 2012-01-16 NaN NaT 0.498027 0.380092 0.414815 0.394444 -934.0 Rafael Nadal Alex Kuznetsov Australian Open, Melbourne 2012-01-16 0 Alex Kuznetsov Rafael Nadal NaN NaT Rafael Nadal 2012-01-16 -0.670139 -0.423057 -0.445623 -0.574767 997.0 Roger Federer Alexander Kudryavtsev Australian Open, Melbourne 2012-01-16 0 Alexander Kudryavtsev Roger Federer NaN NaT Roger Federer 2012-01-16 -0.721415 -0.449516 -0.360255 -0.668090 996.0 Modelling We will trian two models here, one for mens and one for womens. For training, we will use all available data from the second year (too many missing values in the first year) up until 2017. For validation, we will test the model on the 2018 Australian Open data This setup allows us to 'mimic' the final prediction (using historical matches to predict 2019 results) df_train_atp = df_atp_final.loc [( df_atp_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & ( df_atp_final.Tournament_Date > '2012-01-16' )] # excluding first year df_valid_atp = df_atp_final.loc [ df_atp_final.Tournament_Date == '2018-01-15' ] # Australian Open 2018 only df_train_wta = df_wta_final.loc [( df_wta_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & ( df_wta_final.Tournament_Date > '2014-01-13' )] # excluding first year df_train_atp.head () Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Daniel Brands Adrian Ungur U.S. Open, New York 2012-08-27 0 Adrian Ungur Daniel Brands NaN NaT Daniel Brands 2012-08-27 -0.535211 -0.300000 -0.043478 -0.434783 870.0 Richard Gasquet Albert Montanes U.S. Open, New York 2012-08-27 1 Richard Gasquet Albert Montanes Richard Gasquet 2012-08-27 Albert Montanes 2012-08-27 0.080003 0.077451 0.180847 0.131108 -37.0 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 1 Martin Klizan Alejandro Falla Martin Klizan 2012-08-27 Alejandro Falla 2012-08-27 0.077117 -0.044716 -0.087362 0.068180 -2.0 Andy Murray Alex Bogomolov Jr. U.S. Open, New York 2012-08-27 1 Andy Murray Alex Bogomolov Jr. Andy Murray 2012-08-27 Alex Bogomolov Jr. 2012-08-27 0.039641 0.031701 0.094722 0.059010 -69.0 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 1 Tommy Robredo Andreas Seppi Tommy Robredo 2012-08-27 Andreas Seppi 2012-08-27 -0.026814 0.006442 -0.009930 -0.067780 151.0 # target variable target = 'player_1_win' # features being fed into the models feats = [ 'Player_Serve_Win_Ratio_diff' , 'Player_Return_Win_Ratio_diff' , 'Player_BreakPoints_Per_Return_Game_diff' , 'Player_Game_Win_Percentage_diff' , 'Player_Rank_diff' ] print ( feats ) H2O model for ATP h2o.init () # Convert to an h2o frame df_train_atp_h2o = h2o.H2OFrame ( df_train_atp ) df_valid_atp_h2o = h2o.H2OFrame ( df_valid_atp ) # For binary classification, response should be a factor df_train_atp_h2o [ target ] = df_train_atp_h2o [ target ] .asfactor () df_valid_atp_h2o [ target ] = df_valid_atp_h2o [ target ] .asfactor () # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_atp = h2o.automl.H2OAutoML ( max_runtime_secs = 300 , max_models = 100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True , seed = 183 ) aml_atp.train ( x = feats , y = target , training_frame = df_train_atp_h2o , validation_frame = df_valid_atp_h2o ) # View the AutoML Leaderboard lb = aml_atp.leaderboard lb.head () model_id auc logloss mean_per_class_error rmse mse GBM_5_AutoML_20181221_094949 0.790281 0.554852 0.281363 0.431379 0.186088 GBM_grid_1_AutoML_20181221_094949_model_15 0.789329 0.556804 0.29856 0.431931 0.186564 GBM_grid_1_AutoML_20181221_094949_model_7 0.788013 0.557808 0.295899 0.432968 0.187461 StackedEnsemble_BestOfFamily_AutoML_20181221_094949 0.788131 0.558028 0.285321 0.432849 0.187358 GBM_grid_1_AutoML_20181221_094949_model_20 0.785633 0.561094 0.283932 0.43479 0.189043 StackedEnsemble_AllModels_AutoML_20181221_094949 0.784411 0.561587 0.293244 0.434667 0.188935 GBM_grid_1_AutoML_20181221_094949_model_25 0.785311 0.561783 0.291912 0.434888 0.189127 GBM_grid_1_AutoML_20181221_094949_model_17 0.774832 0.570883 0.295836 0.439375 0.193051 DeepLearning_1_AutoML_20181221_094949 0.779388 0.572823 0.311737 0.438479 0.192264 GBM_grid_1_AutoML_20181221_094949_model_14 0.7718 0.578867 0.285835 0.441373 0.19481 H2O model for WTA # Convert to an h2o frame df_train_wta_h2o = h2o.H2OFrame ( df_train_wta ) df_valid_wta_h2o = h2o.H2OFrame ( df_valid_wta ) # For binary classification, response should be a factor df_train_wta_h2o [ target ] = df_train_wta_h2o [ target ] .asfactor () df_valid_wta_h2o [ target ] = df_valid_wta_h2o [ target ] .asfactor () # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_wta = h2o.automl.H2OAutoML ( max_runtime_secs = 300 , max_models = 100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True , seed = 183 ) aml_wta.train ( x = feats , y = target , training_frame = df_train_wta_h2o , validation_frame = df_valid_wta_h2o ) # View the AutoML Leaderboard lb = aml_wta.leaderboard lb.head () model_id auc logloss mean_per_class_error rmse mse StackedEnsemble_AllModels_AutoML_20181221_095400 0.726046 0.60827 0.321222 0.457117 0.208956 StackedEnsemble_BestOfFamily_AutoML_20181221_095400 0.724911 0.609329 0.337847 0.457659 0.209452 DeepLearning_grid_1_AutoML_20181221_095400_model_3 0.729152 0.612669 0.315971 0.45641 0.20831 GBM_grid_1_AutoML_20181221_095400_model_7 0.721204 0.615763 0.336848 0.460885 0.212415 GBM_5_AutoML_20181221_095400 0.719252 0.616535 0.319179 0.461055 0.212572 GBM_grid_1_AutoML_20181221_095400_model_15 0.715921 0.619263 0.318673 0.462215 0.213643 GLM_grid_1_AutoML_20181221_095400_model_1 0.726048 0.622989 0.366124 0.463099 0.214461 GBM_grid_1_AutoML_20181221_095400_model_17 0.709261 0.624902 0.34876 0.465628 0.216809 GBM_grid_1_AutoML_20181221_095400_model_18 0.70946 0.625704 0.393556 0.466147 0.217293 DeepLearning_grid_1_AutoML_20181221_095400_model_2 0.713419 0.628008 0.311334 0.463638 0.21496 Use the models to predict and make submissions Now let's use the models we just created to make the submissions df_predict_atp = pd.read_csv ( \"data/men_dummy_submission_file.csv\" ) df_predict_wta = pd.read_csv ( \"data/women_dummy_submission_file.csv\" , encoding = 'latin1' ) # for womens, there are some names need a different encoding df_predict_wta.head ( 2 ) player_1 player_2 player_1_win_probability Simona Halep Angelique Kerber 0.5 Simona Halep Caroline Wozniacki 0.5 Get the features for the predict df We need to join the features to the 2019 players # Before we join the features by the names and the dates, we need to convert any non-english characters to english first translationTable = str.maketrans ( \"\u00e9\u00e0\u00e8\u00f9\u00e2\u00ea\u00ee\u00f4\u00fb\u00e7\u00f1\u00e1\" , \"eaeuaeioucna\" ) df_predict_atp [ 'player_1' ] = df_predict_atp.player_1.apply ( lambda x : x.translate ( translationTable )) df_predict_atp [ 'player_2' ] = df_predict_atp.player_2.apply ( lambda x : x.translate ( translationTable )) df_predict_wta [ 'player_1' ] = df_predict_wta.player_1.apply ( lambda x : x.translate ( translationTable )) df_predict_wta [ 'player_2' ] = df_predict_wta.player_2.apply ( lambda x : x.translate ( translationTable )) # Also we need to convert the names into lower cases df_predict_atp [ 'player_1' ] = df_predict_atp [ 'player_1' ] .str.lower () df_predict_atp [ 'player_2' ] = df_predict_atp [ 'player_2' ] .str.lower () df_predict_wta [ 'player_1' ] = df_predict_wta [ 'player_1' ] .str.lower () df_predict_wta [ 'player_2' ] = df_predict_wta [ 'player_2' ] .str.lower () df_rolling_atp [ 'Player' ] = df_rolling_atp [ 'Player' ] .str.lower () df_rolling_wta [ 'Player' ] = df_rolling_wta [ 'Player' ] .str.lower () # Lastly, some players have slightly difference names in the submission data and the match data. So we are editing them here manually df_predict_atp.loc [ df_predict_atp.player_1 == 'jaume munar' , 'player_1' ] = 'jaume antoni munar clar' df_predict_atp.loc [ df_predict_atp.player_2 == 'jaume munar' , 'player_2' ] = 'jaume antoni munar clar' df_predict_wta.loc [ df_predict_wta.player_1 == 'daria kasatkina' , 'player_1' ] = 'darya kasatkina' df_predict_wta.loc [ df_predict_wta.player_2 == 'daria kasatkina' , 'player_2' ] = 'darya kasatkina' df_predict_wta.loc [ df_predict_wta.player_1 == 'lesia tsurenko' , 'player_1' ] = 'lesya tsurenko' df_predict_wta.loc [ df_predict_wta.player_2 == 'lesia tsurenko' , 'player_2' ] = 'lesya tsurenko' df_predict_wta.loc [ df_predict_wta.player_1 == 'danielle collins' , 'player_1' ] = 'danielle rose collins' df_predict_wta.loc [ df_predict_wta.player_2 == 'danielle collins' , 'player_2' ] = 'danielle rose collins' df_predict_wta.loc [ df_predict_wta.player_1 == 'anna karolina schmiedlova' , 'player_1' ] = 'anna schmiedlova' df_predict_wta.loc [ df_predict_wta.player_2 == 'anna karolina schmiedlova' , 'player_2' ] = 'anna schmiedlova' df_predict_wta.loc [ df_predict_wta.player_1 == 'georgina garcia perez' , 'player_1' ] = 'georgina garcia-perez' df_predict_wta.loc [ df_predict_wta.player_2 == 'georgina garcia perez' , 'player_2' ] = 'georgina garcia-perez' # create and tournament date column and set it to 2019 so we can join the lastest features df_predict_atp [ 'Tournament_Date' ] = pd.to_datetime ( '2019-01-15' ) df_predict_wta [ 'Tournament_Date' ] = pd.to_datetime ( '2019-01-15' ) # Get the rolling features for player 1 df_predict_atp = df_predict_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_predict_wta = df_predict_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # For duplicate columns, we will use '_p1' to denote player 1 and '_p2' for player 2 df_predict_atp = df_predict_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_predict_wta = df_predict_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_predict_atp.head ( 2 ) player_1 player_2 player_1_win_probability Tournament_Date Player_p1 Player_Serve_Win_Ratio_p1 Player_Return_Win_Ratio_p1 Player_BreakPoints_Per_Return_Game_p1 Player_Game_Win_Percentage_p1 Player_Rank_p1 tournament_date_index_p1 Player_p2 Player_Serve_Win_Ratio_p2 Player_Return_Win_Ratio_p2 Player_BreakPoints_Per_Return_Game_p2 Player_Game_Win_Percentage_p2 Player_Rank_p2 tournament_date_index_p2 novak djokovic rafael nadal 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 rafael nadal 0.622425 0.401028 0.334270 0.570833 1.0 2019-01-15 novak djokovic roger federer 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 roger federer 0.620070 0.389781 0.269224 0.564244 3.0 2019-01-15 # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the mens' .format ( df_predict_atp.loc [ df_predict_atp.Player_p1.isna (), 'player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament in the mens' .format ( df_predict_atp.loc [ df_predict_atp.Player_p2.isna (), 'player_2' ] .nunique ())) 3 player_1s do Not have previous match history before the tournament in the mens 3 player_2s do Not have previous match history before the tournament in the mens # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the womens' .format ( df_predict_wta.loc [ df_predict_wta.Player_p1.isna (), 'player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament in the womens' .format ( df_predict_wta.loc [ df_predict_wta.Player_p2.isna (), 'player_2' ] .nunique ())) 0 player_1s do Not have previous match history before the tournament in the womens 0 player_2s do Not have previous match history before the tournament in the womens print ( df_predict_atp.loc [ df_predict_atp.Player_p1.isna (), 'player_1' ] .unique () .tolist ()) [ 'christian garin' , 'pedro sousa' , 'hugo dellien' ] print ( df_predict_wta.loc [ df_predict_wta.Player_p1.isna (), 'player_1' ] .unique () .tolist ()) [] We will do the differencing again for the prediction data frames exactly like what we did for training # Apply the function and get the difference between player 1 and 2 df_predict_atp , _ = get_player_difference ( df_predict_atp , diff_cols = diff_cols ) df_predict_wta , _ = get_player_difference ( df_predict_wta , diff_cols = diff_cols ) Make the prediction df_predict_atp_h2o = h2o.H2OFrame ( df_predict_atp [ feats ]) df_predict_wta_h2o = h2o.H2OFrame ( df_predict_wta [ feats ]) atp_preds = aml_atp.predict ( df_predict_atp_h2o )[ 'p1' ] .as_data_frame () wta_preds = aml_wta.predict ( df_predict_wta_h2o )[ 'p1' ] .as_data_frame () df_predict_atp [ 'player_1_win_probability' ] = atp_preds df_predict_wta [ 'player_1_win_probability' ] = wta_preds atp_submission = df_predict_atp [[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] wta_submission = df_predict_wta [[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] atp_submission.head () player_1 player_2 player_1_win_probability novak djokovic rafael nadal 0.571588 novak djokovic roger federer 0.662511 novak djokovic juan martin del potro 0.544306 novak djokovic alexander zverev 0.709483 novak djokovic kevin anderson 0.687195 wta_submission.head () player_1 player_2 player_1_win_probability simona halep angelique kerber 0.455224 simona halep caroline wozniacki 0.546276 simona halep elina svitolina 0.408014 simona halep naomi osaka 0.285125 simona halep sloane stephens 0.576643 Let's look at who has the highest win rate from our models atp_submission.groupby ( 'player_1' )[ 'player_1_win_probability' ] .mean () \\ .reset_index () .sort_values ( 'player_1_win_probability' , ascending = False ) .head () player_1 player_1_win_probability novak djokovic 0.846377 juan martin del potro 0.787337 karen khachanov 0.782963 rafael nadal 0.778707 roger federer 0.767337 wta_submission.groupby ( 'player_1' )[ 'player_1_win_probability' ] .mean () \\ .reset_index () .sort_values ( 'player_1_win_probability' , ascending = False ) .head () player_1 player_1_win_probability madison keys 0.750580 naomi osaka 0.749195 caroline wozniacki 0.722409 kiki bertens 0.713904 aryna sabalenka 0.707368 Now we can output the predictions as csvs atp_submission.to_csv ( 'submission/atp_submission_python.csv' , index = False ) wta_submission.to_csv ( 'submission/wta_submission_pthon.csv' , index = False ) atp_submission.shape (16256, 3) Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Aus Open Python Tutorial"},{"location":"modelling/AusOpenPythonTutorial/#australian-open-datathon-python-tutorial","text":"","title":"Australian Open Datathon Python Tutorial"},{"location":"modelling/AusOpenPythonTutorial/#overview","text":"","title":"Overview"},{"location":"modelling/AusOpenPythonTutorial/#the-task","text":"This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodology and thought process, read this article.","title":"The Task"},{"location":"modelling/AusOpenPythonTutorial/#intention","text":"This notebook will demonstrate how to: Process the raw data sets Produce simple features Run a predictive model on H2O Outputs the final predictions for the submissions Load the data and required packages import numpy as np import pandas as pd import os import gc import sys import warnings warnings.filterwarnings ( 'ignore' ) import h2o from h2o.automl import H2OAutoML pd.options.display.max_columns = 999 # We are loading both the mens and womens match csvs df_atp = pd.read_csv ( \"data/ATP_matches.csv\" ) df_wta = pd.read_csv ( \"data/WTA_matches.csv\" )","title":"Intention"},{"location":"modelling/AusOpenPythonTutorial/#data-pre-processing","text":"Filter the matches to hard and indoor hard only due to the fact that Australian Open is on hard surface and we want the models to train specifically for hard surfaces matches Convert the columns in both datasets to the correct types. For example, we want to make sure the date columns are in the datetime format and numerical columns are either integer or floats. This will help reduce the memory in use and make the feature engineering process easier ### Include hard and indoor hard only df_atp = df_atp.loc [ df_atp.Court_Surface.isin ([ 'Hard' , 'Indoor Hard' ])] df_wta = df_wta.loc [ df_wta.Court_Surface.isin ([ 'Hard' , 'Indoor Hard' ])] ### Exclude qualifying rounds df_atp = df_atp.loc [ df_atp.Round_Description != 'Qualifying' ] df_wta = df_wta.loc [ df_wta.Round_Description != 'Qualifying' ] # Store the shape of the data for reference check later atp_shape = df_atp.shape wta_shape = df_wta.shape numeric_columns = [ 'Winner_Rank' , 'Loser_Rank' , 'Retirement_Ind' , 'Winner_Sets_Won' , 'Winner_Games_Won' , 'Winner_Aces' , 'Winner_DoubleFaults' , 'Winner_FirstServes_Won' , 'Winner_FirstServes_In' , 'Winner_SecondServes_Won' , 'Winner_SecondServes_In' , 'Winner_BreakPoints_Won' , 'Winner_BreakPoints' , 'Winner_ReturnPoints_Won' , 'Winner_ReturnPoints_Faced' , 'Winner_TotalPoints_Won' , 'Loser_Sets_Won' , 'Loser_Games_Won' , 'Loser_Aces' , 'Loser_DoubleFaults' , 'Loser_FirstServes_Won' , 'Loser_FirstServes_In' , 'Loser_SecondServes_Won' , 'Loser_SecondServes_In' , 'Loser_BreakPoints_Won' , 'Loser_BreakPoints' , 'Loser_ReturnPoints_Won' , 'Loser_ReturnPoints_Faced' , 'Loser_TotalPoints_Won' ] text_columns = [ 'Winner' , 'Loser' , 'Tournament' , 'Court_Surface' , 'Round_Description' ] date_columns = [ 'Tournament_Date' ] # we set the **erros** to coerce so any non-numerical values (text,special characters) will return an NA df_atp [ numeric_columns ] = df_atp [ numeric_columns ] .apply ( pd.to_numeric , errors = 'coerce' ) df_wta [ numeric_columns ] = df_wta [ numeric_columns ] .apply ( pd.to_numeric , errors = 'coerce' ) df_atp [ date_columns ] = df_atp [ date_columns ] .apply ( pd.to_datetime ) df_wta [ date_columns ] = df_wta [ date_columns ] .apply ( pd.to_datetime )","title":"Data pre-processing"},{"location":"modelling/AusOpenPythonTutorial/#feature-engineering","text":"The raw datasets are constructed in a way that each row will have the seperate stats for both the winner and loser of that match. However, we want to reshape the data so that each row we will only have one player randomly selected from the winner/loser columns and the features are the difference between opponents statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. In addition, for the features, we will take the rolling average of the player's most recent 15 matches before the particular tournament starts. For example, if the match is the second round of the Australian Open 2018, the features will be the last 15 matches before the first round of Australian Open 2018. The reason of not including the stats in the first round is that we would not have known the player's stats in the first round for the final submissions A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, difference in ELO rating etc. The target variable will be whether or not Player A wins (1=Player A wins and 0=lose). The steps we take are: Convert the raw data frames into long format: Create some new features Take the rolling average for each player and each match Since we will be only training our models on US Open and Australian Open, we will only be creating features for those matches. However, the rolling average will take into account any hard surface matches before those tournaments Calculate the difference of averages for each match in the data frames","title":"Feature Engineering"},{"location":"modelling/AusOpenPythonTutorial/#convert-the-raw-data-frames-into-long-format","text":"# Before we split the data frame into winner and loser, we want to create a feature that captures the total number of games the match takes. # We have to do it before the split or we will lose this information df_atp [ 'Total_Games' ] = df_atp.Winner_Games_Won + df_atp.Loser_Games_Won df_wta [ 'Total_Games' ] = df_wta.Winner_Games_Won + df_wta.Loser_Games_Won # Get the column names for the winner and loser stats winner_cols = [ col for col in df_atp.columns if col.startswith ( 'Winner' )] loser_cols = [ col for col in df_atp.columns if col.startswith ( 'Loser' )] # create a winner data frame to store the winner stats and a loser data frame for the losers # In addition to the winner and loser columns, we are adding common columns as well (e.g. tournamnt dates) common_cols = [ 'Total_Games' , 'Tournament' , 'Tournament_Date' , 'Court_Surface' , 'Round_Description' ] df_winner_atp = df_atp [ winner_cols + common_cols ] df_loser_atp = df_atp [ loser_cols + common_cols ] df_winner_wta = df_wta [ winner_cols + common_cols ] df_loser_wta = df_wta [ loser_cols + common_cols ] # Create a new column to show whether the player has won or not. df_winner_atp [ \"won\" ] = 1 df_loser_atp [ \"won\" ] = 0 df_winner_wta [ \"won\" ] = 1 df_loser_wta [ \"won\" ] = 0 # Rename the columns for the winner and loser data frames so we can append them later on. # We will rename the Winner_ / Loser_ columns to Player_ new_column_names = [ col.replace ( 'Winner' , 'Player' ) for col in winner_cols ] df_winner_atp.columns = new_column_names + common_cols + [ 'won' ] # They all should be the same df_loser_atp.columns = df_winner_atp.columns df_winner_wta.columns = df_winner_atp.columns df_loser_wta.columns = df_winner_atp.columns # append the winner and loser data frames df_long_atp = df_winner_atp.append ( df_loser_atp ) df_long_wta = df_winner_wta.append ( df_loser_wta ) So now our data frames are in long format and should looks like this df_long_atp.head () Player Player_Rank Player_Sets_Won Player_Games_Won Player_Aces Player_DoubleFaults Player_FirstServes_Won Player_FirstServes_In Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Total_Games Tournament Tournament_Date Court_Surface Round_Description won Edouard Roger-Vasselin 106.0 2.0 12 5.0 2.0 22 30 12 19 4.0 7.0 25.0 59.0 59 19 Chennai 2012-01-02 Hard First Round 1 Dudi Sela 83.0 2.0 12 2.0 0.0 14 17 11 16 6.0 14.0 36.0 58.0 61 13 Chennai 2012-01-02 Hard First Round 1 Go Soeda 120.0 2.0 19 6.0 1.0 48 64 19 39 5.0 11.0 42.0 105.0 109 33 Chennai 2012-01-02 Hard First Round 1 Yuki Bhambri 345.0 2.0 12 1.0 2.0 22 29 9 17 5.0 13.0 34.0 62.0 65 17 Chennai 2012-01-02 Hard First Round 1 Yuichi Sugita 235.0 2.0 12 3.0 1.0 37 51 11 27 3.0 7.0 22.0 54.0 70 19 Chennai 2012-01-02 Hard First Round 1","title":"Convert the raw data frames into long format:"},{"location":"modelling/AusOpenPythonTutorial/#create-some-new-features","text":"Thinking about the dynamics of tennis, we know that players often will matches by \u201cbreaking\u201d the opponent\u2019s serve (i.e. winning a game when the opponent is serving). This is especially important in tennis. Let\u2019s create a feature called Player_BreakPoints_Per_Game, which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let\u2019s also create a feature called Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \u201cholding\u201d serve is important (i.e. winning a game when you are serving). Let\u2019s create a feature called Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let\u2019s create a feature called Player_Game_Win_Percentage which is the propotion of games that a player wins. So the four new features we will create are: Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage # Here, we will define a function so we can apply it to both atp and wta data frames def get_new_features ( df ) : # Input: # df: data frame to get the data from # Return: the df with the new features # Point Win ratio when serving df [ 'Player_Serve_Win_Ratio' ] = ( df.Player_FirstServes_Won + df.Player_SecondServes_Won - df.Player_DoubleFaults ) \\ / ( df.Player_FirstServes_In + df.Player_SecondServes_In + df.Player_DoubleFaults ) # Point win ratio when returning df [ 'Player_Return_Win_Ratio' ] = df.Player_ReturnPoints_Won / df.Player_ReturnPoints_Faced # Breakpoints per receiving game df [ 'Player_BreakPoints_Per_Return_Game' ] = df.Player_BreakPoints / df.Total_Games df [ 'Player_Game_Win_Percentage' ] = df.Player_Games_Won / df.Total_Games return df # Apply the function we just created to the long data frames df_long_atp = get_new_features ( df_long_atp ) df_long_wta = get_new_features ( df_long_wta ) # The long table should have exactly twice of the rows of the original data assert df_long_atp.shape [ 0 ] == atp_shape [ 0 ] * 2 assert df_long_wta.shape [ 0 ] == wta_shape [ 0 ] * 2","title":"Create some new features"},{"location":"modelling/AusOpenPythonTutorial/#take-the-rolling-average-for-each-player-and-each-match","text":"To train our models, we cannot simply use the player stats for that current match. In fact, we wont be able to use any stats from the same tournament. The logic behind this is that when we try to predict the results in 2019, we would not know the stats of any of the matches in the Australian Open 2019 tournament. As a result, we will use the players' past performance. Here, we will do a rolling average of the most recent 15 matches before the tournament. To do the above, we will follow the steps below: List all the tournament dates for US and Australian Opens Loop through the dates from point 1, for each date, we filter the data to only include matches before that date and take the most recent 15 games Take the average of those 15 games # the two tournaments we will be using for training and thus the feature generation tournaments = [ 'U.S. Open, New York' , 'Australian Open, Melbourne' ] # Store the dates for the loops tournament_dates_atp = df_atp.loc [ df_atp.Tournament.isin ( tournaments )] .groupby ([ 'Tournament' , 'Tournament_Date' ]) \\ .size () .reset_index ()[[ 'Tournament' , 'Tournament_Date' ]] tournament_dates_wta = df_wta.loc [ df_wta.Tournament.isin ( tournaments )] .groupby ([ 'Tournament' , 'Tournament_Date' ]) \\ .size () .reset_index ()[[ 'Tournament' , 'Tournament_Date' ]] # We are adding one more date for the final prediction tournament_dates_atp.loc [ -1 ] = [ 'Australian Open, Melbourne' , pd.to_datetime ( '2019-01-15' )] tournament_dates_wta.loc [ -1 ] = [ 'Australian Open, Melbourne' , pd.to_datetime ( '2019-01-15' )] Following are the dates for each tournament tournament_dates_atp Tournament Tournament_Date Australian Open, Melbourne 2012-01-16 Australian Open, Melbourne 2013-01-1 Australian Open, Melbourne 2014-01-1 Australian Open, Melbourne 2015-01-1 Australian Open, Melbourne 2016-01-1 Australian Open, Melbourne 2017-01-1 Australian Open, Melbourne 2018-01-1 U.S. Open, New York 2012-08-2 U.S. Open, New York 2013-08-2 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 tournament_dates_wta Tournament Tournament_Date Australian Open, Melbourne 2014-01-13 Australian Open, Melbourne 2015-01-19 Australian Open, Melbourne 2016-01-18 Australian Open, Melbourne 2017-01-16 Australian Open, Melbourne 2018-01-15 U.S. Open, New York 2014-08-25 U.S. Open, New York 2015-08-31 U.S. Open, New York 2016-08-29 U.S. Open, New York 2017-08-28 U.S. Open, New York 2018-08-27 Australian Open, Melbourne 2019-01-15 They look fine but it is interesting that for men's, we have two more years of data from 2012 to 2013 # Let's define a function to calculate the rolling averages def get_rolling_features ( df , date_df = None , rolling_cols = None , last_cols = None ) : # Input: # df: data frame to get the data from # date_df: data frame that has the start dates for each tournament # rolling_cols: columns to get the rolling averages # last_cols: columns to get the last value (most recent) # Return: the df with the new features # Sort the data by player and dates so the most recent matches are at the bottom df = df.sort_values ([ 'Player' , 'Tournament_Date' , 'Tournament' ], ascending = True ) # For each tournament, get the rolling averages of that player's past matches before the tournament start date for index , tournament_date in enumerate ( date_df.Tournament_Date ) : # create a temp df to store the interim results df_temp = df.loc [ df.Tournament_Date < tournament_date ] # for ranks, we only take the last one. (comment this out if want to take avg of rank) df_temp_last = df_temp.groupby ( 'Player' )[ last_cols ] .last () .reset_index () # take the most recent 15 matches for the rolling average df_temp = df_temp.groupby ( 'Player' )[ rolling_cols ] .rolling ( 15 , min_periods = 1 ) .mean () .reset_index () df_temp = df_temp.groupby ( 'Player' ) .tail ( 1 ) # take the last row of the above df_temp = df_temp.merge ( df_temp_last , on = 'Player' , how = 'left' ) if index == 0 : df_result = df_temp df_result [ 'tournament_date_index' ] = tournament_date # so we know which tournament this feature is for else : df_temp [ 'tournament_date_index' ] = tournament_date df_result = df_result.append ( df_temp ) df_result.drop ( 'level_1' , axis = 1 , inplace = True ) return df_result # columns we are applying the rolling averages on rolling_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' ] # columns we are taking the most recent values on # For the player rank, we think we can just use the latest rank (before the tournament starts) # as it should refect the most recent performance of the player last_cols = [ 'Player_Rank' ] # Apply the rolling average function to the long data frames (it will take a few mins to run) df_rolling_atp = get_rolling_features ( df_long_atp , tournament_dates_atp , rolling_cols , last_cols = last_cols ) df_rolling_wta = get_rolling_features ( df_long_wta , tournament_dates_wta , rolling_cols , last_cols = last_cols ) df_rolling_atp.head ( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Adrian Mannarino 0.623408 0.353397 0.257859 0.447246 87.0 2012-01-16 Albert Montanes 0.507246 0.195652 0.000000 0.294118 50.0 2012-01-16 df_rolling_wta.head ( 2 ) Player Player_Serve_Win_Ratio Player_Return_Win_Ratio Player_BreakPoints_Per_Return_Game Player_Game_Win_Percentage Player_Rank tournament_date_index Agnieszka Radwanska 0.413333 0.475410 0.350000 0.350000 5.0 2014-01-13 Ajla Tomljanovic 0.468457 0.407319 0.242253 0.462634 75.0 2014-01-13","title":"Take the rolling average for each player and each match"},{"location":"modelling/AusOpenPythonTutorial/#calculate-the-difference-of-averages-for-each-match-in-the-data-frames","text":"In the original data frames, the first column is always the winner and followed by the loser. Same for the player stats. Thus, we cannot simply calculate the difference between winner and loser and create a target variable indicating player 1 will win or not because it will always be the winner in this case (target always = 1). As a result, we need to pick a player randomly so the player might or might not be the winner In addition, instead of using both the features for player 1 and 2, we will take the difference of averages between the randomised player 1 and 2. The main benefit is that it will reduce the number of features to half Steps: We will create a random number for each player which only return 0 or 1 If it is zero, we will assign the winner to player 1 and loser to player 2 We will join the features to the player 1 and 2. The join will be on the player names and the tournament date (tournament_index in the feature data frames) For players who do not have any history, we will fill the stats by zeros and rank by 999 # Randomise the match_wide dataset so the first player is not always the winner # set a seed so the random number is reproducable np.random.seed ( 2 ) # randomise a number 0/1 with 50% chance each # if 0 then take the winner, 1 then take loser df_atp [ 'random_number' ] = np.random.randint ( 2 , size = len ( df_atp )) df_atp [ 'randomised_player_1' ] = np.where ( df_atp [ 'random_number' ] == 0 , df_atp [ 'Winner' ], df_atp [ 'Loser' ]) df_atp [ 'randomised_player_2' ] = np.where ( df_atp [ 'random_number' ] == 0 , df_atp [ 'Loser' ], df_atp [ 'Winner' ]) df_wta [ 'random_number' ] = np.random.randint ( 2 , size = len ( df_wta )) df_wta [ 'randomised_player_1' ] = np.where ( df_wta [ 'random_number' ] == 0 , df_wta [ 'Winner' ], df_wta [ 'Loser' ]) df_wta [ 'randomised_player_2' ] = np.where ( df_wta [ 'random_number' ] == 0 , df_wta [ 'Loser' ], df_wta [ 'Winner' ]) # set the target (win/loss) based on the new randomise number df_atp [ 'player_1_win' ] = np.where ( df_atp [ 'random_number' ] == 0 , 1 , 0 ) df_wta [ 'player_1_win' ] = np.where ( df_wta [ 'random_number' ] == 0 , 1 , 0 ) print ( 'After shuffling, the win rate for player 1 for the mens is {}%' .format ( df_atp [ 'player_1_win' ] .mean () * 100 )) print ( 'After shuffling, the win rate for player 1 for the womens is {}%' .format ( df_wta [ 'player_1_win' ] .mean () * 100 )) After shuffling, the win rate for player 1 for the mens is 49.64798919857267% After shuffling, the win rate for player 1 for the womens is 49.697671426733564% The win rates are close enough to 50%. So we are good to go # To get our data frames ready for model training, we will exclude other tournaments from the data now because we have gotten the rolling averages from them and # for training, we only need US and Australian Open matches df_atp = df_atp.loc [ df_atp.Tournament.isin ( tournaments )] df_wta = df_wta.loc [ df_wta.Tournament.isin ( tournaments )] # now we can remove other stats columns because we will be using the differences cols_to_keep = [ 'Winner' , 'Loser' , 'Tournament' , 'Tournament_Date' , 'player_1_win' , 'randomised_player_1' , 'randomised_player_2' ] df_atp = df_atp [ cols_to_keep ] df_wta = df_wta [ cols_to_keep ] # Here, we are joining the rolling average data frames to the individual matches. # We need to do it twice. One for player 1 and one for player 2 # Get the rolling features for player 1 df_atp = df_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_wta = df_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'randomised_player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # we will use '_p1' to denote player 1 and '_p2' for player 2 df_atp = df_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_wta = df_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'randomised_player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' .format ( df_atp.loc [ df_atp.Player_p1.isna (), 'randomised_player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament' .format ( df_atp.loc [ df_atp.Player_p2.isna (), 'randomised_player_2' ] .nunique ())) 59 player_1s do Not have previous match history before the tournament 56 player_2s do Not have previous match history before the tournament # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament' .format ( df_wta.loc [ df_wta.Player_p1.isna (), 'randomised_player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament' .format ( df_wta.loc [ df_wta.Player_p2.isna (), 'randomised_player_2' ] .nunique ())) 41 player_1s do Not have previous match history before the tournament 37 player_2s do Not have previous match history before the tournament # Most of the missing are for the early years which makes sense as we dont have enough history for them df_wta.loc [ df_wta.Player_p1.isna (), 'Tournament_Date' ] .value_counts () 2014-01-13 29 2014-08-25 7 2015-08-31 5 2015-01-19 3 2017-08-28 3 2018-01-15 3 2018-08-27 3 Name: Tournament_Date, dtype: int64 df_atp.loc [ df_atp.Player_p1.isna (), 'Tournament_Date' ] .value_counts () 2012-01-16 29 2012-08-27 9 2014-01-13 5 2013-08-26 5 2016-01-18 5 2013-01-14 4 2014-08-25 3 2018-01-15 3 2017-08-28 3 2018-08-27 2 2016-08-29 2 2015-01-19 1 Name: Tournament_Date, dtype: int64 Now we have gotten the rolling averages for both player 1 and 2. What we need to do next is to simply calculate their difference. To calculate the difference, we need to: Split the data frames into two new data frames: Player 1 and Player 2 Take the difference between the two data frames def get_player_difference ( df , diff_cols = None ) : # Input: # df: data frame to get the data from # diff_cols: columns we take the difference on. For example is diff_cols = win rate. This function will calculate the # difference of the win rates between player 1 and player 2 # Return: the df with the new features p1_cols = [ i + '_p1' for i in diff_cols ] # column names for player 1 stats p2_cols = [ i + '_p2' for i in diff_cols ] # column names for player 2 stats # For any missing values, we will fill them by zeros except the ranking where we will use 999 df [ 'Player_Rank_p1' ] = df [ 'Player_Rank_p1' ] .fillna ( 999 ) df [ p1_cols ] = df [ p1_cols ] .fillna ( 0 ) df [ 'Player_Rank_p2' ] = df [ 'Player_Rank_p2' ] .fillna ( 999 ) df [ p2_cols ] = df [ p2_cols ] .fillna ( 0 ) new_column_name = [ i + '_diff' for i in diff_cols ] # Take the difference df_p1 = df [ p1_cols ] df_p2 = df [ p2_cols ] df_p1.columns = new_column_name df_p2.columns = new_column_name df_diff = df_p1 - df_p2 df_diff.columns = new_column_name # drop the p1 and p2 columns because We have the differences now df.drop ( p1_cols + p2_cols , axis = 1 , inplace = True ) # Concat the df_diff and raw_df df = pd.concat ([ df , df_diff ], axis = 1 ) return df , new_column_name diff_cols = [ 'Player_Serve_Win_Ratio' , 'Player_Return_Win_Ratio' , 'Player_BreakPoints_Per_Return_Game' , 'Player_Game_Win_Percentage' , 'Player_Rank' ] # Apply the function and get the difference between player 1 and 2 df_atp , _ = get_player_difference ( df_atp , diff_cols = diff_cols ) df_wta , _ = get_player_difference ( df_wta , diff_cols = diff_cols ) # Make a copy of the data frames in case we need to come back to check the values df_atp_final = df_atp.copy () df_wta_final = df_wta.copy () df_atp_final.head () Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Juan Martin del Potro Adrian Mannarino Australian Open, Melbourne 2012-01-16 1 Juan Martin del Potro Adrian Mannarino Juan Martin del Potro 2012-01-16 Adrian Mannarino 2012-01-16 0.035030 -0.021271 -0.025975 0.103479 -76.0 Pere Riba Albert Montanes Australian Open, Melbourne 2012-01-16 1 Pere Riba Albert Montanes Pere Riba 2012-01-16 Albert Montanes 2012-01-16 -0.156369 0.008893 0.066667 -0.094118 39.0 Tomas Berdych Albert Ramos-Vinolas Australian Open, Melbourne 2012-01-16 0 Albert Ramos-Vinolas Tomas Berdych Albert Ramos-Vinolas 2012-01-16 NaN NaT 0.498027 0.380092 0.414815 0.394444 -934.0 Rafael Nadal Alex Kuznetsov Australian Open, Melbourne 2012-01-16 0 Alex Kuznetsov Rafael Nadal NaN NaT Rafael Nadal 2012-01-16 -0.670139 -0.423057 -0.445623 -0.574767 997.0 Roger Federer Alexander Kudryavtsev Australian Open, Melbourne 2012-01-16 0 Alexander Kudryavtsev Roger Federer NaN NaT Roger Federer 2012-01-16 -0.721415 -0.449516 -0.360255 -0.668090 996.0","title":"Calculate the difference of averages for each match in the data frames"},{"location":"modelling/AusOpenPythonTutorial/#modelling","text":"We will trian two models here, one for mens and one for womens. For training, we will use all available data from the second year (too many missing values in the first year) up until 2017. For validation, we will test the model on the 2018 Australian Open data This setup allows us to 'mimic' the final prediction (using historical matches to predict 2019 results) df_train_atp = df_atp_final.loc [( df_atp_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & ( df_atp_final.Tournament_Date > '2012-01-16' )] # excluding first year df_valid_atp = df_atp_final.loc [ df_atp_final.Tournament_Date == '2018-01-15' ] # Australian Open 2018 only df_train_wta = df_wta_final.loc [( df_wta_final.Tournament_Date != '2018-01-15' ) # excluding Aus Open 2018, and & ( df_wta_final.Tournament_Date > '2014-01-13' )] # excluding first year df_train_atp.head () Winner Loser Tournament Tournament_Date player_1_win randomised_player_1 randomised_player_2 Player_p1 tournament_date_index_p1 Player_p2 tournament_date_index_p2 Player_Serve_Win_Ratio_diff Player_Return_Win_Ratio_diff Player_BreakPoints_Per_Return_Game_diff Player_Game_Win_Percentage_diff Player_Rank_diff Daniel Brands Adrian Ungur U.S. Open, New York 2012-08-27 0 Adrian Ungur Daniel Brands NaN NaT Daniel Brands 2012-08-27 -0.535211 -0.300000 -0.043478 -0.434783 870.0 Richard Gasquet Albert Montanes U.S. Open, New York 2012-08-27 1 Richard Gasquet Albert Montanes Richard Gasquet 2012-08-27 Albert Montanes 2012-08-27 0.080003 0.077451 0.180847 0.131108 -37.0 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 1 Martin Klizan Alejandro Falla Martin Klizan 2012-08-27 Alejandro Falla 2012-08-27 0.077117 -0.044716 -0.087362 0.068180 -2.0 Andy Murray Alex Bogomolov Jr. U.S. Open, New York 2012-08-27 1 Andy Murray Alex Bogomolov Jr. Andy Murray 2012-08-27 Alex Bogomolov Jr. 2012-08-27 0.039641 0.031701 0.094722 0.059010 -69.0 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 1 Tommy Robredo Andreas Seppi Tommy Robredo 2012-08-27 Andreas Seppi 2012-08-27 -0.026814 0.006442 -0.009930 -0.067780 151.0 # target variable target = 'player_1_win' # features being fed into the models feats = [ 'Player_Serve_Win_Ratio_diff' , 'Player_Return_Win_Ratio_diff' , 'Player_BreakPoints_Per_Return_Game_diff' , 'Player_Game_Win_Percentage_diff' , 'Player_Rank_diff' ] print ( feats )","title":"Modelling"},{"location":"modelling/AusOpenPythonTutorial/#h2o-model-for-atp","text":"h2o.init () # Convert to an h2o frame df_train_atp_h2o = h2o.H2OFrame ( df_train_atp ) df_valid_atp_h2o = h2o.H2OFrame ( df_valid_atp ) # For binary classification, response should be a factor df_train_atp_h2o [ target ] = df_train_atp_h2o [ target ] .asfactor () df_valid_atp_h2o [ target ] = df_valid_atp_h2o [ target ] .asfactor () # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_atp = h2o.automl.H2OAutoML ( max_runtime_secs = 300 , max_models = 100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True , seed = 183 ) aml_atp.train ( x = feats , y = target , training_frame = df_train_atp_h2o , validation_frame = df_valid_atp_h2o ) # View the AutoML Leaderboard lb = aml_atp.leaderboard lb.head () model_id auc logloss mean_per_class_error rmse mse GBM_5_AutoML_20181221_094949 0.790281 0.554852 0.281363 0.431379 0.186088 GBM_grid_1_AutoML_20181221_094949_model_15 0.789329 0.556804 0.29856 0.431931 0.186564 GBM_grid_1_AutoML_20181221_094949_model_7 0.788013 0.557808 0.295899 0.432968 0.187461 StackedEnsemble_BestOfFamily_AutoML_20181221_094949 0.788131 0.558028 0.285321 0.432849 0.187358 GBM_grid_1_AutoML_20181221_094949_model_20 0.785633 0.561094 0.283932 0.43479 0.189043 StackedEnsemble_AllModels_AutoML_20181221_094949 0.784411 0.561587 0.293244 0.434667 0.188935 GBM_grid_1_AutoML_20181221_094949_model_25 0.785311 0.561783 0.291912 0.434888 0.189127 GBM_grid_1_AutoML_20181221_094949_model_17 0.774832 0.570883 0.295836 0.439375 0.193051 DeepLearning_1_AutoML_20181221_094949 0.779388 0.572823 0.311737 0.438479 0.192264 GBM_grid_1_AutoML_20181221_094949_model_14 0.7718 0.578867 0.285835 0.441373 0.19481 H2O model for WTA # Convert to an h2o frame df_train_wta_h2o = h2o.H2OFrame ( df_train_wta ) df_valid_wta_h2o = h2o.H2OFrame ( df_valid_wta ) # For binary classification, response should be a factor df_train_wta_h2o [ target ] = df_train_wta_h2o [ target ] .asfactor () df_valid_wta_h2o [ target ] = df_valid_wta_h2o [ target ] .asfactor () # Run AutoML for 20 base models (limited to 1 hour max runtime by default) aml_wta = h2o.automl.H2OAutoML ( max_runtime_secs = 300 , max_models = 100 , stopping_metric = 'logloss' , sort_metric = 'logloss' , balance_classes = True , seed = 183 ) aml_wta.train ( x = feats , y = target , training_frame = df_train_wta_h2o , validation_frame = df_valid_wta_h2o ) # View the AutoML Leaderboard lb = aml_wta.leaderboard lb.head () model_id auc logloss mean_per_class_error rmse mse StackedEnsemble_AllModels_AutoML_20181221_095400 0.726046 0.60827 0.321222 0.457117 0.208956 StackedEnsemble_BestOfFamily_AutoML_20181221_095400 0.724911 0.609329 0.337847 0.457659 0.209452 DeepLearning_grid_1_AutoML_20181221_095400_model_3 0.729152 0.612669 0.315971 0.45641 0.20831 GBM_grid_1_AutoML_20181221_095400_model_7 0.721204 0.615763 0.336848 0.460885 0.212415 GBM_5_AutoML_20181221_095400 0.719252 0.616535 0.319179 0.461055 0.212572 GBM_grid_1_AutoML_20181221_095400_model_15 0.715921 0.619263 0.318673 0.462215 0.213643 GLM_grid_1_AutoML_20181221_095400_model_1 0.726048 0.622989 0.366124 0.463099 0.214461 GBM_grid_1_AutoML_20181221_095400_model_17 0.709261 0.624902 0.34876 0.465628 0.216809 GBM_grid_1_AutoML_20181221_095400_model_18 0.70946 0.625704 0.393556 0.466147 0.217293 DeepLearning_grid_1_AutoML_20181221_095400_model_2 0.713419 0.628008 0.311334 0.463638 0.21496","title":"H2O model for ATP"},{"location":"modelling/AusOpenPythonTutorial/#use-the-models-to-predict-and-make-submissions","text":"Now let's use the models we just created to make the submissions df_predict_atp = pd.read_csv ( \"data/men_dummy_submission_file.csv\" ) df_predict_wta = pd.read_csv ( \"data/women_dummy_submission_file.csv\" , encoding = 'latin1' ) # for womens, there are some names need a different encoding df_predict_wta.head ( 2 ) player_1 player_2 player_1_win_probability Simona Halep Angelique Kerber 0.5 Simona Halep Caroline Wozniacki 0.5","title":"Use the models to predict and make submissions"},{"location":"modelling/AusOpenPythonTutorial/#get-the-features-for-the-predict-df","text":"We need to join the features to the 2019 players # Before we join the features by the names and the dates, we need to convert any non-english characters to english first translationTable = str.maketrans ( \"\u00e9\u00e0\u00e8\u00f9\u00e2\u00ea\u00ee\u00f4\u00fb\u00e7\u00f1\u00e1\" , \"eaeuaeioucna\" ) df_predict_atp [ 'player_1' ] = df_predict_atp.player_1.apply ( lambda x : x.translate ( translationTable )) df_predict_atp [ 'player_2' ] = df_predict_atp.player_2.apply ( lambda x : x.translate ( translationTable )) df_predict_wta [ 'player_1' ] = df_predict_wta.player_1.apply ( lambda x : x.translate ( translationTable )) df_predict_wta [ 'player_2' ] = df_predict_wta.player_2.apply ( lambda x : x.translate ( translationTable )) # Also we need to convert the names into lower cases df_predict_atp [ 'player_1' ] = df_predict_atp [ 'player_1' ] .str.lower () df_predict_atp [ 'player_2' ] = df_predict_atp [ 'player_2' ] .str.lower () df_predict_wta [ 'player_1' ] = df_predict_wta [ 'player_1' ] .str.lower () df_predict_wta [ 'player_2' ] = df_predict_wta [ 'player_2' ] .str.lower () df_rolling_atp [ 'Player' ] = df_rolling_atp [ 'Player' ] .str.lower () df_rolling_wta [ 'Player' ] = df_rolling_wta [ 'Player' ] .str.lower () # Lastly, some players have slightly difference names in the submission data and the match data. So we are editing them here manually df_predict_atp.loc [ df_predict_atp.player_1 == 'jaume munar' , 'player_1' ] = 'jaume antoni munar clar' df_predict_atp.loc [ df_predict_atp.player_2 == 'jaume munar' , 'player_2' ] = 'jaume antoni munar clar' df_predict_wta.loc [ df_predict_wta.player_1 == 'daria kasatkina' , 'player_1' ] = 'darya kasatkina' df_predict_wta.loc [ df_predict_wta.player_2 == 'daria kasatkina' , 'player_2' ] = 'darya kasatkina' df_predict_wta.loc [ df_predict_wta.player_1 == 'lesia tsurenko' , 'player_1' ] = 'lesya tsurenko' df_predict_wta.loc [ df_predict_wta.player_2 == 'lesia tsurenko' , 'player_2' ] = 'lesya tsurenko' df_predict_wta.loc [ df_predict_wta.player_1 == 'danielle collins' , 'player_1' ] = 'danielle rose collins' df_predict_wta.loc [ df_predict_wta.player_2 == 'danielle collins' , 'player_2' ] = 'danielle rose collins' df_predict_wta.loc [ df_predict_wta.player_1 == 'anna karolina schmiedlova' , 'player_1' ] = 'anna schmiedlova' df_predict_wta.loc [ df_predict_wta.player_2 == 'anna karolina schmiedlova' , 'player_2' ] = 'anna schmiedlova' df_predict_wta.loc [ df_predict_wta.player_1 == 'georgina garcia perez' , 'player_1' ] = 'georgina garcia-perez' df_predict_wta.loc [ df_predict_wta.player_2 == 'georgina garcia perez' , 'player_2' ] = 'georgina garcia-perez' # create and tournament date column and set it to 2019 so we can join the lastest features df_predict_atp [ 'Tournament_Date' ] = pd.to_datetime ( '2019-01-15' ) df_predict_wta [ 'Tournament_Date' ] = pd.to_datetime ( '2019-01-15' ) # Get the rolling features for player 1 df_predict_atp = df_predict_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) df_predict_wta = df_predict_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'player_1' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' ) # Get the rolling features for player 2 # For duplicate columns, we will use '_p1' to denote player 1 and '_p2' for player 2 df_predict_atp = df_predict_atp.merge ( df_rolling_atp , how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_predict_wta = df_predict_wta.merge ( df_rolling_wta , how = 'left' , left_on = [ 'player_2' , 'Tournament_Date' ], right_on = [ 'Player' , 'tournament_date_index' ], validate = 'm:1' , suffixes = ( '_p1' , '_p2' )) df_predict_atp.head ( 2 ) player_1 player_2 player_1_win_probability Tournament_Date Player_p1 Player_Serve_Win_Ratio_p1 Player_Return_Win_Ratio_p1 Player_BreakPoints_Per_Return_Game_p1 Player_Game_Win_Percentage_p1 Player_Rank_p1 tournament_date_index_p1 Player_p2 Player_Serve_Win_Ratio_p2 Player_Return_Win_Ratio_p2 Player_BreakPoints_Per_Return_Game_p2 Player_Game_Win_Percentage_p2 Player_Rank_p2 tournament_date_index_p2 novak djokovic rafael nadal 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 rafael nadal 0.622425 0.401028 0.334270 0.570833 1.0 2019-01-15 novak djokovic roger federer 0.5 2019-01-15 novak djokovic 0.685903 0.426066 0.42378 0.640857 2.0 2019-01-15 roger federer 0.620070 0.389781 0.269224 0.564244 3.0 2019-01-15 # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the mens' .format ( df_predict_atp.loc [ df_predict_atp.Player_p1.isna (), 'player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament in the mens' .format ( df_predict_atp.loc [ df_predict_atp.Player_p2.isna (), 'player_2' ] .nunique ())) 3 player_1s do Not have previous match history before the tournament in the mens 3 player_2s do Not have previous match history before the tournament in the mens # How many players do not have previous match history print ( '{} player_1s do Not have previous match history before the tournament in the womens' .format ( df_predict_wta.loc [ df_predict_wta.Player_p1.isna (), 'player_1' ] .nunique ())) print ( '{} player_2s do Not have previous match history before the tournament in the womens' .format ( df_predict_wta.loc [ df_predict_wta.Player_p2.isna (), 'player_2' ] .nunique ())) 0 player_1s do Not have previous match history before the tournament in the womens 0 player_2s do Not have previous match history before the tournament in the womens print ( df_predict_atp.loc [ df_predict_atp.Player_p1.isna (), 'player_1' ] .unique () .tolist ()) [ 'christian garin' , 'pedro sousa' , 'hugo dellien' ] print ( df_predict_wta.loc [ df_predict_wta.Player_p1.isna (), 'player_1' ] .unique () .tolist ()) [] We will do the differencing again for the prediction data frames exactly like what we did for training # Apply the function and get the difference between player 1 and 2 df_predict_atp , _ = get_player_difference ( df_predict_atp , diff_cols = diff_cols ) df_predict_wta , _ = get_player_difference ( df_predict_wta , diff_cols = diff_cols )","title":"Get the features for the predict df"},{"location":"modelling/AusOpenPythonTutorial/#make-the-prediction","text":"df_predict_atp_h2o = h2o.H2OFrame ( df_predict_atp [ feats ]) df_predict_wta_h2o = h2o.H2OFrame ( df_predict_wta [ feats ]) atp_preds = aml_atp.predict ( df_predict_atp_h2o )[ 'p1' ] .as_data_frame () wta_preds = aml_wta.predict ( df_predict_wta_h2o )[ 'p1' ] .as_data_frame () df_predict_atp [ 'player_1_win_probability' ] = atp_preds df_predict_wta [ 'player_1_win_probability' ] = wta_preds atp_submission = df_predict_atp [[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] wta_submission = df_predict_wta [[ 'player_1' , 'player_2' , 'player_1_win_probability' ]] atp_submission.head () player_1 player_2 player_1_win_probability novak djokovic rafael nadal 0.571588 novak djokovic roger federer 0.662511 novak djokovic juan martin del potro 0.544306 novak djokovic alexander zverev 0.709483 novak djokovic kevin anderson 0.687195 wta_submission.head () player_1 player_2 player_1_win_probability simona halep angelique kerber 0.455224 simona halep caroline wozniacki 0.546276 simona halep elina svitolina 0.408014 simona halep naomi osaka 0.285125 simona halep sloane stephens 0.576643 Let's look at who has the highest win rate from our models atp_submission.groupby ( 'player_1' )[ 'player_1_win_probability' ] .mean () \\ .reset_index () .sort_values ( 'player_1_win_probability' , ascending = False ) .head () player_1 player_1_win_probability novak djokovic 0.846377 juan martin del potro 0.787337 karen khachanov 0.782963 rafael nadal 0.778707 roger federer 0.767337 wta_submission.groupby ( 'player_1' )[ 'player_1_win_probability' ] .mean () \\ .reset_index () .sort_values ( 'player_1_win_probability' , ascending = False ) .head () player_1 player_1_win_probability madison keys 0.750580 naomi osaka 0.749195 caroline wozniacki 0.722409 kiki bertens 0.713904 aryna sabalenka 0.707368 Now we can output the predictions as csvs atp_submission.to_csv ( 'submission/atp_submission_python.csv' , index = False ) wta_submission.to_csv ( 'submission/wta_submission_pthon.csv' , index = False ) atp_submission.shape (16256, 3)","title":"Make the prediction"},{"location":"modelling/AusOpenPythonTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/AusOpenRTutorial/","text":"Australian Open Datathon R Tutorial Overview The Task This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodology and thought process, read this article. Exploring the Data First we need to get an idea of what the data looks like. Let's read the men's data in and get an idea of what it looks like. Note that you will need to install all the packages listed below unless you already have them. Note that for this tutorial I will be using dplyr , if you are not familiar with the syntax I encourage you to read up on the basics . # Import libraries library ( dplyr ) library ( readr ) library ( tidyr ) library ( RcppRoll ) library ( tidyselect ) library ( lubridate ) library ( stringr ) library ( zoo ) library ( purrr ) library ( h2o ) library ( DT ) mens = readr :: read_csv ( 'data/ATP_matches.csv' , na = \".\" ) # NAs are indicated by . mens %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Winner Loser Tournament Tournament_Date Court_Surface Round_Description Winner_Rank Loser_Rank Retirement_Ind Winner_Sets_Won ... Loser_DoubleFaults Loser_FirstServes_Won Loser_FirstServes_In Loser_SecondServes_Won Loser_SecondServes_In Loser_BreakPoints_Won Loser_BreakPoints Loser_ReturnPoints_Won Loser_ReturnPoints_Faced Loser_TotalPoints_Won Edouard Roger-Vasselin Eric Prodon Chennai 2-Jan-12 Hard First Round 106 97 0 2 ... 3 21 33 13 26 1 3 15 49 49 Dudi Sela Fabio Fognini Chennai 2-Jan-12 Hard First Round 83 48 0 2 ... 4 17 32 5 26 0 1 8 33 30 Go Soeda Frederico Gil Chennai 2-Jan-12 Hard First Round 120 102 0 2 ... 2 45 70 18 35 2 4 36 103 99 Yuki Bhambri Karol Beck Chennai 2-Jan-12 Hard First Round 345 101 0 2 ... 1 15 33 13 29 2 3 15 46 43 Yuichi Sugita Olivier Rochus Chennai 2-Jan-12 Hard First Round 235 67 0 2 ... 0 19 32 13 22 1 7 30 78 62 Benoit Paire Pere Riba Chennai 2-Jan-12 Hard First Round 95 89 0 2 ... 5 13 20 12 32 0 1 9 44 34 Victor Hanescu Sam Querrey Chennai 2-Jan-12 Hard First Round 90 93 0 2 ... 8 29 41 7 24 1 3 17 57 53 Yen-Hsun Lu Thiemo de Bakker Chennai 2-Jan-12 Hard First Round 82 223 0 2 ... 0 20 32 10 24 1 1 19 57 49 Andreas Beck Vasek Pospisil Chennai 2-Jan-12 Hard First Round 98 119 0 2 ... 3 39 57 16 38 1 5 24 74 79 Ivan Dodig Vishnu Vardhan Chennai 2-Jan-12 Hard First Round 36 313 0 2 ... 5 41 59 13 27 2 8 34 101 88 David Goffin Xavier Malisse Chennai 2-Jan-12 Hard First Round 174 49 0 2 ... 1 31 43 19 34 1 4 27 85 77 David Goffin Andreas Beck Chennai 2-Jan-12 Hard Second Round 174 98 0 2 ... 0 43 71 14 27 2 8 27 82 84 Dudi Sela Benoit Paire Chennai 2-Jan-12 Hard Second Round 83 95 0 2 ... 5 40 58 21 46 1 7 26 87 87 Stan Wawrinka Edouard Roger-Vasselin Chennai 2-Jan-12 Hard Second Round 17 106 0 2 ... 0 43 70 16 34 4 6 28 82 87 Go Soeda Ivan Dodig Chennai 2-Jan-12 Hard Second Round 120 36 0 2 ... 2 31 41 11 28 1 4 23 73 65 Milos Raonic Victor Hanescu Chennai 2-Jan-12 Hard Second Round 31 90 0 2 ... 1 25 38 5 14 0 4 15 56 45 Yuichi Sugita Yen-Hsun Lu Chennai 2-Jan-12 Hard Second Round 235 82 0 2 ... 4 34 45 12 34 2 9 38 93 84 Janko Tipsarevic Yuki Bhambri Chennai 2-Jan-12 Hard Second Round 9 345 0 2 ... 2 12 22 9 17 0 1 8 41 29 Janko Tipsarevic David Goffin Chennai 2-Jan-12 Hard Quarter-finals 9 174 0 2 ... 5 34 51 19 40 1 2 18 67 71 Milos Raonic Dudi Sela Chennai 2-Jan-12 Hard Quarter-finals 31 83 0 2 ... 2 23 31 19 28 0 3 16 69 58 Go Soeda Stan Wawrinka Chennai 2-Jan-12 Hard Quarter-finals 120 17 0 2 ... 4 18 34 13 31 3 7 31 74 62 Nicolas Almagro Yuichi Sugita Chennai 2-Jan-12 Hard Quarter-finals 10 235 0 2 ... 1 36 65 30 40 3 12 45 123 111 Janko Tipsarevic Go Soeda Chennai 2-Jan-12 Hard Semi-finals 9 120 0 2 ... 1 21 33 10 28 1 1 10 44 41 Milos Raonic Nicolas Almagro Chennai 2-Jan-12 Hard Semi-finals 31 10 0 2 ... 0 31 45 8 15 0 3 12 54 51 Milos Raonic Janko Tipsarevic Chennai 2-Jan-12 Hard Finals 31 9 0 2 ... 2 59 83 34 55 0 4 25 113 118 Igor Andreev Adrian Mannarino Brisbane 2-Jan-12 Hard First Round 115 87 0 2 ... 3 24 35 13 25 1 3 21 70 58 Alexandr Dolgopolov Alejandro Falla Brisbane 2-Jan-12 Hard First Round 15 74 0 2 ... 3 16 33 12 25 3 7 33 75 61 Tatsuma Ito Benjamin Mitchell Brisbane 2-Jan-12 Hard First Round 122 227 0 2 ... 6 30 44 7 24 0 2 13 52 50 Kei Nishikori Cedrik-Marcel Stebe Brisbane 2-Jan-12 Hard First Round 25 81 0 2 ... 2 27 49 23 41 3 6 28 75 78 Denis Istomin Florian Mayer Brisbane 2-Jan-12 Hard First Round 73 23 1 1 ... 1 28 38 11 17 0 2 15 56 54 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Malek Jaziri Fernando Verdasco Paris 29-Oct-18 Indoor Hard Second Round 55 27 0 2 ... 6 46 60 16 35 3 13 39 104 101 Alexander Zverev Frances Tiafoe Paris 29-Oct-18 Indoor Hard Second Round 5 44 0 2 ... 4 26 40 16 36 2 10 27 72 69 Dominic Thiem Gilles Simon Paris 29-Oct-18 Indoor Hard Second Round 8 31 0 2 ... 1 13 26 12 25 2 2 23 59 48 Novak Djokovic Joao Sousa Paris 29-Oct-18 Indoor Hard Second Round 2 48 0 2 ... 2 25 35 6 22 1 10 27 74 58 Karen Khachanov Matthew Ebden Paris 29-Oct-18 Indoor Hard Second Round 18 39 1 2 ... 6 8 18 5 20 1 2 10 30 23 John Isner Mikhail Kukushkin Paris 29-Oct-18 Indoor Hard Second Round 9 54 0 2 ... 1 54 80 24 39 0 1 13 90 91 Kevin Anderson Nikoloz Basilashvili Paris 29-Oct-18 Indoor Hard Second Round 6 22 0 2 ... 7 43 54 30 49 0 3 26 106 99 Marin Cilic Philipp Kohlschreiber Paris 29-Oct-18 Indoor Hard Second Round 7 43 0 2 ... 1 19 34 12 20 1 1 17 55 48 Jack Sock Richard Gasquet Paris 29-Oct-18 Indoor Hard Second Round 23 28 0 2 ... 4 18 33 16 29 0 4 19 59 53 Grigor Dimitrov Roberto Bautista Agut Paris 29-Oct-18 Indoor Hard Second Round 10 25 0 2 ... 0 34 48 11 20 2 4 27 76 72 Damir Dzumhur Stefanos Tsitsipas Paris 29-Oct-18 Indoor Hard Second Round 52 16 0 2 ... 3 14 26 15 30 2 2 17 52 46 Dominic Thiem Borna Coric Paris 29-Oct-18 Indoor Hard Third Round 8 13 0 2 ... 1 39 57 16 38 2 2 27 88 82 Novak Djokovic Damir Dzumhur Paris 29-Oct-18 Indoor Hard Third Round 2 52 1 2 ... 4 15 28 7 18 0 0 8 28 30 Alexander Zverev Diego Schwartzman Paris 29-Oct-18 Indoor Hard Third Round 5 19 0 2 ... 2 22 37 12 24 0 4 18 58 52 Roger Federer Fabio Fognini Paris 29-Oct-18 Indoor Hard Third Round 3 14 0 2 ... 6 22 32 15 37 1 5 16 54 53 Marin Cilic Grigor Dimitrov Paris 29-Oct-18 Indoor Hard Third Round 7 10 0 2 ... 1 37 55 14 32 1 5 22 71 73 Karen Khachanov John Isner Paris 29-Oct-18 Indoor Hard Third Round 18 9 0 2 ... 4 67 80 19 38 0 0 17 100 103 Kei Nishikori Kevin Anderson Paris 29-Oct-18 Indoor Hard Third Round 11 6 0 2 ... 1 26 33 11 19 0 0 11 51 48 Jack Sock Malek Jaziri Paris 29-Oct-18 Indoor Hard Third Round 23 55 0 2 ... 6 13 21 10 24 0 0 9 41 32 Karen Khachanov Alexander Zverev Paris 29-Oct-18 Indoor Hard Quarter-finals 18 5 0 2 ... 7 26 47 4 21 1 3 10 36 40 Dominic Thiem Jack Sock Paris 29-Oct-18 Indoor Hard Quarter-finals 8 23 0 2 ... 5 44 59 19 37 2 10 34 97 97 Roger Federer Kei Nishikori Paris 29-Oct-18 Indoor Hard Quarter-finals 3 11 0 2 ... 0 21 37 16 26 0 1 12 56 49 Novak Djokovic Marin Cilic Paris 29-Oct-18 Indoor Hard Quarter-finals 2 7 0 2 ... 0 38 55 11 28 2 5 29 85 78 Karen Khachanov Dominic Thiem Paris 29-Oct-18 Indoor Hard Semi-finals 18 8 0 2 ... 0 19 29 8 26 1 3 15 47 42 Novak Djokovic Roger Federer Paris 29-Oct-18 Indoor Hard Semi-finals 2 3 0 2 ... 2 69 93 25 46 1 2 29 113 123 Karen Khachanov Novak Djokovic Paris 29-Oct-18 Indoor Hard Finals 18 2 0 2 ... 1 30 43 14 28 1 5 20 66 64 Jaume Antoni Munar Clar Frances Tiafoe Milan 5-Nov-18 Indoor Hard NA 76 40 0 3 ... 3 21 29 6 17 0 2 5 46 32 Frances Tiafoe Hubert Hurkacz Milan 5-Nov-18 Indoor Hard NA 40 85 0 3 ... 4 35 48 10 19 1 7 22 78 67 Hubert Hurkacz Jaume Antoni Munar Clar Milan 5-Nov-18 Indoor Hard NA 85 76 0 3 ... 1 43 63 15 35 3 9 29 80 87 Andrey Rublev Liam Caruana Milan 5-Nov-18 Indoor Hard NA 68 NA 0 3 ... 1 28 39 4 14 1 3 18 57 50 As we can see, we have a Winner column, a Loser column, as well as other columns detailing the match details, and other columns which have the stats for that match. As we have a Winner column, if we use the current data structure to train a model we will leak the result. The model will simply learn that the actual winner comes from the Winner column, rather than learning from other features that we can create, such as First Serve % . To avoid this problem, let's reshape the data from wide to long, then shuffle the data. For this, we will define a function, split_winner_loser_columns , which splits the raw data frame into two data frames, appends them together, and then shuffles the data. Let's also remove all Grass and Clay matches from our data, as we will be modelling the Australian Open which is a hardcourt surface. Additionally, we will add a few columns, such as Match_Id and Total_Games . These will be useful later. split_winner_loser_columns <- function ( df ) { # This function splits the raw data into two data frames and appends them together then shuffles them # This output is a data frame with only one player's stats on each row (i.e. in long format) # Grab a df with only the Winner's stats winner = df %>% select ( - contains ( \"Loser\" )) %>% # Select only the Winner columns + extra game info columns as a df rename_at ( # Rename all columns containing \"Winner\" to \"Player\" vars ( contains ( \"Winner\" )), ~ str_replace ( . , \"Winner\" , \"Player\" ) ) %>% mutate ( Winner = 1 ) # Create a target column # Repeat the process with the loser's stats loser = df %>% select ( - contains ( \"Winner\" )) %>% rename_at ( vars ( contains ( \"Loser\" )), ~ str_replace ( . , \"Loser\" , \"Player\" ) ) %>% mutate ( Winner = 0 ) set.seed ( 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) # Create a df that appends both the Winner and loser df together combined_df = winner %>% rbind ( loser ) %>% # Append the loser df to the Winner df slice ( sample ( 1 : n ())) %>% # Randomise row order arrange ( Match_Id ) %>% # Arrange by Match_Id return () } # Read in men and womens data; randomise the data to avoid result leakage mens = readr :: read_csv ( 'data/ATP_matches.csv' , na = \".\" ) %>% filter ( Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% # Filter to only use hardcourt games mutate ( Match_Id = row_number (), # Add a match ID column to be used as a key Tournament_Date = dmy ( Tournament_Date ), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won ) %>% # Add a total games played column split_winner_loser_columns () # Change the data frame from wide to long mens %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Player Tournament Tournament_Date Court_Surface Round_Description Player_Rank Retirement_Ind Player_Sets_Won Player_Games_Won Player_Aces ... Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Match_Id Total_Games Winner Eric Prodon Chennai 2012-01-02 Hard First Round 97 0 0 7 2 ... 13 26 1 3 15 49 49 1 19 0 Edouard Roger-Vasselin Chennai 2012-01-02 Hard First Round 106 0 2 12 5 ... 12 19 4 7 25 59 59 1 19 1 Dudi Sela Chennai 2012-01-02 Hard First Round 83 0 2 12 2 ... 11 16 6 14 36 58 61 2 13 1 Fabio Fognini Chennai 2012-01-02 Hard First Round 48 0 0 1 1 ... 5 26 0 1 8 33 30 2 13 0 Frederico Gil Chennai 2012-01-02 Hard First Round 102 0 1 14 5 ... 18 35 2 4 36 103 99 3 33 0 Go Soeda Chennai 2012-01-02 Hard First Round 120 0 2 19 6 ... 19 39 5 11 42 105 109 3 33 1 Feature Creation Now that we have a fairly good understanding of what the data looks like, let's add some features. To do this we will define a function. Ideally we want to add features which will provide predictive power to our model. Thinking about the dynamics of tennis, we know that players often will matches by \"breaking\" the opponent's serve (i.e. winning a game when the opponent is serving). This is especially important in mens tennis. Let's create a feature called F_Player_BreakPoints_Per_Game , which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let's also create a feature called F_Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \"holding\" serve is important (i.e. winning a game when you are serving). Let's create a feature called F_Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let's create a feature called F_Player_Game_Win_Percentage which is the propotion of games that a player wins. add_ratio_features <- function ( df ) { # This function adds ratio features to a long df df %>% mutate ( # Point Win ratio when serving F_Player_Serve_Win_Ratio = ( Player_FirstServes_Won + Player_SecondServes_Won - Player_DoubleFaults ) / ( Player_FirstServes_In + Player_SecondServes_In + Player_DoubleFaults ), # Point win ratio when returning F_Player_Return_Win_Ratio = Player_ReturnPoints_Won / Player_ReturnPoints_Faced , # Breakpoints per receiving game F_Player_BreakPoints_Per_Game = Player_BreakPoints / Total_Games , F_Player_Game_Win_Percentage = Player_Games_Won / Total_Games ) %>% mutate_at ( vars ( colnames ( . ), - contains ( \"Rank\" ), - Tournament_Date ), # Replace all NAs with0 apart from Rank, Date ~ ifelse ( is.na ( . ), 0 , . ) ) %>% return () } mens = mens %>% add_ratio_features () # Add features Now that we have added our features, we need to create rolling averages for them. We cannot simply use current match statistics, as they will leak the result to the model. Instead, we need to use past match statistics to predict future matches. Here we will use a rolling mean with a window of 15. If the player hasn't played 15 games, we will instead use a cumulative mean. We will also lag the result so as to not leak the result. This next chunk of code simply takes all the columns starting with F_ and calculates these means. mens = mens %>% group_by ( Player ) %>% # Group by player mutate_at ( # Create a rolling mean with window 15 for each player. vars ( starts_with ( \"F_\" )), # If the player hasn't played 15 games, use a cumulative mean ~ coalesce ( rollmean ( . , k = 15 , align = \"right\" , fill = NA_real_ ), cummean ( . )) %>% lag () ) %>% ungroup () Creating a Training Feature Matrix In predictive modelling language - features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options We can train the model on every tennis match in the data set, or We can only train the model on Australian Open matches. Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. We have decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface - hard court. However, we also need to train our model in the same way that will be used to predict the 2019 Australian Open. When predicting the 2nd round, we won't have data from the 1st round. So we will need to build our training feature matrix with this in mind. We should extract features for a player from past games at the start of the tournament and apply them to every matchup that that player plays. To do this, we will create a function, extract_latest_features_for_tournament , which maps over our feature data frame for the dates in the first round of a tournament and grabs features. First, we need the Australian Open and US Open results - let's grab these and then apply our function. # Get Australian Open and US Open Results aus_us_open_results = mens %>% filter (( Tournament == \"Australian Open, Melbourne\" | Tournament == \"U.S. Open, New York\" ) & Round_Description != \"Qualifying\" & Tournament_Date != \"2012-01-16\" ) %>% # Filter out qualifiers select ( Match_Id , Player , Tournament , Tournament_Date , Round_Description , Winner ) # Create a function which extracts features for each tournament extract_latest_features_for_tournament = function ( df , dte ) { df %>% # Filter for the 1st round filter ( Tournament_Date == dte , Round_Description == \"First Round\" , Tournament_Date != \"2012-01-16\" ) %>% group_by ( Player ) %>% # Group by player select_at ( vars ( Match_Id , starts_with ( \"F_\" ), Player_Rank ) # Grab the players' features ) %>% rename ( F_Player_Rank = Player_Rank ) %>% ungroup () %>% mutate ( Feature_Date = dte ) %>% select ( Player , Feature_Date , everything ()) } # Create a feature matrix in long format feature_matrix_long = aus_us_open_results %>% distinct ( Tournament_Date ) %>% # Pull all Tournament Dates pull () %>% map_dfr ( ~ extract_latest_features_for_tournament ( mens , . ) # Get the features ) %>% filter ( Feature_Date != \"2012-01-16\" ) %>% # Filter out the first Aus Open mutate_at ( # Replace NAs with the mean vars ( starts_with ( \"F_\" )), ~ ifelse ( is.na ( . ), mean ( . , na.rm = TRUE ), . ) ) Now that we have a feature matrix in long format, we need to convert it to wide format so that the features are on the same row. To do this we will define a function gather_df , which converts the data frame from long to wide. Let's also join the results to the matrix and convert the Winner column to a factor. Finally, we will take the difference of player1 and player2's features, so as to reduce the dimensionality of the model. gather_df <- function ( df ) { # This function puts the df back into its original format of each row containing stats for both players df %>% arrange ( Match_Id ) %>% filter ( row_number () %% 2 != 0 ) %>% # Filter for every 2nd row, starting at the 1st index. e.g. 1, 3, 5 rename_at ( # Rename columns to player_1 vars ( contains ( \"Player\" )), ~ str_replace ( . , \"Player\" , \"player_1\" ) ) %>% inner_join ( df %>% filter ( row_number () %% 2 == 0 ) %>% rename_at ( vars ( contains ( \"Player\" )), # Rename columns to player_2 ~ str_replace ( . , \"Player\" , \"player_2\" ) ) %>% select ( Match_Id , contains ( \"Player\" )), by = c ( 'Match_Id' ) ) %>% select ( Match_Id , player_1 , player_2 , Winner , everything ()) %>% return () } # Joining results to features feature_matrix_wide = aus_us_open_results %>% inner_join ( feature_matrix_long %>% select ( - Match_Id ), by = c ( \"Player\" , \"Tournament_Date\" = \"Feature_Date\" )) %>% gather_df () %>% mutate ( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio , F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio , F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage , F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game , F_Rank_Diff = ( F_player_1_Rank - F_player_2_Rank ), Winner = as.factor ( Winner ) ) %>% select ( Match_Id , player_1 , player_2 , Tournament , Tournament_Date , Round_Description , Winner , contains ( \"Diff\" )) train = feature_matrix_wide train %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Match_Id player_1 player_2 Tournament Tournament_Date Round_Description Winner F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff 1139 Adrian Ungur Daniel Brands U.S. Open, New York 2012-08-27 First Round 0 0.03279412 -0.014757229 0.002877458 0.073938088 -13 1140 Albert Montanes Richard Gasquet U.S. Open, New York 2012-08-27 First Round 0 -0.08000322 -0.077451342 -0.131108056 -0.180846832 97 1141 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 First Round 1 0.07711693 -0.044715517 0.068179841 -0.087361962 1 1142 Alex Bogomolov Jr. Andy Murray U.S. Open, New York 2012-08-27 First Round 0 -0.03964074 -0.031700826 -0.059010072 -0.094721700 69 1143 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 First Round 1 -0.02681392 0.006442134 -0.067779660 -0.009930089 151 1144 Ryan Harrison Benjamin Becker U.S. Open, New York 2012-08-27 First Round 1 0.04251983 0.018604623 0.026486753 -0.003548973 -24 Creating the Feature Matrix for the 2019 Australian Open Now that we have our training set, train , we need to create a feature matrix to create predictions on. To do this, we need to generate features again. We could simply append a player list to our raw data frame, create a mock date and then use the extract_latest_features_for_tournament function that we used before. Instead, we're going to create a lookup table for each unique player in the 2019 Australian Open. We will need to get their last 15 games and then find the mean for each feature so that our features are the same. Let's first explore what the dummy submission file looks like, then use it to get the unique players. read_csv ( 'data/men_dummy_submission_file.csv' ) %>% glimpse () As we can see, the dummy submission file contains every potential match up for the Open. This will be updated a few days before the Open starts with the actual players playing. Let's now create the lookup feature table. # Get a vector of unique players in this years' open using the dummy submission file unique_players = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% pull ( player_1 ) %>% unique () # Get the last 15 games played for each unique player and find their features lookup_feature_table = read_csv ( 'data/ATP_matches.csv' , na = \".\" ) %>% filter ( Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% mutate ( Match_Id = row_number (), # Add a match ID column to be used as a key Tournament_Date = dmy ( Tournament_Date ), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won ) %>% # Add a total games played column # clean_missing_data() %>% # Clean missing data split_winner_loser_columns () %>% # Change the data frame from wide to long add_ratio_features () %>% filter ( Player %in% unique_players ) %>% group_by ( Player ) %>% top_n ( 15 , Match_Id ) %>% summarise ( F_Player_Serve_Win_Ratio = mean ( F_Player_Serve_Win_Ratio ), F_Player_Return_Win_Ratio = mean ( F_Player_Return_Win_Ratio ), F_Player_BreakPoints_Per_Game = mean ( F_Player_BreakPoints_Per_Game ), F_Player_Game_Win_Percentage = mean ( F_Player_Game_Win_Percentage ), F_Player_Rank = last ( Player_Rank ) ) Now let's create features for every single combination. To do this we'll join our lookup_feature_table to the player_1 and player_2 columns in the dummy_submission_file . # Create feature matrix for the Australian Open for all player 1s features_player_1 = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% select ( player_1 ) %>% inner_join ( lookup_feature_table , by = c ( \"player_1\" = \"Player\" )) %>% rename ( F_player_1_Serve_Win_Ratio = F_Player_Serve_Win_Ratio , F_player_1_Return_Win_Ratio = F_Player_Return_Win_Ratio , F_player_1_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game , F_player_1_Game_Win_Percentage = F_Player_Game_Win_Percentage , F_player_1_Rank = F_Player_Rank ) # Create feature matrix for the Australian Open for all player 2s features_player_2 = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% select ( player_2 ) %>% inner_join ( lookup_feature_table , by = c ( \"player_2\" = \"Player\" )) %>% rename ( F_player_2_Serve_Win_Ratio = F_Player_Serve_Win_Ratio , F_player_2_Return_Win_Ratio = F_Player_Return_Win_Ratio , F_player_2_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game , F_player_2_Game_Win_Percentage = F_Player_Game_Win_Percentage , F_player_2_Rank = F_Player_Rank ) # Join the two dfs together and subtract features to create Difference features aus_open_2019_features = features_player_1 %>% bind_cols ( features_player_2 ) %>% select ( player_1 , player_2 , everything ()) %>% mutate ( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio , F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio , F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage , F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game , F_Rank_Diff = ( F_player_1_Rank - F_player_2_Rank ) ) %>% select ( player_1 , player_2 , contains ( \"Diff\" )) aus_open_2019_features %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) player_1 player_2 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff Novak Djokovic Rafael Nadal 0.06347805 0.02503802 0.07002382 0.08951024 1 Novak Djokovic Roger Federer 0.06583364 0.03628491 0.07661295 0.15455628 -1 Novak Djokovic Juan Martin del Potro 0.01067079 0.03436023 0.06382353 0.11259979 -2 Novak Djokovic Alexander Zverev 0.11117863 0.03125651 0.11055585 0.08661036 -3 Novak Djokovic Kevin Anderson 0.02132375 0.10449337 0.11184503 0.23684083 -4 Novak Djokovic Marin Cilic 0.08410746 0.02434916 0.07653035 0.08355134 -5 Generating 2019 Australian Open Predictions Now that we have our features, we can finally train our model and generate predictions for the 2019 Australian Open. Due to its simplicity, we will use h2o's Auto Machine Learning function h2o.automl . This will train a heap of different models and optimise the hyperparameters, as well as creating stacked ensembles automatically for us. We will use optimising by log loss. First, we must create h2o frames for our training and feature data frames. Then we will run h2o.automl . Note that we can set the max_runtime_secs parameter. As this is a notebook, I have set it for 30 seconds - but I suggest you give it 10 minutes to create the best model. We can then create our predictions and assign them back to our aus_open_2019_features data frame. Finally, we will group_by player and find the best player, on average. ## Setup H2O h2o.init ( ip = \"localhost\" , port = 54321 , enable_assertions = TRUE , nthreads = 2 , max_mem_size = \"24g\" ) ## Sending file to h2o train_h2o = feature_matrix_wide %>% select ( contains ( \"Diff\" ), Winner ) %>% as.h2o ( destination_frame = \"train_h2o\" ) aus_open_2019_features_h2o = aus_open_2019_features %>% select ( contains ( \"Diff\" )) %>% as.h2o ( destination_frame = \"aus_open_2019_features_h2o\" ) ## Running Auto ML mens_model = h2o.automl ( y = \"Winner\" , training_frame = train_h2o , max_runtime_secs = 30 , max_models = 100 , stopping_metric = \"logloss\" , sort_metric = \"logloss\" , balance_classes = TRUE , seed = 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) ## Predictions on test frame predictions = h2o.predict ( mens_model @ leader , aus_open_2019_features_h2o ) %>% as.data.frame () aus_open_2019_features $ prob_player_1 = predictions $ p1 aus_open_2019_features $ prob_player_2 = predictions $ p0 h2o.shutdown ( prompt = FALSE ) Now let's find the best player by taking the mean of the prediction probability by player. aus_open_2019_features %>% select ( player_1 , starts_with ( \"F_\" ), prob_player_1 ) %>% group_by ( player_1 ) %>% summarise_all ( mean ) %>% arrange ( desc ( prob_player_1 )) %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) player_1 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff prob_player_1 Novak Djokovic 0.1109364627 0.076150615 0.1483970690 0.17144300 NA 0.8616486 Karen Khachanov 0.0960639298 0.061436164 0.1059967623 0.04544955 NA 0.8339594 Juan Martin del Potro 0.1003931993 0.042025222 0.0847985439 0.05943767 NA 0.8218308 Rafael Nadal 0.0480432305 0.051531252 0.0790179917 0.08181694 NA 0.8032543 Gilles Simon 0.0646937767 0.084843307 0.0901401318 0.08675350 NA 0.7985995 Roger Federer 0.0452014997 0.040992497 0.0725719954 0.01817046 NA 0.7962289 Kei Nishikori 0.0777155934 0.018720226 0.0800648870 0.02740276 NA 0.7843631 Marin Cilic 0.0285413602 0.053017465 0.0736687072 0.08883055 NA 0.7804876 Tomas Berdych 0.0471654691 0.047289449 0.0737401748 0.10584114 NA 0.7739211 Daniil Medvedev 0.0275430665 0.031121856 0.0721948279 0.01803757 NA 0.7543269 Stefanos Tsitsipas 0.0470382377 0.023825850 0.0577628626 0.02105227 NA 0.7511674 Dominic Thiem 0.0258904189 0.032481624 0.0483707080 0.05857158 NA 0.7451547 Alexander Zverev 0.0006199716 0.044811275 0.0380134371 0.08423392 NA 0.7374897 Kyle Edmund 0.0558006240 0.011963627 0.0478850676 0.05142186 NA 0.7304873 Pablo Carreno Busta 0.0321878318 0.029862068 0.0413674481 -0.00229784 NA 0.7302043 Borna Coric 0.0762084129 -0.010097922 0.0413621283 -0.01924267 NA 0.7268124 Kevin Anderson 0.0907358428 -0.027171681 0.0381421997 -0.06362578 NA 0.7260799 David Goffin -0.0034821911 0.037247336 0.0162572061 0.05603565 NA 0.7155908 Fernando Verdasco 0.0229261365 0.032884054 0.0521212576 0.04668854 NA 0.7120831 Roberto Bautista Agut 0.0047641170 0.049939608 0.0218975349 0.07331023 NA 0.7009891 Milos Raonic 0.0849726089 -0.028732182 0.0385944327 -0.08009382 NA 0.6986865 Fabio Fognini -0.0394792678 0.047935185 0.0226546894 0.06213496 NA 0.6982031 Hyeon Chung 0.0042489153 0.047722133 0.0158096386 0.04823304 NA 0.6958943 Jack Sock -0.0099659903 0.026454984 0.0186547428 0.02307214 NA 0.6757770 Diego Schwartzman -0.0317130675 0.032098381 0.0006215006 0.05621187 NA 0.6631067 John Millman 0.0016290285 0.042676556 0.0119857356 0.06228135 NA 0.6603912 Nikoloz Basilashvili -0.0099968609 0.005561102 0.0473876170 0.03661962 NA 0.6602628 John Isner 0.1346946527 -0.070556940 0.0161348609 -0.11425009 NA 0.6598097 Gael Monfils -0.0074254934 0.024286746 0.0295568649 0.04007519 NA 0.6449506 Richard Gasquet 0.0296009556 -0.011382437 0.0013138324 -0.03972967 NA 0.6442043 ... ... ... ... ... ... ... Laslo Djere -0.042300822 -0.0150684095 -0.064667709 -0.0349151578 NA 0.3606923 David Ferrer -0.036179509 0.0532782117 0.012751020 0.0914824480 NA 0.3488057 Bradley Klahn -0.001248083 -0.0444982448 -0.025987040 -0.1181295700 NA 0.3487806 Marcel Granollers -0.031011830 -0.0094056152 -0.049853664 0.0136841358 NA 0.3460035 Ricardas Berankis -0.022557215 -0.0103782963 -0.047937290 -0.0468488990 NA 0.3454980 Radu Albot -0.040829057 0.0076150564 -0.034891704 0.0443672533 NA 0.3420615 Jordan Thompson -0.068554906 0.0261969117 -0.044349181 0.0206636045 NA 0.3358572 Thomas Fabbiano -0.060583307 0.0275756029 -0.025883493 0.0709707306 NA 0.3319778 Roberto Carballes Baena -0.054016396 -0.0091521177 -0.019093050 0.0347187874 NA 0.3312105 Paolo Lorenzi -0.038613500 -0.0212206827 -0.052602703 0.0199474025 NA 0.3299791 Guido Andreozzi -0.038614385 -0.0133763922 0.029549861 0.0636745661 NA 0.3288762 Peter Polansky 0.007461636 -0.0163389196 -0.024034159 -0.0442144260 NA 0.3216756 Ernests Gulbis -0.062827089 -0.0134699552 -0.027633425 -0.0518663252 NA 0.3123511 Thiago Monteiro 0.001235931 -0.0288349103 -0.043831840 -0.0654744344 NA 0.3122069 Casper Ruud 0.016838968 -0.0178511679 0.015234507 0.0219131874 NA 0.3119321 Marco Trungelliti -0.022148774 -0.0005658242 0.048542554 0.1243537739 NA 0.3092636 Jiri Vesely -0.050204009 -0.0351868278 -0.042887646 -0.0160467165 NA 0.3089287 Guillermo Garcia-Lopez -0.090076100 -0.0108663630 -0.048712763 -0.0124446402 NA 0.3080898 Michael Mmoh -0.063802934 -0.0079053251 -0.011112236 -0.0332042032 NA 0.2822330 Jason Kubler -0.124758873 -0.0202756806 -0.013998570 0.1020895301 NA 0.2814246 Ruben Bemelmans -0.029036164 -0.0138846550 -0.032256254 -0.0363563402 NA 0.2772185 Bjorn Fratangelo -0.014149222 0.0033574304 -0.019931504 -0.0360199607 NA 0.2652527 Pablo Andujar -0.042869833 -0.0488261697 -0.070057834 -0.0164918910 NA 0.2647100 Christian Garin -0.046150875 0.0235799476 -0.006209664 0.0736304057 NA 0.2631607 Ivo Karlovic 0.071597162 -0.1093833837 0.001410787 -0.1237762218 NA 0.2500242 Juan Ignacio Londero -0.026454456 -0.0715665271 -0.016749898 -0.0363353678 NA 0.2351747 Ramkumar Ramanathan -0.005371622 -0.0606138479 -0.041631884 -0.0005573405 NA 0.2272977 Reilly Opelka 0.025704824 -0.0607219257 -0.015474944 -0.0720809006 NA 0.2262993 Carlos Berlocq -0.063580460 0.0074576369 -0.054277974 -0.0165235079 NA 0.2112275 Pedro Sousa -0.197333352 -0.0734557562 -0.161962722 -0.1023311674 NA 0.1502313 Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Aus Open R Tutorial"},{"location":"modelling/AusOpenRTutorial/#australian-open-datathon-r-tutorial","text":"","title":"Australian Open Datathon R Tutorial"},{"location":"modelling/AusOpenRTutorial/#overview","text":"","title":"Overview"},{"location":"modelling/AusOpenRTutorial/#the-task","text":"This notebook will outline how the Betfair Data Scientists went about modelling the Australian Open for Betfair's Australian Open Datathon. The task is simple: we ask you to predict the winner of every possible Australian Open matchup using data which we provide. The metric used to determine the winner will be log loss, based on the actual matchups that happen in the Open. For more information on log loss, click here . For a detailed outline of the task, the prizes, and to sign up, click here . How an outline of our methodology and thought process, read this article.","title":"The Task"},{"location":"modelling/AusOpenRTutorial/#exploring-the-data","text":"First we need to get an idea of what the data looks like. Let's read the men's data in and get an idea of what it looks like. Note that you will need to install all the packages listed below unless you already have them. Note that for this tutorial I will be using dplyr , if you are not familiar with the syntax I encourage you to read up on the basics . # Import libraries library ( dplyr ) library ( readr ) library ( tidyr ) library ( RcppRoll ) library ( tidyselect ) library ( lubridate ) library ( stringr ) library ( zoo ) library ( purrr ) library ( h2o ) library ( DT ) mens = readr :: read_csv ( 'data/ATP_matches.csv' , na = \".\" ) # NAs are indicated by . mens %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Winner Loser Tournament Tournament_Date Court_Surface Round_Description Winner_Rank Loser_Rank Retirement_Ind Winner_Sets_Won ... Loser_DoubleFaults Loser_FirstServes_Won Loser_FirstServes_In Loser_SecondServes_Won Loser_SecondServes_In Loser_BreakPoints_Won Loser_BreakPoints Loser_ReturnPoints_Won Loser_ReturnPoints_Faced Loser_TotalPoints_Won Edouard Roger-Vasselin Eric Prodon Chennai 2-Jan-12 Hard First Round 106 97 0 2 ... 3 21 33 13 26 1 3 15 49 49 Dudi Sela Fabio Fognini Chennai 2-Jan-12 Hard First Round 83 48 0 2 ... 4 17 32 5 26 0 1 8 33 30 Go Soeda Frederico Gil Chennai 2-Jan-12 Hard First Round 120 102 0 2 ... 2 45 70 18 35 2 4 36 103 99 Yuki Bhambri Karol Beck Chennai 2-Jan-12 Hard First Round 345 101 0 2 ... 1 15 33 13 29 2 3 15 46 43 Yuichi Sugita Olivier Rochus Chennai 2-Jan-12 Hard First Round 235 67 0 2 ... 0 19 32 13 22 1 7 30 78 62 Benoit Paire Pere Riba Chennai 2-Jan-12 Hard First Round 95 89 0 2 ... 5 13 20 12 32 0 1 9 44 34 Victor Hanescu Sam Querrey Chennai 2-Jan-12 Hard First Round 90 93 0 2 ... 8 29 41 7 24 1 3 17 57 53 Yen-Hsun Lu Thiemo de Bakker Chennai 2-Jan-12 Hard First Round 82 223 0 2 ... 0 20 32 10 24 1 1 19 57 49 Andreas Beck Vasek Pospisil Chennai 2-Jan-12 Hard First Round 98 119 0 2 ... 3 39 57 16 38 1 5 24 74 79 Ivan Dodig Vishnu Vardhan Chennai 2-Jan-12 Hard First Round 36 313 0 2 ... 5 41 59 13 27 2 8 34 101 88 David Goffin Xavier Malisse Chennai 2-Jan-12 Hard First Round 174 49 0 2 ... 1 31 43 19 34 1 4 27 85 77 David Goffin Andreas Beck Chennai 2-Jan-12 Hard Second Round 174 98 0 2 ... 0 43 71 14 27 2 8 27 82 84 Dudi Sela Benoit Paire Chennai 2-Jan-12 Hard Second Round 83 95 0 2 ... 5 40 58 21 46 1 7 26 87 87 Stan Wawrinka Edouard Roger-Vasselin Chennai 2-Jan-12 Hard Second Round 17 106 0 2 ... 0 43 70 16 34 4 6 28 82 87 Go Soeda Ivan Dodig Chennai 2-Jan-12 Hard Second Round 120 36 0 2 ... 2 31 41 11 28 1 4 23 73 65 Milos Raonic Victor Hanescu Chennai 2-Jan-12 Hard Second Round 31 90 0 2 ... 1 25 38 5 14 0 4 15 56 45 Yuichi Sugita Yen-Hsun Lu Chennai 2-Jan-12 Hard Second Round 235 82 0 2 ... 4 34 45 12 34 2 9 38 93 84 Janko Tipsarevic Yuki Bhambri Chennai 2-Jan-12 Hard Second Round 9 345 0 2 ... 2 12 22 9 17 0 1 8 41 29 Janko Tipsarevic David Goffin Chennai 2-Jan-12 Hard Quarter-finals 9 174 0 2 ... 5 34 51 19 40 1 2 18 67 71 Milos Raonic Dudi Sela Chennai 2-Jan-12 Hard Quarter-finals 31 83 0 2 ... 2 23 31 19 28 0 3 16 69 58 Go Soeda Stan Wawrinka Chennai 2-Jan-12 Hard Quarter-finals 120 17 0 2 ... 4 18 34 13 31 3 7 31 74 62 Nicolas Almagro Yuichi Sugita Chennai 2-Jan-12 Hard Quarter-finals 10 235 0 2 ... 1 36 65 30 40 3 12 45 123 111 Janko Tipsarevic Go Soeda Chennai 2-Jan-12 Hard Semi-finals 9 120 0 2 ... 1 21 33 10 28 1 1 10 44 41 Milos Raonic Nicolas Almagro Chennai 2-Jan-12 Hard Semi-finals 31 10 0 2 ... 0 31 45 8 15 0 3 12 54 51 Milos Raonic Janko Tipsarevic Chennai 2-Jan-12 Hard Finals 31 9 0 2 ... 2 59 83 34 55 0 4 25 113 118 Igor Andreev Adrian Mannarino Brisbane 2-Jan-12 Hard First Round 115 87 0 2 ... 3 24 35 13 25 1 3 21 70 58 Alexandr Dolgopolov Alejandro Falla Brisbane 2-Jan-12 Hard First Round 15 74 0 2 ... 3 16 33 12 25 3 7 33 75 61 Tatsuma Ito Benjamin Mitchell Brisbane 2-Jan-12 Hard First Round 122 227 0 2 ... 6 30 44 7 24 0 2 13 52 50 Kei Nishikori Cedrik-Marcel Stebe Brisbane 2-Jan-12 Hard First Round 25 81 0 2 ... 2 27 49 23 41 3 6 28 75 78 Denis Istomin Florian Mayer Brisbane 2-Jan-12 Hard First Round 73 23 1 1 ... 1 28 38 11 17 0 2 15 56 54 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Malek Jaziri Fernando Verdasco Paris 29-Oct-18 Indoor Hard Second Round 55 27 0 2 ... 6 46 60 16 35 3 13 39 104 101 Alexander Zverev Frances Tiafoe Paris 29-Oct-18 Indoor Hard Second Round 5 44 0 2 ... 4 26 40 16 36 2 10 27 72 69 Dominic Thiem Gilles Simon Paris 29-Oct-18 Indoor Hard Second Round 8 31 0 2 ... 1 13 26 12 25 2 2 23 59 48 Novak Djokovic Joao Sousa Paris 29-Oct-18 Indoor Hard Second Round 2 48 0 2 ... 2 25 35 6 22 1 10 27 74 58 Karen Khachanov Matthew Ebden Paris 29-Oct-18 Indoor Hard Second Round 18 39 1 2 ... 6 8 18 5 20 1 2 10 30 23 John Isner Mikhail Kukushkin Paris 29-Oct-18 Indoor Hard Second Round 9 54 0 2 ... 1 54 80 24 39 0 1 13 90 91 Kevin Anderson Nikoloz Basilashvili Paris 29-Oct-18 Indoor Hard Second Round 6 22 0 2 ... 7 43 54 30 49 0 3 26 106 99 Marin Cilic Philipp Kohlschreiber Paris 29-Oct-18 Indoor Hard Second Round 7 43 0 2 ... 1 19 34 12 20 1 1 17 55 48 Jack Sock Richard Gasquet Paris 29-Oct-18 Indoor Hard Second Round 23 28 0 2 ... 4 18 33 16 29 0 4 19 59 53 Grigor Dimitrov Roberto Bautista Agut Paris 29-Oct-18 Indoor Hard Second Round 10 25 0 2 ... 0 34 48 11 20 2 4 27 76 72 Damir Dzumhur Stefanos Tsitsipas Paris 29-Oct-18 Indoor Hard Second Round 52 16 0 2 ... 3 14 26 15 30 2 2 17 52 46 Dominic Thiem Borna Coric Paris 29-Oct-18 Indoor Hard Third Round 8 13 0 2 ... 1 39 57 16 38 2 2 27 88 82 Novak Djokovic Damir Dzumhur Paris 29-Oct-18 Indoor Hard Third Round 2 52 1 2 ... 4 15 28 7 18 0 0 8 28 30 Alexander Zverev Diego Schwartzman Paris 29-Oct-18 Indoor Hard Third Round 5 19 0 2 ... 2 22 37 12 24 0 4 18 58 52 Roger Federer Fabio Fognini Paris 29-Oct-18 Indoor Hard Third Round 3 14 0 2 ... 6 22 32 15 37 1 5 16 54 53 Marin Cilic Grigor Dimitrov Paris 29-Oct-18 Indoor Hard Third Round 7 10 0 2 ... 1 37 55 14 32 1 5 22 71 73 Karen Khachanov John Isner Paris 29-Oct-18 Indoor Hard Third Round 18 9 0 2 ... 4 67 80 19 38 0 0 17 100 103 Kei Nishikori Kevin Anderson Paris 29-Oct-18 Indoor Hard Third Round 11 6 0 2 ... 1 26 33 11 19 0 0 11 51 48 Jack Sock Malek Jaziri Paris 29-Oct-18 Indoor Hard Third Round 23 55 0 2 ... 6 13 21 10 24 0 0 9 41 32 Karen Khachanov Alexander Zverev Paris 29-Oct-18 Indoor Hard Quarter-finals 18 5 0 2 ... 7 26 47 4 21 1 3 10 36 40 Dominic Thiem Jack Sock Paris 29-Oct-18 Indoor Hard Quarter-finals 8 23 0 2 ... 5 44 59 19 37 2 10 34 97 97 Roger Federer Kei Nishikori Paris 29-Oct-18 Indoor Hard Quarter-finals 3 11 0 2 ... 0 21 37 16 26 0 1 12 56 49 Novak Djokovic Marin Cilic Paris 29-Oct-18 Indoor Hard Quarter-finals 2 7 0 2 ... 0 38 55 11 28 2 5 29 85 78 Karen Khachanov Dominic Thiem Paris 29-Oct-18 Indoor Hard Semi-finals 18 8 0 2 ... 0 19 29 8 26 1 3 15 47 42 Novak Djokovic Roger Federer Paris 29-Oct-18 Indoor Hard Semi-finals 2 3 0 2 ... 2 69 93 25 46 1 2 29 113 123 Karen Khachanov Novak Djokovic Paris 29-Oct-18 Indoor Hard Finals 18 2 0 2 ... 1 30 43 14 28 1 5 20 66 64 Jaume Antoni Munar Clar Frances Tiafoe Milan 5-Nov-18 Indoor Hard NA 76 40 0 3 ... 3 21 29 6 17 0 2 5 46 32 Frances Tiafoe Hubert Hurkacz Milan 5-Nov-18 Indoor Hard NA 40 85 0 3 ... 4 35 48 10 19 1 7 22 78 67 Hubert Hurkacz Jaume Antoni Munar Clar Milan 5-Nov-18 Indoor Hard NA 85 76 0 3 ... 1 43 63 15 35 3 9 29 80 87 Andrey Rublev Liam Caruana Milan 5-Nov-18 Indoor Hard NA 68 NA 0 3 ... 1 28 39 4 14 1 3 18 57 50 As we can see, we have a Winner column, a Loser column, as well as other columns detailing the match details, and other columns which have the stats for that match. As we have a Winner column, if we use the current data structure to train a model we will leak the result. The model will simply learn that the actual winner comes from the Winner column, rather than learning from other features that we can create, such as First Serve % . To avoid this problem, let's reshape the data from wide to long, then shuffle the data. For this, we will define a function, split_winner_loser_columns , which splits the raw data frame into two data frames, appends them together, and then shuffles the data. Let's also remove all Grass and Clay matches from our data, as we will be modelling the Australian Open which is a hardcourt surface. Additionally, we will add a few columns, such as Match_Id and Total_Games . These will be useful later. split_winner_loser_columns <- function ( df ) { # This function splits the raw data into two data frames and appends them together then shuffles them # This output is a data frame with only one player's stats on each row (i.e. in long format) # Grab a df with only the Winner's stats winner = df %>% select ( - contains ( \"Loser\" )) %>% # Select only the Winner columns + extra game info columns as a df rename_at ( # Rename all columns containing \"Winner\" to \"Player\" vars ( contains ( \"Winner\" )), ~ str_replace ( . , \"Winner\" , \"Player\" ) ) %>% mutate ( Winner = 1 ) # Create a target column # Repeat the process with the loser's stats loser = df %>% select ( - contains ( \"Winner\" )) %>% rename_at ( vars ( contains ( \"Loser\" )), ~ str_replace ( . , \"Loser\" , \"Player\" ) ) %>% mutate ( Winner = 0 ) set.seed ( 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) # Create a df that appends both the Winner and loser df together combined_df = winner %>% rbind ( loser ) %>% # Append the loser df to the Winner df slice ( sample ( 1 : n ())) %>% # Randomise row order arrange ( Match_Id ) %>% # Arrange by Match_Id return () } # Read in men and womens data; randomise the data to avoid result leakage mens = readr :: read_csv ( 'data/ATP_matches.csv' , na = \".\" ) %>% filter ( Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% # Filter to only use hardcourt games mutate ( Match_Id = row_number (), # Add a match ID column to be used as a key Tournament_Date = dmy ( Tournament_Date ), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won ) %>% # Add a total games played column split_winner_loser_columns () # Change the data frame from wide to long mens %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Player Tournament Tournament_Date Court_Surface Round_Description Player_Rank Retirement_Ind Player_Sets_Won Player_Games_Won Player_Aces ... Player_SecondServes_Won Player_SecondServes_In Player_BreakPoints_Won Player_BreakPoints Player_ReturnPoints_Won Player_ReturnPoints_Faced Player_TotalPoints_Won Match_Id Total_Games Winner Eric Prodon Chennai 2012-01-02 Hard First Round 97 0 0 7 2 ... 13 26 1 3 15 49 49 1 19 0 Edouard Roger-Vasselin Chennai 2012-01-02 Hard First Round 106 0 2 12 5 ... 12 19 4 7 25 59 59 1 19 1 Dudi Sela Chennai 2012-01-02 Hard First Round 83 0 2 12 2 ... 11 16 6 14 36 58 61 2 13 1 Fabio Fognini Chennai 2012-01-02 Hard First Round 48 0 0 1 1 ... 5 26 0 1 8 33 30 2 13 0 Frederico Gil Chennai 2012-01-02 Hard First Round 102 0 1 14 5 ... 18 35 2 4 36 103 99 3 33 0 Go Soeda Chennai 2012-01-02 Hard First Round 120 0 2 19 6 ... 19 39 5 11 42 105 109 3 33 1","title":"Exploring the Data"},{"location":"modelling/AusOpenRTutorial/#feature-creation","text":"Now that we have a fairly good understanding of what the data looks like, let's add some features. To do this we will define a function. Ideally we want to add features which will provide predictive power to our model. Thinking about the dynamics of tennis, we know that players often will matches by \"breaking\" the opponent's serve (i.e. winning a game when the opponent is serving). This is especially important in mens tennis. Let's create a feature called F_Player_BreakPoints_Per_Game , which is the number of breakpoints a player gets per game that they play (even though they can only get breakpoints every second game, we will use total games). Let's also create a feature called F_Player_Return_Win_Ratio which is the proportion of points won when returning. Similarly, \"holding\" serve is important (i.e. winning a game when you are serving). Let's create a feature called F_Player_Serve_Win_Ratio which is the proportion of points won when serving. Finally, you only win a set of tennis by winning more sets than your opponent. To win a set, you need to win games. Let's create a feature called F_Player_Game_Win_Percentage which is the propotion of games that a player wins. add_ratio_features <- function ( df ) { # This function adds ratio features to a long df df %>% mutate ( # Point Win ratio when serving F_Player_Serve_Win_Ratio = ( Player_FirstServes_Won + Player_SecondServes_Won - Player_DoubleFaults ) / ( Player_FirstServes_In + Player_SecondServes_In + Player_DoubleFaults ), # Point win ratio when returning F_Player_Return_Win_Ratio = Player_ReturnPoints_Won / Player_ReturnPoints_Faced , # Breakpoints per receiving game F_Player_BreakPoints_Per_Game = Player_BreakPoints / Total_Games , F_Player_Game_Win_Percentage = Player_Games_Won / Total_Games ) %>% mutate_at ( vars ( colnames ( . ), - contains ( \"Rank\" ), - Tournament_Date ), # Replace all NAs with0 apart from Rank, Date ~ ifelse ( is.na ( . ), 0 , . ) ) %>% return () } mens = mens %>% add_ratio_features () # Add features Now that we have added our features, we need to create rolling averages for them. We cannot simply use current match statistics, as they will leak the result to the model. Instead, we need to use past match statistics to predict future matches. Here we will use a rolling mean with a window of 15. If the player hasn't played 15 games, we will instead use a cumulative mean. We will also lag the result so as to not leak the result. This next chunk of code simply takes all the columns starting with F_ and calculates these means. mens = mens %>% group_by ( Player ) %>% # Group by player mutate_at ( # Create a rolling mean with window 15 for each player. vars ( starts_with ( \"F_\" )), # If the player hasn't played 15 games, use a cumulative mean ~ coalesce ( rollmean ( . , k = 15 , align = \"right\" , fill = NA_real_ ), cummean ( . )) %>% lag () ) %>% ungroup ()","title":"Feature Creation"},{"location":"modelling/AusOpenRTutorial/#creating-a-training-feature-matrix","text":"In predictive modelling language - features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options We can train the model on every tennis match in the data set, or We can only train the model on Australian Open matches. Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. We have decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface - hard court. However, we also need to train our model in the same way that will be used to predict the 2019 Australian Open. When predicting the 2nd round, we won't have data from the 1st round. So we will need to build our training feature matrix with this in mind. We should extract features for a player from past games at the start of the tournament and apply them to every matchup that that player plays. To do this, we will create a function, extract_latest_features_for_tournament , which maps over our feature data frame for the dates in the first round of a tournament and grabs features. First, we need the Australian Open and US Open results - let's grab these and then apply our function. # Get Australian Open and US Open Results aus_us_open_results = mens %>% filter (( Tournament == \"Australian Open, Melbourne\" | Tournament == \"U.S. Open, New York\" ) & Round_Description != \"Qualifying\" & Tournament_Date != \"2012-01-16\" ) %>% # Filter out qualifiers select ( Match_Id , Player , Tournament , Tournament_Date , Round_Description , Winner ) # Create a function which extracts features for each tournament extract_latest_features_for_tournament = function ( df , dte ) { df %>% # Filter for the 1st round filter ( Tournament_Date == dte , Round_Description == \"First Round\" , Tournament_Date != \"2012-01-16\" ) %>% group_by ( Player ) %>% # Group by player select_at ( vars ( Match_Id , starts_with ( \"F_\" ), Player_Rank ) # Grab the players' features ) %>% rename ( F_Player_Rank = Player_Rank ) %>% ungroup () %>% mutate ( Feature_Date = dte ) %>% select ( Player , Feature_Date , everything ()) } # Create a feature matrix in long format feature_matrix_long = aus_us_open_results %>% distinct ( Tournament_Date ) %>% # Pull all Tournament Dates pull () %>% map_dfr ( ~ extract_latest_features_for_tournament ( mens , . ) # Get the features ) %>% filter ( Feature_Date != \"2012-01-16\" ) %>% # Filter out the first Aus Open mutate_at ( # Replace NAs with the mean vars ( starts_with ( \"F_\" )), ~ ifelse ( is.na ( . ), mean ( . , na.rm = TRUE ), . ) ) Now that we have a feature matrix in long format, we need to convert it to wide format so that the features are on the same row. To do this we will define a function gather_df , which converts the data frame from long to wide. Let's also join the results to the matrix and convert the Winner column to a factor. Finally, we will take the difference of player1 and player2's features, so as to reduce the dimensionality of the model. gather_df <- function ( df ) { # This function puts the df back into its original format of each row containing stats for both players df %>% arrange ( Match_Id ) %>% filter ( row_number () %% 2 != 0 ) %>% # Filter for every 2nd row, starting at the 1st index. e.g. 1, 3, 5 rename_at ( # Rename columns to player_1 vars ( contains ( \"Player\" )), ~ str_replace ( . , \"Player\" , \"player_1\" ) ) %>% inner_join ( df %>% filter ( row_number () %% 2 == 0 ) %>% rename_at ( vars ( contains ( \"Player\" )), # Rename columns to player_2 ~ str_replace ( . , \"Player\" , \"player_2\" ) ) %>% select ( Match_Id , contains ( \"Player\" )), by = c ( 'Match_Id' ) ) %>% select ( Match_Id , player_1 , player_2 , Winner , everything ()) %>% return () } # Joining results to features feature_matrix_wide = aus_us_open_results %>% inner_join ( feature_matrix_long %>% select ( - Match_Id ), by = c ( \"Player\" , \"Tournament_Date\" = \"Feature_Date\" )) %>% gather_df () %>% mutate ( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio , F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio , F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage , F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game , F_Rank_Diff = ( F_player_1_Rank - F_player_2_Rank ), Winner = as.factor ( Winner ) ) %>% select ( Match_Id , player_1 , player_2 , Tournament , Tournament_Date , Round_Description , Winner , contains ( \"Diff\" )) train = feature_matrix_wide train %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) Match_Id player_1 player_2 Tournament Tournament_Date Round_Description Winner F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff 1139 Adrian Ungur Daniel Brands U.S. Open, New York 2012-08-27 First Round 0 0.03279412 -0.014757229 0.002877458 0.073938088 -13 1140 Albert Montanes Richard Gasquet U.S. Open, New York 2012-08-27 First Round 0 -0.08000322 -0.077451342 -0.131108056 -0.180846832 97 1141 Martin Klizan Alejandro Falla U.S. Open, New York 2012-08-27 First Round 1 0.07711693 -0.044715517 0.068179841 -0.087361962 1 1142 Alex Bogomolov Jr. Andy Murray U.S. Open, New York 2012-08-27 First Round 0 -0.03964074 -0.031700826 -0.059010072 -0.094721700 69 1143 Tommy Robredo Andreas Seppi U.S. Open, New York 2012-08-27 First Round 1 -0.02681392 0.006442134 -0.067779660 -0.009930089 151 1144 Ryan Harrison Benjamin Becker U.S. Open, New York 2012-08-27 First Round 1 0.04251983 0.018604623 0.026486753 -0.003548973 -24","title":"Creating a Training Feature Matrix"},{"location":"modelling/AusOpenRTutorial/#creating-the-feature-matrix-for-the-2019-australian-open","text":"Now that we have our training set, train , we need to create a feature matrix to create predictions on. To do this, we need to generate features again. We could simply append a player list to our raw data frame, create a mock date and then use the extract_latest_features_for_tournament function that we used before. Instead, we're going to create a lookup table for each unique player in the 2019 Australian Open. We will need to get their last 15 games and then find the mean for each feature so that our features are the same. Let's first explore what the dummy submission file looks like, then use it to get the unique players. read_csv ( 'data/men_dummy_submission_file.csv' ) %>% glimpse () As we can see, the dummy submission file contains every potential match up for the Open. This will be updated a few days before the Open starts with the actual players playing. Let's now create the lookup feature table. # Get a vector of unique players in this years' open using the dummy submission file unique_players = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% pull ( player_1 ) %>% unique () # Get the last 15 games played for each unique player and find their features lookup_feature_table = read_csv ( 'data/ATP_matches.csv' , na = \".\" ) %>% filter ( Court_Surface == \"Hard\" | Court_Surface == \"Indoor Hard\" ) %>% mutate ( Match_Id = row_number (), # Add a match ID column to be used as a key Tournament_Date = dmy ( Tournament_Date ), # Change Tournament to datetime Total_Games = Winner_Games_Won + Loser_Games_Won ) %>% # Add a total games played column # clean_missing_data() %>% # Clean missing data split_winner_loser_columns () %>% # Change the data frame from wide to long add_ratio_features () %>% filter ( Player %in% unique_players ) %>% group_by ( Player ) %>% top_n ( 15 , Match_Id ) %>% summarise ( F_Player_Serve_Win_Ratio = mean ( F_Player_Serve_Win_Ratio ), F_Player_Return_Win_Ratio = mean ( F_Player_Return_Win_Ratio ), F_Player_BreakPoints_Per_Game = mean ( F_Player_BreakPoints_Per_Game ), F_Player_Game_Win_Percentage = mean ( F_Player_Game_Win_Percentage ), F_Player_Rank = last ( Player_Rank ) ) Now let's create features for every single combination. To do this we'll join our lookup_feature_table to the player_1 and player_2 columns in the dummy_submission_file . # Create feature matrix for the Australian Open for all player 1s features_player_1 = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% select ( player_1 ) %>% inner_join ( lookup_feature_table , by = c ( \"player_1\" = \"Player\" )) %>% rename ( F_player_1_Serve_Win_Ratio = F_Player_Serve_Win_Ratio , F_player_1_Return_Win_Ratio = F_Player_Return_Win_Ratio , F_player_1_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game , F_player_1_Game_Win_Percentage = F_Player_Game_Win_Percentage , F_player_1_Rank = F_Player_Rank ) # Create feature matrix for the Australian Open for all player 2s features_player_2 = read_csv ( 'data/men_dummy_submission_file.csv' ) %>% select ( player_2 ) %>% inner_join ( lookup_feature_table , by = c ( \"player_2\" = \"Player\" )) %>% rename ( F_player_2_Serve_Win_Ratio = F_Player_Serve_Win_Ratio , F_player_2_Return_Win_Ratio = F_Player_Return_Win_Ratio , F_player_2_BreakPoints_Per_Game = F_Player_BreakPoints_Per_Game , F_player_2_Game_Win_Percentage = F_Player_Game_Win_Percentage , F_player_2_Rank = F_Player_Rank ) # Join the two dfs together and subtract features to create Difference features aus_open_2019_features = features_player_1 %>% bind_cols ( features_player_2 ) %>% select ( player_1 , player_2 , everything ()) %>% mutate ( F_Serve_Win_Ratio_Diff = F_player_1_Serve_Win_Ratio - F_player_2_Serve_Win_Ratio , F_Return_Win_Ratio_Diff = F_player_1_Return_Win_Ratio - F_player_2_Return_Win_Ratio , F_Game_Win_Percentage_Diff = F_player_1_Game_Win_Percentage - F_player_2_Game_Win_Percentage , F_BreakPoints_Per_Game_Diff = F_player_1_BreakPoints_Per_Game - F_player_2_BreakPoints_Per_Game , F_Rank_Diff = ( F_player_1_Rank - F_player_2_Rank ) ) %>% select ( player_1 , player_2 , contains ( \"Diff\" )) aus_open_2019_features %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) player_1 player_2 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff Novak Djokovic Rafael Nadal 0.06347805 0.02503802 0.07002382 0.08951024 1 Novak Djokovic Roger Federer 0.06583364 0.03628491 0.07661295 0.15455628 -1 Novak Djokovic Juan Martin del Potro 0.01067079 0.03436023 0.06382353 0.11259979 -2 Novak Djokovic Alexander Zverev 0.11117863 0.03125651 0.11055585 0.08661036 -3 Novak Djokovic Kevin Anderson 0.02132375 0.10449337 0.11184503 0.23684083 -4 Novak Djokovic Marin Cilic 0.08410746 0.02434916 0.07653035 0.08355134 -5","title":"Creating the Feature Matrix for the 2019 Australian Open"},{"location":"modelling/AusOpenRTutorial/#generating-2019-australian-open-predictions","text":"Now that we have our features, we can finally train our model and generate predictions for the 2019 Australian Open. Due to its simplicity, we will use h2o's Auto Machine Learning function h2o.automl . This will train a heap of different models and optimise the hyperparameters, as well as creating stacked ensembles automatically for us. We will use optimising by log loss. First, we must create h2o frames for our training and feature data frames. Then we will run h2o.automl . Note that we can set the max_runtime_secs parameter. As this is a notebook, I have set it for 30 seconds - but I suggest you give it 10 minutes to create the best model. We can then create our predictions and assign them back to our aus_open_2019_features data frame. Finally, we will group_by player and find the best player, on average. ## Setup H2O h2o.init ( ip = \"localhost\" , port = 54321 , enable_assertions = TRUE , nthreads = 2 , max_mem_size = \"24g\" ) ## Sending file to h2o train_h2o = feature_matrix_wide %>% select ( contains ( \"Diff\" ), Winner ) %>% as.h2o ( destination_frame = \"train_h2o\" ) aus_open_2019_features_h2o = aus_open_2019_features %>% select ( contains ( \"Diff\" )) %>% as.h2o ( destination_frame = \"aus_open_2019_features_h2o\" ) ## Running Auto ML mens_model = h2o.automl ( y = \"Winner\" , training_frame = train_h2o , max_runtime_secs = 30 , max_models = 100 , stopping_metric = \"logloss\" , sort_metric = \"logloss\" , balance_classes = TRUE , seed = 183 ) # Set seed to replicate results - 183 is the most games played in a tennis match (Isner-Mahut) ## Predictions on test frame predictions = h2o.predict ( mens_model @ leader , aus_open_2019_features_h2o ) %>% as.data.frame () aus_open_2019_features $ prob_player_1 = predictions $ p1 aus_open_2019_features $ prob_player_2 = predictions $ p0 h2o.shutdown ( prompt = FALSE ) Now let's find the best player by taking the mean of the prediction probability by player. aus_open_2019_features %>% select ( player_1 , starts_with ( \"F_\" ), prob_player_1 ) %>% group_by ( player_1 ) %>% summarise_all ( mean ) %>% arrange ( desc ( prob_player_1 )) %>% datatable ( rownames = FALSE , extensions = 'Scroller' , options = list ( dom = \"t\" , scrollY = 450 , scroller = TRUE , scrollX = 600 , fixedColumns = TRUE )) %>% formatRound ( columns = pluck ( . , \"x\" , \"data\" ) %>% colnames (), digits = 3 ) player_1 F_Serve_Win_Ratio_Diff F_Return_Win_Ratio_Diff F_Game_Win_Percentage_Diff F_BreakPoints_Per_Game_Diff F_Rank_Diff prob_player_1 Novak Djokovic 0.1109364627 0.076150615 0.1483970690 0.17144300 NA 0.8616486 Karen Khachanov 0.0960639298 0.061436164 0.1059967623 0.04544955 NA 0.8339594 Juan Martin del Potro 0.1003931993 0.042025222 0.0847985439 0.05943767 NA 0.8218308 Rafael Nadal 0.0480432305 0.051531252 0.0790179917 0.08181694 NA 0.8032543 Gilles Simon 0.0646937767 0.084843307 0.0901401318 0.08675350 NA 0.7985995 Roger Federer 0.0452014997 0.040992497 0.0725719954 0.01817046 NA 0.7962289 Kei Nishikori 0.0777155934 0.018720226 0.0800648870 0.02740276 NA 0.7843631 Marin Cilic 0.0285413602 0.053017465 0.0736687072 0.08883055 NA 0.7804876 Tomas Berdych 0.0471654691 0.047289449 0.0737401748 0.10584114 NA 0.7739211 Daniil Medvedev 0.0275430665 0.031121856 0.0721948279 0.01803757 NA 0.7543269 Stefanos Tsitsipas 0.0470382377 0.023825850 0.0577628626 0.02105227 NA 0.7511674 Dominic Thiem 0.0258904189 0.032481624 0.0483707080 0.05857158 NA 0.7451547 Alexander Zverev 0.0006199716 0.044811275 0.0380134371 0.08423392 NA 0.7374897 Kyle Edmund 0.0558006240 0.011963627 0.0478850676 0.05142186 NA 0.7304873 Pablo Carreno Busta 0.0321878318 0.029862068 0.0413674481 -0.00229784 NA 0.7302043 Borna Coric 0.0762084129 -0.010097922 0.0413621283 -0.01924267 NA 0.7268124 Kevin Anderson 0.0907358428 -0.027171681 0.0381421997 -0.06362578 NA 0.7260799 David Goffin -0.0034821911 0.037247336 0.0162572061 0.05603565 NA 0.7155908 Fernando Verdasco 0.0229261365 0.032884054 0.0521212576 0.04668854 NA 0.7120831 Roberto Bautista Agut 0.0047641170 0.049939608 0.0218975349 0.07331023 NA 0.7009891 Milos Raonic 0.0849726089 -0.028732182 0.0385944327 -0.08009382 NA 0.6986865 Fabio Fognini -0.0394792678 0.047935185 0.0226546894 0.06213496 NA 0.6982031 Hyeon Chung 0.0042489153 0.047722133 0.0158096386 0.04823304 NA 0.6958943 Jack Sock -0.0099659903 0.026454984 0.0186547428 0.02307214 NA 0.6757770 Diego Schwartzman -0.0317130675 0.032098381 0.0006215006 0.05621187 NA 0.6631067 John Millman 0.0016290285 0.042676556 0.0119857356 0.06228135 NA 0.6603912 Nikoloz Basilashvili -0.0099968609 0.005561102 0.0473876170 0.03661962 NA 0.6602628 John Isner 0.1346946527 -0.070556940 0.0161348609 -0.11425009 NA 0.6598097 Gael Monfils -0.0074254934 0.024286746 0.0295568649 0.04007519 NA 0.6449506 Richard Gasquet 0.0296009556 -0.011382437 0.0013138324 -0.03972967 NA 0.6442043 ... ... ... ... ... ... ... Laslo Djere -0.042300822 -0.0150684095 -0.064667709 -0.0349151578 NA 0.3606923 David Ferrer -0.036179509 0.0532782117 0.012751020 0.0914824480 NA 0.3488057 Bradley Klahn -0.001248083 -0.0444982448 -0.025987040 -0.1181295700 NA 0.3487806 Marcel Granollers -0.031011830 -0.0094056152 -0.049853664 0.0136841358 NA 0.3460035 Ricardas Berankis -0.022557215 -0.0103782963 -0.047937290 -0.0468488990 NA 0.3454980 Radu Albot -0.040829057 0.0076150564 -0.034891704 0.0443672533 NA 0.3420615 Jordan Thompson -0.068554906 0.0261969117 -0.044349181 0.0206636045 NA 0.3358572 Thomas Fabbiano -0.060583307 0.0275756029 -0.025883493 0.0709707306 NA 0.3319778 Roberto Carballes Baena -0.054016396 -0.0091521177 -0.019093050 0.0347187874 NA 0.3312105 Paolo Lorenzi -0.038613500 -0.0212206827 -0.052602703 0.0199474025 NA 0.3299791 Guido Andreozzi -0.038614385 -0.0133763922 0.029549861 0.0636745661 NA 0.3288762 Peter Polansky 0.007461636 -0.0163389196 -0.024034159 -0.0442144260 NA 0.3216756 Ernests Gulbis -0.062827089 -0.0134699552 -0.027633425 -0.0518663252 NA 0.3123511 Thiago Monteiro 0.001235931 -0.0288349103 -0.043831840 -0.0654744344 NA 0.3122069 Casper Ruud 0.016838968 -0.0178511679 0.015234507 0.0219131874 NA 0.3119321 Marco Trungelliti -0.022148774 -0.0005658242 0.048542554 0.1243537739 NA 0.3092636 Jiri Vesely -0.050204009 -0.0351868278 -0.042887646 -0.0160467165 NA 0.3089287 Guillermo Garcia-Lopez -0.090076100 -0.0108663630 -0.048712763 -0.0124446402 NA 0.3080898 Michael Mmoh -0.063802934 -0.0079053251 -0.011112236 -0.0332042032 NA 0.2822330 Jason Kubler -0.124758873 -0.0202756806 -0.013998570 0.1020895301 NA 0.2814246 Ruben Bemelmans -0.029036164 -0.0138846550 -0.032256254 -0.0363563402 NA 0.2772185 Bjorn Fratangelo -0.014149222 0.0033574304 -0.019931504 -0.0360199607 NA 0.2652527 Pablo Andujar -0.042869833 -0.0488261697 -0.070057834 -0.0164918910 NA 0.2647100 Christian Garin -0.046150875 0.0235799476 -0.006209664 0.0736304057 NA 0.2631607 Ivo Karlovic 0.071597162 -0.1093833837 0.001410787 -0.1237762218 NA 0.2500242 Juan Ignacio Londero -0.026454456 -0.0715665271 -0.016749898 -0.0363353678 NA 0.2351747 Ramkumar Ramanathan -0.005371622 -0.0606138479 -0.041631884 -0.0005573405 NA 0.2272977 Reilly Opelka 0.025704824 -0.0607219257 -0.015474944 -0.0720809006 NA 0.2262993 Carlos Berlocq -0.063580460 0.0074576369 -0.054277974 -0.0165235079 NA 0.2112275 Pedro Sousa -0.197333352 -0.0734557562 -0.161962722 -0.1023311674 NA 0.1502313","title":"Generating 2019 Australian Open Predictions"},{"location":"modelling/AusOpenRTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/EPLmlPython/","text":"EPL Machine Learning Walkthrough 01. Data Acquisition & Exploration Welcome to the first part of this Machine Learning Walkthrough. This tutorial will be made of two parts; how we actually acquired our data (programmatically) and exploring the data to find potential features to use in the next tutorial . Data Acquisition We will be grabbing our data from football-data.co.uk , which has an enormous amount of soccer data dating back to the 90s. They also generously allow us to use it for free! However, the data is in separate CSVs based on the season. That means we would need to manually download 20 different files if we wanted the past 20 seasons. Rather than do this laborious and boring task, let's create a function which downloads the files for us, and appends them all into one big CSV. To do this, we will use BeautifulSoup, a Python library which helps to pull data from HTML and XML files. We will then define a function which collates all the data for us into one DataFrame. # Import Modules import pandas as pd import requests from bs4 import BeautifulSoup import datetime pd . set_option ( 'display.max_columns' , 100 ) import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline from data_preparation_functions import * def grab_epl_data (): # Connect to football-data.co.uk res = requests . get ( \"http://www.football-data.co.uk/englandm.php\" ) # Create a BeautifulSoup object soup = BeautifulSoup ( res . content , 'lxml' ) # Find the tables with the links to the data in them. table = soup . find_all ( 'table' , { 'align' : 'center' , 'cellspacing' : '0' , 'width' : '800' })[ 1 ] body = table . find_all ( 'td' , { 'valign' : 'top' })[ 1 ] # Grab the urls for the csv files links = [ link . get ( 'href' ) for link in body . find_all ( 'a' )] links_text = [ link_text . text for link_text in body . find_all ( 'a' )] data_urls = [] # Create a list of links prefix = 'http://www.football-data.co.uk/' for i , text in enumerate ( links_text ): if text == 'Premier League' : data_urls . append ( prefix + links [ i ]) # Get rid of last 11 uls as these don't include match stats and odds, and we # only want from 2005 onwards data_urls = data_urls [: - 12 ] df = pd . DataFrame () # Iterate over the urls for url in data_urls : # Get the season and make it a column season = url . split ( '/' )[ 4 ] print ( f \"Getting data for season { season } \" ) # Read the data from the url into a DataFrame temp_df = pd . read_csv ( url ) temp_df [ 'season' ] = season # Create helpful columns like Day, Month, Year, Date etc. so that our data is clean temp_df = ( temp_df . dropna ( axis = 'columns' , thresh = temp_df . shape [ 0 ] - 30 ) . assign ( Day = lambda df : df . Date . str . split ( '/' ) . str [ 0 ], Month = lambda df : df . Date . str . split ( '/' ) . str [ 1 ], Year = lambda df : df . Date . str . split ( '/' ) . str [ 2 ]) . assign ( Date = lambda df : df . Month + '/' + df . Day + '/' + df . Year ) . assign ( Date = lambda df : pd . to_datetime ( df . Date )) . dropna ()) # Append the temp_df to the main df df = df . append ( temp_df , sort = True ) # Drop all NAs df = df . dropna ( axis = 1 ) . dropna () . sort_values ( by = 'Date' ) print ( \"Finished grabbing data.\" ) return df df = grab_epl_data () # df.to_csv(\"data/epl_data.csv\", index=False) Getting data for season 1819 Getting data for season 1718 Getting data for season 1617 Getting data for season 1516 Getting data for season 1415 Getting data for season 1314 Getting data for season 1213 Getting data for season 1112 Getting data for season 1011 Getting data for season 0910 Getting data for season 0809 Getting data for season 0708 Getting data for season 0607 Getting data for season 0506 Finished grabbing data . Whenever we want to update our data (for example if we want the most recent Gameweek included), all we have to do is run that function and then save the data to a csv with the commented out line above. Data Exploration Now that we have our data, let's explore it. Let's first look at home team win rates since 2005 to see if there is a consistent trend. To get an idea of what our data looks like, we'll look at the tail of the dataset first. df . tail ( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season 28 3.0 11.0 0.0 9.0 3.0 2.0 Crystal Palace 3.00 3.25 2.60 2.95 3.1 2.55 42.0 20.0 -0.25 1.71 2.13 2.92 1.73 2.16 3.22 2.55 1.79 2.21 3.04 1.77 2.23 3.36 2.66 39.0 2018-08-26 26 E0 1.0 2.0 H 6.0 14.0 0.0 13.0 5.0 0.0 0.0 D 4.0 Watford 2.95 3.20 2.5 2.90 3.1 2.50 08 A Taylor 2.90 3.3 2.6 18 1819 27 5.0 8.0 0.0 15.0 3.0 1.0 Chelsea 1.66 4.00 5.75 1.67 3.8 5.25 42.0 22.0 1.00 1.92 1.88 1.67 2.18 1.71 3.90 5.25 2.01 1.95 1.71 2.28 1.76 4.17 5.75 40.0 2018-08-26 26 E0 2.0 1.0 A 4.0 16.0 0.0 6.0 2.0 0.0 0.0 D 3.0 Newcastle 1.70 3.75 5.0 1.67 3.8 5.25 08 P Tierney 1.67 4.0 5.5 18 1819 29 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.90 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.00 1.76 2.25 3.40 2.67 40.0 2018-08-27 27 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.6 2.75 3.2 2.55 08 C Pawson 2.90 3.3 2.6 18 1819 # Create Home Win, Draw Win and Away Win columns df = df . assign ( homeWin = lambda df : df . apply ( lambda row : 1 if row . FTHG > row . FTAG else 0 , axis = 'columns' ), draw = lambda df : df . apply ( lambda row : 1 if row . FTHG == row . FTAG else 0 , axis = 'columns' ), awayWin = lambda df : df . apply ( lambda row : 1 if row . FTHG < row . FTAG else 0 , axis = 'columns' )) Home Ground Advantage win_rates = \\ ( df . groupby ( 'season' ) . mean () . loc [:, [ 'homeWin' , 'draw' , 'awayWin' ]]) win_rates homeWin draw awayWin season 0506 0.505263 0.202632 0.292105 0607 0.477573 0.258575 0.263852 0708 0.463158 0.263158 0.273684 0809 0.453826 0.255937 0.290237 0910 0.507895 0.252632 0.239474 1011 0.471053 0.292105 0.236842 1112 0.450000 0.244737 0.305263 1213 0.433862 0.285714 0.280423 1314 0.472973 0.208108 0.318919 1415 0.453826 0.245383 0.300792 1516 0.414248 0.282322 0.303430 1617 0.492105 0.221053 0.286842 1718 0.455263 0.260526 0.284211 1819 0.466667 0.200000 0.333333 Findings As we can see, winrates across home team wins, draws and away team wins are very consistent. It seems that the home team wins around 46-47% of the time, the draw happens about 25% of the time, and the away team wins about 27% of the time. Let's plot this DataFrame so that we can see the trend more easily. # Set the style plt . style . use ( 'ggplot' ) fig = plt . figure () ax = fig . add_subplot ( 111 ) home_line = ax . plot ( win_rates . homeWin , label = 'Home Win Rate' ) away_line = ax . plot ( win_rates . awayWin , label = 'Away Win Rate' ) draw_line = ax . plot ( win_rates . draw , label = 'Draw Win Rate' ) ax . set_xlabel ( \"season\" ) ax . set_ylabel ( \"Win Rate\" ) plt . title ( \"Win Rates\" , fontsize = 16 ) # Add the legend locations home_legend = plt . legend ( handles = home_line , loc = 'upper right' , bbox_to_anchor = ( 1 , 1 )) ax = plt . gca () . add_artist ( home_legend ) away_legend = plt . legend ( handles = away_line , loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.4 )) ax = plt . gca () . add_artist ( away_legend ) draw_legend = plt . legend ( handles = draw_line , loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.06 )) As we can see, the winrates are relatively stable each season, except for in 14/15 when the home win rate drops dramatically. Out of interest, let's also have a look at which team has the best home ground advantage. Let's define HGA as home win rate - away win rate. And then plot some of the big clubs' HGA against each other. home_win_rates = \\ ( df . groupby ([ 'HomeTeam' ]) . homeWin . mean ()) away_win_rates = \\ ( df . groupby ([ 'AwayTeam' ]) . awayWin . mean ()) hga = ( home_win_rates - away_win_rates ) . reset_index () . rename ( columns = { 0 : 'HGA' }) . sort_values ( by = 'HGA' , ascending = False ) hga . head ( 10 ) HomeTeam HGA 15 Fulham 0.315573 7 Brighton 0.304762 20 Man City 0.244980 14 Everton 0.241935 30 Stoke 0.241131 10 Charlton 0.236842 0 Arsenal 0.236140 27 Reading 0.234962 33 Tottenham 0.220207 21 Man United 0.215620 So the club with the best HGA is Fulham - interesting. This is most likely because Fulham have won 100% of home games in 2018 so far which is skewing the mean. Let's see how the HGA for some of the big clubs based compare over seasons. big_clubs = [ 'Liverpool' , 'Man City' , 'Man United' , 'Chelsea' , 'Arsenal' ] home_win_rates_5 = df [ df . HomeTeam . isin ( big_clubs )] . groupby ([ 'HomeTeam' , 'season' ]) . homeWin . mean () away_win_rates_5 = df [ df . AwayTeam . isin ( big_clubs )] . groupby ([ 'AwayTeam' , 'season' ]) . awayWin . mean () hga_top_5 = home_win_rates_5 - away_win_rates_5 hga_top_5 . unstack ( level = 0 ) HomeTeam Arsenal Chelsea Liverpool Man City Man United season 0506 0.421053 0.368421 0.263158 0.263158 0.052632 0607 0.263158 0.000000 0.421053 -0.052632 0.105263 0708 0.210526 -0.052632 0.157895 0.368421 0.368421 0809 0.105263 -0.157895 -0.052632 0.578947 0.210526 0910 0.368421 0.368421 0.421053 0.315789 0.263158 1011 0.157895 0.368421 0.368421 0.263158 0.684211 1112 0.157895 0.315789 -0.105263 0.421053 0.105263 1213 0.052632 0.105263 0.105263 0.248538 0.201754 1314 0.143275 0.251462 0.307018 0.362573 -0.026316 1415 0.131579 0.210526 0.105263 0.210526 0.421053 1516 0.210526 -0.105263 0.000000 0.263158 0.263158 1617 0.263158 0.210526 0.105263 -0.052632 -0.105263 1718 0.578947 0.052632 0.157895 0.000000 0.263158 1819 0.500000 0.000000 0.000000 0.500000 0.500000 Now let's plot it. sns . lineplot ( x = 'season' , y = 'HGA' , hue = 'team' , data = hga_top_5 . reset_index () . rename ( columns = { 0 : 'HGA' , 'HomeTeam' : 'team' })) plt . legend ( loc = 'lower center' , ncol = 6 , bbox_to_anchor = ( 0.45 , - 0.2 )) plt . title ( \"HGA Among the top 5 clubs\" , fontsize = 14 ) plt . show () The results here seem to be quite erratic, although it seems that Arsenal consistently has a HGA above 0. Let's now look at the distributions of each of our columns. The odds columns are likely to be highly skewed, so we may have to account for this later. for col in df . select_dtypes ( 'number' ) . columns : sns . distplot ( df [ col ]) plt . title ( f \"Distribution for { col } \" ) plt . show () Exploring Referee Home Ground Bias What may be of interest is whether certain referees are correlated with the home team winning more often. Let's explore referee home ground bias for referees for the top 10 Referees based on games. print ( 'Overall Home Win Rate: {:.4} %' . format ( df . homeWin . mean () * 100 )) # Get the top 10 refs based on games top_10_refs = df . Referee . value_counts () . head ( 10 ) . index df [ df . Referee . isin ( top_10_refs )] . groupby ( 'Referee' ) . homeWin . mean () . sort_values ( ascending = False ) Overall Home Win Rate: 46.55% Referee L Mason 0.510373 C Foy 0.500000 M Clattenburg 0.480000 M Jones 0.475248 P Dowd 0.469880 M Atkinson 0.469565 M Oliver 0.466019 H Webb 0.456604 A Marriner 0.455516 M Dean 0.442049 Name: homeWin, dtype: float64 It seems that L Mason may be the most influenced by the home crowd. Whilst the overall home win rate is 46.5%, the home win rate when he is the Referee is 51%. However it should be noted that this doesn't mean that he causes the win through bias. It could just be that he referees the best clubs, so naturally their home win rate is high. Variable Correlation With Margin Let's now explore different variables' relationships with margin. First, we'll create a margin column, then we will pick a few different variables to look at the correlations amongst each other, using a correlation heatmap. df [ 'margin' ] = df [ 'FTHG' ] - df [ 'FTAG' ] stat_cols = [ 'AC' , 'AF' , 'AR' , 'AS' , 'AST' , 'AY' , 'HC' , 'HF' , 'HR' , 'HS' , 'HST' , 'HTR' , 'HY' , 'margin' ] stat_correlations = df [ stat_cols ] . corr () stat_correlations [ 'margin' ] . sort_values () AST - 0.345703 AS - 0.298665 HY - 0.153806 HR - 0.129393 AC - 0.073204 HF - 0.067469 AF 0.005474 AY 0.013746 HC 0.067433 AR 0.103528 HS 0.275847 HST 0.367591 margin 1.000000 Name : margin , dtype : float64 Unsurprisingly, Home Shots on Target correlate the most with Margin, and Away Reds is also high. What is surprising is that Home Yellows has quite a strong negative correlation with margin - this may be because players will play more aggresively when they are losing to try and get the lead back, and hence receive more yellow cards. Let's now look at the heatmap between variables. sns . heatmap ( stat_correlations , annot = True , annot_kws = { 'size' : 10 }) < matplotlib . axes . _subplots . AxesSubplot at 0x220a4227048 > Analysing Features What we are really interested in, is how our features (creating in the next tutorial), correlate with winning. We will skip ahead here and use a function to create our features for us, and then examine how the moving averages/different features correlate with winning. # Create a cleaned df of all of our data pre_features_df = create_df ( 'data/epl_data.csv' ) # Create our features features = create_feature_df ( pre_features_df ) Creating all games feature DataFrame C : \\ Users \\ wardj \\ Documents \\ Betfair Public Github \\ predictive - models \\ epl \\ data_preparation_functions . py : 419 : RuntimeWarning : invalid value encountered in double_scalars . pipe ( lambda df : ( df . eloAgainst * df [ goalsForOrAgainstCol ]) . sum () / df . eloAgainst . sum ())) Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . features = ( pre_features_df . assign ( margin = lambda df : df . FTHG - df . FTAG ) . loc [:, [ 'gameId' , 'margin' ]] . pipe ( pd . merge , features , on = [ 'gameId' ])) features . corr () . margin . sort_values ( ascending = False )[: 20 ] margin 1.000000 f_awayOdds 0.413893 f_totalMktH % 0.330420 f_defMktH % 0.325392 f_eloAgainstAway 0.317853 f_eloForHome 0.317853 f_midMktH % 0.316080 f_attMktH % 0.312262 f_sizeOfHandicapAway 0.301667 f_goalsForHome 0.298930 f_wtEloGoalsForHome 0.297157 f_shotsForHome 0.286239 f_cornersForHome 0.279917 f_gkMktH % 0.274732 f_homeWinPc38Away 0.271326 f_homeWinPc38Home 0.271326 f_wtEloGoalsAgainstAway 0.269663 f_goalsAgainstAway 0.258418 f_cornersAgainstAway 0.257148 f_drawOdds 0.256807 Name : margin , dtype : float64 As we can see away odds is most highly correlated to margin. This makes sense, as odds generally have most/all information included in the price. What is interesting is that elo seems to also be highly correlated, which is good news for our elo model that we made. Similarly, weighted goals and the the value of the defence relative to other teams ('defMktH%' etc.) is strongly correlated to margin. 02. Data Preparation & Feature Engineering Welcome to the second part of this Machine Learning Walkthrough. This tutorial will focus on data preparation and feature creation, before we dive into modelling in the next tutorial . Specifically, this tutorial will cover a few things: Data wrangling specifically for sport Feature creation - focussing on commonly used features in sports modelling, such as exponential moving averages Using functions to modularise the data preparation process Data Wrangling We will begin by utilising functions we have defined in our data_preparation_functions script to wrangle our data into a format that can be consumed by Machine Learning algorithms. A typical issue faced by aspect of modelling sport is the issue of Machine Learning algorithms requiring all features for the teams playing to be on the same row of a table, whereas when we actual calculate these features, we usually require the teams to be on separate rows as it makes it a lot easier to calculate typical features, such as expontentially weighted moving averages . We will explore this issue and show how we deal with issues like these. # Import libraries from data_preparation_functions import * from sklearn.metrics import log_loss from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold , cross_val_score import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) We have created some functions which prepare the data for you. For thoroughly commented explanation of how the functions work, read through the data_preparation_functions.py script along side this walkthrough. Essentially, each functions wrangles the data through a similar process. It first reads in the data from a csv file, then converts the columns to datatypes that we can work with, such as converting the Date column to a datetime data type. It then adds a Game ID column, so each game is easily identifiable and joined on. We then assign the DataFrame some other columns which may be useful, such as 'Year', 'Result' and 'homeWin'. Finally, we drop redundant column and return the DataFrame. Let us now create six different DataFrames, which we will use to create features. Later, we will join these features back into one main feature DataFrame. Create 6 distinct DataFrames # This table includes all of our data in one big DataFrame df = create_df ( 'data/epl_data.csv' ) df . head ( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.0 2.38 8 A Wiley 2.75 3.25 2.4 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.0 2.10 8 M Riley 3.10 3.25 2.2 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.2 3.75 8 G Poll 1.80 3.30 4.5 2005 0506 3 0 1 away # This includes only the typical soccer stats, like home corners, home shots on target etc. stats = create_stats_df ( 'data/epl_data.csv' ) stats . head ( 3 ) gameId HomeTeam AwayTeam FTHG FTAG HTHG HTAG HS AS HST AST HF AF HC AC HY AY HR AR 0 1 West Ham Blackburn 3.0 1.0 0.0 1.0 13.0 11.0 5.0 5.0 11.0 14.0 2.0 6.0 0.0 1.0 0.0 1.0 1 2 Aston Villa Bolton 2.0 2.0 2.0 2.0 3.0 13.0 2.0 6.0 14.0 16.0 7.0 8.0 0.0 2.0 0.0 0.0 2 3 Everton Man United 0.0 2.0 0.0 1.0 10.0 12.0 5.0 5.0 15.0 14.0 8.0 6.0 3.0 1.0 0.0 0.0 # This includes all of our betting related data, such as win/draw/lose odds, asian handicaps etc. betting = create_betting_df ( 'data/epl_data.csv' ) betting . head ( 3 ) B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Day Div IWA IWD IWH LBA LBD LBH Month VCA VCD VCH Year homeWin awayWin result HomeTeam AwayTeam gameId 0 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 13 E0 2.7 3.0 2.3 2.75 3.0 2.38 8 2.75 3.25 2.4 2005 1 0 home West Ham Blackburn 1 1 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 13 E0 3.1 3.0 2.1 3.20 3.0 2.10 8 3.10 3.25 2.2 2005 0 0 draw Aston Villa Bolton 2 2 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 13 E0 1.8 3.1 3.8 1.83 3.2 3.75 8 1.80 3.30 4.5 2005 0 1 away Everton Man United 3 # This includes all of the team information for each game. team_info = create_team_info_df ( 'data/epl_data.csv' ) team_info . head ( 3 ) gameId Date season HomeTeam AwayTeam FTR HTR Referee 0 1 2005-08-13 0506 West Ham Blackburn H A A Wiley 1 2 2005-08-13 0506 Aston Villa Bolton D D M Riley 2 3 2005-08-13 0506 Everton Man United A A G Poll # Whilst the other DataFrames date back to 2005, this DataFrame has data from 2001 to 2005. historic_games = create_historic_games_df ( 'data/historic_games_pre2005.csv' ) historic_games . head ( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin 0 2001-08-18 Charlton Everton 1 2 -1 20012002 0 1 2001-08-18 Derby Blackburn 2 1 -1 20012002 1 2 2001-08-18 Leeds Southampton 2 0 -1 20012002 1 # This is the historic_games DataFrame appended to the df DataFrame. all_games = create_all_games_df ( 'data/epl_data.csv' , 'data/historic_games_pre2005.csv' ) all_games . head ( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin awayWin homeWinPc5 homeWinPc38 awayWinPc5 awayWinPc38 gameIdHistoric 0 2001-08-18 Charlton Everton 1.0 2.0 -1 20012002 0 1 NaN NaN NaN NaN 1 1 2001-08-18 Derby Blackburn 2.0 1.0 -1 20012002 1 0 NaN NaN NaN NaN 2 2 2001-08-18 Leeds Southampton 2.0 0.0 -1 20012002 1 0 NaN NaN NaN NaN 3 Feature Creation Now that we have all of our pre-prepared DataFrames, and we know that the data is clean, we can move onto feature creation. As is common practice with sports modelling, we are going to start by creating expontentially weighted moving averages (EMA) as features. To get a better understanding of how EMAs work, read here . In short, an EMA is like a simple moving average, except it weights recent instances more than older instances based on an alpha parameter. The documentation for the pandas (emw method)[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html] we will be using states that we can specify alpha in a number of ways. We will specify it in terms of span, where $\\alpha = 2 / (span+1), span \u2265 1 $. Let's first define a function which calculates the exponential moving average for each column in the stats DataFrame. We will then apply this function with other functions we have created, such as create_betting_features_ema, which creates moving averages of betting data. However, we must first change the structure of our data. Notice that currently each row has both the Home Team's data and the Away Team's data on a single row. This makes it difficult to calculate rolling averages, so we will restructure our DataFrames to ensure each row only contains single team's data. To do this, we will define a function, reate_multiline_df_stats. # Define a function which restructures our DataFrame def create_multiline_df_stats ( old_stats_df ): # Create a list of columns we want and their mappings to more interpretable names home_stats_cols = [ 'HomeTeam' , 'FTHG' , 'FTAG' , 'HTHG' , 'HTAG' , 'HS' , 'AS' , 'HST' , 'AST' , 'HF' , 'AF' , 'HC' , 'AC' , 'HY' , 'AY' , 'HR' , 'AR' ] away_stats_cols = [ 'AwayTeam' , 'FTAG' , 'FTHG' , 'HTAG' , 'HTHG' , 'AS' , 'HS' , 'AST' , 'HST' , 'AF' , 'HF' , 'AC' , 'HC' , 'AY' , 'HY' , 'AR' , 'HR' ] stats_cols_mapping = [ 'team' , 'goalsFor' , 'goalsAgainst' , 'halfTimeGoalsFor' , 'halfTimeGoalsAgainst' , 'shotsFor' , 'shotsAgainst' , 'shotsOnTargetFor' , 'shotsOnTargetAgainst' , 'freesFor' , 'freesAgainst' , 'cornersFor' , 'cornersAgainst' , 'yellowsFor' , 'yellowsAgainst' , 'redsFor' , 'redsAgainst' ] # Create a dictionary of the old column names to new column names home_mapping = { old_col : new_col for old_col , new_col in zip ( home_stats_cols , stats_cols_mapping )} away_mapping = { old_col : new_col for old_col , new_col in zip ( away_stats_cols , stats_cols_mapping )} # Put each team onto an individual row multi_line_stats = ( old_stats_df [[ 'gameId' ] + home_stats_cols ] # Filter for only the home team columns . rename ( columns = home_mapping ) # Rename the columns . assign ( homeGame = 1 ) # Assign homeGame=1 so that we can use a general function later . append (( old_stats_df [[ 'gameId' ] + away_stats_cols ]) # Append the away team columns . rename ( columns = away_mapping ) # Rename the away team columns . assign ( homeGame = 0 ), sort = True ) . sort_values ( by = 'gameId' ) # Sort the values . reset_index ( drop = True )) return multi_line_stats # Define a function which creates an EMA DataFrame from the stats DataFrame def create_stats_features_ema ( stats , span ): # Create a restructured DataFrames so that we can calculate EMA multi_line_stats = create_multiline_df_stats ( stats ) # Create a copy of the DataFrame ema_features = multi_line_stats [[ 'gameId' , 'team' , 'homeGame' ]] . copy () # Get the columns that we want to create EMA for feature_names = multi_line_stats . drop ( columns = [ 'gameId' , 'team' , 'homeGame' ]) . columns # Loop over the features for feature_name in feature_names : feature_ema = ( multi_line_stats . groupby ( 'team' )[ feature_name ] # Calculate the EMA . transform ( lambda row : row . ewm ( span = span , min_periods = 2 ) . mean () . shift ( 1 ))) # Shift the data down 1 so we don't leak data ema_features [ feature_name ] = feature_ema # Add the new feature to the DataFrame return ema_features # Apply the function stats_features = create_stats_features_ema ( stats , span = 5 ) stats_features . tail () gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9903 4952 Newcastle 1 4.301743 4.217300 11.789345 12.245066 0.797647 0.833658 0.644214 0.420832 2.323450e-10 3.333631e-01 11.335147 13.265955 3.211345 4.067990 1.848860 1.627140 9904 4953 Burnley 0 4.880132 5.165915 13.326703 8.800033 1.945502 0.667042 0.609440 0.529409 3.874405e-03 3.356120e-10 13.129631 10.642381 4.825874 3.970285 0.963527 0.847939 9905 4953 Fulham 1 4.550255 4.403060 10.188263 8.555589 2.531046 1.003553 0.860573 0.076949 1.002518e-04 8.670776e-03 17.463779 12.278877 8.334019 4.058213 0.980097 1.102974 9906 4954 Man United 1 3.832573 4.759683 11.640608 10.307946 1.397234 1.495032 1.034251 0.809280 6.683080e-05 1.320468e-05 8.963022 10.198642 3.216957 3.776900 1.040077 1.595650 9907 4954 Tottenham 0 3.042034 5.160211 8.991460 9.955635 1.332704 2.514789 0.573728 1.010491 4.522878e-08 1.354409e-05 12.543406 17.761004 3.757437 7.279845 1.478976 1.026601 As we can see, we now have averages for each team. Let's create a quick table to see the top 10 teams' goalsFor average EMAs since 2005. pd . DataFrame ( stats_features . groupby ( 'team' ) . goalsFor . mean () . sort_values ( ascending = False )[: 10 ]) goalsFor team Man United 1.895026 Chelsea 1.888892 Arsenal 1.876770 Man City 1.835863 Liverpool 1.771125 Tottenham 1.655063 Leicester 1.425309 Blackpool 1.390936 Everton 1.387110 Southampton 1.288349 Optimising Alpha It looks like Man United and Chelsea have been two of the best teams since 2005, based on goalsFor. Now that we have our stats features, we may be tempted to move on. However, we have arbitrarily chosen a span of 5. How do we know that this is the best value? We don't. Let's try and optimise this value. To do this, we will use a simple Logistic Regression model to create probabilistic predictions based on the stats features we created before. We will iterate a range of span values, from say, 3 to 15, and choose the value which produces a model with the lowest log loss, based on cross validation. To do this, we need to restructure our DataFrame back to how it was before. def restructure_stats_features ( stats_features ): non_features = [ 'homeGame' , 'team' , 'gameId' ] stats_features_restructured = ( stats_features . query ( 'homeGame == 1' ) . rename ( columns = { col : 'f_' + col + 'Home' for col in stats_features . columns if col not in non_features }) . rename ( columns = { 'team' : 'HomeTeam' }) . pipe ( pd . merge , ( stats_features . query ( 'homeGame == 0' ) . rename ( columns = { 'team' : 'AwayTeam' }) . rename ( columns = { col : 'f_' + col + 'Away' for col in stats_features . columns if col not in non_features })), on = [ 'gameId' ]) . pipe ( pd . merge , df [[ 'gameId' , 'result' ]], on = 'gameId' ) . dropna ()) return stats_features_restructured restructure_stats_features ( stats_features ) . head () gameId HomeTeam homeGame_x f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome AwayTeam homeGame_y f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway result 20 21 Birmingham 1 4.8 7.8 12.0 9.4 1.2 0.6 0.6 0.6 0.0 0.0 11.4 8.2 6.4 2.8 1.0 2.6 Middlesbrough 0 3.0 5.6 14.0 12.8 1.2 0.0 0.0 0.0 0.0 0.4 17.2 8.8 7.6 2.6 3.0 1.4 away 21 22 Portsmouth 1 2.6 4.6 21.8 16.6 2.0 0.6 1.0 0.0 0.0 0.0 8.0 10.4 3.6 4.0 3.2 1.8 Aston Villa 0 9.8 7.0 14.2 18.2 1.4 0.8 0.8 0.8 0.0 0.0 16.0 3.0 9.6 2.6 2.0 0.6 draw 22 23 Sunderland 1 5.0 5.0 11.6 18.0 1.8 0.4 1.0 0.4 0.4 0.6 14.6 6.0 5.2 3.2 1.2 2.6 Man City 0 7.8 3.6 8.6 12.4 0.6 1.2 0.6 0.6 0.0 0.0 10.6 11.4 2.4 6.8 3.0 1.4 away 23 24 Arsenal 1 3.0 7.4 17.0 18.6 0.6 0.8 0.0 0.0 0.4 0.0 6.2 11.4 4.0 6.6 1.6 1.8 Fulham 0 7.2 3.0 20.8 13.2 1.2 0.6 0.6 0.0 0.0 0.0 12.4 10.8 7.0 5.2 2.0 1.6 home 24 25 Blackburn 1 1.4 7.2 12.8 21.2 1.8 1.6 0.0 1.0 0.0 0.4 10.0 14.0 4.4 7.4 1.2 1.6 Tottenham 0 6.4 3.8 11.2 18.8 0.0 2.0 0.0 0.4 0.0 0.0 11.6 15.2 4.6 7.2 0.6 2.6 draw Now let's write a function that optimises our span based on log loss of the output of a Logistic Regression model. def optimise_alpha ( features ): le = LabelEncoder () y = le . fit_transform ( features . result ) # Encode the result from away, draw, home win to 0, 1, 2 X = features [[ col for col in features . columns if col . startswith ( 'f_' )]] # Only get the features - these all start with f_ lr = LogisticRegression () kfold = StratifiedKFold ( n_splits = 5 ) ave_cv_score = cross_val_score ( lr , X , y , scoring = 'neg_log_loss' , cv = kfold ) . mean () return ave_cv_score best_score = np . float ( 'inf' ) best_span = 0 cv_scores = [] # Iterate over a range of spans for span in range ( 1 , 120 , 3 ): stats_features = create_stats_features_ema ( stats , span = span ) restructured_stats_features = restructure_stats_features ( stats_features ) cv_score = optimise_alpha ( restructured_stats_features ) cv_scores . append ( cv_score ) if cv_score * - 1 < best_score : best_score = cv_score * - 1 best_span = span plt . style . use ( 'ggplot' ) plt . plot ( list ( range ( 1 , 120 , 3 )), ( pd . Series ( cv_scores ) *- 1 )) # Plot our results plt . title ( \"Optimising alpha\" ) plt . xlabel ( \"Span\" ) plt . ylabel ( \"Log Loss\" ) plt . show () print ( \"Our lowest log loss ( {:2f} ) occurred at a span of {} \" . format ( best_score , best_span )) Our lowest log loss (0.980835) occurred at a span of 55 The above method is just an example of how you can optimise hyparameters. Obviously this example has many limitations, such as attempting to optimise each statistic with the same alpha. However, for the rest of these tutorial series we will use this span value. Now let's create the rest of our features. For thorough explanations and the actual code behind some of the functions used, please refer to the data_preparation_functions.py script. Creating our Features DataFrame We will utilise pre-made functions to create all of our features in just a few lines of code. As part of this process we will create features which include margin weighted elo, an exponential average for asian handicap data, and odds as features. Our Elo function is essentially the same as the one we created in the AFL tutorial; if you would like to know more about Elo models please read this article. Note that the cell below may take a few minutes to run. # Create feature DataFrames features_all_games = create_all_games_features ( all_games ) C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive-models\\epl\\data_preparation_functions.py:419: RuntimeWarning: invalid value encountered in double_scalars .pipe(lambda df: (df.eloAgainst * df[goalsForOrAgainstCol]).sum() / df.eloAgainst.sum())) The features_all_games df includes elo for each team, as well as their win percentage at home and away over the past 5 and 38 games. For more information on how it was calculated, read through the data_preparation_functions script. features_all_games . head ( 3 ) Date awayWin awayWinPc38 awayWinPc5 eloAgainst eloFor gameId gameIdHistoric goalsAgainst goalsFor homeGame homeWin homeWinPc38 homeWinPc5 season team wtEloGoalsFor wtEloGoalsAgainst 0 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 2.0 1.0 1 0 NaN NaN 20012002 Charlton NaN NaN 1 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 1.0 2.0 0 0 NaN NaN 20012002 Everton NaN NaN 2 2001-08-18 0 NaN NaN 1500.0 1500.0 -1 2 1.0 2.0 1 1 NaN NaN 20012002 Derby NaN NaN The features_stats df includes all the expontential weighted averages for each stat in the stats df. # Create feature stats df features_stats = create_stats_features_ema ( stats , span = best_span ) features_stats . tail ( 3 ) gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9905 4953 Fulham 1 6.006967 5.045733 10.228997 9.965651 2.147069 1.093550 0.630485 0.364246 0.032937 0.043696 16.510067 11.718122 7.184386 4.645762 1.310424 1.389716 9906 4954 Man United 1 4.463018 5.461075 11.605712 10.870367 0.843222 1.586308 0.427065 0.730650 0.042588 0.027488 10.865754 13.003121 3.562675 4.626450 1.740735 1.712785 9907 4954 Tottenham 0 3.868619 6.362901 10.784145 10.140388 0.954928 2.100166 0.439129 0.799968 0.024351 0.026211 9.947515 16.460598 3.370010 6.136120 1.925005 1.364268 The features_odds df includes a moving average of some of the odds data. # Create feature_odds df features_odds = create_betting_features_ema ( betting , span = 10 ) features_odds . tail ( 3 ) gameId team avAsianHandicapOddsAgainst avAsianHandicapOddsFor avgreaterthan2.5 avlessthan2.5 sizeOfHandicap 9905 4953 Fulham 1.884552 1.985978 1.756776 2.128261 0.502253 9906 4954 Man United 1.871586 2.031787 1.900655 1.963478 -0.942445 9907 4954 Tottenham 1.947833 1.919607 1.629089 2.383593 -1.235630 The features market values has market values and the % of total market for each position. These values are in millions. # Create feature market values df features_market_values = create_market_values_features ( df ) # This creates a df with one game per row features_market_values . head ( 3 ) gameId Year HomeTeam AwayTeam defMktValH attMktValH gkMktValH totalMktValH midMktValH defMktValA attMktValA gkMktValA totalMktValA midMktValA attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% 0 1 2005 West Ham Blackburn 16.90 18.50 6.40 46.40 4.60 27.25 13.00 3.25 70.70 27.20 2.252911 1.583126 0.588168 3.477861 2.486940 4.010007 4.524247 2.297469 1.913986 2.916354 1 2 2005 Aston Villa Bolton 27.63 31.85 7.60 105.83 38.75 9.60 24.55 8.50 72.40 29.75 3.878659 2.989673 4.954673 3.803910 4.065926 1.412700 5.372543 6.008766 4.365456 2.986478 2 3 2005 Everton Man United 44.35 31.38 8.55 109.78 25.50 82.63 114.60 9.25 288.48 82.00 3.821423 13.955867 3.260494 10.484727 6.526378 12.159517 6.044111 6.538951 4.528392 11.899714 all_games_cols = [ 'Date' , 'gameId' , 'team' , 'season' , 'homeGame' , 'homeWinPc38' , 'homeWinPc5' , 'awayWinPc38' , 'awayWinPc5' , 'eloFor' , 'eloAgainst' , 'wtEloGoalsFor' , 'wtEloGoalsAgainst' ] # Join the features together features_multi_line = ( features_all_games [ all_games_cols ] . pipe ( pd . merge , features_stats . drop ( columns = 'homeGame' ), on = [ 'gameId' , 'team' ]) . pipe ( pd . merge , features_odds , on = [ 'gameId' , 'team' ])) # Put each instance on an individual row features_with_na = put_features_on_one_line ( features_multi_line ) market_val_feature_names = [ 'attMktH%' , 'attMktA%' , 'midMktH%' , 'midMktA%' , 'defMktH%' , 'defMktA%' , 'gkMktH%' , 'gkMktA%' , 'totalMktH%' , 'totalMktA%' ] # Merge our team values dataframe to features and result from df features_with_na = ( features_with_na . pipe ( pd . merge , ( features_market_values [ market_val_feature_names + [ 'gameId' ]]) . rename ({ col : 'f_' + col for col in market_val_feature_names }), on = 'gameId' ) . pipe ( pd . merge , df [[ 'HomeTeam' , 'AwayTeam' , 'gameId' , 'result' , 'B365A' , 'B365D' , 'B365H' ]], on = [ 'HomeTeam' , 'AwayTeam' , 'gameId' ])) # Drop NAs from calculating the rolling averages - don't drop Win Pc 38 and Win Pc 5 columns features = features_with_na . dropna ( subset = features_with_na . drop ( columns = [ col for col in features_with_na . columns if 'WinPc' in col ]) . columns ) # Fill NAs for the Win Pc columns features = features . fillna ( features . mean ()) features . head ( 3 ) Date gameId HomeTeam season homeGame f_homeWinPc38Home f_homeWinPc5Home f_awayWinPc38Home f_awayWinPc5Home f_eloForHome f_eloAgainstHome f_wtEloGoalsForHome f_wtEloGoalsAgainstHome f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome f_avAsianHandicapOddsAgainstHome f_avAsianHandicapOddsForHome f_avgreaterthan2.5Home f_avlessthan2.5Home f_sizeOfHandicapHome AwayTeam f_homeWinPc38Away f_homeWinPc5Away f_awayWinPc38Away f_awayWinPc5Away f_eloForAway f_eloAgainstAway f_wtEloGoalsForAway f_wtEloGoalsAgainstAway f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway f_avAsianHandicapOddsAgainstAway f_avAsianHandicapOddsForAway f_avgreaterthan2.5Away f_avlessthan2.5Away f_sizeOfHandicapAway attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% result B365A B365D B365H 20 2005-08-23 21 Birmingham 0506 1 0.394737 0.4 0.263158 0.2 1478.687038 1492.866048 1.061763 1.260223 4.981818 7.527273 12.000000 9.945455 1.018182 0.509091 0.509091 0.509091 0.000000 0.000000 11.945455 8.018182 6.490909 2.981818 1.000000 2.509091 1.9090 1.9455 2.0510 1.6735 -0.1375 Middlesbrough 0.394737 0.4 0.263158 0.2 1492.866048 1478.687038 1.12994 1.279873 2.545455 5.509091 13.545455 13.436364 1.018182 0.000000 0.000000 0.000000 0.0 0.490909 17.018182 8.072727 7.509091 2.509091 3.0 1.490909 1.9395 1.9095 2.0035 1.7155 0.3875 5.132983 5.260851 3.341048 4.289788 3.502318 4.168935 2.332815 3.216457 3.934396 4.522205 away 2.75 3.2 2.50 21 2005-08-23 22 Portsmouth 0506 1 0.447368 0.4 0.263158 0.4 1405.968416 1489.229314 1.147101 1.503051 2.509091 4.963636 21.981818 16.054545 2.000000 0.509091 1.000000 0.000000 0.000000 0.000000 8.454545 10.490909 3.963636 4.454545 3.018182 1.527273 1.8965 1.9690 2.0040 1.7005 0.2500 Aston Villa 0.447368 0.4 0.263158 0.4 1489.229314 1405.968416 1.17516 1.263229 9.527273 7.000000 14.472727 17.563636 1.490909 0.981818 0.981818 0.981818 0.0 0.000000 15.545455 3.000000 9.054545 2.509091 2.0 0.509091 1.8565 1.9770 1.8505 1.8485 0.7125 3.738614 3.878659 4.494368 4.954673 2.884262 4.065926 3.746642 5.372543 3.743410 4.365456 draw 2.75 3.2 2.50 22 2005-08-23 23 Sunderland 0506 1 0.236842 0.0 0.236842 0.4 1277.888970 1552.291880 0.650176 1.543716 5.000000 5.000000 12.418182 17.545455 1.981818 0.490909 1.000000 0.490909 0.490909 0.509091 14.509091 6.909091 5.018182 3.927273 1.018182 2.509091 1.8520 1.9915 1.8535 1.8500 0.7125 Man City 0.236842 0.0 0.236842 0.4 1552.291880 1277.888970 1.28875 1.287367 7.527273 3.509091 8.963636 12.490909 0.509091 1.018182 0.509091 0.509091 0.0 0.000000 10.963636 11.945455 2.490909 6.981818 3.0 1.490909 1.8150 2.0395 2.0060 1.7095 -0.2000 0.706318 3.750792 1.476812 1.070209 2.634096 4.455890 0.777605 4.913050 1.499427 3.151477 away 2.50 3.2 2.75 We now have a features DataFrame ready, with all the feature columns beginning with the \"f_\". In the next section, we will walk through the modelling process to try and find the best type of model to use. 03. Model Building & Hyperparameter Tuning Welcome to the third part of this Machine Learning Walkthrough. This tutorial will focus on the model building process, including how to tune hyperparameters. In the [next tutorial], we will create weekly predictions based on the model we have created here. Specifically, this tutorial will cover a few things: Choosing which Machine Learning algorithm to use from a variety of choices Hyperparameter Tuning Overfitting/Underfitting Choosing an Algorithm The best way to decide on specific algorithm to use, is to try them all! To do this, we will define a function which we first used in our AFL Predictions tutorial. This will iterate over a number of algorithms and give us a good indication of which algorithms are suited for this dataset and exercise. Let's first use grab the features we created in the last tutorial. This may take a minute or two to run. ## Import libraries from data_preparation_functions import * import pandas as pd import numpy as np import matplotlib as plt import seaborn as sns import warnings from sklearn import linear_model , tree , discriminant_analysis , naive_bayes , ensemble , gaussian_process from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV from sklearn.metrics import log_loss , confusion_matrix warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.max_columns' , 100 ) features = create_feature_df () Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . To start our modelling process, we need to make a training set, a test set and a holdout set. As we are using cross validation, we will make our training set all of the seasons up until 2017/18, and we will use the 2017/18 season as the test set. feature_list = [ col for col in features . columns if col . startswith ( \"f_\" )] betting_features = [] le = LabelEncoder () # Initiate a label encoder to transform the labels 'away', 'draw', 'home' to 0, 1, 2 # Grab all seasons except for 17/18 to use CV with all_x = features . loc [ features . season != '1718' , [ 'gameId' ] + feature_list ] all_y = features . loc [ features . season != '1718' , 'result' ] all_y = le . fit_transform ( all_y ) # Create our training vector as the seasons except 16/17 and 17/18 train_x = features . loc [ ~ features . season . isin ([ '1617' , '1718' ]), [ 'gameId' ] + feature_list ] train_y = le . transform ( features . loc [ ~ features . season . isin ([ '1617' , '1718' ]), 'result' ]) # Create our holdout vectors as the 16/17 season holdout_x = features . loc [ features . season == '1617' , [ 'gameId' ] + feature_list ] holdout_y = le . transform ( features . loc [ features . season == '1617' , 'result' ]) # Create our test vectors as the 17/18 season test_x = features . loc [ features . season == '1718' , [ 'gameId' ] + feature_list ] test_y = le . transform ( features . loc [ features . season == '1718' , 'result' ]) # Create a list of standard classifiers classifiers = [ #GLM linear_model . LogisticRegressionCV (), #Navies Bayes naive_bayes . BernoulliNB (), naive_bayes . GaussianNB (), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis (), discriminant_analysis . QuadraticDiscriminantAnalysis (), #Ensemble Methods ensemble . AdaBoostClassifier (), ensemble . BaggingClassifier (), ensemble . ExtraTreesClassifier (), ensemble . GradientBoostingClassifier (), ensemble . RandomForestClassifier (), #Gaussian Processes gaussian_process . GaussianProcessClassifier (), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # xgb.XGBClassifier() ] def find_best_algorithms ( classifier_list , X , y ): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold ( n_splits = 5 ) # Grab the cross validation scores for each algorithm cv_results = [ cross_val_score ( classifier , X , y , scoring = \"neg_log_loss\" , cv = kfold ) for classifier in classifier_list ] cv_means = [ cv_result . mean () * - 1 for cv_result in cv_results ] cv_std = [ cv_result . std () for cv_result in cv_results ] algorithm_names = [ alg . __class__ . __name__ for alg in classifiers ] # Create a DataFrame of all the CV results cv_results = pd . DataFrame ({ \"Mean Log Loss\" : cv_means , \"Log Loss Std\" : cv_std , \"Algorithm\" : algorithm_names }) . sort_values ( by = 'Mean Log Loss' ) return cv_results algorithm_results = find_best_algorithms ( classifiers , all_x , all_y ) algorithm_results Mean Log Loss Log Loss Std Algorithm 0 0.966540 0.020347 LogisticRegressionCV 3 0.986679 0.015601 LinearDiscriminantAnalysis 1 1.015197 0.017466 BernoulliNB 10 1.098612 0.000000 GaussianProcessClassifier 5 1.101281 0.044383 AdaBoostClassifier 8 1.137778 0.153391 GradientBoostingClassifier 7 2.093981 0.284831 ExtraTreesClassifier 9 2.095088 0.130367 RandomForestClassifier 6 2.120571 0.503132 BaggingClassifier 4 4.065796 1.370119 QuadraticDiscriminantAnalysis 2 5.284171 0.826991 GaussianNB We can see that LogisticRegression seems to perform the best out of all the algorithms, and some algorithms have a very high log loss. This is most likely due to overfitting. It would definitely be useful to condense our features down to reduce the dimensionality of the dataset. Hyperparameter Tuning For now, however, we will use logistic regression. Let's first try and tune a logistic regression model with cross validation. To do this, we will use grid search . Grid search essentially tries out each combination of values and finds the model with the lowest error metric, which in our case is log loss. 'C' in logistic regression determines the amount of regularization. Lower values increase regularization. # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.01 , 0.05 , 0.2 , 1 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } kfold = StratifiedKFold ( n_splits = 5 ) gs = GridSearchCV ( LogisticRegression (), param_grid = lr_grid , cv = kfold , scoring = 'neg_log_loss' ) gs . fit ( all_x , all_y ) print ( \"Best log loss: {} \" . format ( gs . best_score_ *- 1 )) best_lr_params = gs . best_params_ Best log loss : 0.9669551970849734 Defining a Baseline We should also define a baseline, as we don't really know if our log loss is good or bad. Randomly assigning a 1/3 chance to each selection yields a log loss of log3 = 1.09. However, what we are really interested in, is how our model performs relative to the odds. So let's find the log loss of the odds. # Finding the log loss of the odds log_loss ( all_y , 1 / all_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) 0.9590114943474463 This is good news: our algorithm almost beats the bookies in terms of log loss. It would be great if we could beat this result. Analysing the Errors Made Now that we have a logistic regression model tuned, let's see what type of errors it made. To do this we will look at the confusion matrix produced when we predict our holdout set. lr = LogisticRegression ( ** best_lr_params ) # Instantiate the model lr . fit ( train_x , train_y ) # Fit our model lr_predict = lr . predict ( holdout_x ) # Predict the holdout values # Create a confusion matrix c_matrix = ( pd . DataFrame ( confusion_matrix ( holdout_y , lr_predict ), columns = le . classes_ , index = le . classes_ ) . rename_axis ( 'Actual' ) . rename_axis ( 'Predicted' , axis = 'columns' )) c_matrix Predicted away draw home Actual away 77 0 32 draw 26 3 55 home 33 7 147 As we can see, when we predicted 'away' as the result, we correctly predicted 79 / 109 results, a hit rate of 70.6%. However, when we look at our draw hit rate, we only predicted 6 / 84 correctly, meaning we only had a hit rate of around 8.3%. For a more in depth analysis of our predictions, please skip to the Analysing Predictions & Staking Strategies section of the tutorial. Before we move on, however, let's use our model to predict the 17/18 season and compare how we went with the odds. # Get test predictions test_lr = LogisticRegression ( ** best_lr_params ) test_lr . fit ( all_x , all_y ) test_predictions_probs = lr . predict_proba ( test_x ) test_predictions = lr . predict ( test_x ) test_ll = log_loss ( test_y , test_predictions_probs ) test_accuracy = ( test_predictions == test_y ) . mean () print ( \"Our predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.2f} \" . format ( test_ll , test_accuracy )) Our predictions for the 2017/18 season have a log loss of: 0.95767 and an accuracy of: 0.56 # Get accuracy and log loss based on the odds odds_ll = log_loss ( test_y , 1 / test_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) odds_predictions = test_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]] . apply ( lambda row : row . idxmin ()[ 2 : 6 ], axis = 1 ) . values odds_accuracy = ( odds_predictions == le . inverse_transform ( test_y )) . mean () print ( \"Odds predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.3f} \" . format ( odds_ll , odds_accuracy )) Odds predictions for the 2017/18 season have a log loss of: 0.94635 and an accuracy of: 0.545 Results There we have it! The odds predicted 54.5% of EPL games correctly in the 2017/18 season, whilst our model predicted 54% correctly. This is a decent result for the first iteration of our model. In future iterations, we could wait a certain number of matches each season and calculate EMAs for on those first n games. This may help the issue of players switching clubs and teams becoming relatively stronger/weaker compared to previous seasons. 04. Weekly Predictions Welcome to the third part of this Machine Learning Walkthrough. This tutorial will be a walk through of creating weekly EPL predictions from the basic logistic regression model we built in the previous tutorial. We will then analyse our predictions and create staking strategies in the next tutorial. Specifically, this tutorial will cover a few things: Obtaining Weekly Odds / Game Info Using Betfair's API Data Wrangling This Week's Game Info Into Our Feature Set Obtaining Weekly Odds / Game Info Using Betfair's API The first thing we need to do to create weekly predictions is get both the games being played this week, as well as match odds from Betfair to be used as features. To make this process easier, I have created a csv file with the fixture for the 2018/19 season. Let's load that now. ## Import libraries import pandas as pd from weekly_prediction_functions import * from data_preparation_functions import * from sklearn.metrics import log_loss , confusion_matrix import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.max_columns' , 100 ) fixture = ( pd . read_csv ( 'data/fixture.csv' ) . assign ( Date = lambda df : pd . to_datetime ( df . Date ))) fixture . head () Date Time (AEST) HomeTeam AwayTeam Venue TV Year round season 0 2018-08-11 5:00 AM Man United Leicester Old Trafford, Manchester Optus, Fox Sports (delay) 2018 1 1819 1 2018-08-11 9:30 PM Newcastle Tottenham St.James\u2019 Park, Newcastle Optus, SBS 2018 1 1819 2 2018-08-12 12:00 AM Bournemouth Cardiff Vitality Stadium, Bournemouth Optus 2018 1 1819 3 2018-08-12 12:00 AM Fulham Crystal Palace Craven Cottage, London Optus 2018 1 1819 4 2018-08-12 12:00 AM Huddersfield Chelsea John Smith\u2019s Stadium, Huddersfield Optus, Fox Sports (delay) 2018 1 1819 Now we are going to connect to the API and retrieve game level information for the next week. To do this, we will use an R script. If you are not familiar with R, don't worry, it is relatively simple to read through. For this, we will run the script weekly_game_info_puller.R. Go ahead and run that script now. Note that for this step, you will require a Betfair API App Key. If you don't have one, visit this page and follow the instructions . I will upload an updated weekly file, so you can follow along regardless of if you have an App Key or not. Let's load that file in now. game_info = create_game_info_df ( \"data/weekly_game_info.csv\" ) game_info . head ( 3 ) AwayTeam HomeTeam awaySelectionId drawSelectionId homeSelectionId draw marketId marketStartTime totalMatched eventId eventName homeOdds drawOdds awayOdds competitionId Date localMarketStartTime 0 Arsenal Cardiff 1096 58805 79343 The Draw 1.146897152 2018-09-02 12:30:00 30123.595116 28852020 Cardiff v Arsenal 7.00 4.3 1.62 10932509 2018-09-02 Sun September 2, 10:30PM 1 Bournemouth Chelsea 1141 58805 55190 The Draw 1.146875421 2018-09-01 14:00:00 30821.329656 28851426 Chelsea v Bournemouth 1.32 6.8 12.00 10932509 2018-09-01 Sun September 2, 12:00AM 2 Fulham Brighton 56764 58805 18567 The Draw 1.146875746 2018-09-01 14:00:00 16594.833096 28851429 Brighton v Fulham 2.36 3.5 3.50 10932509 2018-09-01 Sun September 2, 12:00AM Finally, we will use the API to grab the weekly odds. This R script is also provided, but I have also included the weekly odds csv for convenience. odds = ( pd . read_csv ( 'data/weekly_epl_odds.csv' ) . replace ({ 'Man Utd' : 'Man United' , 'C Palace' : 'Crystal Palace' })) odds . head ( 3 ) HomeTeam AwayTeam f_homeOdds f_drawOdds f_awayOdds 0 Leicester Liverpool 7.80 5.1 1.48 1 Brighton Fulham 2.36 3.5 3.50 2 Everton Huddersfield 1.54 4.4 8.20 Data Wrangling This Week's Game Info Into Our Feature Set Now we have the arduous task of wrangling all of this info into a feature set that we can use to predict this week's games. Luckily our functions we created earlier should work if we just append the non-features to our main dataframe. df = create_df ( 'data/epl_data.csv' ) df . head () AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.50 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.90 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.00 2.38 8 A Wiley 2.75 3.25 2.40 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.30 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.40 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.00 2.10 8 M Riley 3.10 3.25 2.20 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.00 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.80 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.20 3.75 8 G Poll 1.80 3.30 4.50 2005 0506 3 0 1 away 3 6.0 13.0 0.0 7.0 4.0 2.0 Birmingham 2.87 3.25 2.37 2.80 3.20 2.30 56.0 21.0 0.00 1.69 2.04 2.87 2.05 1.81 3.16 2.31 1.77 2.24 3.05 2.11 1.85 3.30 2.60 36.0 2005-08-13 13 E0 0.0 0.0 D 6.0 12.0 0.0 15.0 7.0 0.0 0.0 D 1.0 Fulham 2.9 3.0 2.2 2.88 3.00 2.25 8 R Styles 2.80 3.25 2.35 2005 0506 4 0 0 draw 4 6.0 11.0 0.0 13.0 3.0 3.0 West Brom 5.00 3.40 1.72 4.80 3.45 1.65 55.0 23.0 -0.75 1.77 1.94 4.79 1.76 2.10 3.38 1.69 1.90 2.10 5.60 1.83 2.19 3.63 1.80 36.0 2005-08-13 13 E0 0.0 0.0 D 3.0 13.0 0.0 15.0 8.0 0.0 0.0 D 2.0 Man City 4.2 3.2 1.7 4.50 3.25 1.67 8 C Foy 5.00 3.25 1.75 2005 0506 5 0 0 draw Now we need to specify which game week we would like to predict. We will then filter the fixture for this game week and append this info to the main DataFrame round_to_predict = int ( input ( \"Which game week would you like to predict? Please input next week's Game Week \\n \" )) Which game week would you like to predict? Please input next week's Game Week 4 future_predictions = ( fixture . loc [ fixture [ 'round' ] == round_to_predict , [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]] . pipe ( pd . merge , odds , on = [ 'HomeTeam' , 'AwayTeam' ]) . rename ( columns = { 'f_homeOdds' : 'B365H' , 'f_awayOdds' : 'B365A' , 'f_drawOdds' : 'B365D' }) . assign ( season = lambda df : df . season . astype ( str ))) df_including_future_games = ( pd . read_csv ( 'data/epl_data.csv' , dtype = { 'season' : str }) . assign ( Date = lambda df : pd . to_datetime ( df . Date )) . pipe ( lambda df : df . dropna ( thresh = len ( df ) - 2 , axis = 1 )) # Drop cols with NAs . dropna ( axis = 0 ) # Drop rows with NAs . sort_values ( 'Date' ) . append ( future_predictions , sort = True ) . reset_index ( drop = True ) . assign ( gameId = lambda df : list ( df . index + 1 ), Year = lambda df : df . Date . apply ( lambda row : row . year ), homeWin = lambda df : df . apply ( lambda row : 1 if row . FTHG > row . FTAG else 0 , axis = 1 ), awayWin = lambda df : df . apply ( lambda row : 1 if row . FTAG > row . FTHG else 0 , axis = 1 ), result = lambda df : df . apply ( lambda row : 'home' if row . FTHG > row . FTAG else ( 'draw' if row . FTHG == row . FTAG else 'away' ), axis = 1 ))) df_including_future_games . tail ( 12 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 4952 4.0 8.0 0.0 12.0 2.0 1.0 Burnley 4.33 3.40 2.00 4.0 3.3 2.00 39.0 20.0 -0.25 1.65 2.22 4.14 2.22 1.69 3.36 1.98 1.72 2.31 4.5 2.32 1.74 3.57 2.04 36.0 2018-08-26 26.0 E0 2.0 4.0 H 6.0 11.0 0.0 25.0 12.0 2.0 3.0 H 2.0 Fulham 4.10 3.35 1.97 3.90 3.2 2.00 8.0 D Coote 4.33 3.4 2.0 2018 1819 4953 1 0 home 4953 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.9 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.0 1.76 2.25 3.40 2.67 40.0 2018-08-27 27.0 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.60 2.75 3.2 2.55 8.0 C Pawson 2.90 3.3 2.6 2018 1819 4954 0 1 away 4954 NaN NaN NaN NaN NaN NaN Liverpool 1.48 5.10 7.80 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-01 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Leicester NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4955 0 0 away 4955 NaN NaN NaN NaN NaN NaN Fulham 3.50 3.50 2.36 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Brighton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4956 0 0 away 4956 NaN NaN NaN NaN NaN NaN Man United 1.70 3.90 6.60 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Burnley NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4957 0 0 away 4957 NaN NaN NaN NaN NaN NaN Bournemouth 12.00 6.80 1.32 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Chelsea NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4958 0 0 away 4958 NaN NaN NaN NaN NaN NaN Southampton 4.50 3.55 2.04 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Crystal Palace NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4959 0 0 away 4959 NaN NaN NaN NaN NaN NaN Huddersfield 8.20 4.40 1.54 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Everton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4960 0 0 away 4960 NaN NaN NaN NaN NaN NaN Wolves 2.98 3.50 2.62 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN West Ham NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4961 0 0 away 4961 NaN NaN NaN NaN NaN NaN Newcastle 32.00 12.50 1.12 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Man City NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4962 0 0 away 4962 NaN NaN NaN NaN NaN NaN Arsenal 1.62 4.30 7.00 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Cardiff NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4963 0 0 away 4963 NaN NaN NaN NaN NaN NaN Tottenham 1.68 4.30 5.90 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-03 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Watford NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4964 0 0 away As we can see, what we have done is appended the Game information to our main DataFrame. The rest of the info is left as NAs, but this will be filled when we created our rolling average features. This is a 'hacky' type of way to complete this task, but works well as we can use the same functions that we created in the previous tutorials on this DataFrame. We now need to add the odds from our odds DataFrame, then we can just run our create features functions as usual. Predicting Next Gameweek's Results Now that we have our feature DataFrame, all we need to do is split the feature DataFrame up into a training set and next week's games, then use the model we tuned in the last tutorial to create predictions! features = create_feature_df ( df = df_including_future_games ) Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . # Create a feature DataFrame for this week's games. production_df = pd . merge ( future_predictions , features , on = [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]) # Create a training DataFrame training_df = features [ ~ features . gameId . isin ( production_df . gameId )] feature_names = [ col for col in training_df if col . startswith ( 'f_' )] le = LabelEncoder () train_y = le . fit_transform ( training_df . result ) train_x = training_df [ feature_names ] lr = LogisticRegression ( C = 0.01 , solver = 'liblinear' ) lr . fit ( train_x , train_y ) predicted_probs = lr . predict_proba ( production_df [ feature_names ]) predicted_odds = 1 / predicted_probs # Assign the modelled odds to our predictions df predictions_df = ( production_df . loc [:, [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'B365H' , 'B365D' , 'B365A' ]] . assign ( homeModelledOdds = [ i [ 2 ] for i in predicted_odds ], drawModelledOdds = [ i [ 1 ] for i in predicted_odds ], awayModelledOdds = [ i [ 0 ] for i in predicted_odds ]) . rename ( columns = { 'B365H' : 'BetfairHomeOdds' , 'B365D' : 'BetfairDrawOdds' , 'B365A' : 'BetfairAwayOdds' })) predictions_df Date HomeTeam AwayTeam BetfairHomeOdds BetfairDrawOdds BetfairAwayOdds homeModelledOdds drawModelledOdds awayModelledOdds 0 2018-09-01 Leicester Liverpool 7.80 5.10 1.48 5.747661 5.249857 1.573478 1 2018-09-02 Brighton Fulham 2.36 3.50 3.50 2.183193 3.803120 3.584057 2 2018-09-02 Burnley Man United 6.60 3.90 1.70 5.282620 4.497194 1.699700 3 2018-09-02 Chelsea Bournemouth 1.32 6.80 12.00 1.308366 6.079068 14.047070 4 2018-09-02 Crystal Palace Southampton 2.04 3.55 4.50 2.202871 4.213695 3.239122 5 2018-09-02 Everton Huddersfield 1.54 4.40 8.20 1.641222 3.759249 8.020055 6 2018-09-02 West Ham Wolves 2.62 3.50 2.98 1.999816 4.000456 4.000279 7 2018-09-02 Man City Newcastle 1.12 12.50 32.00 1.043103 29.427939 136.231983 8 2018-09-02 Cardiff Arsenal 7.00 4.30 1.62 6.256929 4.893445 1.572767 9 2018-09-03 Watford Tottenham 5.90 4.30 1.68 5.643663 4.338926 1.688224 Above are the predictions for this Gameweek's matches. In the next tutorial we will explore the errors our model has made, and work on creating a profitable betting strategy. Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"EPL ML walk through in Python"},{"location":"modelling/EPLmlPython/#epl-machine-learning-walkthrough","text":"","title":"EPL Machine Learning Walkthrough"},{"location":"modelling/EPLmlPython/#01-data-acquisition-exploration","text":"Welcome to the first part of this Machine Learning Walkthrough. This tutorial will be made of two parts; how we actually acquired our data (programmatically) and exploring the data to find potential features to use in the next tutorial .","title":"01. Data Acquisition &amp; Exploration"},{"location":"modelling/EPLmlPython/#data-acquisition","text":"We will be grabbing our data from football-data.co.uk , which has an enormous amount of soccer data dating back to the 90s. They also generously allow us to use it for free! However, the data is in separate CSVs based on the season. That means we would need to manually download 20 different files if we wanted the past 20 seasons. Rather than do this laborious and boring task, let's create a function which downloads the files for us, and appends them all into one big CSV. To do this, we will use BeautifulSoup, a Python library which helps to pull data from HTML and XML files. We will then define a function which collates all the data for us into one DataFrame. # Import Modules import pandas as pd import requests from bs4 import BeautifulSoup import datetime pd . set_option ( 'display.max_columns' , 100 ) import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline from data_preparation_functions import * def grab_epl_data (): # Connect to football-data.co.uk res = requests . get ( \"http://www.football-data.co.uk/englandm.php\" ) # Create a BeautifulSoup object soup = BeautifulSoup ( res . content , 'lxml' ) # Find the tables with the links to the data in them. table = soup . find_all ( 'table' , { 'align' : 'center' , 'cellspacing' : '0' , 'width' : '800' })[ 1 ] body = table . find_all ( 'td' , { 'valign' : 'top' })[ 1 ] # Grab the urls for the csv files links = [ link . get ( 'href' ) for link in body . find_all ( 'a' )] links_text = [ link_text . text for link_text in body . find_all ( 'a' )] data_urls = [] # Create a list of links prefix = 'http://www.football-data.co.uk/' for i , text in enumerate ( links_text ): if text == 'Premier League' : data_urls . append ( prefix + links [ i ]) # Get rid of last 11 uls as these don't include match stats and odds, and we # only want from 2005 onwards data_urls = data_urls [: - 12 ] df = pd . DataFrame () # Iterate over the urls for url in data_urls : # Get the season and make it a column season = url . split ( '/' )[ 4 ] print ( f \"Getting data for season { season } \" ) # Read the data from the url into a DataFrame temp_df = pd . read_csv ( url ) temp_df [ 'season' ] = season # Create helpful columns like Day, Month, Year, Date etc. so that our data is clean temp_df = ( temp_df . dropna ( axis = 'columns' , thresh = temp_df . shape [ 0 ] - 30 ) . assign ( Day = lambda df : df . Date . str . split ( '/' ) . str [ 0 ], Month = lambda df : df . Date . str . split ( '/' ) . str [ 1 ], Year = lambda df : df . Date . str . split ( '/' ) . str [ 2 ]) . assign ( Date = lambda df : df . Month + '/' + df . Day + '/' + df . Year ) . assign ( Date = lambda df : pd . to_datetime ( df . Date )) . dropna ()) # Append the temp_df to the main df df = df . append ( temp_df , sort = True ) # Drop all NAs df = df . dropna ( axis = 1 ) . dropna () . sort_values ( by = 'Date' ) print ( \"Finished grabbing data.\" ) return df df = grab_epl_data () # df.to_csv(\"data/epl_data.csv\", index=False) Getting data for season 1819 Getting data for season 1718 Getting data for season 1617 Getting data for season 1516 Getting data for season 1415 Getting data for season 1314 Getting data for season 1213 Getting data for season 1112 Getting data for season 1011 Getting data for season 0910 Getting data for season 0809 Getting data for season 0708 Getting data for season 0607 Getting data for season 0506 Finished grabbing data . Whenever we want to update our data (for example if we want the most recent Gameweek included), all we have to do is run that function and then save the data to a csv with the commented out line above.","title":"Data Acquisition"},{"location":"modelling/EPLmlPython/#data-exploration","text":"Now that we have our data, let's explore it. Let's first look at home team win rates since 2005 to see if there is a consistent trend. To get an idea of what our data looks like, we'll look at the tail of the dataset first. df . tail ( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season 28 3.0 11.0 0.0 9.0 3.0 2.0 Crystal Palace 3.00 3.25 2.60 2.95 3.1 2.55 42.0 20.0 -0.25 1.71 2.13 2.92 1.73 2.16 3.22 2.55 1.79 2.21 3.04 1.77 2.23 3.36 2.66 39.0 2018-08-26 26 E0 1.0 2.0 H 6.0 14.0 0.0 13.0 5.0 0.0 0.0 D 4.0 Watford 2.95 3.20 2.5 2.90 3.1 2.50 08 A Taylor 2.90 3.3 2.6 18 1819 27 5.0 8.0 0.0 15.0 3.0 1.0 Chelsea 1.66 4.00 5.75 1.67 3.8 5.25 42.0 22.0 1.00 1.92 1.88 1.67 2.18 1.71 3.90 5.25 2.01 1.95 1.71 2.28 1.76 4.17 5.75 40.0 2018-08-26 26 E0 2.0 1.0 A 4.0 16.0 0.0 6.0 2.0 0.0 0.0 D 3.0 Newcastle 1.70 3.75 5.0 1.67 3.8 5.25 08 P Tierney 1.67 4.0 5.5 18 1819 29 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.90 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.00 1.76 2.25 3.40 2.67 40.0 2018-08-27 27 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.6 2.75 3.2 2.55 08 C Pawson 2.90 3.3 2.6 18 1819 # Create Home Win, Draw Win and Away Win columns df = df . assign ( homeWin = lambda df : df . apply ( lambda row : 1 if row . FTHG > row . FTAG else 0 , axis = 'columns' ), draw = lambda df : df . apply ( lambda row : 1 if row . FTHG == row . FTAG else 0 , axis = 'columns' ), awayWin = lambda df : df . apply ( lambda row : 1 if row . FTHG < row . FTAG else 0 , axis = 'columns' ))","title":"Data Exploration"},{"location":"modelling/EPLmlPython/#home-ground-advantage","text":"win_rates = \\ ( df . groupby ( 'season' ) . mean () . loc [:, [ 'homeWin' , 'draw' , 'awayWin' ]]) win_rates homeWin draw awayWin season 0506 0.505263 0.202632 0.292105 0607 0.477573 0.258575 0.263852 0708 0.463158 0.263158 0.273684 0809 0.453826 0.255937 0.290237 0910 0.507895 0.252632 0.239474 1011 0.471053 0.292105 0.236842 1112 0.450000 0.244737 0.305263 1213 0.433862 0.285714 0.280423 1314 0.472973 0.208108 0.318919 1415 0.453826 0.245383 0.300792 1516 0.414248 0.282322 0.303430 1617 0.492105 0.221053 0.286842 1718 0.455263 0.260526 0.284211 1819 0.466667 0.200000 0.333333","title":"Home Ground Advantage"},{"location":"modelling/EPLmlPython/#findings","text":"As we can see, winrates across home team wins, draws and away team wins are very consistent. It seems that the home team wins around 46-47% of the time, the draw happens about 25% of the time, and the away team wins about 27% of the time. Let's plot this DataFrame so that we can see the trend more easily. # Set the style plt . style . use ( 'ggplot' ) fig = plt . figure () ax = fig . add_subplot ( 111 ) home_line = ax . plot ( win_rates . homeWin , label = 'Home Win Rate' ) away_line = ax . plot ( win_rates . awayWin , label = 'Away Win Rate' ) draw_line = ax . plot ( win_rates . draw , label = 'Draw Win Rate' ) ax . set_xlabel ( \"season\" ) ax . set_ylabel ( \"Win Rate\" ) plt . title ( \"Win Rates\" , fontsize = 16 ) # Add the legend locations home_legend = plt . legend ( handles = home_line , loc = 'upper right' , bbox_to_anchor = ( 1 , 1 )) ax = plt . gca () . add_artist ( home_legend ) away_legend = plt . legend ( handles = away_line , loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.4 )) ax = plt . gca () . add_artist ( away_legend ) draw_legend = plt . legend ( handles = draw_line , loc = 'center right' , bbox_to_anchor = ( 0.95 , 0.06 )) As we can see, the winrates are relatively stable each season, except for in 14/15 when the home win rate drops dramatically. Out of interest, let's also have a look at which team has the best home ground advantage. Let's define HGA as home win rate - away win rate. And then plot some of the big clubs' HGA against each other. home_win_rates = \\ ( df . groupby ([ 'HomeTeam' ]) . homeWin . mean ()) away_win_rates = \\ ( df . groupby ([ 'AwayTeam' ]) . awayWin . mean ()) hga = ( home_win_rates - away_win_rates ) . reset_index () . rename ( columns = { 0 : 'HGA' }) . sort_values ( by = 'HGA' , ascending = False ) hga . head ( 10 ) HomeTeam HGA 15 Fulham 0.315573 7 Brighton 0.304762 20 Man City 0.244980 14 Everton 0.241935 30 Stoke 0.241131 10 Charlton 0.236842 0 Arsenal 0.236140 27 Reading 0.234962 33 Tottenham 0.220207 21 Man United 0.215620 So the club with the best HGA is Fulham - interesting. This is most likely because Fulham have won 100% of home games in 2018 so far which is skewing the mean. Let's see how the HGA for some of the big clubs based compare over seasons. big_clubs = [ 'Liverpool' , 'Man City' , 'Man United' , 'Chelsea' , 'Arsenal' ] home_win_rates_5 = df [ df . HomeTeam . isin ( big_clubs )] . groupby ([ 'HomeTeam' , 'season' ]) . homeWin . mean () away_win_rates_5 = df [ df . AwayTeam . isin ( big_clubs )] . groupby ([ 'AwayTeam' , 'season' ]) . awayWin . mean () hga_top_5 = home_win_rates_5 - away_win_rates_5 hga_top_5 . unstack ( level = 0 ) HomeTeam Arsenal Chelsea Liverpool Man City Man United season 0506 0.421053 0.368421 0.263158 0.263158 0.052632 0607 0.263158 0.000000 0.421053 -0.052632 0.105263 0708 0.210526 -0.052632 0.157895 0.368421 0.368421 0809 0.105263 -0.157895 -0.052632 0.578947 0.210526 0910 0.368421 0.368421 0.421053 0.315789 0.263158 1011 0.157895 0.368421 0.368421 0.263158 0.684211 1112 0.157895 0.315789 -0.105263 0.421053 0.105263 1213 0.052632 0.105263 0.105263 0.248538 0.201754 1314 0.143275 0.251462 0.307018 0.362573 -0.026316 1415 0.131579 0.210526 0.105263 0.210526 0.421053 1516 0.210526 -0.105263 0.000000 0.263158 0.263158 1617 0.263158 0.210526 0.105263 -0.052632 -0.105263 1718 0.578947 0.052632 0.157895 0.000000 0.263158 1819 0.500000 0.000000 0.000000 0.500000 0.500000 Now let's plot it. sns . lineplot ( x = 'season' , y = 'HGA' , hue = 'team' , data = hga_top_5 . reset_index () . rename ( columns = { 0 : 'HGA' , 'HomeTeam' : 'team' })) plt . legend ( loc = 'lower center' , ncol = 6 , bbox_to_anchor = ( 0.45 , - 0.2 )) plt . title ( \"HGA Among the top 5 clubs\" , fontsize = 14 ) plt . show () The results here seem to be quite erratic, although it seems that Arsenal consistently has a HGA above 0. Let's now look at the distributions of each of our columns. The odds columns are likely to be highly skewed, so we may have to account for this later. for col in df . select_dtypes ( 'number' ) . columns : sns . distplot ( df [ col ]) plt . title ( f \"Distribution for { col } \" ) plt . show ()","title":"Findings"},{"location":"modelling/EPLmlPython/#exploring-referee-home-ground-bias","text":"What may be of interest is whether certain referees are correlated with the home team winning more often. Let's explore referee home ground bias for referees for the top 10 Referees based on games. print ( 'Overall Home Win Rate: {:.4} %' . format ( df . homeWin . mean () * 100 )) # Get the top 10 refs based on games top_10_refs = df . Referee . value_counts () . head ( 10 ) . index df [ df . Referee . isin ( top_10_refs )] . groupby ( 'Referee' ) . homeWin . mean () . sort_values ( ascending = False ) Overall Home Win Rate: 46.55% Referee L Mason 0.510373 C Foy 0.500000 M Clattenburg 0.480000 M Jones 0.475248 P Dowd 0.469880 M Atkinson 0.469565 M Oliver 0.466019 H Webb 0.456604 A Marriner 0.455516 M Dean 0.442049 Name: homeWin, dtype: float64 It seems that L Mason may be the most influenced by the home crowd. Whilst the overall home win rate is 46.5%, the home win rate when he is the Referee is 51%. However it should be noted that this doesn't mean that he causes the win through bias. It could just be that he referees the best clubs, so naturally their home win rate is high.","title":"Exploring Referee Home Ground Bias"},{"location":"modelling/EPLmlPython/#variable-correlation-with-margin","text":"Let's now explore different variables' relationships with margin. First, we'll create a margin column, then we will pick a few different variables to look at the correlations amongst each other, using a correlation heatmap. df [ 'margin' ] = df [ 'FTHG' ] - df [ 'FTAG' ] stat_cols = [ 'AC' , 'AF' , 'AR' , 'AS' , 'AST' , 'AY' , 'HC' , 'HF' , 'HR' , 'HS' , 'HST' , 'HTR' , 'HY' , 'margin' ] stat_correlations = df [ stat_cols ] . corr () stat_correlations [ 'margin' ] . sort_values () AST - 0.345703 AS - 0.298665 HY - 0.153806 HR - 0.129393 AC - 0.073204 HF - 0.067469 AF 0.005474 AY 0.013746 HC 0.067433 AR 0.103528 HS 0.275847 HST 0.367591 margin 1.000000 Name : margin , dtype : float64 Unsurprisingly, Home Shots on Target correlate the most with Margin, and Away Reds is also high. What is surprising is that Home Yellows has quite a strong negative correlation with margin - this may be because players will play more aggresively when they are losing to try and get the lead back, and hence receive more yellow cards. Let's now look at the heatmap between variables. sns . heatmap ( stat_correlations , annot = True , annot_kws = { 'size' : 10 }) < matplotlib . axes . _subplots . AxesSubplot at 0x220a4227048 >","title":"Variable Correlation With Margin"},{"location":"modelling/EPLmlPython/#analysing-features","text":"What we are really interested in, is how our features (creating in the next tutorial), correlate with winning. We will skip ahead here and use a function to create our features for us, and then examine how the moving averages/different features correlate with winning. # Create a cleaned df of all of our data pre_features_df = create_df ( 'data/epl_data.csv' ) # Create our features features = create_feature_df ( pre_features_df ) Creating all games feature DataFrame C : \\ Users \\ wardj \\ Documents \\ Betfair Public Github \\ predictive - models \\ epl \\ data_preparation_functions . py : 419 : RuntimeWarning : invalid value encountered in double_scalars . pipe ( lambda df : ( df . eloAgainst * df [ goalsForOrAgainstCol ]) . sum () / df . eloAgainst . sum ())) Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . features = ( pre_features_df . assign ( margin = lambda df : df . FTHG - df . FTAG ) . loc [:, [ 'gameId' , 'margin' ]] . pipe ( pd . merge , features , on = [ 'gameId' ])) features . corr () . margin . sort_values ( ascending = False )[: 20 ] margin 1.000000 f_awayOdds 0.413893 f_totalMktH % 0.330420 f_defMktH % 0.325392 f_eloAgainstAway 0.317853 f_eloForHome 0.317853 f_midMktH % 0.316080 f_attMktH % 0.312262 f_sizeOfHandicapAway 0.301667 f_goalsForHome 0.298930 f_wtEloGoalsForHome 0.297157 f_shotsForHome 0.286239 f_cornersForHome 0.279917 f_gkMktH % 0.274732 f_homeWinPc38Away 0.271326 f_homeWinPc38Home 0.271326 f_wtEloGoalsAgainstAway 0.269663 f_goalsAgainstAway 0.258418 f_cornersAgainstAway 0.257148 f_drawOdds 0.256807 Name : margin , dtype : float64 As we can see away odds is most highly correlated to margin. This makes sense, as odds generally have most/all information included in the price. What is interesting is that elo seems to also be highly correlated, which is good news for our elo model that we made. Similarly, weighted goals and the the value of the defence relative to other teams ('defMktH%' etc.) is strongly correlated to margin.","title":"Analysing Features"},{"location":"modelling/EPLmlPython/#02-data-preparation-feature-engineering","text":"Welcome to the second part of this Machine Learning Walkthrough. This tutorial will focus on data preparation and feature creation, before we dive into modelling in the next tutorial . Specifically, this tutorial will cover a few things: Data wrangling specifically for sport Feature creation - focussing on commonly used features in sports modelling, such as exponential moving averages Using functions to modularise the data preparation process","title":"02. Data Preparation &amp; Feature Engineering"},{"location":"modelling/EPLmlPython/#data-wrangling","text":"We will begin by utilising functions we have defined in our data_preparation_functions script to wrangle our data into a format that can be consumed by Machine Learning algorithms. A typical issue faced by aspect of modelling sport is the issue of Machine Learning algorithms requiring all features for the teams playing to be on the same row of a table, whereas when we actual calculate these features, we usually require the teams to be on separate rows as it makes it a lot easier to calculate typical features, such as expontentially weighted moving averages . We will explore this issue and show how we deal with issues like these. # Import libraries from data_preparation_functions import * from sklearn.metrics import log_loss from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold , cross_val_score import matplotlib.pyplot as plt pd . set_option ( 'display.max_columns' , 100 ) We have created some functions which prepare the data for you. For thoroughly commented explanation of how the functions work, read through the data_preparation_functions.py script along side this walkthrough. Essentially, each functions wrangles the data through a similar process. It first reads in the data from a csv file, then converts the columns to datatypes that we can work with, such as converting the Date column to a datetime data type. It then adds a Game ID column, so each game is easily identifiable and joined on. We then assign the DataFrame some other columns which may be useful, such as 'Year', 'Result' and 'homeWin'. Finally, we drop redundant column and return the DataFrame. Let us now create six different DataFrames, which we will use to create features. Later, we will join these features back into one main feature DataFrame.","title":"Data Wrangling"},{"location":"modelling/EPLmlPython/#create-6-distinct-dataframes","text":"# This table includes all of our data in one big DataFrame df = create_df ( 'data/epl_data.csv' ) df . head ( 3 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.0 2.38 8 A Wiley 2.75 3.25 2.4 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.0 2.10 8 M Riley 3.10 3.25 2.2 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.2 3.75 8 G Poll 1.80 3.30 4.5 2005 0506 3 0 1 away # This includes only the typical soccer stats, like home corners, home shots on target etc. stats = create_stats_df ( 'data/epl_data.csv' ) stats . head ( 3 ) gameId HomeTeam AwayTeam FTHG FTAG HTHG HTAG HS AS HST AST HF AF HC AC HY AY HR AR 0 1 West Ham Blackburn 3.0 1.0 0.0 1.0 13.0 11.0 5.0 5.0 11.0 14.0 2.0 6.0 0.0 1.0 0.0 1.0 1 2 Aston Villa Bolton 2.0 2.0 2.0 2.0 3.0 13.0 2.0 6.0 14.0 16.0 7.0 8.0 0.0 2.0 0.0 0.0 2 3 Everton Man United 0.0 2.0 0.0 1.0 10.0 12.0 5.0 5.0 15.0 14.0 8.0 6.0 3.0 1.0 0.0 0.0 # This includes all of our betting related data, such as win/draw/lose odds, asian handicaps etc. betting = create_betting_df ( 'data/epl_data.csv' ) betting . head ( 3 ) B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Day Div IWA IWD IWH LBA LBD LBH Month VCA VCD VCH Year homeWin awayWin result HomeTeam AwayTeam gameId 0 2.75 3.20 2.5 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.9 2.08 1.86 3.35 2.60 35.0 13 E0 2.7 3.0 2.3 2.75 3.0 2.38 8 2.75 3.25 2.4 2005 1 0 home West Ham Blackburn 1 1 3.00 3.25 2.3 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.4 1.92 2.10 3.30 2.40 36.0 13 E0 3.1 3.0 2.1 3.20 3.0 2.10 8 3.10 3.25 2.2 2005 0 0 draw Aston Villa Bolton 2 2 1.72 3.40 5.0 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.8 1.93 2.05 3.70 5.65 36.0 13 E0 1.8 3.1 3.8 1.83 3.2 3.75 8 1.80 3.30 4.5 2005 0 1 away Everton Man United 3 # This includes all of the team information for each game. team_info = create_team_info_df ( 'data/epl_data.csv' ) team_info . head ( 3 ) gameId Date season HomeTeam AwayTeam FTR HTR Referee 0 1 2005-08-13 0506 West Ham Blackburn H A A Wiley 1 2 2005-08-13 0506 Aston Villa Bolton D D M Riley 2 3 2005-08-13 0506 Everton Man United A A G Poll # Whilst the other DataFrames date back to 2005, this DataFrame has data from 2001 to 2005. historic_games = create_historic_games_df ( 'data/historic_games_pre2005.csv' ) historic_games . head ( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin 0 2001-08-18 Charlton Everton 1 2 -1 20012002 0 1 2001-08-18 Derby Blackburn 2 1 -1 20012002 1 2 2001-08-18 Leeds Southampton 2 0 -1 20012002 1 # This is the historic_games DataFrame appended to the df DataFrame. all_games = create_all_games_df ( 'data/epl_data.csv' , 'data/historic_games_pre2005.csv' ) all_games . head ( 3 ) Date HomeTeam AwayTeam FTHG FTAG gameId season homeWin awayWin homeWinPc5 homeWinPc38 awayWinPc5 awayWinPc38 gameIdHistoric 0 2001-08-18 Charlton Everton 1.0 2.0 -1 20012002 0 1 NaN NaN NaN NaN 1 1 2001-08-18 Derby Blackburn 2.0 1.0 -1 20012002 1 0 NaN NaN NaN NaN 2 2 2001-08-18 Leeds Southampton 2.0 0.0 -1 20012002 1 0 NaN NaN NaN NaN 3","title":"Create 6 distinct DataFrames"},{"location":"modelling/EPLmlPython/#feature-creation","text":"Now that we have all of our pre-prepared DataFrames, and we know that the data is clean, we can move onto feature creation. As is common practice with sports modelling, we are going to start by creating expontentially weighted moving averages (EMA) as features. To get a better understanding of how EMAs work, read here . In short, an EMA is like a simple moving average, except it weights recent instances more than older instances based on an alpha parameter. The documentation for the pandas (emw method)[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html] we will be using states that we can specify alpha in a number of ways. We will specify it in terms of span, where $\\alpha = 2 / (span+1), span \u2265 1 $. Let's first define a function which calculates the exponential moving average for each column in the stats DataFrame. We will then apply this function with other functions we have created, such as create_betting_features_ema, which creates moving averages of betting data. However, we must first change the structure of our data. Notice that currently each row has both the Home Team's data and the Away Team's data on a single row. This makes it difficult to calculate rolling averages, so we will restructure our DataFrames to ensure each row only contains single team's data. To do this, we will define a function, reate_multiline_df_stats. # Define a function which restructures our DataFrame def create_multiline_df_stats ( old_stats_df ): # Create a list of columns we want and their mappings to more interpretable names home_stats_cols = [ 'HomeTeam' , 'FTHG' , 'FTAG' , 'HTHG' , 'HTAG' , 'HS' , 'AS' , 'HST' , 'AST' , 'HF' , 'AF' , 'HC' , 'AC' , 'HY' , 'AY' , 'HR' , 'AR' ] away_stats_cols = [ 'AwayTeam' , 'FTAG' , 'FTHG' , 'HTAG' , 'HTHG' , 'AS' , 'HS' , 'AST' , 'HST' , 'AF' , 'HF' , 'AC' , 'HC' , 'AY' , 'HY' , 'AR' , 'HR' ] stats_cols_mapping = [ 'team' , 'goalsFor' , 'goalsAgainst' , 'halfTimeGoalsFor' , 'halfTimeGoalsAgainst' , 'shotsFor' , 'shotsAgainst' , 'shotsOnTargetFor' , 'shotsOnTargetAgainst' , 'freesFor' , 'freesAgainst' , 'cornersFor' , 'cornersAgainst' , 'yellowsFor' , 'yellowsAgainst' , 'redsFor' , 'redsAgainst' ] # Create a dictionary of the old column names to new column names home_mapping = { old_col : new_col for old_col , new_col in zip ( home_stats_cols , stats_cols_mapping )} away_mapping = { old_col : new_col for old_col , new_col in zip ( away_stats_cols , stats_cols_mapping )} # Put each team onto an individual row multi_line_stats = ( old_stats_df [[ 'gameId' ] + home_stats_cols ] # Filter for only the home team columns . rename ( columns = home_mapping ) # Rename the columns . assign ( homeGame = 1 ) # Assign homeGame=1 so that we can use a general function later . append (( old_stats_df [[ 'gameId' ] + away_stats_cols ]) # Append the away team columns . rename ( columns = away_mapping ) # Rename the away team columns . assign ( homeGame = 0 ), sort = True ) . sort_values ( by = 'gameId' ) # Sort the values . reset_index ( drop = True )) return multi_line_stats # Define a function which creates an EMA DataFrame from the stats DataFrame def create_stats_features_ema ( stats , span ): # Create a restructured DataFrames so that we can calculate EMA multi_line_stats = create_multiline_df_stats ( stats ) # Create a copy of the DataFrame ema_features = multi_line_stats [[ 'gameId' , 'team' , 'homeGame' ]] . copy () # Get the columns that we want to create EMA for feature_names = multi_line_stats . drop ( columns = [ 'gameId' , 'team' , 'homeGame' ]) . columns # Loop over the features for feature_name in feature_names : feature_ema = ( multi_line_stats . groupby ( 'team' )[ feature_name ] # Calculate the EMA . transform ( lambda row : row . ewm ( span = span , min_periods = 2 ) . mean () . shift ( 1 ))) # Shift the data down 1 so we don't leak data ema_features [ feature_name ] = feature_ema # Add the new feature to the DataFrame return ema_features # Apply the function stats_features = create_stats_features_ema ( stats , span = 5 ) stats_features . tail () gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9903 4952 Newcastle 1 4.301743 4.217300 11.789345 12.245066 0.797647 0.833658 0.644214 0.420832 2.323450e-10 3.333631e-01 11.335147 13.265955 3.211345 4.067990 1.848860 1.627140 9904 4953 Burnley 0 4.880132 5.165915 13.326703 8.800033 1.945502 0.667042 0.609440 0.529409 3.874405e-03 3.356120e-10 13.129631 10.642381 4.825874 3.970285 0.963527 0.847939 9905 4953 Fulham 1 4.550255 4.403060 10.188263 8.555589 2.531046 1.003553 0.860573 0.076949 1.002518e-04 8.670776e-03 17.463779 12.278877 8.334019 4.058213 0.980097 1.102974 9906 4954 Man United 1 3.832573 4.759683 11.640608 10.307946 1.397234 1.495032 1.034251 0.809280 6.683080e-05 1.320468e-05 8.963022 10.198642 3.216957 3.776900 1.040077 1.595650 9907 4954 Tottenham 0 3.042034 5.160211 8.991460 9.955635 1.332704 2.514789 0.573728 1.010491 4.522878e-08 1.354409e-05 12.543406 17.761004 3.757437 7.279845 1.478976 1.026601 As we can see, we now have averages for each team. Let's create a quick table to see the top 10 teams' goalsFor average EMAs since 2005. pd . DataFrame ( stats_features . groupby ( 'team' ) . goalsFor . mean () . sort_values ( ascending = False )[: 10 ]) goalsFor team Man United 1.895026 Chelsea 1.888892 Arsenal 1.876770 Man City 1.835863 Liverpool 1.771125 Tottenham 1.655063 Leicester 1.425309 Blackpool 1.390936 Everton 1.387110 Southampton 1.288349","title":"Feature Creation"},{"location":"modelling/EPLmlPython/#optimising-alpha","text":"It looks like Man United and Chelsea have been two of the best teams since 2005, based on goalsFor. Now that we have our stats features, we may be tempted to move on. However, we have arbitrarily chosen a span of 5. How do we know that this is the best value? We don't. Let's try and optimise this value. To do this, we will use a simple Logistic Regression model to create probabilistic predictions based on the stats features we created before. We will iterate a range of span values, from say, 3 to 15, and choose the value which produces a model with the lowest log loss, based on cross validation. To do this, we need to restructure our DataFrame back to how it was before. def restructure_stats_features ( stats_features ): non_features = [ 'homeGame' , 'team' , 'gameId' ] stats_features_restructured = ( stats_features . query ( 'homeGame == 1' ) . rename ( columns = { col : 'f_' + col + 'Home' for col in stats_features . columns if col not in non_features }) . rename ( columns = { 'team' : 'HomeTeam' }) . pipe ( pd . merge , ( stats_features . query ( 'homeGame == 0' ) . rename ( columns = { 'team' : 'AwayTeam' }) . rename ( columns = { col : 'f_' + col + 'Away' for col in stats_features . columns if col not in non_features })), on = [ 'gameId' ]) . pipe ( pd . merge , df [[ 'gameId' , 'result' ]], on = 'gameId' ) . dropna ()) return stats_features_restructured restructure_stats_features ( stats_features ) . head () gameId HomeTeam homeGame_x f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome AwayTeam homeGame_y f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway result 20 21 Birmingham 1 4.8 7.8 12.0 9.4 1.2 0.6 0.6 0.6 0.0 0.0 11.4 8.2 6.4 2.8 1.0 2.6 Middlesbrough 0 3.0 5.6 14.0 12.8 1.2 0.0 0.0 0.0 0.0 0.4 17.2 8.8 7.6 2.6 3.0 1.4 away 21 22 Portsmouth 1 2.6 4.6 21.8 16.6 2.0 0.6 1.0 0.0 0.0 0.0 8.0 10.4 3.6 4.0 3.2 1.8 Aston Villa 0 9.8 7.0 14.2 18.2 1.4 0.8 0.8 0.8 0.0 0.0 16.0 3.0 9.6 2.6 2.0 0.6 draw 22 23 Sunderland 1 5.0 5.0 11.6 18.0 1.8 0.4 1.0 0.4 0.4 0.6 14.6 6.0 5.2 3.2 1.2 2.6 Man City 0 7.8 3.6 8.6 12.4 0.6 1.2 0.6 0.6 0.0 0.0 10.6 11.4 2.4 6.8 3.0 1.4 away 23 24 Arsenal 1 3.0 7.4 17.0 18.6 0.6 0.8 0.0 0.0 0.4 0.0 6.2 11.4 4.0 6.6 1.6 1.8 Fulham 0 7.2 3.0 20.8 13.2 1.2 0.6 0.6 0.0 0.0 0.0 12.4 10.8 7.0 5.2 2.0 1.6 home 24 25 Blackburn 1 1.4 7.2 12.8 21.2 1.8 1.6 0.0 1.0 0.0 0.4 10.0 14.0 4.4 7.4 1.2 1.6 Tottenham 0 6.4 3.8 11.2 18.8 0.0 2.0 0.0 0.4 0.0 0.0 11.6 15.2 4.6 7.2 0.6 2.6 draw Now let's write a function that optimises our span based on log loss of the output of a Logistic Regression model. def optimise_alpha ( features ): le = LabelEncoder () y = le . fit_transform ( features . result ) # Encode the result from away, draw, home win to 0, 1, 2 X = features [[ col for col in features . columns if col . startswith ( 'f_' )]] # Only get the features - these all start with f_ lr = LogisticRegression () kfold = StratifiedKFold ( n_splits = 5 ) ave_cv_score = cross_val_score ( lr , X , y , scoring = 'neg_log_loss' , cv = kfold ) . mean () return ave_cv_score best_score = np . float ( 'inf' ) best_span = 0 cv_scores = [] # Iterate over a range of spans for span in range ( 1 , 120 , 3 ): stats_features = create_stats_features_ema ( stats , span = span ) restructured_stats_features = restructure_stats_features ( stats_features ) cv_score = optimise_alpha ( restructured_stats_features ) cv_scores . append ( cv_score ) if cv_score * - 1 < best_score : best_score = cv_score * - 1 best_span = span plt . style . use ( 'ggplot' ) plt . plot ( list ( range ( 1 , 120 , 3 )), ( pd . Series ( cv_scores ) *- 1 )) # Plot our results plt . title ( \"Optimising alpha\" ) plt . xlabel ( \"Span\" ) plt . ylabel ( \"Log Loss\" ) plt . show () print ( \"Our lowest log loss ( {:2f} ) occurred at a span of {} \" . format ( best_score , best_span )) Our lowest log loss (0.980835) occurred at a span of 55 The above method is just an example of how you can optimise hyparameters. Obviously this example has many limitations, such as attempting to optimise each statistic with the same alpha. However, for the rest of these tutorial series we will use this span value. Now let's create the rest of our features. For thorough explanations and the actual code behind some of the functions used, please refer to the data_preparation_functions.py script.","title":"Optimising Alpha"},{"location":"modelling/EPLmlPython/#creating-our-features-dataframe","text":"We will utilise pre-made functions to create all of our features in just a few lines of code. As part of this process we will create features which include margin weighted elo, an exponential average for asian handicap data, and odds as features. Our Elo function is essentially the same as the one we created in the AFL tutorial; if you would like to know more about Elo models please read this article. Note that the cell below may take a few minutes to run. # Create feature DataFrames features_all_games = create_all_games_features ( all_games ) C:\\Users\\wardj\\Documents\\Betfair Public Github\\predictive-models\\epl\\data_preparation_functions.py:419: RuntimeWarning: invalid value encountered in double_scalars .pipe(lambda df: (df.eloAgainst * df[goalsForOrAgainstCol]).sum() / df.eloAgainst.sum())) The features_all_games df includes elo for each team, as well as their win percentage at home and away over the past 5 and 38 games. For more information on how it was calculated, read through the data_preparation_functions script. features_all_games . head ( 3 ) Date awayWin awayWinPc38 awayWinPc5 eloAgainst eloFor gameId gameIdHistoric goalsAgainst goalsFor homeGame homeWin homeWinPc38 homeWinPc5 season team wtEloGoalsFor wtEloGoalsAgainst 0 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 2.0 1.0 1 0 NaN NaN 20012002 Charlton NaN NaN 1 2001-08-18 1 NaN NaN 1500.0 1500.0 -1 1 1.0 2.0 0 0 NaN NaN 20012002 Everton NaN NaN 2 2001-08-18 0 NaN NaN 1500.0 1500.0 -1 2 1.0 2.0 1 1 NaN NaN 20012002 Derby NaN NaN The features_stats df includes all the expontential weighted averages for each stat in the stats df. # Create feature stats df features_stats = create_stats_features_ema ( stats , span = best_span ) features_stats . tail ( 3 ) gameId team homeGame cornersAgainst cornersFor freesAgainst freesFor goalsAgainst goalsFor halfTimeGoalsAgainst halfTimeGoalsFor redsAgainst redsFor shotsAgainst shotsFor shotsOnTargetAgainst shotsOnTargetFor yellowsAgainst yellowsFor 9905 4953 Fulham 1 6.006967 5.045733 10.228997 9.965651 2.147069 1.093550 0.630485 0.364246 0.032937 0.043696 16.510067 11.718122 7.184386 4.645762 1.310424 1.389716 9906 4954 Man United 1 4.463018 5.461075 11.605712 10.870367 0.843222 1.586308 0.427065 0.730650 0.042588 0.027488 10.865754 13.003121 3.562675 4.626450 1.740735 1.712785 9907 4954 Tottenham 0 3.868619 6.362901 10.784145 10.140388 0.954928 2.100166 0.439129 0.799968 0.024351 0.026211 9.947515 16.460598 3.370010 6.136120 1.925005 1.364268 The features_odds df includes a moving average of some of the odds data. # Create feature_odds df features_odds = create_betting_features_ema ( betting , span = 10 ) features_odds . tail ( 3 ) gameId team avAsianHandicapOddsAgainst avAsianHandicapOddsFor avgreaterthan2.5 avlessthan2.5 sizeOfHandicap 9905 4953 Fulham 1.884552 1.985978 1.756776 2.128261 0.502253 9906 4954 Man United 1.871586 2.031787 1.900655 1.963478 -0.942445 9907 4954 Tottenham 1.947833 1.919607 1.629089 2.383593 -1.235630 The features market values has market values and the % of total market for each position. These values are in millions. # Create feature market values df features_market_values = create_market_values_features ( df ) # This creates a df with one game per row features_market_values . head ( 3 ) gameId Year HomeTeam AwayTeam defMktValH attMktValH gkMktValH totalMktValH midMktValH defMktValA attMktValA gkMktValA totalMktValA midMktValA attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% 0 1 2005 West Ham Blackburn 16.90 18.50 6.40 46.40 4.60 27.25 13.00 3.25 70.70 27.20 2.252911 1.583126 0.588168 3.477861 2.486940 4.010007 4.524247 2.297469 1.913986 2.916354 1 2 2005 Aston Villa Bolton 27.63 31.85 7.60 105.83 38.75 9.60 24.55 8.50 72.40 29.75 3.878659 2.989673 4.954673 3.803910 4.065926 1.412700 5.372543 6.008766 4.365456 2.986478 2 3 2005 Everton Man United 44.35 31.38 8.55 109.78 25.50 82.63 114.60 9.25 288.48 82.00 3.821423 13.955867 3.260494 10.484727 6.526378 12.159517 6.044111 6.538951 4.528392 11.899714 all_games_cols = [ 'Date' , 'gameId' , 'team' , 'season' , 'homeGame' , 'homeWinPc38' , 'homeWinPc5' , 'awayWinPc38' , 'awayWinPc5' , 'eloFor' , 'eloAgainst' , 'wtEloGoalsFor' , 'wtEloGoalsAgainst' ] # Join the features together features_multi_line = ( features_all_games [ all_games_cols ] . pipe ( pd . merge , features_stats . drop ( columns = 'homeGame' ), on = [ 'gameId' , 'team' ]) . pipe ( pd . merge , features_odds , on = [ 'gameId' , 'team' ])) # Put each instance on an individual row features_with_na = put_features_on_one_line ( features_multi_line ) market_val_feature_names = [ 'attMktH%' , 'attMktA%' , 'midMktH%' , 'midMktA%' , 'defMktH%' , 'defMktA%' , 'gkMktH%' , 'gkMktA%' , 'totalMktH%' , 'totalMktA%' ] # Merge our team values dataframe to features and result from df features_with_na = ( features_with_na . pipe ( pd . merge , ( features_market_values [ market_val_feature_names + [ 'gameId' ]]) . rename ({ col : 'f_' + col for col in market_val_feature_names }), on = 'gameId' ) . pipe ( pd . merge , df [[ 'HomeTeam' , 'AwayTeam' , 'gameId' , 'result' , 'B365A' , 'B365D' , 'B365H' ]], on = [ 'HomeTeam' , 'AwayTeam' , 'gameId' ])) # Drop NAs from calculating the rolling averages - don't drop Win Pc 38 and Win Pc 5 columns features = features_with_na . dropna ( subset = features_with_na . drop ( columns = [ col for col in features_with_na . columns if 'WinPc' in col ]) . columns ) # Fill NAs for the Win Pc columns features = features . fillna ( features . mean ()) features . head ( 3 ) Date gameId HomeTeam season homeGame f_homeWinPc38Home f_homeWinPc5Home f_awayWinPc38Home f_awayWinPc5Home f_eloForHome f_eloAgainstHome f_wtEloGoalsForHome f_wtEloGoalsAgainstHome f_cornersAgainstHome f_cornersForHome f_freesAgainstHome f_freesForHome f_goalsAgainstHome f_goalsForHome f_halfTimeGoalsAgainstHome f_halfTimeGoalsForHome f_redsAgainstHome f_redsForHome f_shotsAgainstHome f_shotsForHome f_shotsOnTargetAgainstHome f_shotsOnTargetForHome f_yellowsAgainstHome f_yellowsForHome f_avAsianHandicapOddsAgainstHome f_avAsianHandicapOddsForHome f_avgreaterthan2.5Home f_avlessthan2.5Home f_sizeOfHandicapHome AwayTeam f_homeWinPc38Away f_homeWinPc5Away f_awayWinPc38Away f_awayWinPc5Away f_eloForAway f_eloAgainstAway f_wtEloGoalsForAway f_wtEloGoalsAgainstAway f_cornersAgainstAway f_cornersForAway f_freesAgainstAway f_freesForAway f_goalsAgainstAway f_goalsForAway f_halfTimeGoalsAgainstAway f_halfTimeGoalsForAway f_redsAgainstAway f_redsForAway f_shotsAgainstAway f_shotsForAway f_shotsOnTargetAgainstAway f_shotsOnTargetForAway f_yellowsAgainstAway f_yellowsForAway f_avAsianHandicapOddsAgainstAway f_avAsianHandicapOddsForAway f_avgreaterthan2.5Away f_avlessthan2.5Away f_sizeOfHandicapAway attMktH% attMktA% midMktH% midMktA% defMktH% defMktA% gkMktH% gkMktA% totalMktH% totalMktA% result B365A B365D B365H 20 2005-08-23 21 Birmingham 0506 1 0.394737 0.4 0.263158 0.2 1478.687038 1492.866048 1.061763 1.260223 4.981818 7.527273 12.000000 9.945455 1.018182 0.509091 0.509091 0.509091 0.000000 0.000000 11.945455 8.018182 6.490909 2.981818 1.000000 2.509091 1.9090 1.9455 2.0510 1.6735 -0.1375 Middlesbrough 0.394737 0.4 0.263158 0.2 1492.866048 1478.687038 1.12994 1.279873 2.545455 5.509091 13.545455 13.436364 1.018182 0.000000 0.000000 0.000000 0.0 0.490909 17.018182 8.072727 7.509091 2.509091 3.0 1.490909 1.9395 1.9095 2.0035 1.7155 0.3875 5.132983 5.260851 3.341048 4.289788 3.502318 4.168935 2.332815 3.216457 3.934396 4.522205 away 2.75 3.2 2.50 21 2005-08-23 22 Portsmouth 0506 1 0.447368 0.4 0.263158 0.4 1405.968416 1489.229314 1.147101 1.503051 2.509091 4.963636 21.981818 16.054545 2.000000 0.509091 1.000000 0.000000 0.000000 0.000000 8.454545 10.490909 3.963636 4.454545 3.018182 1.527273 1.8965 1.9690 2.0040 1.7005 0.2500 Aston Villa 0.447368 0.4 0.263158 0.4 1489.229314 1405.968416 1.17516 1.263229 9.527273 7.000000 14.472727 17.563636 1.490909 0.981818 0.981818 0.981818 0.0 0.000000 15.545455 3.000000 9.054545 2.509091 2.0 0.509091 1.8565 1.9770 1.8505 1.8485 0.7125 3.738614 3.878659 4.494368 4.954673 2.884262 4.065926 3.746642 5.372543 3.743410 4.365456 draw 2.75 3.2 2.50 22 2005-08-23 23 Sunderland 0506 1 0.236842 0.0 0.236842 0.4 1277.888970 1552.291880 0.650176 1.543716 5.000000 5.000000 12.418182 17.545455 1.981818 0.490909 1.000000 0.490909 0.490909 0.509091 14.509091 6.909091 5.018182 3.927273 1.018182 2.509091 1.8520 1.9915 1.8535 1.8500 0.7125 Man City 0.236842 0.0 0.236842 0.4 1552.291880 1277.888970 1.28875 1.287367 7.527273 3.509091 8.963636 12.490909 0.509091 1.018182 0.509091 0.509091 0.0 0.000000 10.963636 11.945455 2.490909 6.981818 3.0 1.490909 1.8150 2.0395 2.0060 1.7095 -0.2000 0.706318 3.750792 1.476812 1.070209 2.634096 4.455890 0.777605 4.913050 1.499427 3.151477 away 2.50 3.2 2.75 We now have a features DataFrame ready, with all the feature columns beginning with the \"f_\". In the next section, we will walk through the modelling process to try and find the best type of model to use.","title":"Creating our Features DataFrame"},{"location":"modelling/EPLmlPython/#03-model-building-hyperparameter-tuning","text":"Welcome to the third part of this Machine Learning Walkthrough. This tutorial will focus on the model building process, including how to tune hyperparameters. In the [next tutorial], we will create weekly predictions based on the model we have created here. Specifically, this tutorial will cover a few things: Choosing which Machine Learning algorithm to use from a variety of choices Hyperparameter Tuning Overfitting/Underfitting","title":"03. Model Building &amp; Hyperparameter Tuning"},{"location":"modelling/EPLmlPython/#choosing-an-algorithm","text":"The best way to decide on specific algorithm to use, is to try them all! To do this, we will define a function which we first used in our AFL Predictions tutorial. This will iterate over a number of algorithms and give us a good indication of which algorithms are suited for this dataset and exercise. Let's first use grab the features we created in the last tutorial. This may take a minute or two to run. ## Import libraries from data_preparation_functions import * import pandas as pd import numpy as np import matplotlib as plt import seaborn as sns import warnings from sklearn import linear_model , tree , discriminant_analysis , naive_bayes , ensemble , gaussian_process from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV from sklearn.metrics import log_loss , confusion_matrix warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.max_columns' , 100 ) features = create_feature_df () Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . To start our modelling process, we need to make a training set, a test set and a holdout set. As we are using cross validation, we will make our training set all of the seasons up until 2017/18, and we will use the 2017/18 season as the test set. feature_list = [ col for col in features . columns if col . startswith ( \"f_\" )] betting_features = [] le = LabelEncoder () # Initiate a label encoder to transform the labels 'away', 'draw', 'home' to 0, 1, 2 # Grab all seasons except for 17/18 to use CV with all_x = features . loc [ features . season != '1718' , [ 'gameId' ] + feature_list ] all_y = features . loc [ features . season != '1718' , 'result' ] all_y = le . fit_transform ( all_y ) # Create our training vector as the seasons except 16/17 and 17/18 train_x = features . loc [ ~ features . season . isin ([ '1617' , '1718' ]), [ 'gameId' ] + feature_list ] train_y = le . transform ( features . loc [ ~ features . season . isin ([ '1617' , '1718' ]), 'result' ]) # Create our holdout vectors as the 16/17 season holdout_x = features . loc [ features . season == '1617' , [ 'gameId' ] + feature_list ] holdout_y = le . transform ( features . loc [ features . season == '1617' , 'result' ]) # Create our test vectors as the 17/18 season test_x = features . loc [ features . season == '1718' , [ 'gameId' ] + feature_list ] test_y = le . transform ( features . loc [ features . season == '1718' , 'result' ]) # Create a list of standard classifiers classifiers = [ #GLM linear_model . LogisticRegressionCV (), #Navies Bayes naive_bayes . BernoulliNB (), naive_bayes . GaussianNB (), #Discriminant Analysis discriminant_analysis . LinearDiscriminantAnalysis (), discriminant_analysis . QuadraticDiscriminantAnalysis (), #Ensemble Methods ensemble . AdaBoostClassifier (), ensemble . BaggingClassifier (), ensemble . ExtraTreesClassifier (), ensemble . GradientBoostingClassifier (), ensemble . RandomForestClassifier (), #Gaussian Processes gaussian_process . GaussianProcessClassifier (), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html # xgb.XGBClassifier() ] def find_best_algorithms ( classifier_list , X , y ): # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling # Cross validate model with Kfold stratified cross validation kfold = StratifiedKFold ( n_splits = 5 ) # Grab the cross validation scores for each algorithm cv_results = [ cross_val_score ( classifier , X , y , scoring = \"neg_log_loss\" , cv = kfold ) for classifier in classifier_list ] cv_means = [ cv_result . mean () * - 1 for cv_result in cv_results ] cv_std = [ cv_result . std () for cv_result in cv_results ] algorithm_names = [ alg . __class__ . __name__ for alg in classifiers ] # Create a DataFrame of all the CV results cv_results = pd . DataFrame ({ \"Mean Log Loss\" : cv_means , \"Log Loss Std\" : cv_std , \"Algorithm\" : algorithm_names }) . sort_values ( by = 'Mean Log Loss' ) return cv_results algorithm_results = find_best_algorithms ( classifiers , all_x , all_y ) algorithm_results Mean Log Loss Log Loss Std Algorithm 0 0.966540 0.020347 LogisticRegressionCV 3 0.986679 0.015601 LinearDiscriminantAnalysis 1 1.015197 0.017466 BernoulliNB 10 1.098612 0.000000 GaussianProcessClassifier 5 1.101281 0.044383 AdaBoostClassifier 8 1.137778 0.153391 GradientBoostingClassifier 7 2.093981 0.284831 ExtraTreesClassifier 9 2.095088 0.130367 RandomForestClassifier 6 2.120571 0.503132 BaggingClassifier 4 4.065796 1.370119 QuadraticDiscriminantAnalysis 2 5.284171 0.826991 GaussianNB We can see that LogisticRegression seems to perform the best out of all the algorithms, and some algorithms have a very high log loss. This is most likely due to overfitting. It would definitely be useful to condense our features down to reduce the dimensionality of the dataset.","title":"Choosing an Algorithm"},{"location":"modelling/EPLmlPython/#hyperparameter-tuning","text":"For now, however, we will use logistic regression. Let's first try and tune a logistic regression model with cross validation. To do this, we will use grid search . Grid search essentially tries out each combination of values and finds the model with the lowest error metric, which in our case is log loss. 'C' in logistic regression determines the amount of regularization. Lower values increase regularization. # Define our parameters to run a grid search over lr_grid = { \"C\" : [ 0.0001 , 0.01 , 0.05 , 0.2 , 1 ], \"solver\" : [ \"newton-cg\" , \"lbfgs\" , \"liblinear\" ] } kfold = StratifiedKFold ( n_splits = 5 ) gs = GridSearchCV ( LogisticRegression (), param_grid = lr_grid , cv = kfold , scoring = 'neg_log_loss' ) gs . fit ( all_x , all_y ) print ( \"Best log loss: {} \" . format ( gs . best_score_ *- 1 )) best_lr_params = gs . best_params_ Best log loss : 0.9669551970849734","title":"Hyperparameter Tuning"},{"location":"modelling/EPLmlPython/#defining-a-baseline","text":"We should also define a baseline, as we don't really know if our log loss is good or bad. Randomly assigning a 1/3 chance to each selection yields a log loss of log3 = 1.09. However, what we are really interested in, is how our model performs relative to the odds. So let's find the log loss of the odds. # Finding the log loss of the odds log_loss ( all_y , 1 / all_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) 0.9590114943474463 This is good news: our algorithm almost beats the bookies in terms of log loss. It would be great if we could beat this result.","title":"Defining a Baseline"},{"location":"modelling/EPLmlPython/#analysing-the-errors-made","text":"Now that we have a logistic regression model tuned, let's see what type of errors it made. To do this we will look at the confusion matrix produced when we predict our holdout set. lr = LogisticRegression ( ** best_lr_params ) # Instantiate the model lr . fit ( train_x , train_y ) # Fit our model lr_predict = lr . predict ( holdout_x ) # Predict the holdout values # Create a confusion matrix c_matrix = ( pd . DataFrame ( confusion_matrix ( holdout_y , lr_predict ), columns = le . classes_ , index = le . classes_ ) . rename_axis ( 'Actual' ) . rename_axis ( 'Predicted' , axis = 'columns' )) c_matrix Predicted away draw home Actual away 77 0 32 draw 26 3 55 home 33 7 147 As we can see, when we predicted 'away' as the result, we correctly predicted 79 / 109 results, a hit rate of 70.6%. However, when we look at our draw hit rate, we only predicted 6 / 84 correctly, meaning we only had a hit rate of around 8.3%. For a more in depth analysis of our predictions, please skip to the Analysing Predictions & Staking Strategies section of the tutorial. Before we move on, however, let's use our model to predict the 17/18 season and compare how we went with the odds. # Get test predictions test_lr = LogisticRegression ( ** best_lr_params ) test_lr . fit ( all_x , all_y ) test_predictions_probs = lr . predict_proba ( test_x ) test_predictions = lr . predict ( test_x ) test_ll = log_loss ( test_y , test_predictions_probs ) test_accuracy = ( test_predictions == test_y ) . mean () print ( \"Our predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.2f} \" . format ( test_ll , test_accuracy )) Our predictions for the 2017/18 season have a log loss of: 0.95767 and an accuracy of: 0.56 # Get accuracy and log loss based on the odds odds_ll = log_loss ( test_y , 1 / test_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]]) odds_predictions = test_x [[ 'f_awayOdds' , 'f_drawOdds' , 'f_homeOdds' ]] . apply ( lambda row : row . idxmin ()[ 2 : 6 ], axis = 1 ) . values odds_accuracy = ( odds_predictions == le . inverse_transform ( test_y )) . mean () print ( \"Odds predictions for the 2017/18 season have a log loss of: {0:.5f} and an accuracy of: {1:.3f} \" . format ( odds_ll , odds_accuracy )) Odds predictions for the 2017/18 season have a log loss of: 0.94635 and an accuracy of: 0.545","title":"Analysing the Errors Made"},{"location":"modelling/EPLmlPython/#results","text":"There we have it! The odds predicted 54.5% of EPL games correctly in the 2017/18 season, whilst our model predicted 54% correctly. This is a decent result for the first iteration of our model. In future iterations, we could wait a certain number of matches each season and calculate EMAs for on those first n games. This may help the issue of players switching clubs and teams becoming relatively stronger/weaker compared to previous seasons.","title":"Results"},{"location":"modelling/EPLmlPython/#04-weekly-predictions","text":"Welcome to the third part of this Machine Learning Walkthrough. This tutorial will be a walk through of creating weekly EPL predictions from the basic logistic regression model we built in the previous tutorial. We will then analyse our predictions and create staking strategies in the next tutorial. Specifically, this tutorial will cover a few things: Obtaining Weekly Odds / Game Info Using Betfair's API Data Wrangling This Week's Game Info Into Our Feature Set","title":"04. Weekly Predictions"},{"location":"modelling/EPLmlPython/#obtaining-weekly-odds-game-info-using-betfairs-api","text":"The first thing we need to do to create weekly predictions is get both the games being played this week, as well as match odds from Betfair to be used as features. To make this process easier, I have created a csv file with the fixture for the 2018/19 season. Let's load that now. ## Import libraries import pandas as pd from weekly_prediction_functions import * from data_preparation_functions import * from sklearn.metrics import log_loss , confusion_matrix import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.max_columns' , 100 ) fixture = ( pd . read_csv ( 'data/fixture.csv' ) . assign ( Date = lambda df : pd . to_datetime ( df . Date ))) fixture . head () Date Time (AEST) HomeTeam AwayTeam Venue TV Year round season 0 2018-08-11 5:00 AM Man United Leicester Old Trafford, Manchester Optus, Fox Sports (delay) 2018 1 1819 1 2018-08-11 9:30 PM Newcastle Tottenham St.James\u2019 Park, Newcastle Optus, SBS 2018 1 1819 2 2018-08-12 12:00 AM Bournemouth Cardiff Vitality Stadium, Bournemouth Optus 2018 1 1819 3 2018-08-12 12:00 AM Fulham Crystal Palace Craven Cottage, London Optus 2018 1 1819 4 2018-08-12 12:00 AM Huddersfield Chelsea John Smith\u2019s Stadium, Huddersfield Optus, Fox Sports (delay) 2018 1 1819 Now we are going to connect to the API and retrieve game level information for the next week. To do this, we will use an R script. If you are not familiar with R, don't worry, it is relatively simple to read through. For this, we will run the script weekly_game_info_puller.R. Go ahead and run that script now. Note that for this step, you will require a Betfair API App Key. If you don't have one, visit this page and follow the instructions . I will upload an updated weekly file, so you can follow along regardless of if you have an App Key or not. Let's load that file in now. game_info = create_game_info_df ( \"data/weekly_game_info.csv\" ) game_info . head ( 3 ) AwayTeam HomeTeam awaySelectionId drawSelectionId homeSelectionId draw marketId marketStartTime totalMatched eventId eventName homeOdds drawOdds awayOdds competitionId Date localMarketStartTime 0 Arsenal Cardiff 1096 58805 79343 The Draw 1.146897152 2018-09-02 12:30:00 30123.595116 28852020 Cardiff v Arsenal 7.00 4.3 1.62 10932509 2018-09-02 Sun September 2, 10:30PM 1 Bournemouth Chelsea 1141 58805 55190 The Draw 1.146875421 2018-09-01 14:00:00 30821.329656 28851426 Chelsea v Bournemouth 1.32 6.8 12.00 10932509 2018-09-01 Sun September 2, 12:00AM 2 Fulham Brighton 56764 58805 18567 The Draw 1.146875746 2018-09-01 14:00:00 16594.833096 28851429 Brighton v Fulham 2.36 3.5 3.50 10932509 2018-09-01 Sun September 2, 12:00AM Finally, we will use the API to grab the weekly odds. This R script is also provided, but I have also included the weekly odds csv for convenience. odds = ( pd . read_csv ( 'data/weekly_epl_odds.csv' ) . replace ({ 'Man Utd' : 'Man United' , 'C Palace' : 'Crystal Palace' })) odds . head ( 3 ) HomeTeam AwayTeam f_homeOdds f_drawOdds f_awayOdds 0 Leicester Liverpool 7.80 5.1 1.48 1 Brighton Fulham 2.36 3.5 3.50 2 Everton Huddersfield 1.54 4.4 8.20","title":"Obtaining Weekly Odds / Game Info Using Betfair's API"},{"location":"modelling/EPLmlPython/#data-wrangling-this-weeks-game-info-into-our-feature-set","text":"Now we have the arduous task of wrangling all of this info into a feature set that we can use to predict this week's games. Luckily our functions we created earlier should work if we just append the non-features to our main dataframe. df = create_df ( 'data/epl_data.csv' ) df . head () AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 0 6.0 14.0 1.0 11.0 5.0 1.0 Blackburn 2.75 3.20 2.50 2.90 3.30 2.20 55.0 20.0 0.00 1.71 2.02 2.74 2.04 1.82 3.16 2.40 1.80 2.25 2.90 2.08 1.86 3.35 2.60 35.0 2005-08-13 13 E0 1.0 3.0 H 2.0 11.0 0.0 13.0 5.0 1.0 0.0 A 0.0 West Ham 2.7 3.0 2.3 2.75 3.00 2.38 8 A Wiley 2.75 3.25 2.40 2005 0506 1 1 0 home 1 8.0 16.0 0.0 13.0 6.0 2.0 Bolton 3.00 3.25 2.30 3.15 3.25 2.10 56.0 22.0 -0.25 1.70 2.01 3.05 1.84 2.01 3.16 2.20 1.87 2.20 3.40 1.92 2.10 3.30 2.40 36.0 2005-08-13 13 E0 2.0 2.0 D 7.0 14.0 0.0 3.0 2.0 2.0 2.0 D 0.0 Aston Villa 3.1 3.0 2.1 3.20 3.00 2.10 8 M Riley 3.10 3.25 2.20 2005 0506 2 0 0 draw 2 6.0 14.0 0.0 12.0 5.0 1.0 Man United 1.72 3.40 5.00 1.75 3.35 4.35 56.0 23.0 0.75 1.79 1.93 1.69 1.86 2.00 3.36 4.69 1.87 2.10 1.80 1.93 2.05 3.70 5.65 36.0 2005-08-13 13 E0 2.0 0.0 A 8.0 15.0 0.0 10.0 5.0 1.0 0.0 A 3.0 Everton 1.8 3.1 3.8 1.83 3.20 3.75 8 G Poll 1.80 3.30 4.50 2005 0506 3 0 1 away 3 6.0 13.0 0.0 7.0 4.0 2.0 Birmingham 2.87 3.25 2.37 2.80 3.20 2.30 56.0 21.0 0.00 1.69 2.04 2.87 2.05 1.81 3.16 2.31 1.77 2.24 3.05 2.11 1.85 3.30 2.60 36.0 2005-08-13 13 E0 0.0 0.0 D 6.0 12.0 0.0 15.0 7.0 0.0 0.0 D 1.0 Fulham 2.9 3.0 2.2 2.88 3.00 2.25 8 R Styles 2.80 3.25 2.35 2005 0506 4 0 0 draw 4 6.0 11.0 0.0 13.0 3.0 3.0 West Brom 5.00 3.40 1.72 4.80 3.45 1.65 55.0 23.0 -0.75 1.77 1.94 4.79 1.76 2.10 3.38 1.69 1.90 2.10 5.60 1.83 2.19 3.63 1.80 36.0 2005-08-13 13 E0 0.0 0.0 D 3.0 13.0 0.0 15.0 8.0 0.0 0.0 D 2.0 Man City 4.2 3.2 1.7 4.50 3.25 1.67 8 C Foy 5.00 3.25 1.75 2005 0506 5 0 0 draw Now we need to specify which game week we would like to predict. We will then filter the fixture for this game week and append this info to the main DataFrame round_to_predict = int ( input ( \"Which game week would you like to predict? Please input next week's Game Week \\n \" )) Which game week would you like to predict? Please input next week's Game Week 4 future_predictions = ( fixture . loc [ fixture [ 'round' ] == round_to_predict , [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]] . pipe ( pd . merge , odds , on = [ 'HomeTeam' , 'AwayTeam' ]) . rename ( columns = { 'f_homeOdds' : 'B365H' , 'f_awayOdds' : 'B365A' , 'f_drawOdds' : 'B365D' }) . assign ( season = lambda df : df . season . astype ( str ))) df_including_future_games = ( pd . read_csv ( 'data/epl_data.csv' , dtype = { 'season' : str }) . assign ( Date = lambda df : pd . to_datetime ( df . Date )) . pipe ( lambda df : df . dropna ( thresh = len ( df ) - 2 , axis = 1 )) # Drop cols with NAs . dropna ( axis = 0 ) # Drop rows with NAs . sort_values ( 'Date' ) . append ( future_predictions , sort = True ) . reset_index ( drop = True ) . assign ( gameId = lambda df : list ( df . index + 1 ), Year = lambda df : df . Date . apply ( lambda row : row . year ), homeWin = lambda df : df . apply ( lambda row : 1 if row . FTHG > row . FTAG else 0 , axis = 1 ), awayWin = lambda df : df . apply ( lambda row : 1 if row . FTAG > row . FTHG else 0 , axis = 1 ), result = lambda df : df . apply ( lambda row : 'home' if row . FTHG > row . FTAG else ( 'draw' if row . FTHG == row . FTAG else 'away' ), axis = 1 ))) df_including_future_games . tail ( 12 ) AC AF AR AS AST AY AwayTeam B365A B365D B365H BWA BWD BWH Bb1X2 BbAH BbAHh BbAv<2.5 BbAv>2.5 BbAvA BbAvAHA BbAvAHH BbAvD BbAvH BbMx<2.5 BbMx>2.5 BbMxA BbMxAHA BbMxAHH BbMxD BbMxH BbOU Date Day Div FTAG FTHG FTR HC HF HR HS HST HTAG HTHG HTR HY HomeTeam IWA IWD IWH LBA LBD LBH Month Referee VCA VCD VCH Year season gameId homeWin awayWin result 4952 4.0 8.0 0.0 12.0 2.0 1.0 Burnley 4.33 3.40 2.00 4.0 3.3 2.00 39.0 20.0 -0.25 1.65 2.22 4.14 2.22 1.69 3.36 1.98 1.72 2.31 4.5 2.32 1.74 3.57 2.04 36.0 2018-08-26 26.0 E0 2.0 4.0 H 6.0 11.0 0.0 25.0 12.0 2.0 3.0 H 2.0 Fulham 4.10 3.35 1.97 3.90 3.2 2.00 8.0 D Coote 4.33 3.4 2.0 2018 1819 4953 1 0 home 4953 2.0 16.0 0.0 9.0 5.0 4.0 Tottenham 2.90 3.30 2.62 2.9 3.2 2.55 42.0 20.0 -0.25 1.79 2.03 2.86 1.72 2.18 3.27 2.56 1.84 2.10 3.0 1.76 2.25 3.40 2.67 40.0 2018-08-27 27.0 E0 3.0 0.0 A 5.0 11.0 0.0 23.0 5.0 0.0 0.0 D 2.0 Man United 2.75 3.25 2.60 2.75 3.2 2.55 8.0 C Pawson 2.90 3.3 2.6 2018 1819 4954 0 1 away 4954 NaN NaN NaN NaN NaN NaN Liverpool 1.48 5.10 7.80 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-01 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Leicester NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4955 0 0 away 4955 NaN NaN NaN NaN NaN NaN Fulham 3.50 3.50 2.36 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Brighton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4956 0 0 away 4956 NaN NaN NaN NaN NaN NaN Man United 1.70 3.90 6.60 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Burnley NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4957 0 0 away 4957 NaN NaN NaN NaN NaN NaN Bournemouth 12.00 6.80 1.32 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Chelsea NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4958 0 0 away 4958 NaN NaN NaN NaN NaN NaN Southampton 4.50 3.55 2.04 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Crystal Palace NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4959 0 0 away 4959 NaN NaN NaN NaN NaN NaN Huddersfield 8.20 4.40 1.54 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Everton NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4960 0 0 away 4960 NaN NaN NaN NaN NaN NaN Wolves 2.98 3.50 2.62 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN West Ham NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4961 0 0 away 4961 NaN NaN NaN NaN NaN NaN Newcastle 32.00 12.50 1.12 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Man City NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4962 0 0 away 4962 NaN NaN NaN NaN NaN NaN Arsenal 1.62 4.30 7.00 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-02 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Cardiff NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4963 0 0 away 4963 NaN NaN NaN NaN NaN NaN Tottenham 1.68 4.30 5.90 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-09-03 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Watford NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 1819 4964 0 0 away As we can see, what we have done is appended the Game information to our main DataFrame. The rest of the info is left as NAs, but this will be filled when we created our rolling average features. This is a 'hacky' type of way to complete this task, but works well as we can use the same functions that we created in the previous tutorials on this DataFrame. We now need to add the odds from our odds DataFrame, then we can just run our create features functions as usual.","title":"Data Wrangling This Week's Game Info Into Our Feature Set"},{"location":"modelling/EPLmlPython/#predicting-next-gameweeks-results","text":"Now that we have our feature DataFrame, all we need to do is split the feature DataFrame up into a training set and next week's games, then use the model we tuned in the last tutorial to create predictions! features = create_feature_df ( df = df_including_future_games ) Creating all games feature DataFrame Creating stats feature DataFrame Creating odds feature DataFrame Creating market values feature DataFrame Filling NAs Merging stats , odds and market values into one features DataFrame Complete . # Create a feature DataFrame for this week's games. production_df = pd . merge ( future_predictions , features , on = [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'season' ]) # Create a training DataFrame training_df = features [ ~ features . gameId . isin ( production_df . gameId )] feature_names = [ col for col in training_df if col . startswith ( 'f_' )] le = LabelEncoder () train_y = le . fit_transform ( training_df . result ) train_x = training_df [ feature_names ] lr = LogisticRegression ( C = 0.01 , solver = 'liblinear' ) lr . fit ( train_x , train_y ) predicted_probs = lr . predict_proba ( production_df [ feature_names ]) predicted_odds = 1 / predicted_probs # Assign the modelled odds to our predictions df predictions_df = ( production_df . loc [:, [ 'Date' , 'HomeTeam' , 'AwayTeam' , 'B365H' , 'B365D' , 'B365A' ]] . assign ( homeModelledOdds = [ i [ 2 ] for i in predicted_odds ], drawModelledOdds = [ i [ 1 ] for i in predicted_odds ], awayModelledOdds = [ i [ 0 ] for i in predicted_odds ]) . rename ( columns = { 'B365H' : 'BetfairHomeOdds' , 'B365D' : 'BetfairDrawOdds' , 'B365A' : 'BetfairAwayOdds' })) predictions_df Date HomeTeam AwayTeam BetfairHomeOdds BetfairDrawOdds BetfairAwayOdds homeModelledOdds drawModelledOdds awayModelledOdds 0 2018-09-01 Leicester Liverpool 7.80 5.10 1.48 5.747661 5.249857 1.573478 1 2018-09-02 Brighton Fulham 2.36 3.50 3.50 2.183193 3.803120 3.584057 2 2018-09-02 Burnley Man United 6.60 3.90 1.70 5.282620 4.497194 1.699700 3 2018-09-02 Chelsea Bournemouth 1.32 6.80 12.00 1.308366 6.079068 14.047070 4 2018-09-02 Crystal Palace Southampton 2.04 3.55 4.50 2.202871 4.213695 3.239122 5 2018-09-02 Everton Huddersfield 1.54 4.40 8.20 1.641222 3.759249 8.020055 6 2018-09-02 West Ham Wolves 2.62 3.50 2.98 1.999816 4.000456 4.000279 7 2018-09-02 Man City Newcastle 1.12 12.50 32.00 1.043103 29.427939 136.231983 8 2018-09-02 Cardiff Arsenal 7.00 4.30 1.62 6.256929 4.893445 1.572767 9 2018-09-03 Watford Tottenham 5.90 4.30 1.68 5.643663 4.338926 1.688224 Above are the predictions for this Gameweek's matches. In the next tutorial we will explore the errors our model has made, and work on creating a profitable betting strategy.","title":"Predicting Next Gameweek's Results"},{"location":"modelling/EPLmlPython/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/brownlowModelTutorial/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Modelling the Brownlow This walkthrough will provide a brief, yet effective tutorial on how to model the Brownlow medal. We will use data from 2010 to 2018, which includes Supercoach points and other useful stats. The output will be the number of votes predicted for each player in each match, and we will aggregate these to create aggregates for each team and for the whole competition. No doubt we'll have Mitchell right up the top, and if we don't, then we know we've done something wrong! # Import modules libraries import pandas as pd import h2o from h2o.automl import H2OAutoML import numpy as np from sklearn.preprocessing import StandardScaler import os import pickle # Change notebook settings pd . options . display . max_columns = None pd . options . display . max_rows = 300 from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" This walkthrough uses H2O's AutoML to automate machine learning. We have saved the models created by AutoML into the Github repo which can be loaded into the notebook to save you time training the model. However a drawback of H2O is that you can only load models if your currently installed verson of H2O is the same as the version used to create the model. You can check what version of H2O you have installed by running h2o.__version__ : # Checking the current version of H2O print ( f 'Current version of H2O installed: { h2o . __version__ } ' ) Current version of H2O installed: 3.36.0.3 If you have a different version of H2O installed you have two options. You can train the models yourself, all the code to do that is commented out in the notebook. Or you can pip uninstall H2O and then pip install H2O again, but specifically the 3.36.0.3 version. The code to do this is below and will only take 1 or 2 minutes: # # Uncomment the code below if you need to uninstall the current version of H2O and reinstall version 3.36.0.3 # # The following command removes the H2O module for Python. # pip uninstall h2o # # Next, use pip to install this version of the H2O Python module. # pip install http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/3/Python/h2o-3.36.0.3-py2.py3-none-any.whl EDA Read in the data I have collated this data using the fitzRoy R package and merging the afltables dataset with the footywire dataset, so that we can Supercoach and other advanced stats with Brownlow votes. Let's read in the data and have a sneak peak at what it looks like. brownlow_data = pd . read_csv ( 'data/afl_brownlow_data.csv' ) brownlow_data . tail ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 76585 25/08/2018 2018 23 Optus Stadium 12587 9711 J Stephenson 35 Collingwood Fremantle Away 76 67 9 NaN 3 5 5 62.5 0 0 3 2 0 87 6 2 8 3 2 1 3 0 0 0 1 0 0 0 56 56 76586 25/08/2018 2018 23 Optus Stadium 12144 9711 J Thomas 24 Collingwood Fremantle Away 76 67 9 NaN 6 11 14 82.4 0 2 0 1 0 79 11 6 17 4 1 0 3 0 4 1 4 1 1 3 67 89 76587 25/08/2018 2018 23 Optus Stadium 11549 9711 T Varcoe 18 Collingwood Fremantle Away 76 67 9 NaN 7 4 7 53.8 0 1 0 2 0 73 6 7 13 1 0 0 3 0 2 0 4 0 1 1 45 53 It looks like we've got about 76,000 rows of data and have stats like hitouts, clangers, effective disposals etc. Let's explore some certain scenarios. Using my domain knowledge of footy, I can hypothesise that if a player kicks 5 goals, he is pretty likely to poll votes. Similarly, if a player gets 30 possessions and 2+ goals, he is also probably likely to poll votes. Let's have a look at the mean votes for players for both of these situations. Exploring votes if a bag is kicked (5+ goals) brownlow_data . query ( 'G >= 5' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player has kicked a bag:\" , brownlow_data . query ( 'G >= 5' ) . brownlow_votes . mean ()) season 2010 1.420455 2011 1.313433 2012 1.413333 2013 1.253731 2014 1.915254 2015 1.765625 2016 1.788732 2017 2.098361 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player has kicked a bag: 1.4708818635607321 Exploring votes if the player has 30+ possies & 2+ goals brownlow_data . query ( 'G >= 2 and D >= 30' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player has 30 possies and kicks 2+ goals:\" , brownlow_data . query ( 'G >= 2 and D >= 30' ) . brownlow_votes . mean ()) season 2010 1.826923 2011 1.756410 2012 2.118421 2013 2.000000 2014 2.253731 2015 2.047619 2016 2.103448 2017 2.050000 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player has 30 possies and kicks 2+ goals: 1.8741379310344828 As suspected, the average votes for these two situations is 1.87! That's huge. Let's get an idea of the average votes for each player. It should be around 6/44, as there are always 6 votes per match and around 44 players per match. brownlow_data . brownlow_votes . mean () 0.12347341475121326 So the average vote is 0.12. Let's see how this changes is the player is a captain. I have collected data on if players are captains from wikipedia and collated it into a csv. Let's load this in and create a \"Is the player captain\" feature, then check the average votes for captains. Create Is Player Captain Feature captains = pd . read_csv ( 'data/captains.csv' ) . set_index ( 'player' ) def is_captain_for_that_season ( captains_df , player , year ): if player in captains_df . index : # Get years they were captain seasons = captains_df . loc [ player ] . season . split ( '-' ) if len ( seasons ) == 1 : seasons_captain = list ( map ( int , seasons )) elif len ( seasons ) == 2 : if seasons [ 1 ] == '' : seasons_captain = list ( range ( int ( seasons [ 0 ]), 2019 )) else : seasons_captain = list ( range ( int ( seasons [ 0 ]), int ( seasons [ 1 ]) + 1 )) if year in seasons_captain : return 1 return 0 brownlow_data [ 'is_captain' ] = brownlow_data . apply ( lambda x : is_captain_for_that_season ( captains , x . player , x . season ), axis = 'columns' ) brownlow_data . query ( 'is_captain == 1' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player is captain:\" , brownlow_data . query ( 'is_captain == 1' ) . brownlow_votes . mean ()) season 2010 0.408497 2011 0.429936 2012 0.274194 2013 0.438725 2014 0.519663 2015 0.447222 2016 0.347826 2017 0.425806 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player is captain: 0.36661698956780925 This is significantly higher than if they aren't captain. What would be interesting is to look at the average difference in votes between when they were captain and when they weren't, to try and find if there is a 'captain bias' in brownlow votes. Go ahead and try. For now, we're going to move onto feature creation Feature Creation Let's make a range of features, including: * Ratios of each statistic per game * If the player is a captain * If they kicked a bag (4/5+) * If they kicked 2 and had 30+ possies First we will make features of ratios. What is important is not how many of a certain stat a player has, but how much of that stat a player has relative to everyone else in the same match. It doesn't matter if Dusty Martin has 31 possessions if Tom Mitchell has had 50 - Mitchell is probably more likely to poll (assuming all else is equal). So rather than using the actual number of possessions for example, we can divide these possessions by the total amount of possessions in the game. To do this we'll use pandas groupby and transform methods. Create Ratios As Features %% time # Get a list of stats of which to create ratios for ratio_cols = [ 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'TOG' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] # Create a ratios df ratios = ( brownlow_data . copy () . loc [:, [ 'match_id' ] + ratio_cols ] . groupby ( 'match_id' ) . transform ( lambda x : x / x . sum ())) feature_cols = [ 'date' , 'season' , 'round' , 'venue' , 'ID' , 'match_id' , 'player' , 'jumper_no' , 'team' , 'opposition' , 'status' , 'team_score' , 'opposition_score' , 'margin' , 'brownlow_votes' ] # Create a features df - join the ratios to this df features = ( brownlow_data [ feature_cols ] . copy () . join ( ratios )) Wall time: 17.1 s features . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 0 25/03/2010 2010 1 MCG 11559 5089 J Anderson 26 Carlton Richmond Away 120 64 56 0.0 0.024194 0.021186 0.020599 0.021931 0.00 0.000000 0.000000 0.000000 0.0 0.024411 0.017032 0.028481 0.022008 0.019802 0.000000 0.0000 0.029197 0.0 0.000000 0.000000 0.010204 0.014286 0.00000 0.025 0.020075 0.019353 1 25/03/2010 2010 1 MCG 4060 5089 E Betts 19 Carlton Richmond Away 120 64 56 0.0 0.044355 0.014831 0.024345 0.021804 0.05 0.133333 0.130435 0.033333 0.0 0.025243 0.021898 0.031646 0.026135 0.029703 0.111111 0.1875 0.043796 0.0 0.000000 0.014925 0.010204 0.000000 0.00000 0.000 0.034504 0.041125 2 25/03/2010 2010 1 MCG 3281 5089 P Bower 18 Carlton Richmond Away 120 64 56 0.0 0.016129 0.038136 0.044944 0.030602 0.10 0.000000 0.000000 0.100000 0.0 0.026352 0.036496 0.031646 0.034388 0.054455 0.000000 0.0000 0.036496 0.0 0.009804 0.000000 0.000000 0.014286 0.00000 0.000 0.037014 0.039311 3 25/03/2010 2010 1 MCG 4056 5089 A Carrazzo 44 Carlton Richmond Away 120 64 56 2.0 0.032258 0.069915 0.059925 0.025501 0.00 0.000000 0.000000 0.000000 0.0 0.024133 0.051095 0.060127 0.055021 0.024752 0.000000 0.0000 0.029197 0.0 0.078431 0.074627 0.040816 0.057143 0.04878 0.025 0.041092 0.040822 4 25/03/2010 2010 1 MCG 11535 5089 B Gibbs 4 Carlton Richmond Away 120 64 56 1.0 0.032258 0.031780 0.029963 0.022186 0.10 0.000000 0.130435 0.000000 0.0 0.023024 0.036496 0.025316 0.031637 0.029703 0.074074 0.0625 0.029197 0.0 0.009804 0.044776 0.010204 0.000000 0.02439 0.025 0.033250 0.033868 Kicked A Bag Feature features [ 'kicked_a_bag' ] = brownlow_data . G . apply ( lambda x : 1 if x >= 5 else 0 ) Is Captain Feature features [ 'is_captain' ] = features . apply ( lambda x : is_captain_for_that_season ( captains , x . player , x . season ), axis = 'columns' ) Won the Game Feature features [ 'team_won' ] = np . where ( features . margin > 0 , 1 , 0 ) 30+ & 2+ Goals Feature features [ 'got_30_possies_2_goals' ] = np . where (( brownlow_data . G >= 2 ) & ( brownlow_data . D >= 30 ), 1 , 0 ) Previous Top 10 Finish Feature I have a strong feeling that past performance may be a predictor of future performance in the brownlow. For example, last year Dusty Martin won the Brownlow. The umpires may have a bias towards Dusty this year because he is known to be on their radar as being a good player. Let's create a feature which is categorical and is 1 if the player has previously finished in the top 10. Let's create a function for this and then apply it to the afltables dataset, which has data back to 1897. We will then create a lookup table for the top 10 for each season and merge this table with our current features df. afltables = pd . read_csv ( 'data/afltables_stats.csv' ) . query ( 'Season >= 2000' ) def replace_special_characters ( name ): name = name . replace ( \"'\" , \"\" ) . replace ( \"-\" , \" \" ) . lower () name_split = name . split () if len ( name_split ) > 2 : first_name = name_split [ 0 ] last_name = name_split [ - 1 ] name = first_name + ' ' + last_name name_split_2 = name . split () name = name_split_2 [ 0 ][ 0 ] + ' ' + name_split_2 [ 1 ] return name . title () afltables = ( afltables . assign ( player = lambda df : df [ 'First.name' ] + ' ' + df . Surname ) . assign ( player = lambda df : df . player . apply ( replace_special_characters )) . rename ( columns = { 'Brownlow.Votes' : 'brownlow_votes' , 'Season' : 'season' , 'Playing.for' : 'team' })) ### Create Top 10 rank look up table brownlow_votes_yearly = ( afltables . groupby ([ 'season' , 'player' , 'team' ], as_index = False ) . brownlow_votes . sum ()) brownlow_votes_yearly [ 'yearly_rank' ] = ( brownlow_votes_yearly . groupby ( 'season' ) . brownlow_votes . rank ( method = 'max' , ascending = False )) # Filter to only get a dataframe since 2000 and only the top 10 players from each season brownlow_votes_top_10 = brownlow_votes_yearly . query ( 'yearly_rank < 11 & season >= 2000' ) brownlow_votes_top_10 . head ( 3 ) def how_many_times_top_10 ( top_10_df , player , year ): times = len ( top_10_df [( top_10_df . player == player ) & ( top_10_df . season < year )]) return times features [ 'times_in_top_10' ] = features . apply ( lambda x : how_many_times_top_10 ( brownlow_votes_top_10 , x . player , x . season ), axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } season player team brownlow_votes yearly_rank 27 2000.0 A Koutoufides Carlton 19.0 4.0 36 2000.0 A Mcleod Adelaide 20.0 3.0 105 2000.0 B Ratten Carlton 18.0 6.0 Average Brownlow Votes Per Game Last Season Feature # Create a brownlow votes lookup table brownlow_votes_lookup_table = ( brownlow_data . groupby ([ 'player' , 'team' , 'season' ], as_index = False ) . brownlow_votes . mean () . assign ( next_season = lambda df : df . season + 1 ) . rename ( columns = { 'brownlow_votes' : 'ave_votes_last_season' })) # Have a look at Cripps to check if it's working brownlow_votes_lookup_table [ brownlow_votes_lookup_table . player == 'P Cripps' ] # Merge it to our features df features_with_votes_last_season = ( pd . merge ( features , brownlow_votes_lookup_table . drop ( columns = 'season' ), left_on = [ 'player' , 'team' , 'season' ], right_on = [ 'player' , 'team' , 'next_season' ], how = 'left' ) . drop ( columns = [ 'next_season' ]) . fillna ( 0 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } player team season ave_votes_last_season next_season 4377 P Cripps Carlton 2014 0.000000 2015 4378 P Cripps Carlton 2015 0.300000 2016 4379 P Cripps Carlton 2016 0.857143 2017 4380 P Cripps Carlton 2017 0.333333 2018 4381 P Cripps Carlton 2018 0.000000 2019 Historic Performance Relative To Model Feature It is well known that some players are good Brownlow performers. For whatever reason, they always poll much better than their stats may suggest. Lance Franklin and Bontempelli are probably in this category. Perhaps these players have an X-factor that Machine Learning models struggle to pick up on. To get around this, let's create a feature which looks at the player's performance relative to the model's prediction. To do this, we'll need to train and predict 7 different models - from 2011 to 2017. To create a model for each season, we will use h2o's AutoML. If you're new to h2o, please read about it here. It can be used in both R and Python. The metric we will use for loss in Mean Absolute Error (MAE). As we are using regression, some values are negative. We will convert these negative values to 0 as it doesn't make sense to poll negative brownlow votes. Similarly, some matches won't predict exactly 6 votes, so we will scale these predictions so that we predict exactly 6 votes for each match. So that you don't have to train these models yourself, I have saved the models and we will load them in. If you are keen to train the models yourself, simply uncomment out the code below and run the cell. To bulk uncomment, highlight the rows and press ctrl + '/' h2o . init () # Uncomment the code below if you want to train the models yourself - otherwise, we will load them in the load cell from disk ## Join to our features df # aml_yearly_model_objects = {} # yearly_predictions_dfs = {} # feature_cols = ['margin', 'CP', 'UP', 'ED', 'DE', # 'CM', 'GA', 'MI5', 'one_perc', 'BO', 'TOG', 'K', 'HB', 'D', 'M', 'G', # 'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', 'FF', 'FA', 'AF', 'SC'] # for year in range(2011, 2018): # # Filter the data to only include past data # train_historic = brownlow_data[brownlow_data.season < year].copy() # # Convert to an h2o frame # train_h2o_historic = h2o.H2OFrame(train_historic) # # Create an AutoML object # aml = H2OAutoML(max_runtime_secs=30, # balance_classes=True, # seed=42) # # Train the model # aml.train(y='brownlow_votes', x=feature_cols, training_frame=train_h2o_historic) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/yearly_model_{year}') # # Append the best model to a list # aml_yearly_model_objects[year] = aml.leader # # Make predictions on test set for that year # test_historic = brownlow_data[brownlow_data.season == year].copy() # test_h2o_historic = h2o.H2OFrame(test_historic) # preds = aml.predict(test_h2o_historic).as_data_frame() # test_historic['predicted_votes'] = preds.values # # Convert negative predictions to 0 # test_historic['predicted_votes_neg_to_0'] = test_historic.predicted_votes.apply(lambda x: 0 if x < 0 else x) # # Create a total match votes column - which calculates the number of votes predicted in each game when the predictions # # are unscaled # test_historic['unscaled_match_votes'] = test_historic.groupby('match_id').predicted_votes_neg_to_0.transform('sum') # # Scale predictions # test_historic['predicted_votes_scaled'] = test_historic.predicted_votes_neg_to_0 / test_historic.unscaled_match_votes * 6 # # Aggregate the predictions # test_grouped = (test_historic.groupby(['player', 'team'], as_index=False) # .sum() # .sort_values(by='brownlow_votes', ascending=False) # .assign(mae=lambda df: abs(df.predicted_votes_scaled - df.brownlow_votes))) # test_grouped['error'] = test_grouped.predicted_votes_scaled - test_grouped.brownlow_votes # test_grouped['next_year'] = year + 1 # # Add this years predictions df to a dictionary to use later # yearly_predictions_dfs[year] = test_grouped # preds_errors = None # for key, value in yearly_predictions_dfs.items(): # if preds_errors is None: # preds_errors = value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']] # else: # preds_errors = preds_errors.append(value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']], sort=True) # with open('data/prediction_errors_df.pickle', 'wb') as handle: # pickle.dump(preds_errors, handle) Checking whether there is an H2O instance running at http://localhost:54321 ..... not found. Attempting to start a local H2O server... ; Java HotSpot(TM) Client VM (build 25.301-b09, mixed mode) Starting server from C:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar Ice root: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp JVM stdout: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp\\h2o_ZhouI_started_from_python.out JVM stderr: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp\\h2o_ZhouI_started_from_python.err C:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\h2o\\backend\\server.py:386: UserWarning: You have a 32-bit version of Java. H2O works best with 64-bit Java. Please download the latest 64-bit Java SE JDK from Oracle. warn(\" You have a 32-bit version of Java. H2O works best with 64-bit Java.\\n\" Server is running at http://127.0.0.1:54325 Connecting to H2O server at http://127.0.0.1:54325 ... successful. H2O_cluster_uptime: 03 secs H2O_cluster_timezone: Australia/Sydney H2O_data_parsing_timezone: UTC H2O_cluster_version: 3.36.0.3 H2O_cluster_version_age: 5 days H2O_cluster_name: H2O_from_python_ZhouI_yobrjv H2O_cluster_total_nodes: 1 H2O_cluster_free_memory: 247.5 Mb H2O_cluster_total_cores: 12 H2O_cluster_allowed_cores: 12 H2O_cluster_status: locked, healthy H2O_connection_url: http://127.0.0.1:54325 H2O_connection_proxy: {\"http\": null, \"https\": null} H2O_internal_security: False Python_version: 3.9.6 final # Load predictions error df with open ( 'data/prediction_errors_df.pickle' , 'rb' ) as handle : preds_errors = pickle . load ( handle ) # Look at last years predictions preds_errors . query ( 'next_year == 2018' ) . sort_values ( by = 'brownlow_votes' , ascending = False ) . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } brownlow_votes error next_year player predicted_votes_scaled season 139 36.0 -3.431041 2018 D Martin 32.568959 44374 486 33.0 0.065131 2018 P Dangerfield 33.065131 42357 619 25.0 0.243510 2018 T Mitchell 25.243510 44374 279 23.0 -9.267806 2018 J Kennedy 13.732194 38323 376 22.0 -7.621163 2018 L Franklin 14.378837 44374 278 21.0 -4.494170 2018 J Kelly 16.505830 42357 519 20.0 -2.909334 2018 R Sloane 17.090666 44374 410 19.0 -7.124114 2018 M Bontempelli 11.875886 44374 483 18.0 -3.734412 2018 O Wines 14.265588 44374 121 17.0 -4.034390 2018 D Beams 12.965610 38323 390 16.0 -2.797669 2018 L Parker 13.202331 44374 561 15.0 -5.458874 2018 S Pendlebury 9.541126 32272 463 15.0 -4.892862 2018 N Fyfe 10.107138 42357 42 15.0 -4.379072 2018 B Ebert 10.620928 44374 651 15.0 1.011272 2018 Z Merrett 16.011272 42357 578 14.0 2.069385 2018 T Adams 16.069385 44374 34 14.0 -4.886668 2018 B Brown 9.113332 44374 172 14.0 0.211867 2018 D Zorko 14.211867 42357 184 14.0 -0.620086 2018 G Ablett 13.379914 28238 389 14.0 -2.811169 2018 L Neale 11.188831 42357 Look at that! A simple Machine Learning ensemble model, using AutoML predicted last year's winner! That's impressive. As we can see it also predicted Bontempelli would only score 11.26, when he actually scored 19 - a huge discrepency. Let's use this as a feature. features_with_historic_perf_relative_to_model = \\ ( features_with_votes_last_season . pipe ( pd . merge , preds_errors [[ 'player' , 'next_year' , 'error' ]], left_on = [ 'player' , 'season' ], right_on = [ 'player' , 'next_year' ], how = 'left' ) . fillna ( 0 ) . rename ( columns = { 'error' : 'error_last_season' }) . drop_duplicates ( subset = [ 'player' , 'round' , 'SC' ])) Filtering the data to only include the top 20 SC for each match Logically, it is extremely unlikely that a player will poll votes if their Supercoach score is not in the top 20 players. By eliminating the other 20+ players, we can reduce the noise in the data, as we are almost certain the players won't poll from the bottom half. Let's explore how many players poll if they're not in the top 20, and then filter our df if this number is not significant. # Find number of players who vote when in top 15 SC brownlow_data [ 'SC_rank_match' ] = brownlow_data . groupby ( 'match_id' ) . SC . rank ( method = 'max' , ascending = False ) brownlow_data . query ( 'SC_rank_match > 20 and season > 2014' ) . brownlow_votes . value_counts () 0.0 18330 1.0 14 2.0 8 3.0 2 Name: brownlow_votes, dtype: int64 Since 2014, there have only been 24 players who have voted and not been in the top 20 SC. features_with_sc_rank = features_with_historic_perf_relative_to_model . copy () features_with_sc_rank [ 'SC_rank_match' ] = features_with_sc_rank . groupby ( 'match_id' ) . SC . rank ( method = 'max' , ascending = False ) # Filter out rows with a SC rank of below 20 features_with_sc_rank_filtered = features_with_sc_rank . query ( 'SC_rank_match <= 20' ) # Filter out 2010 and 2011 as we used these seasons to create historic model performance features features_last_before_train = features_with_sc_rank_filtered . query ( 'season != 2010 and season != 2011' ) . reset_index ( drop = True ) features_last_before_train . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC kicked_a_bag is_captain team_won got_30_possies_2_goals times_in_top_10 ave_votes_last_season next_year error_last_season SC_rank_match 0 24/03/2012 2012 1 ANZ Stadium 11635 5343 C Bird 14 Sydney GWS Away 100 37 63 0.0 0.035842 0.041002 0.030769 0.019378 0.000000 0.083333 0.055556 0.019802 0.0 0.023064 0.034562 0.037383 0.035762 0.039409 0.105263 0.111111 0.049296 0.0000 0.060606 0.093333 0.063636 0.027778 0.025 0.075 0.039041 0.032662 0 0 1 0 0 0.052632 2012.0 0.136311 6.0 1 24/03/2012 2012 1 ANZ Stadium 1013 5343 J Bolton 24 Sydney GWS Away 100 37 63 0.0 0.039427 0.036446 0.028846 0.019607 0.000000 0.000000 0.000000 0.049505 0.0 0.024203 0.027650 0.040498 0.033113 0.009852 0.052632 0.000000 0.042254 0.0125 0.040404 0.066667 0.009091 0.000000 0.025 0.025 0.029819 0.038767 0 0 1 0 0 0.526316 2012.0 -0.833178 3.0 2 24/03/2012 2012 1 ANZ Stadium 1012 5343 A Goodes 37 Sydney GWS Away 100 37 63 0.0 0.032258 0.029613 0.028846 0.023332 0.037037 0.083333 0.055556 0.009901 0.0 0.026196 0.029954 0.024922 0.027815 0.029557 0.000000 0.055556 0.014085 0.0125 0.070707 0.040000 0.027273 0.041667 0.025 0.000 0.025822 0.027473 0 1 1 0 5 0.761905 2012.0 -5.718150 14.0 Modeling The 2017 Brownlow Now that we have all of our features, we can simply create a training set (2012-2016), and a test set (2017), and make our predictions for last year! We will use AutoML for this process again. Again, rather than waiting for the model to train, I will save the model so you can simply load it in. We will also scale our features. We can then see how our model went in predicting last year's brownlow, creating a baseline for this years' predictions. We will then predict this year's vote count. train_baseline = features_last_before_train . query ( \"season < 2017\" ) holdout = features_last_before_train . query ( \"season == 2017\" ) scale_cols = [ 'team_score' , 'opposition_score' , 'margin' , 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] other_feature_cols = [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' , 'times_in_top_10' , 'ave_votes_last_season' , 'error_last_season' , 'SC_rank_match' ] all_feature_cols = scale_cols + other_feature_cols # Scale features scaler = StandardScaler () train_baseline_scaled = train_baseline . copy () train_baseline_scaled [ scale_cols ] = scaler . fit_transform ( train_baseline [ scale_cols ]) holdout_scaled = holdout . copy () holdout_scaled [ scale_cols ] = scaler . transform ( holdout [ scale_cols ]) # Convert categorical columns to categoricals train_baseline_h2o = h2o . H2OFrame ( train_baseline_scaled ) holdout_h2o = h2o . H2OFrame ( holdout_scaled ) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_baseline_h2o [ col ] = train_baseline_h2o [ col ] . asfactor () holdout_h2o [ col ] = holdout_h2o [ col ] . asfactor () Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% Below I have commented out training and saving the 2017 model. Rather than training it again, we will just load it in. Uncomment this part out if you want to train it yourself. # aml_2017_model = H2OAutoML(max_runtime_secs = 60*30, # balance_classes=True, # seed=42) # aml_2017_model.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_baseline_h2o) # # save the model # model_path = h2o.save_model(model=aml_2017_model.leader, path=\"models\", force=True) # # Get model id # model_name = aml_2017_model.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2017_model_v1') # Load model in aml_2017_model = h2o . load_model ( 'models/brownlow_2017_model_v1' ) # Predict the 2017 brownlow count preds_final_2017_model = aml_2017_model . predict ( holdout_h2o ) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total holdout = ( holdout . assign ( predicted_votes = preds_final_2017_model . as_data_frame () . values ) . assign ( predicted_votes_neg_to_0 = lambda df : df . predicted_votes . apply ( lambda x : 0 if x < 0 else x )) . assign ( unscaled_match_votes = lambda df : df . groupby ( 'match_id' ) . predicted_votes_neg_to_0 . transform ( 'sum' )) . assign ( predicted_votes_scaled = lambda df : df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2017 = ( holdout . groupby ([ 'player' , 'team' ], as_index = False ) . agg ({ 'brownlow_votes' : sum , 'predicted_votes_scaled' : sum , 'SC' : 'mean' , 'G' : 'mean' }) . sort_values ( by = 'brownlow_votes' , ascending = False ) . assign ( mae = lambda df : abs ( df . brownlow_votes - df . predicted_votes_scaled )) . reset_index ( drop = True )) glm prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% agg_predictions_2017 . head ( 15 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } player team brownlow_votes predicted_votes_scaled SC G mae 0 D Martin Richmond 36.0 25.007991 0.037862 0.064869 10.992009 1 P Dangerfield Geelong 33.0 25.568804 0.042441 0.070819 7.431196 2 T Mitchell Hawthorn 25.0 19.078639 0.036040 0.016928 5.921361 3 L Franklin Sydney 22.0 12.574692 0.034640 0.149203 9.425308 4 J Kelly GWS 21.0 15.198580 0.034652 0.033772 5.801420 5 R Sloane Adelaide 20.0 15.755535 0.037068 0.034821 4.244465 6 J Kennedy Sydney 20.0 11.065483 0.032014 0.030508 8.934517 7 M Bontempelli Western Bulldogs 19.0 13.836173 0.033233 0.040498 5.163827 8 D Beams Brisbane 17.0 10.073366 0.034848 0.044998 6.926634 9 O Wines Port Adelaide 16.0 10.511818 0.031601 0.021967 5.488182 10 N Fyfe Fremantle 15.0 12.782723 0.033761 0.031680 2.217277 11 S Pendlebury Collingwood 15.0 9.135615 0.033855 0.013660 5.864385 12 B Ebert Port Adelaide 15.0 7.941416 0.032795 0.008431 7.058584 13 L Parker Sydney 15.0 12.431929 0.031366 0.030311 2.568071 14 Z Merrett Essendon 15.0 15.017767 0.033737 0.015362 0.017767 So whilst our model predicted Dangerfield to win, it was pretty damn accurate! Let's find the MAE for the top 100, 50, 25, and 10, and then compare it to 2018's MAE in week, when the Brownlow has been counted. for top_x in [ 10 , 25 , 50 , 100 ]: temp_mae = round ( agg_predictions_2017 . iloc [: top_x ] . mae . mean (), 3 ) print ( f \"The Average Mean Absolute Error for the top { top_x } is { temp_mae } \" ) The Average Mean Absolute Error for the top 10 is 7.033 The Average Mean Absolute Error for the top 25 is 4.962 The Average Mean Absolute Error for the top 50 is 3.744 The Average Mean Absolute Error for the top 100 is 2.696 Modelling This Year's Brownlow Let's now predict this year's vote count. These predictions will be on the front page of the GitHub. train = features_last_before_train . query ( \"season < 2018\" ) test = features_last_before_train . query ( \"season == 2018\" ) # Scale features scaler = StandardScaler () train_scaled = train . copy () train_scaled [ scale_cols ] = scaler . fit_transform ( train [ scale_cols ]) test_scaled = test . copy () test_scaled [ scale_cols ] = scaler . transform ( test [ scale_cols ]) # Convert categorical columns to categoricals train_h2o = h2o . H2OFrame ( train_scaled ) test_h2o = h2o . H2OFrame ( test_scaled ) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_h2o [ col ] = train_h2o [ col ] . asfactor () test_h2o [ col ] = test_h2o [ col ] . asfactor () Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% # # Train the model - this part is commented out as we will just load our model from disk # aml = H2OAutoML(max_runtime_secs = 60*30, # balance_classes=True, # seed=42) # aml.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_h2o) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2018_model_v1') # Load model in aml = h2o . load_model ( 'models/brownlow_2018_model_v1' ) # Predict the 2018 brownlow count preds_final_2018_model = aml . predict ( test_h2o ) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total test = ( test . assign ( predicted_votes = preds_final_2018_model . as_data_frame () . values ) . assign ( predicted_votes_neg_to_0 = lambda df : df . predicted_votes . apply ( lambda x : 0 if x < 0 else x )) . assign ( unscaled_match_votes = lambda df : df . groupby ( 'match_id' ) . predicted_votes_neg_to_0 . transform ( 'sum' )) . assign ( predicted_votes_scaled = lambda df : df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2018 = ( test . groupby ([ 'player' , 'team' ], as_index = False ) . agg ({ 'predicted_votes_scaled' : sum , 'match_id' : 'count' }) # shows how many games they played . sort_values ( by = 'predicted_votes_scaled' , ascending = False ) . reset_index ( drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% # Show the top 25 predictions agg_predictions_2018 . head ( 25 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 36.577208 20 1 M Gawn Melbourne 20.686854 22 2 C Oliver Melbourne 19.705244 20 3 D Martin Richmond 19.674984 19 4 B Grundy Collingwood 19.136407 22 5 J Macrae Western Bulldogs 18.396814 17 6 P Dangerfield Geelong 18.207483 21 7 D Beams Brisbane 17.280375 15 8 A Gaff West Coast 16.217552 18 9 L Neale Fremantle 16.038782 21 10 E Yeo West Coast 15.866418 20 11 J Selwood Geelong 15.749850 18 12 D Heppell Essendon 15.073503 19 13 N Fyfe Fremantle 14.864971 11 14 Z Merrett Essendon 14.597278 18 15 S Sidebottom Collingwood 14.403396 18 16 J Kennedy Sydney 14.297521 16 17 M Crouch Adelaide 13.464205 16 18 G Ablett Geelong 13.403062 15 19 P Cripps Carlton 13.140569 21 20 R Laird Adelaide 13.016825 19 21 L Franklin Sydney 12.562711 13 22 S Coniglio GWS 12.484317 20 23 J Kelly GWS 12.245194 14 24 J Lloyd Sydney 12.019714 20 print ( agg_predictions_2018 . head ( 15 )) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 36.577208 20 1 M Gawn Melbourne 20.686854 22 2 C Oliver Melbourne 19.705244 20 3 D Martin Richmond 19.674984 19 4 B Grundy Collingwood 19.136407 22 5 J Macrae Western Bulldogs 18.396814 17 6 P Dangerfield Geelong 18.207483 21 7 D Beams Brisbane 17.280375 15 8 A Gaff West Coast 16.217552 18 9 L Neale Fremantle 16.038782 21 10 E Yeo West Coast 15.866418 20 11 J Selwood Geelong 15.749850 18 12 D Heppell Essendon 15.073503 19 13 N Fyfe Fremantle 14.864971 11 14 Z Merrett Essendon 14.597278 18 Now that we have the top 25, let's also look at the top 3 from each team. agg_predictions_2018 . sort_values ( by = [ 'team' , 'predicted_votes_scaled' ], ascending = [ True , False ]) . groupby ( 'team' ) . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } player team predicted_votes_scaled match_id 17 M Crouch Adelaide 13.464205 16 20 R Laird Adelaide 13.016825 19 42 B Gibbs Adelaide 8.976507 18 7 D Beams Brisbane 17.280375 15 48 D Zorko Brisbane 8.039436 14 58 S Martin Brisbane 6.835975 19 19 P Cripps Carlton 13.140569 21 51 K Simpson Carlton 7.497976 18 91 E Curnow Carlton 3.734706 19 4 B Grundy Collingwood 19.136407 22 15 S Sidebottom Collingwood 14.403396 18 26 A Treloar Collingwood 11.173963 11 12 D Heppell Essendon 15.073503 19 14 Z Merrett Essendon 14.597278 18 61 D Smith Essendon 6.367458 20 9 L Neale Fremantle 16.038782 21 13 N Fyfe Fremantle 14.864971 11 75 D Mundy Fremantle 4.895438 19 22 S Coniglio GWS 12.484317 20 23 J Kelly GWS 12.245194 14 25 C Ward GWS 11.251450 19 6 P Dangerfield Geelong 18.207483 21 11 J Selwood Geelong 15.749850 18 18 G Ablett Geelong 13.403062 15 76 J Witts Gold Coast 4.745802 13 79 J Lyons Gold Coast 4.482626 14 114 D Swallow Gold Coast 2.731594 15 0 T Mitchell Hawthorn 36.577208 20 41 L Breust Hawthorn 9.132324 16 50 J Gunston Hawthorn 7.641945 19 1 M Gawn Melbourne 20.686854 22 2 C Oliver Melbourne 19.705244 20 30 J Hogan Melbourne 10.510504 13 29 S Higgins North Melbourne 10.682736 19 40 B Brown North Melbourne 9.211568 13 43 B Cunnington North Melbourne 8.949073 17 28 O Wines Port Adelaide 10.948001 16 35 R Gray Port Adelaide 10.017038 17 53 J Westhoff Port Adelaide 7.425425 21 3 D Martin Richmond 19.674984 19 36 J Riewoldt Richmond 9.981867 15 57 K Lambert Richmond 6.897522 12 38 S Ross St Kilda 9.756485 17 46 J Steven St Kilda 8.103860 17 90 J Steele St Kilda 3.739189 16 16 J Kennedy Sydney 14.297521 16 21 L Franklin Sydney 12.562711 13 24 J Lloyd Sydney 12.019714 20 8 A Gaff West Coast 16.217552 18 10 E Yeo West Coast 15.866418 20 34 J Redden West Coast 10.200779 16 5 J Macrae Western Bulldogs 18.396814 17 39 M Bontempelli Western Bulldogs 9.317955 16 45 L Hunter Western Bulldogs 8.758631 17 If you're looking for a round by round breakdown, just have a look at the test dataframe. test [[ 'date' , 'round' , 'player' , 'team' , 'opposition' , 'margin' , 'SC' , 'predicted_votes_scaled' ]] . tail ( 25 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date round player team opposition margin SC predicted_votes_scaled 27236 24/08/2018 23 J Laverde Essendon Port Adelaide 22 0.028190 0.098214 27237 24/08/2018 23 Z Merrett Essendon Port Adelaide 22 0.030918 0.316848 27238 24/08/2018 23 D Parish Essendon Port Adelaide 22 0.030615 0.031097 27239 24/08/2018 23 A Saad Essendon Port Adelaide 22 0.026069 0.000000 27240 24/08/2018 23 D Smith Essendon Port Adelaide 22 0.028190 0.714852 27241 24/08/2018 23 D Zaharakis Essendon Port Adelaide 22 0.034859 0.331842 27242 25/08/2018 23 H Ballantyne Fremantle Collingwood -9 0.025773 0.594485 27243 25/08/2018 23 T Duman Fremantle Collingwood -9 0.038205 0.068379 27244 25/08/2018 23 J Hamling Fremantle Collingwood -9 0.024560 0.000000 27245 25/08/2018 23 B Hill Fremantle Collingwood -9 0.031837 0.755081 27246 25/08/2018 23 E Langdon Fremantle Collingwood -9 0.033354 0.420121 27247 25/08/2018 23 D Mundy Fremantle Collingwood -9 0.023954 0.000000 27248 25/08/2018 23 L Neale Fremantle Collingwood -9 0.042450 1.012576 27249 25/08/2018 23 S Switkowski Fremantle Collingwood -9 0.027289 0.000000 27250 25/08/2018 23 T Adams Collingwood Fremantle 9 0.030018 0.207985 27251 25/08/2018 23 M Cox Collingwood Fremantle 9 0.024864 0.017404 27252 25/08/2018 23 J Crisp Collingwood Fremantle 9 0.031534 0.022959 27253 25/08/2018 23 B Grundy Collingwood Fremantle 9 0.045482 1.118902 27254 25/08/2018 23 B Maynard Collingwood Fremantle 9 0.026076 0.000000 27255 25/08/2018 23 B Mihocek Collingwood Fremantle 9 0.025167 0.000000 27256 25/08/2018 23 S Pendlebury Collingwood Fremantle 9 0.029715 0.888746 27257 25/08/2018 23 T Phillips Collingwood Fremantle 9 0.028199 0.009869 27258 25/08/2018 23 S Sidebottom Collingwood Fremantle 9 0.039115 0.880034 27259 25/08/2018 23 B Sier Collingwood Fremantle 9 0.023044 0.002407 27260 25/08/2018 23 J Thomas Collingwood Fremantle 9 0.026986 0.001053 And there we have it! In a single notebook we have made a fairly good Brownlow predictive model. Enjoy. Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Modelling the Brownlow Medal in Python"},{"location":"modelling/brownlowModelTutorial/#modelling-the-brownlow","text":"This walkthrough will provide a brief, yet effective tutorial on how to model the Brownlow medal. We will use data from 2010 to 2018, which includes Supercoach points and other useful stats. The output will be the number of votes predicted for each player in each match, and we will aggregate these to create aggregates for each team and for the whole competition. No doubt we'll have Mitchell right up the top, and if we don't, then we know we've done something wrong! # Import modules libraries import pandas as pd import h2o from h2o.automl import H2OAutoML import numpy as np from sklearn.preprocessing import StandardScaler import os import pickle # Change notebook settings pd . options . display . max_columns = None pd . options . display . max_rows = 300 from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" This walkthrough uses H2O's AutoML to automate machine learning. We have saved the models created by AutoML into the Github repo which can be loaded into the notebook to save you time training the model. However a drawback of H2O is that you can only load models if your currently installed verson of H2O is the same as the version used to create the model. You can check what version of H2O you have installed by running h2o.__version__ : # Checking the current version of H2O print ( f 'Current version of H2O installed: { h2o . __version__ } ' ) Current version of H2O installed: 3.36.0.3 If you have a different version of H2O installed you have two options. You can train the models yourself, all the code to do that is commented out in the notebook. Or you can pip uninstall H2O and then pip install H2O again, but specifically the 3.36.0.3 version. The code to do this is below and will only take 1 or 2 minutes: # # Uncomment the code below if you need to uninstall the current version of H2O and reinstall version 3.36.0.3 # # The following command removes the H2O module for Python. # pip uninstall h2o # # Next, use pip to install this version of the H2O Python module. # pip install http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/3/Python/h2o-3.36.0.3-py2.py3-none-any.whl","title":"Modelling the Brownlow"},{"location":"modelling/brownlowModelTutorial/#eda","text":"","title":"EDA"},{"location":"modelling/brownlowModelTutorial/#read-in-the-data","text":"I have collated this data using the fitzRoy R package and merging the afltables dataset with the footywire dataset, so that we can Supercoach and other advanced stats with Brownlow votes. Let's read in the data and have a sneak peak at what it looks like. brownlow_data = pd . read_csv ( 'data/afl_brownlow_data.csv' ) brownlow_data . tail ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 76585 25/08/2018 2018 23 Optus Stadium 12587 9711 J Stephenson 35 Collingwood Fremantle Away 76 67 9 NaN 3 5 5 62.5 0 0 3 2 0 87 6 2 8 3 2 1 3 0 0 0 1 0 0 0 56 56 76586 25/08/2018 2018 23 Optus Stadium 12144 9711 J Thomas 24 Collingwood Fremantle Away 76 67 9 NaN 6 11 14 82.4 0 2 0 1 0 79 11 6 17 4 1 0 3 0 4 1 4 1 1 3 67 89 76587 25/08/2018 2018 23 Optus Stadium 11549 9711 T Varcoe 18 Collingwood Fremantle Away 76 67 9 NaN 7 4 7 53.8 0 1 0 2 0 73 6 7 13 1 0 0 3 0 2 0 4 0 1 1 45 53 It looks like we've got about 76,000 rows of data and have stats like hitouts, clangers, effective disposals etc. Let's explore some certain scenarios. Using my domain knowledge of footy, I can hypothesise that if a player kicks 5 goals, he is pretty likely to poll votes. Similarly, if a player gets 30 possessions and 2+ goals, he is also probably likely to poll votes. Let's have a look at the mean votes for players for both of these situations.","title":"Read in the data"},{"location":"modelling/brownlowModelTutorial/#exploring-votes-if-a-bag-is-kicked-5-goals","text":"brownlow_data . query ( 'G >= 5' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player has kicked a bag:\" , brownlow_data . query ( 'G >= 5' ) . brownlow_votes . mean ()) season 2010 1.420455 2011 1.313433 2012 1.413333 2013 1.253731 2014 1.915254 2015 1.765625 2016 1.788732 2017 2.098361 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player has kicked a bag: 1.4708818635607321","title":"Exploring votes if a bag is kicked (5+ goals)"},{"location":"modelling/brownlowModelTutorial/#exploring-votes-if-the-player-has-30-possies-2-goals","text":"brownlow_data . query ( 'G >= 2 and D >= 30' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player has 30 possies and kicks 2+ goals:\" , brownlow_data . query ( 'G >= 2 and D >= 30' ) . brownlow_votes . mean ()) season 2010 1.826923 2011 1.756410 2012 2.118421 2013 2.000000 2014 2.253731 2015 2.047619 2016 2.103448 2017 2.050000 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player has 30 possies and kicks 2+ goals: 1.8741379310344828 As suspected, the average votes for these two situations is 1.87! That's huge. Let's get an idea of the average votes for each player. It should be around 6/44, as there are always 6 votes per match and around 44 players per match. brownlow_data . brownlow_votes . mean () 0.12347341475121326 So the average vote is 0.12. Let's see how this changes is the player is a captain. I have collected data on if players are captains from wikipedia and collated it into a csv. Let's load this in and create a \"Is the player captain\" feature, then check the average votes for captains.","title":"Exploring votes if the player has 30+ possies &amp; 2+ goals"},{"location":"modelling/brownlowModelTutorial/#create-is-player-captain-feature","text":"captains = pd . read_csv ( 'data/captains.csv' ) . set_index ( 'player' ) def is_captain_for_that_season ( captains_df , player , year ): if player in captains_df . index : # Get years they were captain seasons = captains_df . loc [ player ] . season . split ( '-' ) if len ( seasons ) == 1 : seasons_captain = list ( map ( int , seasons )) elif len ( seasons ) == 2 : if seasons [ 1 ] == '' : seasons_captain = list ( range ( int ( seasons [ 0 ]), 2019 )) else : seasons_captain = list ( range ( int ( seasons [ 0 ]), int ( seasons [ 1 ]) + 1 )) if year in seasons_captain : return 1 return 0 brownlow_data [ 'is_captain' ] = brownlow_data . apply ( lambda x : is_captain_for_that_season ( captains , x . player , x . season ), axis = 'columns' ) brownlow_data . query ( 'is_captain == 1' ) . groupby ( 'season' ) . brownlow_votes . mean () print ( \"Mean votes if a player is captain:\" , brownlow_data . query ( 'is_captain == 1' ) . brownlow_votes . mean ()) season 2010 0.408497 2011 0.429936 2012 0.274194 2013 0.438725 2014 0.519663 2015 0.447222 2016 0.347826 2017 0.425806 2018 0.000000 Name: brownlow_votes, dtype: float64 Mean votes if a player is captain: 0.36661698956780925 This is significantly higher than if they aren't captain. What would be interesting is to look at the average difference in votes between when they were captain and when they weren't, to try and find if there is a 'captain bias' in brownlow votes. Go ahead and try. For now, we're going to move onto feature creation","title":"Create Is Player Captain Feature"},{"location":"modelling/brownlowModelTutorial/#feature-creation","text":"Let's make a range of features, including: * Ratios of each statistic per game * If the player is a captain * If they kicked a bag (4/5+) * If they kicked 2 and had 30+ possies First we will make features of ratios. What is important is not how many of a certain stat a player has, but how much of that stat a player has relative to everyone else in the same match. It doesn't matter if Dusty Martin has 31 possessions if Tom Mitchell has had 50 - Mitchell is probably more likely to poll (assuming all else is equal). So rather than using the actual number of possessions for example, we can divide these possessions by the total amount of possessions in the game. To do this we'll use pandas groupby and transform methods.","title":"Feature Creation"},{"location":"modelling/brownlowModelTutorial/#create-ratios-as-features","text":"%% time # Get a list of stats of which to create ratios for ratio_cols = [ 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'TOG' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] # Create a ratios df ratios = ( brownlow_data . copy () . loc [:, [ 'match_id' ] + ratio_cols ] . groupby ( 'match_id' ) . transform ( lambda x : x / x . sum ())) feature_cols = [ 'date' , 'season' , 'round' , 'venue' , 'ID' , 'match_id' , 'player' , 'jumper_no' , 'team' , 'opposition' , 'status' , 'team_score' , 'opposition_score' , 'margin' , 'brownlow_votes' ] # Create a features df - join the ratios to this df features = ( brownlow_data [ feature_cols ] . copy () . join ( ratios )) Wall time: 17.1 s features . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC 0 25/03/2010 2010 1 MCG 11559 5089 J Anderson 26 Carlton Richmond Away 120 64 56 0.0 0.024194 0.021186 0.020599 0.021931 0.00 0.000000 0.000000 0.000000 0.0 0.024411 0.017032 0.028481 0.022008 0.019802 0.000000 0.0000 0.029197 0.0 0.000000 0.000000 0.010204 0.014286 0.00000 0.025 0.020075 0.019353 1 25/03/2010 2010 1 MCG 4060 5089 E Betts 19 Carlton Richmond Away 120 64 56 0.0 0.044355 0.014831 0.024345 0.021804 0.05 0.133333 0.130435 0.033333 0.0 0.025243 0.021898 0.031646 0.026135 0.029703 0.111111 0.1875 0.043796 0.0 0.000000 0.014925 0.010204 0.000000 0.00000 0.000 0.034504 0.041125 2 25/03/2010 2010 1 MCG 3281 5089 P Bower 18 Carlton Richmond Away 120 64 56 0.0 0.016129 0.038136 0.044944 0.030602 0.10 0.000000 0.000000 0.100000 0.0 0.026352 0.036496 0.031646 0.034388 0.054455 0.000000 0.0000 0.036496 0.0 0.009804 0.000000 0.000000 0.014286 0.00000 0.000 0.037014 0.039311 3 25/03/2010 2010 1 MCG 4056 5089 A Carrazzo 44 Carlton Richmond Away 120 64 56 2.0 0.032258 0.069915 0.059925 0.025501 0.00 0.000000 0.000000 0.000000 0.0 0.024133 0.051095 0.060127 0.055021 0.024752 0.000000 0.0000 0.029197 0.0 0.078431 0.074627 0.040816 0.057143 0.04878 0.025 0.041092 0.040822 4 25/03/2010 2010 1 MCG 11535 5089 B Gibbs 4 Carlton Richmond Away 120 64 56 1.0 0.032258 0.031780 0.029963 0.022186 0.10 0.000000 0.130435 0.000000 0.0 0.023024 0.036496 0.025316 0.031637 0.029703 0.074074 0.0625 0.029197 0.0 0.009804 0.044776 0.010204 0.000000 0.02439 0.025 0.033250 0.033868","title":"Create Ratios As Features"},{"location":"modelling/brownlowModelTutorial/#kicked-a-bag-feature","text":"features [ 'kicked_a_bag' ] = brownlow_data . G . apply ( lambda x : 1 if x >= 5 else 0 )","title":"Kicked A Bag Feature"},{"location":"modelling/brownlowModelTutorial/#is-captain-feature","text":"features [ 'is_captain' ] = features . apply ( lambda x : is_captain_for_that_season ( captains , x . player , x . season ), axis = 'columns' )","title":"Is Captain Feature"},{"location":"modelling/brownlowModelTutorial/#won-the-game-feature","text":"features [ 'team_won' ] = np . where ( features . margin > 0 , 1 , 0 )","title":"Won the Game Feature"},{"location":"modelling/brownlowModelTutorial/#30-2-goals-feature","text":"features [ 'got_30_possies_2_goals' ] = np . where (( brownlow_data . G >= 2 ) & ( brownlow_data . D >= 30 ), 1 , 0 )","title":"30+ &amp; 2+ Goals Feature"},{"location":"modelling/brownlowModelTutorial/#previous-top-10-finish-feature","text":"I have a strong feeling that past performance may be a predictor of future performance in the brownlow. For example, last year Dusty Martin won the Brownlow. The umpires may have a bias towards Dusty this year because he is known to be on their radar as being a good player. Let's create a feature which is categorical and is 1 if the player has previously finished in the top 10. Let's create a function for this and then apply it to the afltables dataset, which has data back to 1897. We will then create a lookup table for the top 10 for each season and merge this table with our current features df. afltables = pd . read_csv ( 'data/afltables_stats.csv' ) . query ( 'Season >= 2000' ) def replace_special_characters ( name ): name = name . replace ( \"'\" , \"\" ) . replace ( \"-\" , \" \" ) . lower () name_split = name . split () if len ( name_split ) > 2 : first_name = name_split [ 0 ] last_name = name_split [ - 1 ] name = first_name + ' ' + last_name name_split_2 = name . split () name = name_split_2 [ 0 ][ 0 ] + ' ' + name_split_2 [ 1 ] return name . title () afltables = ( afltables . assign ( player = lambda df : df [ 'First.name' ] + ' ' + df . Surname ) . assign ( player = lambda df : df . player . apply ( replace_special_characters )) . rename ( columns = { 'Brownlow.Votes' : 'brownlow_votes' , 'Season' : 'season' , 'Playing.for' : 'team' })) ### Create Top 10 rank look up table brownlow_votes_yearly = ( afltables . groupby ([ 'season' , 'player' , 'team' ], as_index = False ) . brownlow_votes . sum ()) brownlow_votes_yearly [ 'yearly_rank' ] = ( brownlow_votes_yearly . groupby ( 'season' ) . brownlow_votes . rank ( method = 'max' , ascending = False )) # Filter to only get a dataframe since 2000 and only the top 10 players from each season brownlow_votes_top_10 = brownlow_votes_yearly . query ( 'yearly_rank < 11 & season >= 2000' ) brownlow_votes_top_10 . head ( 3 ) def how_many_times_top_10 ( top_10_df , player , year ): times = len ( top_10_df [( top_10_df . player == player ) & ( top_10_df . season < year )]) return times features [ 'times_in_top_10' ] = features . apply ( lambda x : how_many_times_top_10 ( brownlow_votes_top_10 , x . player , x . season ), axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } season player team brownlow_votes yearly_rank 27 2000.0 A Koutoufides Carlton 19.0 4.0 36 2000.0 A Mcleod Adelaide 20.0 3.0 105 2000.0 B Ratten Carlton 18.0 6.0","title":"Previous Top 10 Finish Feature"},{"location":"modelling/brownlowModelTutorial/#average-brownlow-votes-per-game-last-season-feature","text":"# Create a brownlow votes lookup table brownlow_votes_lookup_table = ( brownlow_data . groupby ([ 'player' , 'team' , 'season' ], as_index = False ) . brownlow_votes . mean () . assign ( next_season = lambda df : df . season + 1 ) . rename ( columns = { 'brownlow_votes' : 'ave_votes_last_season' })) # Have a look at Cripps to check if it's working brownlow_votes_lookup_table [ brownlow_votes_lookup_table . player == 'P Cripps' ] # Merge it to our features df features_with_votes_last_season = ( pd . merge ( features , brownlow_votes_lookup_table . drop ( columns = 'season' ), left_on = [ 'player' , 'team' , 'season' ], right_on = [ 'player' , 'team' , 'next_season' ], how = 'left' ) . drop ( columns = [ 'next_season' ]) . fillna ( 0 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } player team season ave_votes_last_season next_season 4377 P Cripps Carlton 2014 0.000000 2015 4378 P Cripps Carlton 2015 0.300000 2016 4379 P Cripps Carlton 2016 0.857143 2017 4380 P Cripps Carlton 2017 0.333333 2018 4381 P Cripps Carlton 2018 0.000000 2019","title":"Average Brownlow Votes Per Game Last Season Feature"},{"location":"modelling/brownlowModelTutorial/#historic-performance-relative-to-model-feature","text":"It is well known that some players are good Brownlow performers. For whatever reason, they always poll much better than their stats may suggest. Lance Franklin and Bontempelli are probably in this category. Perhaps these players have an X-factor that Machine Learning models struggle to pick up on. To get around this, let's create a feature which looks at the player's performance relative to the model's prediction. To do this, we'll need to train and predict 7 different models - from 2011 to 2017. To create a model for each season, we will use h2o's AutoML. If you're new to h2o, please read about it here. It can be used in both R and Python. The metric we will use for loss in Mean Absolute Error (MAE). As we are using regression, some values are negative. We will convert these negative values to 0 as it doesn't make sense to poll negative brownlow votes. Similarly, some matches won't predict exactly 6 votes, so we will scale these predictions so that we predict exactly 6 votes for each match. So that you don't have to train these models yourself, I have saved the models and we will load them in. If you are keen to train the models yourself, simply uncomment out the code below and run the cell. To bulk uncomment, highlight the rows and press ctrl + '/' h2o . init () # Uncomment the code below if you want to train the models yourself - otherwise, we will load them in the load cell from disk ## Join to our features df # aml_yearly_model_objects = {} # yearly_predictions_dfs = {} # feature_cols = ['margin', 'CP', 'UP', 'ED', 'DE', # 'CM', 'GA', 'MI5', 'one_perc', 'BO', 'TOG', 'K', 'HB', 'D', 'M', 'G', # 'B', 'T', 'HO', 'I50', 'CL', 'CG', 'R50', 'FF', 'FA', 'AF', 'SC'] # for year in range(2011, 2018): # # Filter the data to only include past data # train_historic = brownlow_data[brownlow_data.season < year].copy() # # Convert to an h2o frame # train_h2o_historic = h2o.H2OFrame(train_historic) # # Create an AutoML object # aml = H2OAutoML(max_runtime_secs=30, # balance_classes=True, # seed=42) # # Train the model # aml.train(y='brownlow_votes', x=feature_cols, training_frame=train_h2o_historic) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/yearly_model_{year}') # # Append the best model to a list # aml_yearly_model_objects[year] = aml.leader # # Make predictions on test set for that year # test_historic = brownlow_data[brownlow_data.season == year].copy() # test_h2o_historic = h2o.H2OFrame(test_historic) # preds = aml.predict(test_h2o_historic).as_data_frame() # test_historic['predicted_votes'] = preds.values # # Convert negative predictions to 0 # test_historic['predicted_votes_neg_to_0'] = test_historic.predicted_votes.apply(lambda x: 0 if x < 0 else x) # # Create a total match votes column - which calculates the number of votes predicted in each game when the predictions # # are unscaled # test_historic['unscaled_match_votes'] = test_historic.groupby('match_id').predicted_votes_neg_to_0.transform('sum') # # Scale predictions # test_historic['predicted_votes_scaled'] = test_historic.predicted_votes_neg_to_0 / test_historic.unscaled_match_votes * 6 # # Aggregate the predictions # test_grouped = (test_historic.groupby(['player', 'team'], as_index=False) # .sum() # .sort_values(by='brownlow_votes', ascending=False) # .assign(mae=lambda df: abs(df.predicted_votes_scaled - df.brownlow_votes))) # test_grouped['error'] = test_grouped.predicted_votes_scaled - test_grouped.brownlow_votes # test_grouped['next_year'] = year + 1 # # Add this years predictions df to a dictionary to use later # yearly_predictions_dfs[year] = test_grouped # preds_errors = None # for key, value in yearly_predictions_dfs.items(): # if preds_errors is None: # preds_errors = value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']] # else: # preds_errors = preds_errors.append(value[['player', 'season', 'next_year', 'brownlow_votes', 'predicted_votes_scaled', 'error']], sort=True) # with open('data/prediction_errors_df.pickle', 'wb') as handle: # pickle.dump(preds_errors, handle) Checking whether there is an H2O instance running at http://localhost:54321 ..... not found. Attempting to start a local H2O server... ; Java HotSpot(TM) Client VM (build 25.301-b09, mixed mode) Starting server from C:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar Ice root: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp JVM stdout: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp\\h2o_ZhouI_started_from_python.out JVM stderr: C:\\Users\\zhoui\\AppData\\Local\\Temp\\tmp6umhk9gp\\h2o_ZhouI_started_from_python.err C:\\Users\\zhoui\\greyhounds_bruno\\greyhound-modelling\\venv_greyhounds\\lib\\site-packages\\h2o\\backend\\server.py:386: UserWarning: You have a 32-bit version of Java. H2O works best with 64-bit Java. Please download the latest 64-bit Java SE JDK from Oracle. warn(\" You have a 32-bit version of Java. H2O works best with 64-bit Java.\\n\" Server is running at http://127.0.0.1:54325 Connecting to H2O server at http://127.0.0.1:54325 ... successful. H2O_cluster_uptime: 03 secs H2O_cluster_timezone: Australia/Sydney H2O_data_parsing_timezone: UTC H2O_cluster_version: 3.36.0.3 H2O_cluster_version_age: 5 days H2O_cluster_name: H2O_from_python_ZhouI_yobrjv H2O_cluster_total_nodes: 1 H2O_cluster_free_memory: 247.5 Mb H2O_cluster_total_cores: 12 H2O_cluster_allowed_cores: 12 H2O_cluster_status: locked, healthy H2O_connection_url: http://127.0.0.1:54325 H2O_connection_proxy: {\"http\": null, \"https\": null} H2O_internal_security: False Python_version: 3.9.6 final # Load predictions error df with open ( 'data/prediction_errors_df.pickle' , 'rb' ) as handle : preds_errors = pickle . load ( handle ) # Look at last years predictions preds_errors . query ( 'next_year == 2018' ) . sort_values ( by = 'brownlow_votes' , ascending = False ) . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } brownlow_votes error next_year player predicted_votes_scaled season 139 36.0 -3.431041 2018 D Martin 32.568959 44374 486 33.0 0.065131 2018 P Dangerfield 33.065131 42357 619 25.0 0.243510 2018 T Mitchell 25.243510 44374 279 23.0 -9.267806 2018 J Kennedy 13.732194 38323 376 22.0 -7.621163 2018 L Franklin 14.378837 44374 278 21.0 -4.494170 2018 J Kelly 16.505830 42357 519 20.0 -2.909334 2018 R Sloane 17.090666 44374 410 19.0 -7.124114 2018 M Bontempelli 11.875886 44374 483 18.0 -3.734412 2018 O Wines 14.265588 44374 121 17.0 -4.034390 2018 D Beams 12.965610 38323 390 16.0 -2.797669 2018 L Parker 13.202331 44374 561 15.0 -5.458874 2018 S Pendlebury 9.541126 32272 463 15.0 -4.892862 2018 N Fyfe 10.107138 42357 42 15.0 -4.379072 2018 B Ebert 10.620928 44374 651 15.0 1.011272 2018 Z Merrett 16.011272 42357 578 14.0 2.069385 2018 T Adams 16.069385 44374 34 14.0 -4.886668 2018 B Brown 9.113332 44374 172 14.0 0.211867 2018 D Zorko 14.211867 42357 184 14.0 -0.620086 2018 G Ablett 13.379914 28238 389 14.0 -2.811169 2018 L Neale 11.188831 42357 Look at that! A simple Machine Learning ensemble model, using AutoML predicted last year's winner! That's impressive. As we can see it also predicted Bontempelli would only score 11.26, when he actually scored 19 - a huge discrepency. Let's use this as a feature. features_with_historic_perf_relative_to_model = \\ ( features_with_votes_last_season . pipe ( pd . merge , preds_errors [[ 'player' , 'next_year' , 'error' ]], left_on = [ 'player' , 'season' ], right_on = [ 'player' , 'next_year' ], how = 'left' ) . fillna ( 0 ) . rename ( columns = { 'error' : 'error_last_season' }) . drop_duplicates ( subset = [ 'player' , 'round' , 'SC' ]))","title":"Historic Performance Relative To Model Feature"},{"location":"modelling/brownlowModelTutorial/#filtering-the-data-to-only-include-the-top-20-sc-for-each-match","text":"Logically, it is extremely unlikely that a player will poll votes if their Supercoach score is not in the top 20 players. By eliminating the other 20+ players, we can reduce the noise in the data, as we are almost certain the players won't poll from the bottom half. Let's explore how many players poll if they're not in the top 20, and then filter our df if this number is not significant. # Find number of players who vote when in top 15 SC brownlow_data [ 'SC_rank_match' ] = brownlow_data . groupby ( 'match_id' ) . SC . rank ( method = 'max' , ascending = False ) brownlow_data . query ( 'SC_rank_match > 20 and season > 2014' ) . brownlow_votes . value_counts () 0.0 18330 1.0 14 2.0 8 3.0 2 Name: brownlow_votes, dtype: int64 Since 2014, there have only been 24 players who have voted and not been in the top 20 SC. features_with_sc_rank = features_with_historic_perf_relative_to_model . copy () features_with_sc_rank [ 'SC_rank_match' ] = features_with_sc_rank . groupby ( 'match_id' ) . SC . rank ( method = 'max' , ascending = False ) # Filter out rows with a SC rank of below 20 features_with_sc_rank_filtered = features_with_sc_rank . query ( 'SC_rank_match <= 20' ) # Filter out 2010 and 2011 as we used these seasons to create historic model performance features features_last_before_train = features_with_sc_rank_filtered . query ( 'season != 2010 and season != 2011' ) . reset_index ( drop = True ) features_last_before_train . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date season round venue ID match_id player jumper_no team opposition status team_score opposition_score margin brownlow_votes CP UP ED DE CM GA MI5 one_perc BO TOG K HB D M G B T HO I50 CL CG R50 FF FA AF SC kicked_a_bag is_captain team_won got_30_possies_2_goals times_in_top_10 ave_votes_last_season next_year error_last_season SC_rank_match 0 24/03/2012 2012 1 ANZ Stadium 11635 5343 C Bird 14 Sydney GWS Away 100 37 63 0.0 0.035842 0.041002 0.030769 0.019378 0.000000 0.083333 0.055556 0.019802 0.0 0.023064 0.034562 0.037383 0.035762 0.039409 0.105263 0.111111 0.049296 0.0000 0.060606 0.093333 0.063636 0.027778 0.025 0.075 0.039041 0.032662 0 0 1 0 0 0.052632 2012.0 0.136311 6.0 1 24/03/2012 2012 1 ANZ Stadium 1013 5343 J Bolton 24 Sydney GWS Away 100 37 63 0.0 0.039427 0.036446 0.028846 0.019607 0.000000 0.000000 0.000000 0.049505 0.0 0.024203 0.027650 0.040498 0.033113 0.009852 0.052632 0.000000 0.042254 0.0125 0.040404 0.066667 0.009091 0.000000 0.025 0.025 0.029819 0.038767 0 0 1 0 0 0.526316 2012.0 -0.833178 3.0 2 24/03/2012 2012 1 ANZ Stadium 1012 5343 A Goodes 37 Sydney GWS Away 100 37 63 0.0 0.032258 0.029613 0.028846 0.023332 0.037037 0.083333 0.055556 0.009901 0.0 0.026196 0.029954 0.024922 0.027815 0.029557 0.000000 0.055556 0.014085 0.0125 0.070707 0.040000 0.027273 0.041667 0.025 0.000 0.025822 0.027473 0 1 1 0 5 0.761905 2012.0 -5.718150 14.0","title":"Filtering the data to only include the top 20 SC for each match"},{"location":"modelling/brownlowModelTutorial/#modeling-the-2017-brownlow","text":"Now that we have all of our features, we can simply create a training set (2012-2016), and a test set (2017), and make our predictions for last year! We will use AutoML for this process again. Again, rather than waiting for the model to train, I will save the model so you can simply load it in. We will also scale our features. We can then see how our model went in predicting last year's brownlow, creating a baseline for this years' predictions. We will then predict this year's vote count. train_baseline = features_last_before_train . query ( \"season < 2017\" ) holdout = features_last_before_train . query ( \"season == 2017\" ) scale_cols = [ 'team_score' , 'opposition_score' , 'margin' , 'CP' , 'UP' , 'ED' , 'DE' , 'CM' , 'GA' , 'MI5' , 'one_perc' , 'BO' , 'K' , 'HB' , 'D' , 'M' , 'G' , 'B' , 'T' , 'HO' , 'I50' , 'CL' , 'CG' , 'R50' , 'FF' , 'FA' , 'AF' , 'SC' ] other_feature_cols = [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' , 'times_in_top_10' , 'ave_votes_last_season' , 'error_last_season' , 'SC_rank_match' ] all_feature_cols = scale_cols + other_feature_cols # Scale features scaler = StandardScaler () train_baseline_scaled = train_baseline . copy () train_baseline_scaled [ scale_cols ] = scaler . fit_transform ( train_baseline [ scale_cols ]) holdout_scaled = holdout . copy () holdout_scaled [ scale_cols ] = scaler . transform ( holdout [ scale_cols ]) # Convert categorical columns to categoricals train_baseline_h2o = h2o . H2OFrame ( train_baseline_scaled ) holdout_h2o = h2o . H2OFrame ( holdout_scaled ) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_baseline_h2o [ col ] = train_baseline_h2o [ col ] . asfactor () holdout_h2o [ col ] = holdout_h2o [ col ] . asfactor () Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% Below I have commented out training and saving the 2017 model. Rather than training it again, we will just load it in. Uncomment this part out if you want to train it yourself. # aml_2017_model = H2OAutoML(max_runtime_secs = 60*30, # balance_classes=True, # seed=42) # aml_2017_model.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_baseline_h2o) # # save the model # model_path = h2o.save_model(model=aml_2017_model.leader, path=\"models\", force=True) # # Get model id # model_name = aml_2017_model.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2017_model_v1') # Load model in aml_2017_model = h2o . load_model ( 'models/brownlow_2017_model_v1' ) # Predict the 2017 brownlow count preds_final_2017_model = aml_2017_model . predict ( holdout_h2o ) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total holdout = ( holdout . assign ( predicted_votes = preds_final_2017_model . as_data_frame () . values ) . assign ( predicted_votes_neg_to_0 = lambda df : df . predicted_votes . apply ( lambda x : 0 if x < 0 else x )) . assign ( unscaled_match_votes = lambda df : df . groupby ( 'match_id' ) . predicted_votes_neg_to_0 . transform ( 'sum' )) . assign ( predicted_votes_scaled = lambda df : df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2017 = ( holdout . groupby ([ 'player' , 'team' ], as_index = False ) . agg ({ 'brownlow_votes' : sum , 'predicted_votes_scaled' : sum , 'SC' : 'mean' , 'G' : 'mean' }) . sort_values ( by = 'brownlow_votes' , ascending = False ) . assign ( mae = lambda df : abs ( df . brownlow_votes - df . predicted_votes_scaled )) . reset_index ( drop = True )) glm prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% agg_predictions_2017 . head ( 15 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } player team brownlow_votes predicted_votes_scaled SC G mae 0 D Martin Richmond 36.0 25.007991 0.037862 0.064869 10.992009 1 P Dangerfield Geelong 33.0 25.568804 0.042441 0.070819 7.431196 2 T Mitchell Hawthorn 25.0 19.078639 0.036040 0.016928 5.921361 3 L Franklin Sydney 22.0 12.574692 0.034640 0.149203 9.425308 4 J Kelly GWS 21.0 15.198580 0.034652 0.033772 5.801420 5 R Sloane Adelaide 20.0 15.755535 0.037068 0.034821 4.244465 6 J Kennedy Sydney 20.0 11.065483 0.032014 0.030508 8.934517 7 M Bontempelli Western Bulldogs 19.0 13.836173 0.033233 0.040498 5.163827 8 D Beams Brisbane 17.0 10.073366 0.034848 0.044998 6.926634 9 O Wines Port Adelaide 16.0 10.511818 0.031601 0.021967 5.488182 10 N Fyfe Fremantle 15.0 12.782723 0.033761 0.031680 2.217277 11 S Pendlebury Collingwood 15.0 9.135615 0.033855 0.013660 5.864385 12 B Ebert Port Adelaide 15.0 7.941416 0.032795 0.008431 7.058584 13 L Parker Sydney 15.0 12.431929 0.031366 0.030311 2.568071 14 Z Merrett Essendon 15.0 15.017767 0.033737 0.015362 0.017767 So whilst our model predicted Dangerfield to win, it was pretty damn accurate! Let's find the MAE for the top 100, 50, 25, and 10, and then compare it to 2018's MAE in week, when the Brownlow has been counted. for top_x in [ 10 , 25 , 50 , 100 ]: temp_mae = round ( agg_predictions_2017 . iloc [: top_x ] . mae . mean (), 3 ) print ( f \"The Average Mean Absolute Error for the top { top_x } is { temp_mae } \" ) The Average Mean Absolute Error for the top 10 is 7.033 The Average Mean Absolute Error for the top 25 is 4.962 The Average Mean Absolute Error for the top 50 is 3.744 The Average Mean Absolute Error for the top 100 is 2.696","title":"Modeling The 2017 Brownlow"},{"location":"modelling/brownlowModelTutorial/#modelling-this-years-brownlow","text":"Let's now predict this year's vote count. These predictions will be on the front page of the GitHub. train = features_last_before_train . query ( \"season < 2018\" ) test = features_last_before_train . query ( \"season == 2018\" ) # Scale features scaler = StandardScaler () train_scaled = train . copy () train_scaled [ scale_cols ] = scaler . fit_transform ( train [ scale_cols ]) test_scaled = test . copy () test_scaled [ scale_cols ] = scaler . transform ( test [ scale_cols ]) # Convert categorical columns to categoricals train_h2o = h2o . H2OFrame ( train_scaled ) test_h2o = h2o . H2OFrame ( test_scaled ) for col in [ 'is_captain' , 'kicked_a_bag' , 'team_won' , 'got_30_possies_2_goals' ]: train_h2o [ col ] = train_h2o [ col ] . asfactor () test_h2o [ col ] = test_h2o [ col ] . asfactor () Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% # # Train the model - this part is commented out as we will just load our model from disk # aml = H2OAutoML(max_runtime_secs = 60*30, # balance_classes=True, # seed=42) # aml.train(y='brownlow_votes', x=all_feature_cols, training_frame=train_h2o) # # save the model # model_path = h2o.save_model(model=aml.leader, path=\"models\", force=True) # # Get model id # model_name = aml.leaderboard[0, 'model_id'] # # Rename the model on disk # os.rename(f'models/{model_name}', f'models/brownlow_2018_model_v1') # Load model in aml = h2o . load_model ( 'models/brownlow_2018_model_v1' ) # Predict the 2018 brownlow count preds_final_2018_model = aml . predict ( test_h2o ) # Scale these predictions - change negatives to 0s and scale so each game predicts 6 votes total test = ( test . assign ( predicted_votes = preds_final_2018_model . as_data_frame () . values ) . assign ( predicted_votes_neg_to_0 = lambda df : df . predicted_votes . apply ( lambda x : 0 if x < 0 else x )) . assign ( unscaled_match_votes = lambda df : df . groupby ( 'match_id' ) . predicted_votes_neg_to_0 . transform ( 'sum' )) . assign ( predicted_votes_scaled = lambda df : df . predicted_votes_neg_to_0 / df . unscaled_match_votes * 6 )) # Create an aggregate votes df and show the average SC points and goals scored agg_predictions_2018 = ( test . groupby ([ 'player' , 'team' ], as_index = False ) . agg ({ 'predicted_votes_scaled' : sum , 'match_id' : 'count' }) # shows how many games they played . sort_values ( by = 'predicted_votes_scaled' , ascending = False ) . reset_index ( drop = True )) stackedensemble prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100% # Show the top 25 predictions agg_predictions_2018 . head ( 25 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 36.577208 20 1 M Gawn Melbourne 20.686854 22 2 C Oliver Melbourne 19.705244 20 3 D Martin Richmond 19.674984 19 4 B Grundy Collingwood 19.136407 22 5 J Macrae Western Bulldogs 18.396814 17 6 P Dangerfield Geelong 18.207483 21 7 D Beams Brisbane 17.280375 15 8 A Gaff West Coast 16.217552 18 9 L Neale Fremantle 16.038782 21 10 E Yeo West Coast 15.866418 20 11 J Selwood Geelong 15.749850 18 12 D Heppell Essendon 15.073503 19 13 N Fyfe Fremantle 14.864971 11 14 Z Merrett Essendon 14.597278 18 15 S Sidebottom Collingwood 14.403396 18 16 J Kennedy Sydney 14.297521 16 17 M Crouch Adelaide 13.464205 16 18 G Ablett Geelong 13.403062 15 19 P Cripps Carlton 13.140569 21 20 R Laird Adelaide 13.016825 19 21 L Franklin Sydney 12.562711 13 22 S Coniglio GWS 12.484317 20 23 J Kelly GWS 12.245194 14 24 J Lloyd Sydney 12.019714 20 print ( agg_predictions_2018 . head ( 15 )) player team predicted_votes_scaled match_id 0 T Mitchell Hawthorn 36.577208 20 1 M Gawn Melbourne 20.686854 22 2 C Oliver Melbourne 19.705244 20 3 D Martin Richmond 19.674984 19 4 B Grundy Collingwood 19.136407 22 5 J Macrae Western Bulldogs 18.396814 17 6 P Dangerfield Geelong 18.207483 21 7 D Beams Brisbane 17.280375 15 8 A Gaff West Coast 16.217552 18 9 L Neale Fremantle 16.038782 21 10 E Yeo West Coast 15.866418 20 11 J Selwood Geelong 15.749850 18 12 D Heppell Essendon 15.073503 19 13 N Fyfe Fremantle 14.864971 11 14 Z Merrett Essendon 14.597278 18 Now that we have the top 25, let's also look at the top 3 from each team. agg_predictions_2018 . sort_values ( by = [ 'team' , 'predicted_votes_scaled' ], ascending = [ True , False ]) . groupby ( 'team' ) . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } player team predicted_votes_scaled match_id 17 M Crouch Adelaide 13.464205 16 20 R Laird Adelaide 13.016825 19 42 B Gibbs Adelaide 8.976507 18 7 D Beams Brisbane 17.280375 15 48 D Zorko Brisbane 8.039436 14 58 S Martin Brisbane 6.835975 19 19 P Cripps Carlton 13.140569 21 51 K Simpson Carlton 7.497976 18 91 E Curnow Carlton 3.734706 19 4 B Grundy Collingwood 19.136407 22 15 S Sidebottom Collingwood 14.403396 18 26 A Treloar Collingwood 11.173963 11 12 D Heppell Essendon 15.073503 19 14 Z Merrett Essendon 14.597278 18 61 D Smith Essendon 6.367458 20 9 L Neale Fremantle 16.038782 21 13 N Fyfe Fremantle 14.864971 11 75 D Mundy Fremantle 4.895438 19 22 S Coniglio GWS 12.484317 20 23 J Kelly GWS 12.245194 14 25 C Ward GWS 11.251450 19 6 P Dangerfield Geelong 18.207483 21 11 J Selwood Geelong 15.749850 18 18 G Ablett Geelong 13.403062 15 76 J Witts Gold Coast 4.745802 13 79 J Lyons Gold Coast 4.482626 14 114 D Swallow Gold Coast 2.731594 15 0 T Mitchell Hawthorn 36.577208 20 41 L Breust Hawthorn 9.132324 16 50 J Gunston Hawthorn 7.641945 19 1 M Gawn Melbourne 20.686854 22 2 C Oliver Melbourne 19.705244 20 30 J Hogan Melbourne 10.510504 13 29 S Higgins North Melbourne 10.682736 19 40 B Brown North Melbourne 9.211568 13 43 B Cunnington North Melbourne 8.949073 17 28 O Wines Port Adelaide 10.948001 16 35 R Gray Port Adelaide 10.017038 17 53 J Westhoff Port Adelaide 7.425425 21 3 D Martin Richmond 19.674984 19 36 J Riewoldt Richmond 9.981867 15 57 K Lambert Richmond 6.897522 12 38 S Ross St Kilda 9.756485 17 46 J Steven St Kilda 8.103860 17 90 J Steele St Kilda 3.739189 16 16 J Kennedy Sydney 14.297521 16 21 L Franklin Sydney 12.562711 13 24 J Lloyd Sydney 12.019714 20 8 A Gaff West Coast 16.217552 18 10 E Yeo West Coast 15.866418 20 34 J Redden West Coast 10.200779 16 5 J Macrae Western Bulldogs 18.396814 17 39 M Bontempelli Western Bulldogs 9.317955 16 45 L Hunter Western Bulldogs 8.758631 17 If you're looking for a round by round breakdown, just have a look at the test dataframe. test [[ 'date' , 'round' , 'player' , 'team' , 'opposition' , 'margin' , 'SC' , 'predicted_votes_scaled' ]] . tail ( 25 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date round player team opposition margin SC predicted_votes_scaled 27236 24/08/2018 23 J Laverde Essendon Port Adelaide 22 0.028190 0.098214 27237 24/08/2018 23 Z Merrett Essendon Port Adelaide 22 0.030918 0.316848 27238 24/08/2018 23 D Parish Essendon Port Adelaide 22 0.030615 0.031097 27239 24/08/2018 23 A Saad Essendon Port Adelaide 22 0.026069 0.000000 27240 24/08/2018 23 D Smith Essendon Port Adelaide 22 0.028190 0.714852 27241 24/08/2018 23 D Zaharakis Essendon Port Adelaide 22 0.034859 0.331842 27242 25/08/2018 23 H Ballantyne Fremantle Collingwood -9 0.025773 0.594485 27243 25/08/2018 23 T Duman Fremantle Collingwood -9 0.038205 0.068379 27244 25/08/2018 23 J Hamling Fremantle Collingwood -9 0.024560 0.000000 27245 25/08/2018 23 B Hill Fremantle Collingwood -9 0.031837 0.755081 27246 25/08/2018 23 E Langdon Fremantle Collingwood -9 0.033354 0.420121 27247 25/08/2018 23 D Mundy Fremantle Collingwood -9 0.023954 0.000000 27248 25/08/2018 23 L Neale Fremantle Collingwood -9 0.042450 1.012576 27249 25/08/2018 23 S Switkowski Fremantle Collingwood -9 0.027289 0.000000 27250 25/08/2018 23 T Adams Collingwood Fremantle 9 0.030018 0.207985 27251 25/08/2018 23 M Cox Collingwood Fremantle 9 0.024864 0.017404 27252 25/08/2018 23 J Crisp Collingwood Fremantle 9 0.031534 0.022959 27253 25/08/2018 23 B Grundy Collingwood Fremantle 9 0.045482 1.118902 27254 25/08/2018 23 B Maynard Collingwood Fremantle 9 0.026076 0.000000 27255 25/08/2018 23 B Mihocek Collingwood Fremantle 9 0.025167 0.000000 27256 25/08/2018 23 S Pendlebury Collingwood Fremantle 9 0.029715 0.888746 27257 25/08/2018 23 T Phillips Collingwood Fremantle 9 0.028199 0.009869 27258 25/08/2018 23 S Sidebottom Collingwood Fremantle 9 0.039115 0.880034 27259 25/08/2018 23 B Sier Collingwood Fremantle 9 0.023044 0.002407 27260 25/08/2018 23 J Thomas Collingwood Fremantle 9 0.026986 0.001053 And there we have it! In a single notebook we have made a fairly good Brownlow predictive model. Enjoy.","title":"Modelling This Year's Brownlow"},{"location":"modelling/brownlowModelTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/fasttrackTutorial/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Greyhound form FastTrack tutorial | Building a model from greyhound historic data to place bets on Betfair Workshop Overview This tutorial will walk through how to retrieve historic greyhound form data from FastTrack by accessing their Data Download Centre (DDC). We will then build a simple model on the data to demonstrate how we can then easily start betting on Betfair using the Betfair API. The tutorial will be broken up into four sections: Download historic greyhound data from FastTrack DDC Build a simple machine learning model Retrieve today's race lineups from FastTrack and Betfair API Run model on today's lineups and start betting Requirements You will need a Betfair API app key. If you don't have one please follow the steps outlined on the The Automation Hub You will need your own FastTrack security key. To apply for one, contact the Betfair automation team . This notebook and accompanying files is shared on betfair-downunder 's Github . You can watch our workshop working through this tutorial on YouTube. # Import libraries import betfairlightweight from betfairlightweight import filters from datetime import datetime from datetime import timedelta from dateutil import tz import math import numpy as np import pandas as pd from scipy.stats import zscore from sklearn.linear_model import LogisticRegression import fasttrack as ft 1. Download historic greyhound data from FastTrack Create a FastTrack object Enter in your FastTrack security key. Create a Fastrack object with this key which will also check whether the key is valid. If the key is vaid, a \"Valid Security Key\" message will be printed. The created 'greys' object will allow us to call a bunch of functions that interact with the FastTrack DDC. seckey = \"your_security_key\" greys = ft . Fasttrack ( seckey ) Valid Security Key Find a list of greyhound tracks and FastTrack track codes Call the listTracks function which creates a DataFrame containing all the greyhound tracks, their track codes and their state. track_codes = greys . listTracks () track_codes . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track_name track_code state 0 Albury 223 NSW 1 Armidale 225 NSW 2 Bathurst 226 NSW 3 Broke Hill 227 NSW 4 Bulli 202 NSW Later on in this tutorial, we will be building a greyhound model on QLD tracks only so let's create a list of the QLD FastTrack track codes which will be used later to filter our data downloads for only QLD tracks. tracks_filter = list ( track_codes [ track_codes [ 'state' ] == 'QLD' ][ 'track_code' ]) tracks_filter ['400', '409', '401', '402', '403', '404', '405', '406', '407', '408', '410', '411', '412', '414', '413'] Call the getRaceResults function Call the getRaceResults function which will retrieve race details and historic results for all races between two dates. The function takes in two parameters and one optional third parameter. Two DataFrames are returned, the first contains all the details for each race and the second contains the dog results for each race. getRaceResults(dt_start, dt_end, tracks = None) dt_start : the start date of the results you want to retrieve (str yyyy-mm-dd) dt_end : the end date of the results you want to retrieve (str yyyy-mm-dd) tracks : optional parameter which will restrict the download to only races in this list. If left blank, all tracks will be downloaded (list of str) In this example, we'll retrieve data from 2018-01-01 to 2021-06-15 and restrict the download to our tracks_filter list which contains only the QLD track codes. race_details , dog_results = greys . getRaceResults ( '2018-01-01' , '2021-06-15' , tracks_filter ) Getting meets for each date .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1262/1262 [10:34<00:00, 1.99it/s] Getting historic results details .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2045/2045 [22:22<00:00, 1.52it/s] race_details . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 285107231 1 UBET - DOWNLOAD THE APP 06:24PM 520m Maiden Albion Park 01 Jan 18 1 285107232 2 THIRTY TALKS @ STUD 06:47PM 600m Restricted Win Albion Park 01 Jan 18 2 285107233 3 BOX 1 PHOTOGRAPHY 07:02PM 331m Grade 5 Albion Park 01 Jan 18 3 285107234 4 ASPLEY LEAGUES CLUB 07:26PM 395m Mixed 4/5 Albion Park 01 Jan 18 4 285107235 5 TWITTER @ BRISGREYS 07:52PM 520m Mixed 3/4 Albion Park 01 Jan 18 dog_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 124886323 1 MERLOT HAYZE 2 2 27.5 $5.10 None 3.00 None 32 0 32 5.76 30.46 1260.00 285107231 12979 T Trigg 1 1362060038 2 SPIN THAT WHEEL 1 1 28.4 $2.70F None 3.00 3.14 11 0 11 5.67 30.68 360.00 285107231 160421 C Schmidt 2 1770370034 3 SOMERVILLE 8 8 32.7 $11.70 None 6.25 3.29 23 0 23 5.75 30.91 180.00 285107231 69795 L Green 3 108391387 4 SYFY LEGEND 6 6 30.4 $8.30 None 15.75 9.43 54 0 54 5.81 31.57 0.00 285107231 82013 S Kleinhans 4 2032540059 5 GET MESSI 5 5 34.4 $10.20 None 17.25 1.57 46 0 46 5.80 31.68 0.00 285107231 87148 S Lawrance Here we do some basic data manipulation and cleansing to get variables into format that we can work with. Also adding on a few variables that will be handy down the track. Nothing too special here. race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) race_details [ 'trackdist' ] = race_details [ 'Track' ] + race_details [ 'Distance' ] . astype ( str ) dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x == None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' ))) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results = dog_results . merge ( race_details [[ 'FastTrack_RaceId' , 'Distance' , 'RaceGrade' , 'Track' , 'date_dt' , 'trackdist' ]], how = 'left' , on = 'FastTrack_RaceId' ) dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x in [ '1' , '1=' ] else 0 ) print ( \"Number of races in dataset: \" + str ( dog_results [ 'FastTrack_RaceId' ] . nunique ())) Number of races in dataset: 20760 2. Build a simple machine learning model * NOTE: This model is not profitable. It is provided for educational purposes only. * Construct some simple features We'll start by constructing some simple features. Normally we'd explore the data, but the objective of this tutorial is to demonstrate how to connect to FastTrack and Betfair so we'll skip the exploration step and jump straight to model building to generate some probability outputs. dog_results = dog_results . sort_values ( by = [ 'FastTrack_DogId' , 'date_dt' ]) dog_results = dog_results . set_index ( 'date_dt' ) # Normalise the runtimes for each trackdist so we can compare runs across different track distance combinations. # We are making an unrealistic assumption that a dog that can run a good time on one trackdistance can run a # good time on a different trackdistance dog_results [ 'RunTime_norm' ] = dog_results . groupby ( 'trackdist' )[ 'RunTime' ] . transform ( lambda x : zscore ( x , nan_policy = 'omit' )) # Feature 1 - Total prize money won over the last 365 Days dog_results [ 'Prizemoney_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'Prizemoney' ] . apply ( lambda x : x . rolling ( \"365D\" ) . sum () . shift ( 1 )) dog_results [ 'Prizemoney_365D' ] . fillna ( 0 , inplace = True ) # Feature 2 - Number of runs over the last 365D dog_results [ 'runs_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'win' ] . apply ( lambda x : x . rolling ( \"365D\" ) . count () . shift ( 1 )) dog_results [ 'runs_365D' ] . fillna ( 0 , inplace = True ) # Feature 3 - win % over the last 365D dog_results [ 'wins_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'win' ] . apply ( lambda x : x . rolling ( \"365D\" ) . sum () . shift ( 1 )) dog_results [ 'wins_365D' ] . fillna ( 0 , inplace = True ) dog_results [ 'win%_365D' ] = dog_results [ 'wins_365D' ] / dog_results [ 'runs_365D' ] # Feature 4 - Best runtime over the last 365D dog_results [ 'RunTime_norm_best_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'RunTime_norm' ] . apply ( lambda x : x . rolling ( \"365D\" ) . min () . shift ( 1 )) # Feature 5 - Median runtime over the last 365D dog_results [ 'RunTime_norm_median_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'RunTime_norm' ] . apply ( lambda x : x . rolling ( \"365D\" ) . median () . shift ( 1 )) dog_results . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 ... Track trackdist win RunTime_norm Prizemoney_365D runs_365D wins_365D win%_365D RunTime_norm_best_365D RunTime_norm_median_365D date_dt 2018-04-08 -2143477289 3 SUNBURNT SWAMPY 3 3 31.6 11.2 None 4.75 1.86 ... Albion Park Albion Park331 0 0.856147 0.0 0.0 0.0 NaN NaN NaN 2018-04-15 -2143477289 6 SUNBURNT SWAMPY 4 4 31.1 38.9 None 12.75 0.14 ... Albion Park Albion Park331 0 0.991574 175.0 1.0 0.0 0.0 0.856147 0.856147 2018-04-22 -2143477289 6 SUNBURNT SWAMPY 5 5 30.7 29.1 None 9.50 4.57 ... Albion Park Albion Park331 0 1.194715 175.0 2.0 0.0 0.0 0.856147 0.923861 2018-07-15 -2143477289 3 SUNBURNT SWAMPY 3 3 31.9 38.1 None 10.00 0.00 ... Albion Park Albion Park331 0 0.675578 175.0 3.0 0.0 0.0 0.856147 0.991574 2018-09-02 -2143477289 6 SUNBURNT SWAMPY 2 2 32.8 11.7 None 8.25 3.57 ... Albion Park Albion Park331 0 0.607864 350.0 4.0 0.0 0.0 0.675578 0.923861 2018-09-09 -2143477289 7 SUNBURNT SWAMPY 6 6 32.6 41.0 None 12.75 3.71 ... Albion Park Albion Park331 0 1.262428 350.0 5.0 0.0 0.0 0.607864 0.856147 2018-09-16 -2143477289 4 SUNBURNT SWAMPY 1 1 32.3 18.0 None 1.50 0.43 ... Albion Park Albion Park331 0 -0.385268 350.0 6.0 0.0 0.0 0.607864 0.923861 2018-10-14 -2143477289 5 SUNBURNT SWAMPY 8 8 32.3 5.5 None 11.25 1.29 ... Albion Park Albion Park331 0 1.217286 350.0 7.0 0.0 0.0 -0.385268 0.856147 2018-11-18 -2143477289 7 SUNBURNT SWAMPY 3 3 32.8 21.0 None 9.25 1.71 ... Albion Park Albion Park331 0 1.262428 350.0 8.0 0.0 0.0 -0.385268 0.923861 2019-05-26 -2143477289 4 SUNBURNT SWAMPY 7 7 31.7 71.0 None 11.00 1.86 ... Albion Park Albion Park331 0 0.517579 350.0 9.0 0.0 0.0 -0.385268 0.991574 10 rows \u00d7 31 columns Convert all features into Z-scores within each race so that the features are on a relative basis when fed into the model dog_results = dog_results . sort_values ( by = [ 'date_dt' , 'FastTrack_RaceId' ]) for col in [ 'Prizemoney_365D' , 'runs_365D' , 'win%_365D' , 'RunTime_norm_best_365D' , 'RunTime_norm_median_365D' ]: dog_results [ col + '_Z' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ col ] . transform ( lambda x : zscore ( x , ddof = 1 )) dog_results [ 'runs_365D_Z' ] . fillna ( 0 , inplace = True ) dog_results [ 'win%_365D_Z' ] . fillna ( 0 , inplace = True ) Train the model Next, we'll train our model. To keep things simple, we'll choose a Logistic Regression from the sklearn package. For modelling purposes, we'll only keep data after 2019 as our features use the last 365 days of history so data in 2018 won't capture an entire 365 day period. Also we'll only keep races where each dog has a value for each feature. The last piece of code is to just double check the DataFrame has no null values. dog_results = dog_results . reset_index () dog_results = dog_results . sort_values ( by = [ 'date_dt' , 'FastTrack_RaceId' ]) # Only keep data aFter 2019 model_df = dog_results [ dog_results [ 'date_dt' ] >= '2019-01-01' ] feature_cols = [ 'Prizemoney_365D_Z' , 'runs_365D_Z' , 'win%_365D_Z' , 'RunTime_norm_best_365D_Z' , 'RunTime_norm_median_365D_Z' ] model_df = model_df [[ 'date_dt' , 'FastTrack_RaceId' , 'DogName' , 'win' , 'StartPrice' ] + feature_cols ] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] # checking if any null values model_df . drop ( columns = 'StartPrice' ) . isnull () . values . any () False We will use pre-2021 as our train dataset and post-2021 as our test dataset which gives us an approximate 80/20 split of train to test data. Note that one issue with training our model this way is that we are training each dog result individually and not in conjunction with the other dogs in the race. Therefore the probabilities are not guaranteed to add up to 1. # Split the data into train and test data train_data = model_df [ model_df [ 'date_dt' ] < '2021-01-01' ] . reset_index ( drop = True ) test_data = model_df [ model_df [ 'date_dt' ] >= '2021-01-01' ] . reset_index ( drop = True ) train_x , train_y = train_data [ feature_cols ], train_data [ 'win' ] test_x , test_y = test_data [ feature_cols ], test_data [ 'win' ] logit_model = LogisticRegression () logit_model . fit ( train_x , train_y ) test_data [ 'prob_unscaled' ] = logit_model . predict_proba ( test_x )[:, 1 ] test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_unscaled' ] . sum () FastTrack_RaceId 626218700 0.840901 626218701 0.731972 626218702 0.754034 626218703 0.986967 626218704 0.990238 ... 680757815 1.178215 680757816 0.847067 680757817 1.043633 680757818 0.805511 680757819 0.782609 Name: prob_unscaled, Length: 2491, dtype: float64 To correct for this, we'll apply a scaling factor to the model's raw outputs to force them to sum to 1. A better way to do this would be to use a conditional logistic regression which in the training process would ensure probabilities sum to unity. # Scale the raw model output so they sum to unity test_data [ 'prob_scaled' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_unscaled' ] . apply ( lambda x : x / sum ( x )) test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_scaled' ] . sum () FastTrack_RaceId 626218700 1.0 626218701 1.0 626218702 1.0 626218703 1.0 626218704 1.0 ... 680757815 1.0 680757816 1.0 680757817 1.0 680757818 1.0 680757819 1.0 Name: prob_scaled, Length: 2491, dtype: float64 As a rudimentary check, let's see how many races the model correctly predicts using the highest probability in a given race as our pick. We'll also do the same for the starting price odds as a comparison. The model predicts the winner in 33% of races which is not great given the starting price predicts it in 41.7% of races ... but it will do for our purposes! # Create a boolean column for whether a dog has the higehst model prediction in a race. Do the same for the starting price # as a comparison test_data [ 'model_win_prediction' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_scaled' ] . apply ( lambda x : x == max ( x )) test_data [ 'odds_win_prediction' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'StartPrice' ] . apply ( lambda x : x == min ( x )) print ( \"Model predicts the winner in {:.2%} of races\" . format ( len ( test_data [( test_data [ 'model_win_prediction' ] == True ) & ( test_data [ 'win' ] == 1 )]) / test_data [ 'FastTrack_RaceId' ] . nunique () )) print ( \"Starting Price Odds predicts the winner in {:.2%} of races\" . format ( len ( test_data [( test_data [ 'odds_win_prediction' ] == True ) & ( test_data [ 'win' ] == 1 )]) / test_data [ 'FastTrack_RaceId' ] . nunique () )) Model predicts the winner in 32.96% of races Starting Price Odds predicts the winner in 41.75% of races 3. Retrieve today's race lineups Retrieve today's lineups from FastTrack Now that we have trained our model. We want to get today's races from FastTrack and run the model over it. We have two options from FastTrack: Basic Plus Format: Contains basic information about the dog lineups such as box, best time, trainer, owner, ratings, speed ratings ... Full Plus Format: Contains everything in the basic format with additional information such as previous start information. getBasicFormat(dt, tracks = None) getFullFormat(dt, tracks = None) The calls will return two dataframes, one with the race information and one with the individual dog information. Again, the tracks parameter is optional and if left blank, all tracks will be returned. As we are only after the dog lineups to run our model on, let's just grab the basic format and again only restrict for QLD tracks. qld_races_today , qld_dogs_today = greys . getBasicFormat ( '2021-06-16' , tracks_filter ) qld_races_today . head () Getting meets for each date .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.08it/s] Getting dog lineups .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00, 1.43it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 ... Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips 0 680665206 1 TAB ORIGIN GREYHOUNDS TOMORROW 03:32PM 16 Jun 21 05:32AM 395m Novice Non Penalty $1750 $500 $250 ... None None NNP None \"\" Albion Park 16 Jun 21 None None None 1 680665207 2 TERRY HILL VS BEN HANNANT 03:52PM 16 Jun 21 05:52AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 2 680665208 3 QLD VS NSW TOMORROW @BRISGREYS 04:17PM 16 Jun 21 06:17AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 3 680665209 4 ORIGIN SPRINT TOMORROW NIGHT 04:38PM 16 Jun 21 06:38AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 4 680665210 5 BEN HANNANT?S QLD MAROONS 04:57PM 16 Jun 21 06:57AM 395m Grade 5 Heat $1750 $500 $250 ... None None 5H None \"\" Albion Park 16 Jun 21 None None None 5 rows \u00d7 27 columns Creat a list of the QLD tracks running today which will be used later when we fetch the Betfair data # Qld tracks running today qld_tracks_today = list ( qld_races_today [ 'Track' ] . unique ()) qld_tracks_today ['Albion Park', 'Ipswich'] Retrieve today's lineups from the Betfair API The FastTrack lineups contain all the dogs in a race, including reserves and scratched dogs. As we only want to run our model on final lineups, we'll need to connect to the Betfair API to update our lineups for any scratchings. Let's first login to the Betfair API. Enter in your username, password and API key and create a betfairlightweight object. my_username = \"your_username\" my_password = \"your_password\" my_app_key = \"your_app_key\" trading = betfairlightweight . APIClient ( my_username , my_password , app_key = my_app_key ) trading . login_interactive () <LoginResource> Next, we'll call the list_events operation which will return all the greyhound events in Australia over the next 24 hours. # Create the market filter greyhounds_event_filter = filters . market_filter ( event_type_ids = [ 4339 ], market_countries = [ 'AU' ], market_start_time = { 'to' : ( datetime . utcnow () + timedelta ( days = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) } ) # Get a list of all greyhound events as objects greyhounds_events = trading . betting . list_events ( filter = greyhounds_event_filter ) # Create a DataFrame with all the events by iterating over each event object greyhounds_events_today = pd . DataFrame ({ 'Event Name' : [ event_object . event . name for event_object in greyhounds_events ], 'Event ID' : [ event_object . event . id for event_object in greyhounds_events ], 'Event Venue' : [ event_object . event . venue for event_object in greyhounds_events ], 'Country Code' : [ event_object . event . country_code for event_object in greyhounds_events ], 'Time Zone' : [ event_object . event . time_zone for event_object in greyhounds_events ], 'Open Date' : [ event_object . event . open_date for event_object in greyhounds_events ], 'Market Count' : [ event_object . market_count for event_object in greyhounds_events ] }) greyhounds_events_today . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 Bend (AUS) 16th Jun 30618018 Bendigo AU Australia/Sydney 2021-06-16 01:37:00 36 1 WPrk (AUS) 16th Jun 30618017 Wentworth Park AU Australia/Sydney 2021-06-16 09:05:00 40 2 MBdg (AUS) 16th Jun 30618832 Murray Bridge AU Australia/Adelaide 2021-06-16 01:55:00 36 3 Cran (AUS) 16th Jun 30618160 Cranbourne AU Australia/Sydney 2021-06-16 08:44:00 34 4 Ball (AUS) 16th Jun 30618165 Ballarat AU Australia/Sydney 2021-06-16 08:58:00 60 Next, let's fetch the market ids. As we know the meets we're interested in today, let's restrict the market pull request for only the QLD tracks that are running today. greyhounds_events_today = greyhounds_events_today [ greyhounds_events_today [ 'Event Venue' ] . isin ( qld_tracks_today )] greyhounds_events_today . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 7 Ipsw (AUS) 16th Jun 30618813 Ipswich AU Australia/Queensland 2021-06-16 08:55:00 40 9 APrk (AUS) 16th Jun 30618188 Albion Park AU Australia/Queensland 2021-06-16 05:32:00 27 market_catalogue_filter = filters . market_filter ( event_ids = list ( greyhounds_events_today [ 'Event ID' ]), market_type_codes = [ 'WIN' ] ) market_catalogue = trading . betting . list_market_catalogue ( filter = market_catalogue_filter , max_results = '1000' , sort = 'FIRST_TO_START' , market_projection = [ 'MARKET_START_TIME' , 'MARKET_DESCRIPTION' , 'RUNNER_DESCRIPTION' , 'EVENT' , 'EVENT_TYPE' ] ) win_markets = [] runners = [] for market_object in market_catalogue : # win_markets_df.append({ # 'Event Name': market_object.event.name, # 'Event ID': market_object.event.id, # 'Event Venue': market_object.event_venue, # 'Market Name': market_object.market_name, # 'Market ID': market_object.market_id, # 'Market start time': market_object.market_start_time, # 'Total Matched': market_object.total_matched # }) win_markets . append ({ 'event_name' : market_object . event . name , 'event_id' : market_object . event . id , 'event_venue' : market_object . event . venue , 'market_name' : market_object . market_name , 'market_id' : market_object . market_id , 'market_start_time' : market_object . market_start_time , 'total_matched' : market_object . total_matched }) for runner in market_object . runners : runners . append ({ 'market_id' : market_object . market_id , 'runner_id' : runner . selection_id , 'runner_name' : runner . runner_name }) win_markets_df = pd . DataFrame ( win_markets ) runners_df = pd . DataFrame ( runners ) For matching purposes, we'll need to extract the race number from the market_name. Also let's add another field 'local_start_time' as the market_start_time field is in UTC format. # Extract race number from market name win_markets_df [ 'race_number' ] = win_markets_df [ 'market_name' ] . apply ( lambda x : x [ 1 : 3 ] . strip () if x [ 0 ] == 'R' else None ) # Functions that returns the time from a newly specified timezone given a time and an old timezone def change_timezone ( time , oldtz , newtz ): from_zone = tz . gettz ( oldtz ) to_zone = tz . gettz ( newtz ) newtime = time . replace ( tzinfo = from_zone ) . astimezone ( to_zone ) . replace ( tzinfo = None ) return newtime # Add in a local_start_time variable win_markets_df [ 'local_start_time' ] = win_markets_df [ 'market_start_time' ] . apply ( lambda x : \\ change_timezone ( x , 'UTC' , 'Australia/Sydney' )) win_markets_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_name event_id event_venue market_name market_id market_start_time total_matched race_number local_start_time 0 APrk (AUS) 16th Jun 30618188 Albion Park R1 395m Nvce 1.184472300 2021-06-16 05:32:00 0.0 1 2021-06-16 15:32:00 1 APrk (AUS) 16th Jun 30618188 Albion Park R2 395m Heat 1.184472302 2021-06-16 05:52:00 0.0 2 2021-06-16 15:52:00 2 APrk (AUS) 16th Jun 30618188 Albion Park R3 395m Heat 1.184472304 2021-06-16 06:17:00 0.0 3 2021-06-16 16:17:00 3 APrk (AUS) 16th Jun 30618188 Albion Park R4 395m Heat 1.184472306 2021-06-16 06:38:00 0.0 4 2021-06-16 16:38:00 4 APrk (AUS) 16th Jun 30618188 Albion Park R5 395m Heat 1.184472308 2021-06-16 06:57:00 0.0 5 2021-06-16 16:57:00 To match the dog names from Betfair and FastTrack, we'll also need to remove the rug number from the start of the runner_name in the runners_df DataFrame. # Remove dog number from runner_name runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [( x . find ( \" \" ) + 1 ):] . upper ()) # Merge on the race number and event venue onto runners_df runners_df = runners_df . merge ( win_markets_df [[ 'market_id' , 'event_venue' , 'race_number' ]], how = 'left' , on = 'market_id' ) runners_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_id runner_name event_venue race_number 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 4 1.184472300 37616746 IM ON FIRE Albion Park 1 Merge race lineups from FastTrack and Betfair Before we can merge, we'll need to do some minor formatting changes to the FastTrack names so we can match onto the Betfair names. Betfair excludes all apostrophes and full stops in their naming convention so we'll create a betfair equivalent dog name on the dataset removing these characters. We'll also tag on the race number to the lineups dataset for merging purposes as well. qld_races_today = qld_races_today . rename ( columns = { '@id' : 'FastTrack_RaceId' }) qld_races_today = qld_races_today [[ 'FastTrack_RaceId' , 'Date' , 'Track' , 'RaceNum' , 'RaceName' , 'RaceTime' , 'Distance' , 'RaceGrade' ]] qld_dogs_today = qld_dogs_today . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) qld_dogs_today = qld_dogs_today . merge ( qld_races_today [[ 'FastTrack_RaceId' , 'Track' , 'RaceNum' ]], how = 'left' , on = 'FastTrack_RaceId' ) qld_dogs_today [ 'DogName_bf' ] = qld_dogs_today [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) Now we can merge on the FastTrack and Betfair lineup dataframes by dog name, track and race number. We'll check that all selections have been matched by making sure there are no null dog ids. # Match on the fastTrack dogId to the runners_df runners_df = runners_df . merge ( qld_dogs_today [[ 'DogName_bf' , 'Track' , 'RaceNum' , 'FastTrack_DogId' ]], how = 'left' , left_on = [ 'runner_name' , 'event_venue' , 'race_number' ], right_on = [ 'DogName_bf' , 'Track' , 'RaceNum' ], ) . drop ([ 'DogName_bf' , 'Track' , 'RaceNum' ], axis = 1 ) # Check all betfair selections are matched to a fastTrack dogId by checking if there are any null dogIds runners_df [ 'FastTrack_DogId' ] . isnull () . any () False runners_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_id runner_name event_venue race_number FastTrack_DogId 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452 4. Run model on today's lineups and start betting Create model features for the runners First we have to create the same model features we used in our logistic regression model on the dogs in the runners_df DataFrame. As our features use historic data over the last 365 days, we'll need to filter our historic results dataset (created in step 1) for only the dog ids we are interested in and only over the last 365 days. runners_historicdata = dog_results [ dog_results [ 'FastTrack_DogId' ] . isin ( runners_df [ 'FastTrack_DogId' ])] runners_historicdata = runners_historicdata . sort_values ( by = [ 'FastTrack_DogId' , 'date_dt' ]) runners_historicdata = runners_historicdata [ runners_historicdata [ 'date_dt' ] >= ( datetime . now () - timedelta ( days = 365 ))] Next we create the features. As our trained model requires a non-null value in each of the features, we'll exclude all markets where at least one dog has a null feature. # Create the feature variables over the last 365 days runners_features = runners_historicdata . groupby ( 'FastTrack_DogId' ) . agg ( Prizemoney_365D = ( 'Prizemoney' , 'sum' ), RunTime_norm_best_365D = ( 'RunTime_norm' , 'min' ), RunTime_norm_median_365D = ( 'RunTime_norm' , 'median' ), runs_365D = ( 'FastTrack_RaceId' , 'count' ), wins_365D = ( 'win' , 'sum' ) ) . reset_index () runners_features [ 'win%_365D' ] = runners_features [ 'wins_365D' ] / runners_features [ 'runs_365D' ] runners_features = runners_features . drop ( 'wins_365D' , axis = 1 ) runners_df = runners_df . merge ( runners_features , how = 'left' , on = 'FastTrack_DogId' ) # Only run on races where every dog has non-null features markets_exclude = runners_df [ runners_df . isnull () . any ( axis = 1 )][ 'market_id' ] . drop_duplicates () runners_df = runners_df [ ~ runners_df [ 'market_id' ] . isin ( markets_exclude )] print ( \" {0} markets are excluded\" . format ( str ( len ( markets_exclude )))) # Convert the feature variables into Z-scores for col in [ 'Prizemoney_365D' , 'runs_365D' , 'win%_365D' , 'RunTime_norm_best_365D' , 'RunTime_norm_median_365D' ]: runners_df [ col + '_Z' ] = runners_df . groupby ( 'market_id' )[ col ] . transform ( lambda x : zscore ( x , ddof = 1 )) runners_df [ 'runs_365D_Z' ] . fillna ( 0 , inplace = True ) runners_df [ 'win%_365D_Z' ] . fillna ( 0 , inplace = True ) 6 markets are excluded Attach the model output onto the runners_df DataFrame. We will also scale the probabilities to sum to unity (same as what we did when assessing the trained model outputs in step 2). Let's also add a column for model fair odds which is just the reciprocal of the prob_scaled . We'll also add another column for the minimum back odds we're willing to take assuming we'd only bet off a 10% model overlay. runners_df [ 'prob_unscaled' ] = logit_model . predict_proba ( runners_df [ feature_cols ])[:, 1 ] runners_df [ 'prob_scaled' ] = runners_df . groupby ( 'market_id' )[ 'prob_unscaled' ] . apply ( lambda x : x / sum ( x )) runners_df [ 'model_fairodds' ] = 1 / runners_df [ 'prob_scaled' ] runners_df [ 'min_odds' ] = ( 0.1 + 1 ) / runners_df [ 'prob_scaled' ] runners_df [[ 'market_id' , 'runner_name' , 'event_venue' , 'prob_scaled' , 'model_fairodds' , 'min_odds' ]] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_name event_venue prob_scaled model_fairodds min_odds 0 1.184472300 LITTLE MISS VANE Albion Park 0.056184 17.798518 19.578370 1 1.184472300 MULGOWIE BELLE Albion Park 0.376277 2.657620 2.923382 2 1.184472300 NIGHT CAPERS Albion Park 0.325158 3.075425 3.382967 3 1.184472300 WRONG GIRL HARRY Albion Park 0.152564 6.554620 7.210082 4 1.184472300 IM ON FIRE Albion Park 0.089817 11.133812 12.247193 Now we can start betting! Now we can start betting! For demonstration, we'll only bet on one market, but it's just as easy to set it up to bet on all markets based on your model probabilities. Let's take the first market only and create a separate DataFrame from runners_df with only those runners in that market. market_id = win_markets_df [ 'market_id' ][ 0 ] bet_df = runners_df [ runners_df [ 'market_id' ] == market_id ] . reset_index ( drop = True ) bet_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_id runner_name event_venue race_number FastTrack_DogId Prizemoney_365D RunTime_norm_best_365D RunTime_norm_median_365D runs_365D win%_365D Prizemoney_365D_Z runs_365D_Z win%_365D_Z RunTime_norm_best_365D_Z RunTime_norm_median_365D_Z prob_unscaled prob_scaled model_fairodds min_odds 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 2175.0 -0.049480 0.538154 12.0 0.083333 -0.114708 0.70791 -0.774043 1.632343 0.269920 0.037524 0.056184 17.798518 19.578370 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 1850.0 -1.029058 -0.897651 2.0 0.500000 -0.818108 -0.97759 1.600323 -0.174870 -1.638741 0.251304 0.376277 2.657620 2.923382 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 2500.0 -1.513865 0.231713 5.0 0.200000 0.588691 -0.47194 -0.109221 -1.069288 -0.137442 0.217164 0.325158 3.075425 3.382967 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 1750.0 -1.151977 0.743528 4.0 0.250000 -1.034539 -0.64049 0.175703 -0.401644 0.542930 0.101893 0.152564 6.554620 7.210082 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452 2865.0 -0.926976 1.059778 16.0 0.062500 1.378664 1.38211 -0.892762 0.013458 0.963332 0.059986 0.089817 11.133812 12.247193 One thing we have to ensure is that the odds that we place adhere to the betfair price increments stucture. For example odds of 19.578370 are not valid odds to place a bet on. If we were to try we would get an INVALID_ODDS error. For more information on valid price increments click here . We'll create a function that rounds odds up to the nearest valid price increment and apply this to our min_odds field. def roundUpOdds ( odds ): if odds < 2 : return math . ceil ( odds * 100 ) / 100 elif odds < 3 : return math . ceil ( odds * 50 ) / 50 elif odds < 4 : return math . ceil ( odds * 20 ) / 20 elif odds < 6 : return math . ceil ( odds * 10 ) / 10 elif odds < 10 : return math . ceil ( odds * 5 ) / 5 elif odds < 20 : return math . ceil ( odds * 2 ) / 2 elif odds < 30 : return math . ceil ( odds * 1 ) / 1 elif odds < 50 : return math . ceil ( odds * 0.5 ) / 0.5 elif odds < 100 : return math . ceil ( odds * 0.2 ) / 0.2 elif odds < 1000 : return math . ceil ( odds * 0.1 ) / 0.1 else : return odds bet_df [ 'min_odds' ] = bet_df [ 'min_odds' ] . apply ( lambda x : roundUpOdds ( x )) bet_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_id runner_name event_venue race_number FastTrack_DogId Prizemoney_365D RunTime_norm_best_365D RunTime_norm_median_365D runs_365D win%_365D Prizemoney_365D_Z runs_365D_Z win%_365D_Z RunTime_norm_best_365D_Z RunTime_norm_median_365D_Z prob_unscaled prob_scaled model_fairodds min_odds 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 2175.0 -0.049480 0.538154 12.0 0.083333 -0.114708 0.70791 -0.774043 1.632343 0.269920 0.037524 0.056184 17.798518 20.00 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 1850.0 -1.029058 -0.897651 2.0 0.500000 -0.818108 -0.97759 1.600323 -0.174870 -1.638741 0.251304 0.376277 2.657620 2.94 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 2500.0 -1.513865 0.231713 5.0 0.200000 0.588691 -0.47194 -0.109221 -1.069288 -0.137442 0.217164 0.325158 3.075425 3.40 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 1750.0 -1.151977 0.743528 4.0 0.250000 -1.034539 -0.64049 0.175703 -0.401644 0.542930 0.101893 0.152564 6.554620 7.40 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452 2865.0 -0.926976 1.059778 16.0 0.062500 1.378664 1.38211 -0.892762 0.013458 0.963332 0.059986 0.089817 11.133812 12.50 Now that we have valid minimum odds that we want to bet on for each selection, we'll start betting. The following function will place a standard limit bet on Betfair on the specified market_id and selection_id at the specified size and price. # Create a function to place a bet using betfairlightweight def placeBackBet ( instance , market_id , selection_id , size , price ): order_filter = filters . limit_order ( size = size , price = price , persistence_type = \"LAPSE\" ) instructions_filter = filters . place_instruction ( selection_id = str ( selection_id ), order_type = \"LIMIT\" , side = \"BACK\" , limit_order = order_filter ) order = instance . betting . place_orders ( market_id = market_id , instructions = [ instructions_filter ] ) print ( \"Bet Place on selection {0} is {1} \" . format ( str ( selection_id ), order . __dict__ [ '_data' ][ 'status' ])) return order Now let's loop through the runners in bet_df and place a bet of $5 on each runner at the minimum odds we're willing to take. for selection_id , min_odds in zip ( bet_df [ 'runner_id' ], bet_df [ 'min_odds' ]): placeBackBet ( trading , market_id , selection_id , 5 , min_odds ) Bet Place on selection 36594055 is SUCCESS Bet Place on selection 39860314 is SUCCESS Bet Place on selection 39860315 is SUCCESS Bet Place on selection 38079770 is SUCCESS Bet Place on selection 37616746 is SUCCESS And success! We have downloaded historical greyhound form data from FastTrack, built a simple model, and bet off this model using the Betfair API. Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Greyhound form FastTrack API"},{"location":"modelling/fasttrackTutorial/#greyhound-form-fasttrack-tutorial","text":"| Building a model from greyhound historic data to place bets on Betfair","title":"Greyhound form FastTrack tutorial"},{"location":"modelling/fasttrackTutorial/#workshop","text":"","title":"Workshop"},{"location":"modelling/fasttrackTutorial/#overview","text":"This tutorial will walk through how to retrieve historic greyhound form data from FastTrack by accessing their Data Download Centre (DDC). We will then build a simple model on the data to demonstrate how we can then easily start betting on Betfair using the Betfair API. The tutorial will be broken up into four sections: Download historic greyhound data from FastTrack DDC Build a simple machine learning model Retrieve today's race lineups from FastTrack and Betfair API Run model on today's lineups and start betting","title":"Overview"},{"location":"modelling/fasttrackTutorial/#requirements","text":"You will need a Betfair API app key. If you don't have one please follow the steps outlined on the The Automation Hub You will need your own FastTrack security key. To apply for one, contact the Betfair automation team . This notebook and accompanying files is shared on betfair-downunder 's Github . You can watch our workshop working through this tutorial on YouTube. # Import libraries import betfairlightweight from betfairlightweight import filters from datetime import datetime from datetime import timedelta from dateutil import tz import math import numpy as np import pandas as pd from scipy.stats import zscore from sklearn.linear_model import LogisticRegression import fasttrack as ft","title":"Requirements"},{"location":"modelling/fasttrackTutorial/#1-download-historic-greyhound-data-from-fasttrack","text":"","title":"1. Download historic greyhound data from FastTrack"},{"location":"modelling/fasttrackTutorial/#create-a-fasttrack-object","text":"Enter in your FastTrack security key. Create a Fastrack object with this key which will also check whether the key is valid. If the key is vaid, a \"Valid Security Key\" message will be printed. The created 'greys' object will allow us to call a bunch of functions that interact with the FastTrack DDC. seckey = \"your_security_key\" greys = ft . Fasttrack ( seckey ) Valid Security Key","title":"Create a FastTrack object"},{"location":"modelling/fasttrackTutorial/#find-a-list-of-greyhound-tracks-and-fasttrack-track-codes","text":"Call the listTracks function which creates a DataFrame containing all the greyhound tracks, their track codes and their state. track_codes = greys . listTracks () track_codes . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track_name track_code state 0 Albury 223 NSW 1 Armidale 225 NSW 2 Bathurst 226 NSW 3 Broke Hill 227 NSW 4 Bulli 202 NSW Later on in this tutorial, we will be building a greyhound model on QLD tracks only so let's create a list of the QLD FastTrack track codes which will be used later to filter our data downloads for only QLD tracks. tracks_filter = list ( track_codes [ track_codes [ 'state' ] == 'QLD' ][ 'track_code' ]) tracks_filter ['400', '409', '401', '402', '403', '404', '405', '406', '407', '408', '410', '411', '412', '414', '413']","title":"Find a list of greyhound tracks and FastTrack track codes"},{"location":"modelling/fasttrackTutorial/#call-the-getraceresults-function","text":"Call the getRaceResults function which will retrieve race details and historic results for all races between two dates. The function takes in two parameters and one optional third parameter. Two DataFrames are returned, the first contains all the details for each race and the second contains the dog results for each race. getRaceResults(dt_start, dt_end, tracks = None) dt_start : the start date of the results you want to retrieve (str yyyy-mm-dd) dt_end : the end date of the results you want to retrieve (str yyyy-mm-dd) tracks : optional parameter which will restrict the download to only races in this list. If left blank, all tracks will be downloaded (list of str) In this example, we'll retrieve data from 2018-01-01 to 2021-06-15 and restrict the download to our tracks_filter list which contains only the QLD track codes. race_details , dog_results = greys . getRaceResults ( '2018-01-01' , '2021-06-15' , tracks_filter ) Getting meets for each date .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1262/1262 [10:34<00:00, 1.99it/s] Getting historic results details .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2045/2045 [22:22<00:00, 1.52it/s] race_details . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 285107231 1 UBET - DOWNLOAD THE APP 06:24PM 520m Maiden Albion Park 01 Jan 18 1 285107232 2 THIRTY TALKS @ STUD 06:47PM 600m Restricted Win Albion Park 01 Jan 18 2 285107233 3 BOX 1 PHOTOGRAPHY 07:02PM 331m Grade 5 Albion Park 01 Jan 18 3 285107234 4 ASPLEY LEAGUES CLUB 07:26PM 395m Mixed 4/5 Albion Park 01 Jan 18 4 285107235 5 TWITTER @ BRISGREYS 07:52PM 520m Mixed 3/4 Albion Park 01 Jan 18 dog_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 124886323 1 MERLOT HAYZE 2 2 27.5 $5.10 None 3.00 None 32 0 32 5.76 30.46 1260.00 285107231 12979 T Trigg 1 1362060038 2 SPIN THAT WHEEL 1 1 28.4 $2.70F None 3.00 3.14 11 0 11 5.67 30.68 360.00 285107231 160421 C Schmidt 2 1770370034 3 SOMERVILLE 8 8 32.7 $11.70 None 6.25 3.29 23 0 23 5.75 30.91 180.00 285107231 69795 L Green 3 108391387 4 SYFY LEGEND 6 6 30.4 $8.30 None 15.75 9.43 54 0 54 5.81 31.57 0.00 285107231 82013 S Kleinhans 4 2032540059 5 GET MESSI 5 5 34.4 $10.20 None 17.25 1.57 46 0 46 5.80 31.68 0.00 285107231 87148 S Lawrance Here we do some basic data manipulation and cleansing to get variables into format that we can work with. Also adding on a few variables that will be handy down the track. Nothing too special here. race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) race_details [ 'trackdist' ] = race_details [ 'Track' ] + race_details [ 'Distance' ] . astype ( str ) dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x == None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' ))) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results = dog_results . merge ( race_details [[ 'FastTrack_RaceId' , 'Distance' , 'RaceGrade' , 'Track' , 'date_dt' , 'trackdist' ]], how = 'left' , on = 'FastTrack_RaceId' ) dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x in [ '1' , '1=' ] else 0 ) print ( \"Number of races in dataset: \" + str ( dog_results [ 'FastTrack_RaceId' ] . nunique ())) Number of races in dataset: 20760","title":"Call the getRaceResults function"},{"location":"modelling/fasttrackTutorial/#2-build-a-simple-machine-learning-model","text":"* NOTE: This model is not profitable. It is provided for educational purposes only. *","title":"2. Build a simple machine learning model"},{"location":"modelling/fasttrackTutorial/#construct-some-simple-features","text":"We'll start by constructing some simple features. Normally we'd explore the data, but the objective of this tutorial is to demonstrate how to connect to FastTrack and Betfair so we'll skip the exploration step and jump straight to model building to generate some probability outputs. dog_results = dog_results . sort_values ( by = [ 'FastTrack_DogId' , 'date_dt' ]) dog_results = dog_results . set_index ( 'date_dt' ) # Normalise the runtimes for each trackdist so we can compare runs across different track distance combinations. # We are making an unrealistic assumption that a dog that can run a good time on one trackdistance can run a # good time on a different trackdistance dog_results [ 'RunTime_norm' ] = dog_results . groupby ( 'trackdist' )[ 'RunTime' ] . transform ( lambda x : zscore ( x , nan_policy = 'omit' )) # Feature 1 - Total prize money won over the last 365 Days dog_results [ 'Prizemoney_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'Prizemoney' ] . apply ( lambda x : x . rolling ( \"365D\" ) . sum () . shift ( 1 )) dog_results [ 'Prizemoney_365D' ] . fillna ( 0 , inplace = True ) # Feature 2 - Number of runs over the last 365D dog_results [ 'runs_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'win' ] . apply ( lambda x : x . rolling ( \"365D\" ) . count () . shift ( 1 )) dog_results [ 'runs_365D' ] . fillna ( 0 , inplace = True ) # Feature 3 - win % over the last 365D dog_results [ 'wins_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'win' ] . apply ( lambda x : x . rolling ( \"365D\" ) . sum () . shift ( 1 )) dog_results [ 'wins_365D' ] . fillna ( 0 , inplace = True ) dog_results [ 'win%_365D' ] = dog_results [ 'wins_365D' ] / dog_results [ 'runs_365D' ] # Feature 4 - Best runtime over the last 365D dog_results [ 'RunTime_norm_best_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'RunTime_norm' ] . apply ( lambda x : x . rolling ( \"365D\" ) . min () . shift ( 1 )) # Feature 5 - Median runtime over the last 365D dog_results [ 'RunTime_norm_median_365D' ] = dog_results . groupby ( 'FastTrack_DogId' )[ 'RunTime_norm' ] . apply ( lambda x : x . rolling ( \"365D\" ) . median () . shift ( 1 )) dog_results . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 ... Track trackdist win RunTime_norm Prizemoney_365D runs_365D wins_365D win%_365D RunTime_norm_best_365D RunTime_norm_median_365D date_dt 2018-04-08 -2143477289 3 SUNBURNT SWAMPY 3 3 31.6 11.2 None 4.75 1.86 ... Albion Park Albion Park331 0 0.856147 0.0 0.0 0.0 NaN NaN NaN 2018-04-15 -2143477289 6 SUNBURNT SWAMPY 4 4 31.1 38.9 None 12.75 0.14 ... Albion Park Albion Park331 0 0.991574 175.0 1.0 0.0 0.0 0.856147 0.856147 2018-04-22 -2143477289 6 SUNBURNT SWAMPY 5 5 30.7 29.1 None 9.50 4.57 ... Albion Park Albion Park331 0 1.194715 175.0 2.0 0.0 0.0 0.856147 0.923861 2018-07-15 -2143477289 3 SUNBURNT SWAMPY 3 3 31.9 38.1 None 10.00 0.00 ... Albion Park Albion Park331 0 0.675578 175.0 3.0 0.0 0.0 0.856147 0.991574 2018-09-02 -2143477289 6 SUNBURNT SWAMPY 2 2 32.8 11.7 None 8.25 3.57 ... Albion Park Albion Park331 0 0.607864 350.0 4.0 0.0 0.0 0.675578 0.923861 2018-09-09 -2143477289 7 SUNBURNT SWAMPY 6 6 32.6 41.0 None 12.75 3.71 ... Albion Park Albion Park331 0 1.262428 350.0 5.0 0.0 0.0 0.607864 0.856147 2018-09-16 -2143477289 4 SUNBURNT SWAMPY 1 1 32.3 18.0 None 1.50 0.43 ... Albion Park Albion Park331 0 -0.385268 350.0 6.0 0.0 0.0 0.607864 0.923861 2018-10-14 -2143477289 5 SUNBURNT SWAMPY 8 8 32.3 5.5 None 11.25 1.29 ... Albion Park Albion Park331 0 1.217286 350.0 7.0 0.0 0.0 -0.385268 0.856147 2018-11-18 -2143477289 7 SUNBURNT SWAMPY 3 3 32.8 21.0 None 9.25 1.71 ... Albion Park Albion Park331 0 1.262428 350.0 8.0 0.0 0.0 -0.385268 0.923861 2019-05-26 -2143477289 4 SUNBURNT SWAMPY 7 7 31.7 71.0 None 11.00 1.86 ... Albion Park Albion Park331 0 0.517579 350.0 9.0 0.0 0.0 -0.385268 0.991574 10 rows \u00d7 31 columns Convert all features into Z-scores within each race so that the features are on a relative basis when fed into the model dog_results = dog_results . sort_values ( by = [ 'date_dt' , 'FastTrack_RaceId' ]) for col in [ 'Prizemoney_365D' , 'runs_365D' , 'win%_365D' , 'RunTime_norm_best_365D' , 'RunTime_norm_median_365D' ]: dog_results [ col + '_Z' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ col ] . transform ( lambda x : zscore ( x , ddof = 1 )) dog_results [ 'runs_365D_Z' ] . fillna ( 0 , inplace = True ) dog_results [ 'win%_365D_Z' ] . fillna ( 0 , inplace = True )","title":"Construct some simple features"},{"location":"modelling/fasttrackTutorial/#train-the-model","text":"Next, we'll train our model. To keep things simple, we'll choose a Logistic Regression from the sklearn package. For modelling purposes, we'll only keep data after 2019 as our features use the last 365 days of history so data in 2018 won't capture an entire 365 day period. Also we'll only keep races where each dog has a value for each feature. The last piece of code is to just double check the DataFrame has no null values. dog_results = dog_results . reset_index () dog_results = dog_results . sort_values ( by = [ 'date_dt' , 'FastTrack_RaceId' ]) # Only keep data aFter 2019 model_df = dog_results [ dog_results [ 'date_dt' ] >= '2019-01-01' ] feature_cols = [ 'Prizemoney_365D_Z' , 'runs_365D_Z' , 'win%_365D_Z' , 'RunTime_norm_best_365D_Z' , 'RunTime_norm_median_365D_Z' ] model_df = model_df [[ 'date_dt' , 'FastTrack_RaceId' , 'DogName' , 'win' , 'StartPrice' ] + feature_cols ] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] # checking if any null values model_df . drop ( columns = 'StartPrice' ) . isnull () . values . any () False We will use pre-2021 as our train dataset and post-2021 as our test dataset which gives us an approximate 80/20 split of train to test data. Note that one issue with training our model this way is that we are training each dog result individually and not in conjunction with the other dogs in the race. Therefore the probabilities are not guaranteed to add up to 1. # Split the data into train and test data train_data = model_df [ model_df [ 'date_dt' ] < '2021-01-01' ] . reset_index ( drop = True ) test_data = model_df [ model_df [ 'date_dt' ] >= '2021-01-01' ] . reset_index ( drop = True ) train_x , train_y = train_data [ feature_cols ], train_data [ 'win' ] test_x , test_y = test_data [ feature_cols ], test_data [ 'win' ] logit_model = LogisticRegression () logit_model . fit ( train_x , train_y ) test_data [ 'prob_unscaled' ] = logit_model . predict_proba ( test_x )[:, 1 ] test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_unscaled' ] . sum () FastTrack_RaceId 626218700 0.840901 626218701 0.731972 626218702 0.754034 626218703 0.986967 626218704 0.990238 ... 680757815 1.178215 680757816 0.847067 680757817 1.043633 680757818 0.805511 680757819 0.782609 Name: prob_unscaled, Length: 2491, dtype: float64 To correct for this, we'll apply a scaling factor to the model's raw outputs to force them to sum to 1. A better way to do this would be to use a conditional logistic regression which in the training process would ensure probabilities sum to unity. # Scale the raw model output so they sum to unity test_data [ 'prob_scaled' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_unscaled' ] . apply ( lambda x : x / sum ( x )) test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_scaled' ] . sum () FastTrack_RaceId 626218700 1.0 626218701 1.0 626218702 1.0 626218703 1.0 626218704 1.0 ... 680757815 1.0 680757816 1.0 680757817 1.0 680757818 1.0 680757819 1.0 Name: prob_scaled, Length: 2491, dtype: float64 As a rudimentary check, let's see how many races the model correctly predicts using the highest probability in a given race as our pick. We'll also do the same for the starting price odds as a comparison. The model predicts the winner in 33% of races which is not great given the starting price predicts it in 41.7% of races ... but it will do for our purposes! # Create a boolean column for whether a dog has the higehst model prediction in a race. Do the same for the starting price # as a comparison test_data [ 'model_win_prediction' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_scaled' ] . apply ( lambda x : x == max ( x )) test_data [ 'odds_win_prediction' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'StartPrice' ] . apply ( lambda x : x == min ( x )) print ( \"Model predicts the winner in {:.2%} of races\" . format ( len ( test_data [( test_data [ 'model_win_prediction' ] == True ) & ( test_data [ 'win' ] == 1 )]) / test_data [ 'FastTrack_RaceId' ] . nunique () )) print ( \"Starting Price Odds predicts the winner in {:.2%} of races\" . format ( len ( test_data [( test_data [ 'odds_win_prediction' ] == True ) & ( test_data [ 'win' ] == 1 )]) / test_data [ 'FastTrack_RaceId' ] . nunique () )) Model predicts the winner in 32.96% of races Starting Price Odds predicts the winner in 41.75% of races","title":"Train the model"},{"location":"modelling/fasttrackTutorial/#3-retrieve-todays-race-lineups","text":"","title":"3. Retrieve today's race lineups"},{"location":"modelling/fasttrackTutorial/#retrieve-todays-lineups-from-fasttrack","text":"Now that we have trained our model. We want to get today's races from FastTrack and run the model over it. We have two options from FastTrack: Basic Plus Format: Contains basic information about the dog lineups such as box, best time, trainer, owner, ratings, speed ratings ... Full Plus Format: Contains everything in the basic format with additional information such as previous start information. getBasicFormat(dt, tracks = None) getFullFormat(dt, tracks = None) The calls will return two dataframes, one with the race information and one with the individual dog information. Again, the tracks parameter is optional and if left blank, all tracks will be returned. As we are only after the dog lineups to run our model on, let's just grab the basic format and again only restrict for QLD tracks. qld_races_today , qld_dogs_today = greys . getBasicFormat ( '2021-06-16' , tracks_filter ) qld_races_today . head () Getting meets for each date .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.08it/s] Getting dog lineups .. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00, 1.43it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime RaceTimeDateUTC Distance RaceGrade PrizeMoney1 PrizeMoney2 PrizeMoney3 ... Handicap TAB GradeCode VICGREYS RaceComment Track Date Quali TipsComments_Bet TipsComments_Tips 0 680665206 1 TAB ORIGIN GREYHOUNDS TOMORROW 03:32PM 16 Jun 21 05:32AM 395m Novice Non Penalty $1750 $500 $250 ... None None NNP None \"\" Albion Park 16 Jun 21 None None None 1 680665207 2 TERRY HILL VS BEN HANNANT 03:52PM 16 Jun 21 05:52AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 2 680665208 3 QLD VS NSW TOMORROW @BRISGREYS 04:17PM 16 Jun 21 06:17AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 3 680665209 4 ORIGIN SPRINT TOMORROW NIGHT 04:38PM 16 Jun 21 06:38AM 395m Maiden Heat $1750 $500 $250 ... None None MH None \"\" Albion Park 16 Jun 21 None None None 4 680665210 5 BEN HANNANT?S QLD MAROONS 04:57PM 16 Jun 21 06:57AM 395m Grade 5 Heat $1750 $500 $250 ... None None 5H None \"\" Albion Park 16 Jun 21 None None None 5 rows \u00d7 27 columns Creat a list of the QLD tracks running today which will be used later when we fetch the Betfair data # Qld tracks running today qld_tracks_today = list ( qld_races_today [ 'Track' ] . unique ()) qld_tracks_today ['Albion Park', 'Ipswich']","title":"Retrieve today's lineups from FastTrack"},{"location":"modelling/fasttrackTutorial/#retrieve-todays-lineups-from-the-betfair-api","text":"The FastTrack lineups contain all the dogs in a race, including reserves and scratched dogs. As we only want to run our model on final lineups, we'll need to connect to the Betfair API to update our lineups for any scratchings. Let's first login to the Betfair API. Enter in your username, password and API key and create a betfairlightweight object. my_username = \"your_username\" my_password = \"your_password\" my_app_key = \"your_app_key\" trading = betfairlightweight . APIClient ( my_username , my_password , app_key = my_app_key ) trading . login_interactive () <LoginResource> Next, we'll call the list_events operation which will return all the greyhound events in Australia over the next 24 hours. # Create the market filter greyhounds_event_filter = filters . market_filter ( event_type_ids = [ 4339 ], market_countries = [ 'AU' ], market_start_time = { 'to' : ( datetime . utcnow () + timedelta ( days = 1 )) . strftime ( \"%Y-%m- %d T%TZ\" ) } ) # Get a list of all greyhound events as objects greyhounds_events = trading . betting . list_events ( filter = greyhounds_event_filter ) # Create a DataFrame with all the events by iterating over each event object greyhounds_events_today = pd . DataFrame ({ 'Event Name' : [ event_object . event . name for event_object in greyhounds_events ], 'Event ID' : [ event_object . event . id for event_object in greyhounds_events ], 'Event Venue' : [ event_object . event . venue for event_object in greyhounds_events ], 'Country Code' : [ event_object . event . country_code for event_object in greyhounds_events ], 'Time Zone' : [ event_object . event . time_zone for event_object in greyhounds_events ], 'Open Date' : [ event_object . event . open_date for event_object in greyhounds_events ], 'Market Count' : [ event_object . market_count for event_object in greyhounds_events ] }) greyhounds_events_today . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 0 Bend (AUS) 16th Jun 30618018 Bendigo AU Australia/Sydney 2021-06-16 01:37:00 36 1 WPrk (AUS) 16th Jun 30618017 Wentworth Park AU Australia/Sydney 2021-06-16 09:05:00 40 2 MBdg (AUS) 16th Jun 30618832 Murray Bridge AU Australia/Adelaide 2021-06-16 01:55:00 36 3 Cran (AUS) 16th Jun 30618160 Cranbourne AU Australia/Sydney 2021-06-16 08:44:00 34 4 Ball (AUS) 16th Jun 30618165 Ballarat AU Australia/Sydney 2021-06-16 08:58:00 60 Next, let's fetch the market ids. As we know the meets we're interested in today, let's restrict the market pull request for only the QLD tracks that are running today. greyhounds_events_today = greyhounds_events_today [ greyhounds_events_today [ 'Event Venue' ] . isin ( qld_tracks_today )] greyhounds_events_today . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Event Name Event ID Event Venue Country Code Time Zone Open Date Market Count 7 Ipsw (AUS) 16th Jun 30618813 Ipswich AU Australia/Queensland 2021-06-16 08:55:00 40 9 APrk (AUS) 16th Jun 30618188 Albion Park AU Australia/Queensland 2021-06-16 05:32:00 27 market_catalogue_filter = filters . market_filter ( event_ids = list ( greyhounds_events_today [ 'Event ID' ]), market_type_codes = [ 'WIN' ] ) market_catalogue = trading . betting . list_market_catalogue ( filter = market_catalogue_filter , max_results = '1000' , sort = 'FIRST_TO_START' , market_projection = [ 'MARKET_START_TIME' , 'MARKET_DESCRIPTION' , 'RUNNER_DESCRIPTION' , 'EVENT' , 'EVENT_TYPE' ] ) win_markets = [] runners = [] for market_object in market_catalogue : # win_markets_df.append({ # 'Event Name': market_object.event.name, # 'Event ID': market_object.event.id, # 'Event Venue': market_object.event_venue, # 'Market Name': market_object.market_name, # 'Market ID': market_object.market_id, # 'Market start time': market_object.market_start_time, # 'Total Matched': market_object.total_matched # }) win_markets . append ({ 'event_name' : market_object . event . name , 'event_id' : market_object . event . id , 'event_venue' : market_object . event . venue , 'market_name' : market_object . market_name , 'market_id' : market_object . market_id , 'market_start_time' : market_object . market_start_time , 'total_matched' : market_object . total_matched }) for runner in market_object . runners : runners . append ({ 'market_id' : market_object . market_id , 'runner_id' : runner . selection_id , 'runner_name' : runner . runner_name }) win_markets_df = pd . DataFrame ( win_markets ) runners_df = pd . DataFrame ( runners ) For matching purposes, we'll need to extract the race number from the market_name. Also let's add another field 'local_start_time' as the market_start_time field is in UTC format. # Extract race number from market name win_markets_df [ 'race_number' ] = win_markets_df [ 'market_name' ] . apply ( lambda x : x [ 1 : 3 ] . strip () if x [ 0 ] == 'R' else None ) # Functions that returns the time from a newly specified timezone given a time and an old timezone def change_timezone ( time , oldtz , newtz ): from_zone = tz . gettz ( oldtz ) to_zone = tz . gettz ( newtz ) newtime = time . replace ( tzinfo = from_zone ) . astimezone ( to_zone ) . replace ( tzinfo = None ) return newtime # Add in a local_start_time variable win_markets_df [ 'local_start_time' ] = win_markets_df [ 'market_start_time' ] . apply ( lambda x : \\ change_timezone ( x , 'UTC' , 'Australia/Sydney' )) win_markets_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_name event_id event_venue market_name market_id market_start_time total_matched race_number local_start_time 0 APrk (AUS) 16th Jun 30618188 Albion Park R1 395m Nvce 1.184472300 2021-06-16 05:32:00 0.0 1 2021-06-16 15:32:00 1 APrk (AUS) 16th Jun 30618188 Albion Park R2 395m Heat 1.184472302 2021-06-16 05:52:00 0.0 2 2021-06-16 15:52:00 2 APrk (AUS) 16th Jun 30618188 Albion Park R3 395m Heat 1.184472304 2021-06-16 06:17:00 0.0 3 2021-06-16 16:17:00 3 APrk (AUS) 16th Jun 30618188 Albion Park R4 395m Heat 1.184472306 2021-06-16 06:38:00 0.0 4 2021-06-16 16:38:00 4 APrk (AUS) 16th Jun 30618188 Albion Park R5 395m Heat 1.184472308 2021-06-16 06:57:00 0.0 5 2021-06-16 16:57:00 To match the dog names from Betfair and FastTrack, we'll also need to remove the rug number from the start of the runner_name in the runners_df DataFrame. # Remove dog number from runner_name runners_df [ 'runner_name' ] = runners_df [ 'runner_name' ] . apply ( lambda x : x [( x . find ( \" \" ) + 1 ):] . upper ()) # Merge on the race number and event venue onto runners_df runners_df = runners_df . merge ( win_markets_df [[ 'market_id' , 'event_venue' , 'race_number' ]], how = 'left' , on = 'market_id' ) runners_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_id runner_name event_venue race_number 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 4 1.184472300 37616746 IM ON FIRE Albion Park 1","title":"Retrieve today's lineups from the Betfair API"},{"location":"modelling/fasttrackTutorial/#merge-race-lineups-from-fasttrack-and-betfair","text":"Before we can merge, we'll need to do some minor formatting changes to the FastTrack names so we can match onto the Betfair names. Betfair excludes all apostrophes and full stops in their naming convention so we'll create a betfair equivalent dog name on the dataset removing these characters. We'll also tag on the race number to the lineups dataset for merging purposes as well. qld_races_today = qld_races_today . rename ( columns = { '@id' : 'FastTrack_RaceId' }) qld_races_today = qld_races_today [[ 'FastTrack_RaceId' , 'Date' , 'Track' , 'RaceNum' , 'RaceName' , 'RaceTime' , 'Distance' , 'RaceGrade' ]] qld_dogs_today = qld_dogs_today . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) qld_dogs_today = qld_dogs_today . merge ( qld_races_today [[ 'FastTrack_RaceId' , 'Track' , 'RaceNum' ]], how = 'left' , on = 'FastTrack_RaceId' ) qld_dogs_today [ 'DogName_bf' ] = qld_dogs_today [ 'DogName' ] . apply ( lambda x : x . replace ( \"'\" , \"\" ) . replace ( \".\" , \"\" ) . replace ( \"Res\" , \"\" ) . strip ()) Now we can merge on the FastTrack and Betfair lineup dataframes by dog name, track and race number. We'll check that all selections have been matched by making sure there are no null dog ids. # Match on the fastTrack dogId to the runners_df runners_df = runners_df . merge ( qld_dogs_today [[ 'DogName_bf' , 'Track' , 'RaceNum' , 'FastTrack_DogId' ]], how = 'left' , left_on = [ 'runner_name' , 'event_venue' , 'race_number' ], right_on = [ 'DogName_bf' , 'Track' , 'RaceNum' ], ) . drop ([ 'DogName_bf' , 'Track' , 'RaceNum' ], axis = 1 ) # Check all betfair selections are matched to a fastTrack dogId by checking if there are any null dogIds runners_df [ 'FastTrack_DogId' ] . isnull () . any () False runners_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_id runner_name event_venue race_number FastTrack_DogId 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452","title":"Merge race lineups from FastTrack and Betfair"},{"location":"modelling/fasttrackTutorial/#4-run-model-on-todays-lineups-and-start-betting","text":"","title":"4. Run model on today's lineups and start betting"},{"location":"modelling/fasttrackTutorial/#create-model-features-for-the-runners","text":"First we have to create the same model features we used in our logistic regression model on the dogs in the runners_df DataFrame. As our features use historic data over the last 365 days, we'll need to filter our historic results dataset (created in step 1) for only the dog ids we are interested in and only over the last 365 days. runners_historicdata = dog_results [ dog_results [ 'FastTrack_DogId' ] . isin ( runners_df [ 'FastTrack_DogId' ])] runners_historicdata = runners_historicdata . sort_values ( by = [ 'FastTrack_DogId' , 'date_dt' ]) runners_historicdata = runners_historicdata [ runners_historicdata [ 'date_dt' ] >= ( datetime . now () - timedelta ( days = 365 ))] Next we create the features. As our trained model requires a non-null value in each of the features, we'll exclude all markets where at least one dog has a null feature. # Create the feature variables over the last 365 days runners_features = runners_historicdata . groupby ( 'FastTrack_DogId' ) . agg ( Prizemoney_365D = ( 'Prizemoney' , 'sum' ), RunTime_norm_best_365D = ( 'RunTime_norm' , 'min' ), RunTime_norm_median_365D = ( 'RunTime_norm' , 'median' ), runs_365D = ( 'FastTrack_RaceId' , 'count' ), wins_365D = ( 'win' , 'sum' ) ) . reset_index () runners_features [ 'win%_365D' ] = runners_features [ 'wins_365D' ] / runners_features [ 'runs_365D' ] runners_features = runners_features . drop ( 'wins_365D' , axis = 1 ) runners_df = runners_df . merge ( runners_features , how = 'left' , on = 'FastTrack_DogId' ) # Only run on races where every dog has non-null features markets_exclude = runners_df [ runners_df . isnull () . any ( axis = 1 )][ 'market_id' ] . drop_duplicates () runners_df = runners_df [ ~ runners_df [ 'market_id' ] . isin ( markets_exclude )] print ( \" {0} markets are excluded\" . format ( str ( len ( markets_exclude )))) # Convert the feature variables into Z-scores for col in [ 'Prizemoney_365D' , 'runs_365D' , 'win%_365D' , 'RunTime_norm_best_365D' , 'RunTime_norm_median_365D' ]: runners_df [ col + '_Z' ] = runners_df . groupby ( 'market_id' )[ col ] . transform ( lambda x : zscore ( x , ddof = 1 )) runners_df [ 'runs_365D_Z' ] . fillna ( 0 , inplace = True ) runners_df [ 'win%_365D_Z' ] . fillna ( 0 , inplace = True ) 6 markets are excluded Attach the model output onto the runners_df DataFrame. We will also scale the probabilities to sum to unity (same as what we did when assessing the trained model outputs in step 2). Let's also add a column for model fair odds which is just the reciprocal of the prob_scaled . We'll also add another column for the minimum back odds we're willing to take assuming we'd only bet off a 10% model overlay. runners_df [ 'prob_unscaled' ] = logit_model . predict_proba ( runners_df [ feature_cols ])[:, 1 ] runners_df [ 'prob_scaled' ] = runners_df . groupby ( 'market_id' )[ 'prob_unscaled' ] . apply ( lambda x : x / sum ( x )) runners_df [ 'model_fairodds' ] = 1 / runners_df [ 'prob_scaled' ] runners_df [ 'min_odds' ] = ( 0.1 + 1 ) / runners_df [ 'prob_scaled' ] runners_df [[ 'market_id' , 'runner_name' , 'event_venue' , 'prob_scaled' , 'model_fairodds' , 'min_odds' ]] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_name event_venue prob_scaled model_fairodds min_odds 0 1.184472300 LITTLE MISS VANE Albion Park 0.056184 17.798518 19.578370 1 1.184472300 MULGOWIE BELLE Albion Park 0.376277 2.657620 2.923382 2 1.184472300 NIGHT CAPERS Albion Park 0.325158 3.075425 3.382967 3 1.184472300 WRONG GIRL HARRY Albion Park 0.152564 6.554620 7.210082 4 1.184472300 IM ON FIRE Albion Park 0.089817 11.133812 12.247193","title":"Create model features for the runners"},{"location":"modelling/fasttrackTutorial/#now-we-can-start-betting","text":"Now we can start betting! For demonstration, we'll only bet on one market, but it's just as easy to set it up to bet on all markets based on your model probabilities. Let's take the first market only and create a separate DataFrame from runners_df with only those runners in that market. market_id = win_markets_df [ 'market_id' ][ 0 ] bet_df = runners_df [ runners_df [ 'market_id' ] == market_id ] . reset_index ( drop = True ) bet_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_id runner_name event_venue race_number FastTrack_DogId Prizemoney_365D RunTime_norm_best_365D RunTime_norm_median_365D runs_365D win%_365D Prizemoney_365D_Z runs_365D_Z win%_365D_Z RunTime_norm_best_365D_Z RunTime_norm_median_365D_Z prob_unscaled prob_scaled model_fairodds min_odds 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 2175.0 -0.049480 0.538154 12.0 0.083333 -0.114708 0.70791 -0.774043 1.632343 0.269920 0.037524 0.056184 17.798518 19.578370 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 1850.0 -1.029058 -0.897651 2.0 0.500000 -0.818108 -0.97759 1.600323 -0.174870 -1.638741 0.251304 0.376277 2.657620 2.923382 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 2500.0 -1.513865 0.231713 5.0 0.200000 0.588691 -0.47194 -0.109221 -1.069288 -0.137442 0.217164 0.325158 3.075425 3.382967 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 1750.0 -1.151977 0.743528 4.0 0.250000 -1.034539 -0.64049 0.175703 -0.401644 0.542930 0.101893 0.152564 6.554620 7.210082 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452 2865.0 -0.926976 1.059778 16.0 0.062500 1.378664 1.38211 -0.892762 0.013458 0.963332 0.059986 0.089817 11.133812 12.247193 One thing we have to ensure is that the odds that we place adhere to the betfair price increments stucture. For example odds of 19.578370 are not valid odds to place a bet on. If we were to try we would get an INVALID_ODDS error. For more information on valid price increments click here . We'll create a function that rounds odds up to the nearest valid price increment and apply this to our min_odds field. def roundUpOdds ( odds ): if odds < 2 : return math . ceil ( odds * 100 ) / 100 elif odds < 3 : return math . ceil ( odds * 50 ) / 50 elif odds < 4 : return math . ceil ( odds * 20 ) / 20 elif odds < 6 : return math . ceil ( odds * 10 ) / 10 elif odds < 10 : return math . ceil ( odds * 5 ) / 5 elif odds < 20 : return math . ceil ( odds * 2 ) / 2 elif odds < 30 : return math . ceil ( odds * 1 ) / 1 elif odds < 50 : return math . ceil ( odds * 0.5 ) / 0.5 elif odds < 100 : return math . ceil ( odds * 0.2 ) / 0.2 elif odds < 1000 : return math . ceil ( odds * 0.1 ) / 0.1 else : return odds bet_df [ 'min_odds' ] = bet_df [ 'min_odds' ] . apply ( lambda x : roundUpOdds ( x )) bet_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } market_id runner_id runner_name event_venue race_number FastTrack_DogId Prizemoney_365D RunTime_norm_best_365D RunTime_norm_median_365D runs_365D win%_365D Prizemoney_365D_Z runs_365D_Z win%_365D_Z RunTime_norm_best_365D_Z RunTime_norm_median_365D_Z prob_unscaled prob_scaled model_fairodds min_odds 0 1.184472300 36594055 LITTLE MISS VANE Albion Park 1 434800466 2175.0 -0.049480 0.538154 12.0 0.083333 -0.114708 0.70791 -0.774043 1.632343 0.269920 0.037524 0.056184 17.798518 20.00 1 1.184472300 39860314 MULGOWIE BELLE Albion Park 1 510731455 1850.0 -1.029058 -0.897651 2.0 0.500000 -0.818108 -0.97759 1.600323 -0.174870 -1.638741 0.251304 0.376277 2.657620 2.94 2 1.184472300 39860315 NIGHT CAPERS Albion Park 1 415994834 2500.0 -1.513865 0.231713 5.0 0.200000 0.588691 -0.47194 -0.109221 -1.069288 -0.137442 0.217164 0.325158 3.075425 3.40 3 1.184472300 38079770 WRONG GIRL HARRY Albion Park 1 443645048 1750.0 -1.151977 0.743528 4.0 0.250000 -1.034539 -0.64049 0.175703 -0.401644 0.542930 0.101893 0.152564 6.554620 7.40 4 1.184472300 37616746 IM ON FIRE Albion Park 1 448841452 2865.0 -0.926976 1.059778 16.0 0.062500 1.378664 1.38211 -0.892762 0.013458 0.963332 0.059986 0.089817 11.133812 12.50 Now that we have valid minimum odds that we want to bet on for each selection, we'll start betting. The following function will place a standard limit bet on Betfair on the specified market_id and selection_id at the specified size and price. # Create a function to place a bet using betfairlightweight def placeBackBet ( instance , market_id , selection_id , size , price ): order_filter = filters . limit_order ( size = size , price = price , persistence_type = \"LAPSE\" ) instructions_filter = filters . place_instruction ( selection_id = str ( selection_id ), order_type = \"LIMIT\" , side = \"BACK\" , limit_order = order_filter ) order = instance . betting . place_orders ( market_id = market_id , instructions = [ instructions_filter ] ) print ( \"Bet Place on selection {0} is {1} \" . format ( str ( selection_id ), order . __dict__ [ '_data' ][ 'status' ])) return order Now let's loop through the runners in bet_df and place a bet of $5 on each runner at the minimum odds we're willing to take. for selection_id , min_odds in zip ( bet_df [ 'runner_id' ], bet_df [ 'min_odds' ]): placeBackBet ( trading , market_id , selection_id , 5 , min_odds ) Bet Place on selection 36594055 is SUCCESS Bet Place on selection 39860314 is SUCCESS Bet Place on selection 39860315 is SUCCESS Bet Place on selection 38079770 is SUCCESS Bet Place on selection 37616746 is SUCCESS And success! We have downloaded historical greyhound form data from FastTrack, built a simple model, and bet off this model using the Betfair API.","title":"Now we can start betting!"},{"location":"modelling/fasttrackTutorial/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/greyhoundModellingPython/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Greyhound modelling in Python | Building a Greyhound Racing model with Scikit-learn Logistic Regression and Ensemble Learning Workshop This tutorial was written by Bruno Chauvet and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the Greyhound form Fasttrack tutorial we shared previously. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements! Overview This tutorial will walk you through the different steps required to generate Greyhound racing winning probabilities Download historic greyhound data from FastTrack API Cleanse and normalise the data Generate features using raw data Build and train classification models Evaluate models' performances Evaluate feature importance Requirements You will need a Betfair API app key. If you don't have one please follow the steps outlined on the The Automation Hub You will need your own FastTrack security key. To apply for one, contact the Betfair automation team . This notebook and accompanying files is shared on betfair-downunder 's Github . # Import libraries import os import sys # Allow imports from src folder module_path = os . path . abspath ( os . path . join ( '../src' )) if module_path not in sys . path : sys . path . append ( module_path ) from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from dateutil import tz from pandas.tseries.offsets import MonthEnd from sklearn.preprocessing import MinMaxScaler import itertools import math import numpy as np import pandas as pd import fasttrack as ft from dotenv import load_dotenv load_dotenv () True Note - FastTrack API key If you follow README instructions to run this notebook locally, you should have configured a .env file with your FastTrack API key. Otherwise you can set your API key below. # Validate FastTrack API connection api_key = os . getenv ( 'FAST_TRACK_API_KEY' , '<REPLACE WITH YOUR KEY>' ) client = ft . Fasttrack ( api_key ) track_codes = client . listTracks () Valid Security Key 1. Download historic greyhound data from FastTrack API The cell below downloads FastTrack AU race data for the past few months. Data is cached locally in the data folder so it can easily be reused for further processing. Depending on the amount of data to retrieve, this can take a few hours. # Import race data excluding NZ races au_tracks_filter = list ( track_codes [ track_codes [ 'state' ] != 'NZ' ][ 'track_code' ]) # Time window to import data # First day of the month 46 months back from now date_from = ( datetime . today () - relativedelta ( months = 46 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # First day of previous month date_to = ( datetime . today () - relativedelta ( months = 1 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # Dataframes to populate data with race_details = pd . DataFrame () dog_results = pd . DataFrame () # For each month, either fetch data from API or use local CSV file if we already have downloaded it for start in pd . date_range ( date_from , date_to , freq = 'MS' ): start_date = start . strftime ( \"%Y-%m- %d \" ) end_date = ( start + MonthEnd ( 1 )) . strftime ( \"%Y-%m- %d \" ) try : filename_races = f 'FT_AU_RACES_ { start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { start_date } .csv' filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' print ( f 'Loading data from { start_date } to { end_date } ' ) if os . path . isfile ( filepath_races ): # Load local CSV file month_race_details = pd . read_csv ( filepath_races ) month_dog_results = pd . read_csv ( filepath_dogs ) else : # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( start_date , end_date , au_tracks_filter ) month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) # Combine monthly data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) except : print ( f 'Could not load data from { start_date } to { end_date } ' ) Loading data from 2017-12-01 to 2017-12-31 Loading data from 2018-01-01 to 2018-01-31 Loading data from 2018-02-01 to 2018-02-28 Loading data from 2018-03-01 to 2018-03-31 Loading data from 2018-04-01 to 2018-04-30 Loading data from 2018-05-01 to 2018-05-31 Loading data from 2018-06-01 to 2018-06-30 Loading data from 2018-07-01 to 2018-07-31 Loading data from 2018-08-01 to 2018-08-31 /home/bruno/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) Loading data from 2018-09-01 to 2018-09-30 Loading data from 2018-10-01 to 2018-10-31 Loading data from 2018-11-01 to 2018-11-30 Loading data from 2018-12-01 to 2018-12-31 Loading data from 2019-01-01 to 2019-01-31 Loading data from 2019-02-01 to 2019-02-28 Loading data from 2019-03-01 to 2019-03-31 Loading data from 2019-04-01 to 2019-04-30 Loading data from 2019-05-01 to 2019-05-31 Loading data from 2019-06-01 to 2019-06-30 Loading data from 2019-07-01 to 2019-07-31 Loading data from 2019-08-01 to 2019-08-31 Loading data from 2019-09-01 to 2019-09-30 Loading data from 2019-10-01 to 2019-10-31 Loading data from 2019-11-01 to 2019-11-30 Loading data from 2019-12-01 to 2019-12-31 Loading data from 2020-01-01 to 2020-01-31 Loading data from 2020-02-01 to 2020-02-29 Loading data from 2020-03-01 to 2020-03-31 Loading data from 2020-04-01 to 2020-04-30 Loading data from 2020-05-01 to 2020-05-31 Loading data from 2020-06-01 to 2020-06-30 Loading data from 2020-07-01 to 2020-07-31 Loading data from 2020-08-01 to 2020-08-31 Loading data from 2020-09-01 to 2020-09-30 Loading data from 2020-10-01 to 2020-10-31 Loading data from 2020-11-01 to 2020-11-30 Loading data from 2020-12-01 to 2020-12-31 Loading data from 2021-01-01 to 2021-01-31 Loading data from 2021-02-01 to 2021-02-28 Loading data from 2021-03-01 to 2021-03-31 Loading data from 2021-04-01 to 2021-04-30 Loading data from 2021-05-01 to 2021-05-31 Loading data from 2021-06-01 to 2021-06-30 Loading data from 2021-07-01 to 2021-07-31 /home/bruno/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (10,12) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) Loading data from 2021-08-01 to 2021-08-31 Loading data from 2021-09-01 to 2021-09-30 To better understand the data we retrieved, let's print the first few rows # Race data race_details . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 278896185 1 TRIPLE M BENDIGO 93.5 02:54PM 425m Grade 6 Bendigo 01 Dec 17 1 278896189 2 GOLDEN CITY CONCRETE PUMPING 03:17PM 500m Mixed 6/7 Bendigo 01 Dec 17 2 275589809 3 RAILWAY STATION HOTEL FINAL 03:38PM 500m Mixed 6/7 Final Bendigo 01 Dec 17 3 278896183 4 MCIVOR RD VETERINARY CLINIC 03:58PM 425m Grade 5 Bendigo 01 Dec 17 4 278896179 5 GRV VIC BRED SERIES HT1 04:24PM 425m Grade 5 Heat Bendigo 01 Dec 17 # Individual dogs results dog_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 124886334 1 VANDA MICK 2.0 2 32.0 $2.80F NaN 0.49 NaN S/231 0 NaN 6.79 24.66 NaN 278896185 66993 M Ellis 1 2027130024 2 DYNA ZAD 7.0 7 24.2 $6.60 NaN 0.49 0.49 M/843 4 NaN 6.95 24.69 NaN 278896185 115912 M Delbridge 2 1448760015 3 KLONDIKE GOLD 4.0 4 33.3 $16.60 NaN 1.83 1.34 M/422 0 NaN 6.81 24.79 NaN 278896185 94459 R Hayes 3 1449650024 4 FROSTY TIARA 3.0 3 26.8 $22.00 NaN 2.94 1.11 S/114 0 NaN 6.75 24.86 NaN 278896185 87428 R Morgan 4 118782592 5 GNOCCHI 1.0 1 29.6 $8.60 NaN 6.50 3.56 S/355 0 NaN 6.80 25.11 NaN 278896185 138164 J La Rosa 2. Cleanse and normalise the data Here we do some basic data manipulation and cleansing to get variables into format that we can work with # Clean up the race dataset race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) # Clean up the dogs results dataset dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) # Combine dogs results with race attributes dog_results = dog_results . merge ( race_details [[ 'FastTrack_RaceId' , 'Distance' , 'RaceGrade' , 'Track' , 'date_dt' ]], how = 'left' , on = 'FastTrack_RaceId' ) # Convert StartPrice to probability dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x is None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' )) if isinstance ( x , str ) else x ) dog_results [ 'StartPrice_probability' ] = ( 1 / dog_results [ 'StartPrice' ]) . fillna ( 0 ) dog_results [ 'StartPrice_probability' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x / x . sum ()) # Discard entries without results (scratched or did not finish) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results [ 'Box' ] = dog_results [ 'Box' ] . astype ( int ) # Clean up other attributes dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'SplitMargin' ] = dog_results [ 'SplitMargin' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) . fillna ( 0 ) dog_results [ 'Place' ] = pd . to_numeric ( dog_results [ 'Place' ] . apply ( lambda x : x . replace ( \"=\" , \"\" ) if isinstance ( x , str ) else 0 ), errors = 'coerce' ) . fillna ( 0 ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x == 1 else 0 ) The cell below shows some normalisation techniques - Apply Log base 10 transformation to Prizemoney and Place - Apply inverse transformation to Place - Combine RunTime and Distance to generate Speed value # Normalise some of the raw values dog_results [ 'Prizemoney_norm' ] = np . log10 ( dog_results [ 'Prizemoney' ] + 1 ) / 12 dog_results [ 'Place_inv' ] = ( 1 / dog_results [ 'Place' ]) . fillna ( 0 ) dog_results [ 'Place_log' ] = np . log10 ( dog_results [ 'Place' ] + 1 ) . fillna ( 0 ) dog_results [ 'RunSpeed' ] = ( dog_results [ 'RunTime' ] / dog_results [ 'Distance' ]) . fillna ( 0 ) 3. Generate features using raw data Calculate median winner time by track/distance To compare individual runner times, we extract the median winner time for each Track/Distance and use it as a reference time. # Calculate median winner time per track/distance win_results = dog_results [ dog_results [ 'win' ] == 1 ] median_win_time = pd . DataFrame ( data = win_results [ win_results [ 'RunTime' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'RunTime' ] . median ()) . rename ( columns = { \"RunTime\" : \"RunTime_median\" }) . reset_index () median_win_split_time = pd . DataFrame ( data = win_results [ win_results [ 'SplitMargin' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'SplitMargin' ] . median ()) . rename ( columns = { \"SplitMargin\" : \"SplitMargin_median\" }) . reset_index () median_win_time . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Track Distance RunTime_median 0 Albion Park 331 19.180 1 Albion Park 395 22.860 2 Albion Park 520 30.220 3 Albion Park 600 35.100 4 Albion Park 710 42.005 Calculate Track speed index Some tracks are run faster than other, we calculate here a speed_index using the track reference time over the travelled distance. The lower the speed_index , the faster the track is. We use MinMaxScaler to scale speed_index values between zero and one. # Calculate track speed index median_win_time [ 'speed_index' ] = ( median_win_time [ 'RunTime_median' ] / median_win_time [ 'Distance' ]) median_win_time [ 'speed_index' ] = MinMaxScaler () . fit_transform ( median_win_time [[ 'speed_index' ]]) median_win_time . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Track Distance RunTime_median speed_index 0 Albion Park 331 19.180 0.471787 1 Albion Park 395 22.860 0.460736 2 Albion Park 520 30.220 0.497773 3 Albion Park 600 35.100 0.556644 4 Albion Park 710 42.005 0.657970 Compare individual times with track reference time For each dog result, we compare the runner time with the reference time using the formula (track reference time) / (runner time) and normalise the result. The higher the value, the quicker the dog was. # Compare dogs finish time with median winner time dog_results = dog_results . merge ( median_win_time , on = [ 'Track' , 'Distance' ], how = 'left' ) dog_results = dog_results . merge ( median_win_split_time , on = [ 'Track' , 'Distance' ], how = 'left' ) # Normalise time comparison dog_results [ 'RunTime_norm' ] = ( dog_results [ 'RunTime_median' ] / dog_results [ 'RunTime' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'RunTime_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'RunTime_norm' ]]) dog_results [ 'SplitMargin_norm' ] = ( dog_results [ 'SplitMargin_median' ] / dog_results [ 'SplitMargin' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'SplitMargin_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'SplitMargin_norm' ]]) dog_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 ... win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm 0 124886334 1.0 VANDA MICK 2 2 32.0 2.8 NaN 0.49 NaN ... 1 0.0 1.000000 0.301030 0.058024 24.21 0.321642 6.63 0.408759 0.382180 1 2027130024 2.0 DYNA ZAD 7 7 24.2 6.6 NaN 0.49 0.49 ... 0 0.0 0.500000 0.477121 0.058094 24.21 0.321642 6.63 0.402795 0.269784 2 1448760015 3.0 KLONDIKE GOLD 4 4 33.3 16.6 NaN 1.83 1.34 ... 0 0.0 0.333333 0.602060 0.058329 24.21 0.321642 6.63 0.383017 0.367841 3 1449650024 4.0 FROSTY TIARA 3 3 26.8 22.0 NaN 2.94 1.11 ... 0 0.0 0.250000 0.698970 0.058494 24.21 0.321642 6.63 0.369268 0.411111 4 118782592 5.0 GNOCCHI 1 1 29.6 8.6 NaN 6.50 3.56 ... 0 0.0 0.200000 0.778151 0.059082 24.21 0.321642 6.63 0.320789 0.375000 5 rows \u00d7 34 columns Barrier winning probabilities The barrier dogs start from play a big part in the race so we calculate the winning percentage for each barrier/track/distance # Calculate box winning percentage for each track/distance box_win_percent = pd . DataFrame ( data = dog_results . groupby ([ 'Track' , 'Distance' , 'Box' ])[ 'win' ] . mean ()) . rename ( columns = { \"win\" : \"box_win_percent\" }) . reset_index () # Add to dog results dataframe dog_results = dog_results . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) # Display example of barrier winning probabilities display ( box_win_percent . head ( 8 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Track Distance Box box_win_percent 0 Albion Park 331 1 0.195652 1 Albion Park 331 2 0.153472 2 Albion Park 331 3 0.125446 3 Albion Park 331 4 0.124615 4 Albion Park 331 5 0.116135 5 Albion Park 331 6 0.105144 6 Albion Park 331 7 0.104770 7 Albion Park 331 8 0.115095 Generate time-based features Now that we have a set of basic features for individual dog results, we need to aggregate them into a single feature vector. To do so, we calculate the min , max , mean , median , std of features previously caculated over different time windows 28 , 91 and 365 days: - RunTime_norm - SplitMargin_norm - Place_inv - Place_log - Prizemoney_norm Depending on the dataset size, this can take several minutes # Generate rolling window features dataset = dog_results . copy () dataset = dataset . set_index ([ 'FastTrack_DogId' , 'date_dt' ]) . sort_index () # Use rolling window of 28, 91 and 365 days rolling_windows = [ '28D' , '91D' , '365D' ] # Features to use for rolling windows calculation features = [ 'RunTime_norm' , 'SplitMargin_norm' , 'Place_inv' , 'Place_log' , 'Prizemoney_norm' ] # Aggregation functions to apply aggregates = [ 'min' , 'max' , 'mean' , 'median' , 'std' ] # Keep track of generated feature names feature_cols = [ 'speed_index' , 'box_win_percent' ] for rolling_window in rolling_windows : print ( f 'Processing rolling window { rolling_window } ' ) rolling_result = ( dataset . reset_index ( level = 0 ) . groupby ( 'FastTrack_DogId' )[ features ] . rolling ( rolling_window ) . agg ( aggregates ) . groupby ( level = 0 ) . shift ( 1 ) ) # Generate list of rolling window feature names (eg: RunTime_norm_min_365D) agg_features_cols = [ f ' { f } _ { a } _ { rolling_window } ' for f , a in itertools . product ( features , aggregates )] # Add features to dataset dataset [ agg_features_cols ] = rolling_result # Keep track of generated feature names feature_cols . extend ( agg_features_cols ) Processing rolling window 28D Processing rolling window 91D Processing rolling window 365D # Replace missing values with 0 dataset . fillna ( 0 , inplace = True ) display ( dataset . head ( 8 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR ... Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D FastTrack_DogId date_dt -2143487296 2017-12-14 6.0 JEWELLED COIN 7 7 26.6 13.1 0.0 8.25 0.14 55 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 2017-12-21 4.0 JEWELLED COIN 5 5 26.8 9.7 0.0 13.50 3.00 555 ... 0.845098 0.845098 0.845098 0.845098 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 2017-12-26 3.0 JEWELLED COIN 3 3 27.1 21.5 0.0 6.75 2.29 642 ... 0.698970 0.845098 0.772034 0.772034 0.103328 0.0 0.142298 0.071149 0.071149 0.100620 2017-12-30 7.0 JEWELLED COIN 7 9 26.4 48.1 0.0 21.75 2.29 7777 ... 0.602060 0.845098 0.715376 0.698970 0.122347 0.0 0.204697 0.115665 0.142298 0.104915 2018-01-02 8.0 JEWELLED COIN 5 5 26.8 32.7 0.0 15.75 0.00 888 ... 0.602060 0.903090 0.762305 0.772034 0.137070 0.0 0.204697 0.086749 0.071149 0.103357 2018-01-08 4.0 JEWELLED COIN 1 1 27.2 2.5 0.0 6.50 1.29 5443 ... 0.602060 0.954243 0.800692 0.845098 0.146490 0.0 0.204697 0.069399 0.000000 0.097556 2018-01-10 2.0 JEWELLED COIN 5 5 27.3 8.5 0.0 2.00 2.14 442 ... 0.602060 0.954243 0.783738 0.772034 0.137448 0.0 0.204697 0.080926 0.069282 0.091711 2018-01-17 2.0 JEWELLED COIN 3 3 27.4 7.3 0.0 5.25 5.14 433 ... 0.477121 0.954243 0.739936 0.698970 0.170804 0.0 0.204697 0.098329 0.138563 0.095547 8 rows \u00d7 108 columns As we use up to a year of data to generate our feature set, we exclude the first year of the dataset from our training dataset # Only keep data after 2018-12-01 model_df = dataset . reset_index () feature_cols = np . unique ( feature_cols ) . tolist () model_df = model_df [ model_df [ 'date_dt' ] >= '2018-12-01' ] model_df = model_df [[ 'date_dt' , 'FastTrack_RaceId' , 'DogName' , 'win' , 'StartPrice_probability' ] + feature_cols ] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] 4. Build and train regression models Logistic regression The dataset is split between training and validation: - train dataset: from 2018-12-01 to 2020-12-31 - validation dataset: from 2021-01-01 The next cell trains a LogisticRegression model and uses the win flag ( 0=lose, 1=win ) as a target. from matplotlib import pyplot from matplotlib.pyplot import figure from sklearn.linear_model import LogisticRegression # Split the data into train and test data train_data = model_df [ model_df [ 'date_dt' ] < '2021-01-01' ] . reset_index ( drop = True ) . sample ( frac = 1 ) test_data = model_df [ model_df [ 'date_dt' ] >= '2021-01-01' ] . reset_index ( drop = True ) # Use our previously built features set columns as Training vector # Use win flag as Target vector train_x , train_y = train_data [ feature_cols ], train_data [ 'win' ] test_x , test_y = test_data [ feature_cols ], test_data [ 'win' ] # Build a LogisticRegression model model = LogisticRegression ( verbose = 0 , solver = 'saga' , n_jobs =- 1 ) # Train the model print ( f 'Training on { len ( train_x ) : , } samples with { len ( feature_cols ) } features' ) model . fit ( train_x , train_y ) Training on 630,306 samples with 77 features LogisticRegression(n_jobs=-1, solver='saga') 5. Evaluate model predictions Now that we have trained our model, we can generate predictions on the test dataset # Generate runner win predictions dog_win_probabilities = model . predict_proba ( test_x )[:, 1 ] test_data [ 'prob_LogisticRegression' ] = dog_win_probabilities # Normalise probabilities test_data [ 'prob_LogisticRegression' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x / sum ( x )) Model strike rate Knowing how often a model correctly predicts the winner is one of the most important metrics # Create a boolean column for whether a dog has the higehst model prediction in a race test_dataset_size = test_data [ 'FastTrack_RaceId' ] . nunique () odds_win_prediction = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x == max ( x )) odds_win_prediction_percent = len ( test_data [( odds_win_prediction == True ) & ( test_data [ 'win' ] == 1 )]) / test_dataset_size print ( f \"LogisticRegression strike rate: { odds_win_prediction_percent : .2% } \" ) LogisticRegression strike rate: 32.57% Brier score The Brier score measures the mean squared difference between the predicted probability and the actual outcome. The smaller the Brier score loss, the better. from sklearn.metrics import brier_score_loss brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ 'prob_LogisticRegression' ]) print ( f 'LogisticRegression Brier score: { brier_score : .8 } ' ) LogisticRegression Brier score: 0.11074995 Predictions' distribution To get a better feel of what our models are predicting, we can plot the generated probabilities' distribution and compare them with Start Prices probabilities' distribution. import matplotlib.pyplot as plt import seaborn as sns bins = 100 sns . displot ( data = [ test_data [ 'prob_LogisticRegression' ], test_data [ 'StartPrice_probability' ]], kind = \"hist\" , bins = bins , height = 7 , aspect = 2 ) plt . title ( 'StartPrice vs LogisticRegression probabilities distribution' ) plt . xlabel ( 'Probability' ) plt . show () Probabilities generated by the logistic regression model follow a slightly different distribution. Scikit-learn framework offers various hyper parameters to fine tune a model and achieve better performances. Predictions calibration We want to ensure that probabilities generated by our model match real world probabilities. Calibration curves help us understand if a model needs to be calibrated. from sklearn.calibration import calibration_curve bins = 100 fig = plt . figure ( figsize = ( 12 , 9 )) # Generate calibration curves based on our probabilities cal_y , cal_x = calibration_curve ( test_data [ 'win' ], test_data [ 'prob_LogisticRegression' ], n_bins = bins ) # Plot against reference line plt . plot ( cal_x , cal_y , marker = 'o' , linewidth = 1 ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], '--' , color = 'gray' ) plt . title ( \"LogisticRegression calibration curve\" ); A model is perfectly calibrated if the grouped values (bins) follow the dotted line. Our model generates probabilities that need to be calibrated. To get our model to generate more accurate probabilities, we would need to generate better features, test various modelling approaches and calibrate generated probabilities. Compare other types of classification models The next cell trains different classification models using Scikit-learn unified API: - GradientBoostingClassifier - RandomForestClassifier - LGBMClassifier - XGBClassifier - CatBoostClassifier Depending on dataset size and compute capacity, this can take several minutes from matplotlib import pyplot from matplotlib.pyplot import figure from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestClassifier # Gradient Boosting Machines libraries from lightgbm import LGBMClassifier from xgboost import XGBClassifier from catboost import CatBoostClassifier # Common models parameters verbose = 0 learning_rate = 0.1 n_estimators = 100 # Train different types of models models = { 'LogisticRegression' : LogisticRegression ( verbose = 0 , solver = 'saga' , n_jobs =- 1 ), 'GradientBoostingClassifier' : GradientBoostingClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators , max_depth = 3 , max_features = 0.25 ), 'RandomForestClassifier' : RandomForestClassifier ( verbose = verbose , n_estimators = n_estimators , max_depth = 8 , max_features = 0.5 , n_jobs =- 1 ), 'LGBMClassifier' : LGBMClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators , force_col_wise = True ), 'XGBClassifier' : XGBClassifier ( verbosity = verbose , learning_rate = learning_rate , n_estimators = n_estimators , objective = 'binary:logistic' , use_label_encoder = False ), 'CatBoostClassifier' : CatBoostClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators ) } print ( f 'Training on { len ( train_x ) : , } samples with { len ( feature_cols ) } features' ) for key , model in models . items (): print ( f 'Fitting model { key } ' ) model . fit ( train_x , train_y ) Training on 630,306 samples with 77 features Fitting model LogisticRegression Fitting model GradientBoostingClassifier Fitting model RandomForestClassifier Fitting model LGBMClassifier Fitting model XGBClassifier Fitting model CatBoostClassifier # Calculate probabilities for each model on the test dataset probs_columns = [ 'StartPrice_probability' ] for key , model in models . items (): probs_column_key = f 'prob_ { key } ' # Calculate runner win probability dog_win_probs = model . predict_proba ( test_x )[:, 1 ] test_data [ probs_column_key ] = dog_win_probs # Normalise probabilities test_data [ probs_column_key ] = test_data . groupby ( 'FastTrack_RaceId' )[ f 'prob_ { key } ' ] . apply ( lambda x : x / sum ( x )) probs_columns . append ( probs_column_key ) Calculate models strike rate and Brier score Here we compare the strike rate of the different models' predictions with the start price strike rate. # Create a boolean column for whether a dog has the higehst model prediction in a race. # Do the same for the starting price as a comparison test_dataset_size = test_data [ 'FastTrack_RaceId' ] . nunique () odds_win_prediction = test_data . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x == max ( x )) odds_win_prediction_percent = len ( test_data [( odds_win_prediction == True ) & ( test_data [ 'win' ] == 1 )]) / test_dataset_size brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ 'StartPrice_probability' ]) print ( f 'Starting Price strike rate: { odds_win_prediction_percent : .2% } Brier score: { brier_score : .8 } ' ) for key , model in models . items (): predicted_winners = test_data . groupby ( 'FastTrack_RaceId' )[ f 'prob_ { key } ' ] . apply ( lambda x : x == max ( x )) strike_rate = len ( test_data [( predicted_winners == True ) & ( test_data [ 'win' ] == 1 )]) / test_data [ 'FastTrack_RaceId' ] . nunique () brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ f 'prob_ { key } ' ]) print ( f ' { key . ljust ( 30 ) } strike rate: { strike_rate : .2% } Brier score: { brier_score : .8 } ' ) Starting Price strike rate: 42.24% Brier score: 0.1008106 LogisticRegression strike rate: 32.57% Brier score: 0.11074995 GradientBoostingClassifier strike rate: 33.31% Brier score: 0.1105322 RandomForestClassifier strike rate: 33.24% Brier score: 0.11110442 LGBMClassifier strike rate: 33.40% Brier score: 0.11024272 XGBClassifier strike rate: 33.45% Brier score: 0.11019414 CatBoostClassifier strike rate: 33.33% Brier score: 0.11038785 Visualise models predictions Here we generate some probabilities using our trained models' and compare them with the start price. In blue the lowest prediction and in red the highest prediction generated by the different models. # Display and format sample results def highlight_max ( s , props = '' ): return np . where ( s == np . nanmax ( s . values ), props , '' ) def highlight_min ( s , props = '' ): return np . where ( s == np . nanmin ( s . values ), props , '' ) test_data [ probs_columns ] . sample ( 20 ) . style \\ . bar ( color = '#FFA07A' , vmin = 0.01 , vmax = 0.25 , axis = 1 ) \\ . apply ( highlight_max , props = 'color:red;' , axis = 1 ) \\ . apply ( highlight_min , props = 'color:blue;' , axis = 1 ) #T_f2591_row0_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 65.8%, transparent 65.8%); color: blue; } #T_f2591_row0_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 91.4%, transparent 91.4%); color: red; } #T_f2591_row0_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 84.8%, transparent 84.8%); } #T_f2591_row0_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 78.7%, transparent 78.7%); } #T_f2591_row0_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 84.4%, transparent 84.4%); } #T_f2591_row0_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 79.7%, transparent 79.7%); } #T_f2591_row0_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 86.1%, transparent 86.1%); } #T_f2591_row1_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 37.4%, transparent 37.4%); } #T_f2591_row1_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 35.2%, transparent 35.2%); } #T_f2591_row1_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 49.5%, transparent 49.5%); } #T_f2591_row1_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 61.3%, transparent 61.3%); color: red; } #T_f2591_row1_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 42.2%, transparent 42.2%); } #T_f2591_row1_col5, #T_f2591_row15_col0, #T_f2591_row16_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 22.6%, transparent 22.6%); color: blue; } #T_f2591_row1_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 48.1%, transparent 48.1%); } #T_f2591_row2_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 1.4%, transparent 1.4%); color: blue; } #T_f2591_row2_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 22.4%, transparent 22.4%); } #T_f2591_row2_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 25.1%, transparent 25.1%); } #T_f2591_row2_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 35.9%, transparent 35.9%); color: red; } #T_f2591_row2_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 28.0%, transparent 28.0%); } #T_f2591_row2_col5, #T_f2591_row2_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 29.0%, transparent 29.0%); } #T_f2591_row3_col0, #T_f2591_row5_col0, #T_f2591_row6_col0, #T_f2591_row7_col1, #T_f2591_row9_col0, #T_f2591_row17_col0, #T_f2591_row19_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 100.0%, transparent 100.0%); color: red; } #T_f2591_row3_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 26.8%, transparent 26.8%); } #T_f2591_row3_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 34.7%, transparent 34.7%); } #T_f2591_row3_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 32.4%, transparent 32.4%); } #T_f2591_row3_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 25.4%, transparent 25.4%); color: blue; } #T_f2591_row3_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 27.0%, transparent 27.0%); } #T_f2591_row3_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 34.3%, transparent 34.3%); } #T_f2591_row4_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 6.5%, transparent 6.5%); } #T_f2591_row4_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 9.1%, transparent 9.1%); } #T_f2591_row4_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 7.8%, transparent 7.8%); } #T_f2591_row4_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 11.4%, transparent 11.4%); color: red; } #T_f2591_row4_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 7.6%, transparent 7.6%); } #T_f2591_row4_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 6.4%, transparent 6.4%); } #T_f2591_row4_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 6.3%, transparent 6.3%); color: blue; } #T_f2591_row5_col1, #T_f2591_row17_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 100.0%, transparent 100.0%); color: blue; } #T_f2591_row5_col2, #T_f2591_row5_col3, #T_f2591_row5_col4, #T_f2591_row5_col5, #T_f2591_row5_col6, #T_f2591_row7_col0, #T_f2591_row17_col1, #T_f2591_row17_col2, #T_f2591_row17_col4, #T_f2591_row17_col5, #T_f2591_row17_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 100.0%, transparent 100.0%); } #T_f2591_row6_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 53.0%, transparent 53.0%); } #T_f2591_row6_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.5%, transparent 43.5%); } #T_f2591_row6_col3, #T_f2591_row18_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 42.9%, transparent 42.9%); } #T_f2591_row6_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 34.7%, transparent 34.7%); color: blue; } #T_f2591_row6_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 40.9%, transparent 40.9%); } #T_f2591_row6_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 48.6%, transparent 48.6%); } #T_f2591_row7_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 80.9%, transparent 80.9%); } #T_f2591_row7_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 74.2%, transparent 74.2%); color: blue; } #T_f2591_row7_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 84.0%, transparent 84.0%); } #T_f2591_row7_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 77.8%, transparent 77.8%); } #T_f2591_row7_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 87.2%, transparent 87.2%); } #T_f2591_row8_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 48.3%, transparent 48.3%); color: red; } #T_f2591_row8_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 40.3%, transparent 40.3%); } #T_f2591_row8_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 37.8%, transparent 37.8%); color: blue; } #T_f2591_row8_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.8%, transparent 43.8%); } #T_f2591_row8_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 42.1%, transparent 42.1%); } #T_f2591_row8_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 45.2%, transparent 45.2%); } #T_f2591_row8_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 40.7%, transparent 40.7%); } #T_f2591_row9_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 63.5%, transparent 63.5%); } #T_f2591_row9_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 62.1%, transparent 62.1%); } #T_f2591_row9_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 49.8%, transparent 49.8%); color: blue; } #T_f2591_row9_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 74.4%, transparent 74.4%); } #T_f2591_row9_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 67.6%, transparent 67.6%); } #T_f2591_row9_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 59.7%, transparent 59.7%); } #T_f2591_row10_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 61.7%, transparent 61.7%); color: blue; } #T_f2591_row10_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 91.0%, transparent 91.0%); } #T_f2591_row10_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 93.2%, transparent 93.2%); color: red; } #T_f2591_row10_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 91.1%, transparent 91.1%); } #T_f2591_row10_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 88.0%, transparent 88.0%); } #T_f2591_row10_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 86.4%, transparent 86.4%); } #T_f2591_row10_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 92.0%, transparent 92.0%); } #T_f2591_row11_col0, #T_f2591_row11_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 12.6%, transparent 12.6%); } #T_f2591_row11_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 12.3%, transparent 12.3%); } #T_f2591_row11_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 14.9%, transparent 14.9%); } #T_f2591_row11_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 16.1%, transparent 16.1%); color: red; } #T_f2591_row11_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 12.2%, transparent 12.2%); color: blue; } #T_f2591_row11_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 13.6%, transparent 13.6%); } #T_f2591_row12_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 10.9%, transparent 10.9%); color: blue; } #T_f2591_row12_col1, #T_f2591_row12_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 31.0%, transparent 31.0%); } #T_f2591_row12_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 34.2%, transparent 34.2%); } #T_f2591_row12_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 36.4%, transparent 36.4%); color: red; } #T_f2591_row12_col5, #T_f2591_row13_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 32.9%, transparent 32.9%); } #T_f2591_row12_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 33.2%, transparent 33.2%); } #T_f2591_row13_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 15.1%, transparent 15.1%); color: blue; } #T_f2591_row13_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 33.6%, transparent 33.6%); } #T_f2591_row13_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 41.5%, transparent 41.5%); } #T_f2591_row13_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 41.6%, transparent 41.6%); color: red; } #T_f2591_row13_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 41.4%, transparent 41.4%); } #T_f2591_row13_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 39.9%, transparent 39.9%); } #T_f2591_row14_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 26.7%, transparent 26.7%); } #T_f2591_row14_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 40.1%, transparent 40.1%); color: red; } #T_f2591_row14_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 27.6%, transparent 27.6%); } #T_f2591_row14_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 29.4%, transparent 29.4%); } #T_f2591_row14_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 25.1%, transparent 25.1%); color: blue; } #T_f2591_row14_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 26.1%, transparent 26.1%); } #T_f2591_row14_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 29.2%, transparent 29.2%); } #T_f2591_row15_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 46.1%, transparent 46.1%); } #T_f2591_row15_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 39.5%, transparent 39.5%); } #T_f2591_row15_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 44.9%, transparent 44.9%); } #T_f2591_row15_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 51.8%, transparent 51.8%); } #T_f2591_row15_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 56.5%, transparent 56.5%); color: red; } #T_f2591_row15_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.4%, transparent 43.4%); } #T_f2591_row16_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 56.4%, transparent 56.4%); color: red; } #T_f2591_row16_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 48.3%, transparent 48.3%); } #T_f2591_row16_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 54.8%, transparent 54.8%); } #T_f2591_row16_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 49.7%, transparent 49.7%); } #T_f2591_row16_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 45.8%, transparent 45.8%); } #T_f2591_row16_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 49.4%, transparent 49.4%); } #T_f2591_row18_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 26.3%, transparent 26.3%); color: blue; } #T_f2591_row18_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 46.6%, transparent 46.6%); color: red; } #T_f2591_row18_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.9%, transparent 43.9%); } #T_f2591_row18_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.0%, transparent 43.0%); } #T_f2591_row18_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 44.2%, transparent 44.2%); } #T_f2591_row18_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 36.8%, transparent 36.8%); } #T_f2591_row19_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 75.6%, transparent 75.6%); } #T_f2591_row19_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 69.4%, transparent 69.4%); } #T_f2591_row19_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 63.6%, transparent 63.6%); } #T_f2591_row19_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 74.0%, transparent 74.0%); } #T_f2591_row19_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 71.9%, transparent 71.9%); } #T_f2591_row19_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 60.2%, transparent 60.2%); color: blue; } StartPrice_probability prob_LogisticRegression prob_GradientBoostingClassifier prob_RandomForestClassifier prob_LGBMClassifier prob_XGBClassifier prob_CatBoostClassifier 103796 0.168011 0.229477 0.213527 0.198978 0.212484 0.201237 0.216716 148438 0.099749 0.094426 0.128795 0.157217 0.111212 0.064149 0.125434 47999 0.013440 0.063647 0.070135 0.096175 0.077220 0.079547 0.079643 226804 0.331895 0.074406 0.093237 0.087665 0.070858 0.074815 0.092281 14964 0.025668 0.031759 0.028652 0.037364 0.028186 0.025395 0.025159 139208 0.537243 0.269383 0.304885 0.300394 0.286015 0.301443 0.296410 43027 0.257494 0.137246 0.114401 0.113054 0.093332 0.108154 0.126703 151933 0.254678 0.280246 0.204094 0.188197 0.211623 0.196675 0.219325 75586 0.126036 0.106833 0.100814 0.115142 0.111017 0.118383 0.107643 114240 0.530708 0.162428 0.159064 0.129457 0.188566 0.172335 0.153399 73858 0.158025 0.228463 0.233774 0.228696 0.221210 0.217358 0.230763 174698 0.040139 0.039564 0.045854 0.048556 0.040328 0.039350 0.042728 121778 0.036096 0.084464 0.092160 0.097381 0.084467 0.088923 0.089644 227215 0.046275 0.088997 0.090732 0.109514 0.109888 0.109474 0.105836 10731 0.074040 0.106214 0.076128 0.080542 0.070221 0.072588 0.080146 88484 0.064234 0.120542 0.104910 0.117783 0.134223 0.145592 0.114074 104690 0.064302 0.145388 0.126006 0.141474 0.129206 0.120031 0.128490 201852 0.602269 0.367840 0.394838 0.364498 0.401240 0.387149 0.393707 218537 0.073158 0.121725 0.115405 0.113319 0.116149 0.098413 0.112932 119928 0.310934 0.191449 0.176635 0.162701 0.187676 0.182536 0.154486 We now have built a simple feature set and trained models using various classification techniques. To improve our model's performance, one should build a more advanced feature set and fine tune the model's hyper parameters. 6. Display models' features' importance from sklearn.preprocessing import normalize total_feature_importances = [] # Individual models feature importance for key , model in models . items (): figure ( figsize = ( 10 , 24 ), dpi = 80 ) if isinstance ( model , LogisticRegression ): feature_importance = model . coef_ [ 0 ] else : feature_importance = model . feature_importances_ feature_importance = normalize ( feature_importance [:, np . newaxis ], axis = 0 ) . ravel () total_feature_importances . append ( feature_importance ) pyplot . barh ( feature_cols , feature_importance ) pyplot . xlabel ( f ' { key } Features Importance' ) pyplot . show () # Overall feature importance avg_feature_importances = np . asarray ( total_feature_importances ) . mean ( axis = 0 ) figure ( figsize = ( 10 , 24 ), dpi = 80 ) pyplot . barh ( feature_cols , avg_feature_importances ) pyplot . xlabel ( 'Overall Features Importance' ) pyplot . show () Conclusion This notebook shows an approach to building a simple feature set and training fundamental models using various classification techniques. Analysing these models using metrics such as strike rate and Beyer score indicates that these models have poor performances compared to market starting prices and therefore probably should not be used for betting. To improve your model's performances, the logical next steps would generally be to create new features to add to the dataset, apply various normalisation and standardisation techniques and fine-tune hyper-parameters when training models. This process does take time and effort to get right but is an approach that can be both fun, challenging and sometimes rewarding. Complete Code Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github # Import libraries import os import sys # Allow imports from src folder module_path = os . path . abspath ( os . path . join ( '../src' )) if module_path not in sys . path : sys . path . append ( module_path ) from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from dateutil import tz from pandas.tseries.offsets import MonthEnd from sklearn.preprocessing import MinMaxScaler import itertools import math import numpy as np import pandas as pd import fasttrack as ft from dotenv import load_dotenv load_dotenv () # Validate FastTrack API connection api_key = os . getenv ( 'FAST_TRACK_API_KEY' , '<REPLACE WITH YOUR KEY>' ) client = ft . Fasttrack ( api_key ) track_codes = client . listTracks () # Import race data excluding NZ races au_tracks_filter = list ( track_codes [ track_codes [ 'state' ] != 'NZ' ][ 'track_code' ]) # Time window to import data # First day of the month 46 months back from now date_from = ( datetime . today () - relativedelta ( months = 46 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # First day of previous month date_to = ( datetime . today () - relativedelta ( months = 1 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # Dataframes to populate data with race_details = pd . DataFrame () dog_results = pd . DataFrame () # For each month, either fetch data from API or use local CSV file if we already have downloaded it for start in pd . date_range ( date_from , date_to , freq = 'MS' ): start_date = start . strftime ( \"%Y-%m- %d \" ) end_date = ( start + MonthEnd ( 1 )) . strftime ( \"%Y-%m- %d \" ) try : filename_races = f 'FT_AU_RACES_ { start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { start_date } .csv' filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' print ( f 'Loading data from { start_date } to { end_date } ' ) if os . path . isfile ( filepath_races ): # Load local CSV file month_race_details = pd . read_csv ( filepath_races ) month_dog_results = pd . read_csv ( filepath_dogs ) else : # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( start_date , end_date , au_tracks_filter ) month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) # Combine monthly data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) except : print ( f 'Could not load data from { start_date } to { end_date } ' ) ## Cleanse and normalise the data # Clean up the race dataset race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) # Clean up the dogs results dataset dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) # Combine dogs results with race attributes dog_results = dog_results . merge ( race_details [[ 'FastTrack_RaceId' , 'Distance' , 'RaceGrade' , 'Track' , 'date_dt' ]], how = 'left' , on = 'FastTrack_RaceId' ) # Convert StartPrice to probability dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x is None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' )) if isinstance ( x , str ) else x ) dog_results [ 'StartPrice_probability' ] = ( 1 / dog_results [ 'StartPrice' ]) . fillna ( 0 ) dog_results [ 'StartPrice_probability' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x / x . sum ()) # Discard entries without results (scratched or did not finish) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results [ 'Box' ] = dog_results [ 'Box' ] . astype ( int ) # Clean up other attributes dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'SplitMargin' ] = dog_results [ 'SplitMargin' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) . fillna ( 0 ) dog_results [ 'Place' ] = pd . to_numeric ( dog_results [ 'Place' ] . apply ( lambda x : x . replace ( \"=\" , \"\" ) if isinstance ( x , str ) else 0 ), errors = 'coerce' ) . fillna ( 0 ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x == 1 else 0 ) # Normalise some of the raw values dog_results [ 'Prizemoney_norm' ] = np . log10 ( dog_results [ 'Prizemoney' ] + 1 ) / 12 dog_results [ 'Place_inv' ] = ( 1 / dog_results [ 'Place' ]) . fillna ( 0 ) dog_results [ 'Place_log' ] = np . log10 ( dog_results [ 'Place' ] + 1 ) . fillna ( 0 ) dog_results [ 'RunSpeed' ] = ( dog_results [ 'RunTime' ] / dog_results [ 'Distance' ]) . fillna ( 0 ) ## Generate features using raw data # Calculate median winner time per track/distance win_results = dog_results [ dog_results [ 'win' ] == 1 ] median_win_time = pd . DataFrame ( data = win_results [ win_results [ 'RunTime' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'RunTime' ] . median ()) . rename ( columns = { \"RunTime\" : \"RunTime_median\" }) . reset_index () median_win_split_time = pd . DataFrame ( data = win_results [ win_results [ 'SplitMargin' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'SplitMargin' ] . median ()) . rename ( columns = { \"SplitMargin\" : \"SplitMargin_median\" }) . reset_index () median_win_time . head () # Calculate track speed index median_win_time [ 'speed_index' ] = ( median_win_time [ 'RunTime_median' ] / median_win_time [ 'Distance' ]) median_win_time [ 'speed_index' ] = MinMaxScaler () . fit_transform ( median_win_time [[ 'speed_index' ]]) median_win_time . head () # Compare dogs finish time with median winner time dog_results = dog_results . merge ( median_win_time , on = [ 'Track' , 'Distance' ], how = 'left' ) dog_results = dog_results . merge ( median_win_split_time , on = [ 'Track' , 'Distance' ], how = 'left' ) # Normalise time comparison dog_results [ 'RunTime_norm' ] = ( dog_results [ 'RunTime_median' ] / dog_results [ 'RunTime' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'RunTime_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'RunTime_norm' ]]) dog_results [ 'SplitMargin_norm' ] = ( dog_results [ 'SplitMargin_median' ] / dog_results [ 'SplitMargin' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'SplitMargin_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'SplitMargin_norm' ]]) dog_results . head () # Calculate box winning percentage for each track/distance box_win_percent = pd . DataFrame ( data = dog_results . groupby ([ 'Track' , 'Distance' , 'Box' ])[ 'win' ] . mean ()) . rename ( columns = { \"win\" : \"box_win_percent\" }) . reset_index () # Add to dog results dataframe dog_results = dog_results . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) # Display example of barrier winning probabilities display ( box_win_percent . head ( 8 )) # Generate rolling window features dataset = dog_results . copy () dataset = dataset . set_index ([ 'FastTrack_DogId' , 'date_dt' ]) . sort_index () # Use rolling window of 28, 91 and 365 days rolling_windows = [ '28D' , '91D' , '365D' ] # Features to use for rolling windows calculation features = [ 'RunTime_norm' , 'SplitMargin_norm' , 'Place_inv' , 'Place_log' , 'Prizemoney_norm' ] # Aggregation functions to apply aggregates = [ 'min' , 'max' , 'mean' , 'median' , 'std' ] # Keep track of generated feature names feature_cols = [ 'speed_index' , 'box_win_percent' ] for rolling_window in rolling_windows : print ( f 'Processing rolling window { rolling_window } ' ) rolling_result = ( dataset . reset_index ( level = 0 ) . groupby ( 'FastTrack_DogId' )[ features ] . rolling ( rolling_window ) . agg ( aggregates ) . shift ( 1 ) ) # Generate list of rolling window feature names (eg: RunTime_norm_min_365D) agg_features_cols = [ f ' { f } _ { a } _ { rolling_window } ' for f , a in itertools . product ( features , aggregates )] # Add features to dataset dataset [ agg_features_cols ] = rolling_result # Keep track of generated feature names feature_cols . extend ( agg_features_cols ) # Replace missing values with 0 dataset . fillna ( 0 , inplace = True ) display ( dataset . head ( 8 )) # Only keep data after 2018-12-01 model_df = dataset . reset_index () feature_cols = np . unique ( feature_cols ) . tolist () model_df = model_df [ model_df [ 'date_dt' ] >= '2018-12-01' ] model_df = model_df [[ 'date_dt' , 'FastTrack_RaceId' , 'DogName' , 'win' , 'StartPrice_probability' ] + feature_cols ] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] ## Build and train Regression models from matplotlib import pyplot from matplotlib.pyplot import figure from sklearn.linear_model import LogisticRegression # Split the data into train and test data train_data = model_df [ model_df [ 'date_dt' ] < '2021-01-01' ] . reset_index ( drop = True ) . sample ( frac = 1 ) test_data = model_df [ model_df [ 'date_dt' ] >= '2021-01-01' ] . reset_index ( drop = True ) # Use our previously built features set columns as Training vector # Use win flag as Target vector train_x , train_y = train_data [ feature_cols ], train_data [ 'win' ] test_x , test_y = test_data [ feature_cols ], test_data [ 'win' ] # Build a LogisticRegression model model = LogisticRegression ( verbose = 0 , solver = 'saga' , n_jobs =- 1 ) # Train the model print ( f 'Training on { len ( train_x ) : , } samples with { len ( feature_cols ) } features' ) model . fit ( train_x , train_y ) # Generate runner win predictions dog_win_probabilities = model . predict_proba ( test_x )[:, 1 ] test_data [ 'prob_LogisticRegression' ] = dog_win_probabilities # Normalise probabilities test_data [ 'prob_LogisticRegression' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x / sum ( x )) # Create a boolean column for whether a dog has the higehst model prediction in a race test_dataset_size = test_data [ 'FastTrack_RaceId' ] . nunique () odds_win_prediction = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x == max ( x )) odds_win_prediction_percent = len ( test_data [( odds_win_prediction == True ) & ( test_data [ 'win' ] == 1 )]) / test_dataset_size print ( f \"LogisticRegression strike rate: { odds_win_prediction_percent : .2% } \" ) from sklearn.metrics import brier_score_loss brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ 'prob_LogisticRegression' ]) print ( f 'LogisticRegression Brier score: { brier_score : .8 } ' ) # Predictions distribution import matplotlib.pyplot as plt import seaborn as sns bins = 100 sns . displot ( data = [ test_data [ 'prob_LogisticRegression' ], test_data [ 'StartPrice_probability' ]], kind = \"hist\" , bins = bins , height = 7 , aspect = 2 ) plt . title ( 'StartPrice vs LogisticRegression probabilities distribution' ) plt . xlabel ( 'Probability' ) plt . show () # Predictions calibration from sklearn.calibration import calibration_curve bins = 100 fig = plt . figure ( figsize = ( 12 , 9 )) # Generate calibration curves based on our probabilities cal_y , cal_x = calibration_curve ( test_data [ 'win' ], test_data [ 'prob_LogisticRegression' ], n_bins = bins ) # Plot against reference line plt . plot ( cal_x , cal_y , marker = 'o' , linewidth = 1 ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], '--' , color = 'gray' ) plt . title ( \"LogisticRegression calibration curve\" ); # Other classification models from matplotlib import pyplot from matplotlib.pyplot import figure from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestClassifier # Gradient Boosting Machines libraries from lightgbm import LGBMClassifier from xgboost import XGBClassifier from catboost import CatBoostClassifier # Common models parameters verbose = 0 learning_rate = 0.1 n_estimators = 100 # Train different types of models models = { 'LogisticRegression' : LogisticRegression ( verbose = 0 , solver = 'saga' , n_jobs =- 1 ), 'GradientBoostingClassifier' : GradientBoostingClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators , max_depth = 3 , max_features = 0.25 ), 'RandomForestClassifier' : RandomForestClassifier ( verbose = verbose , n_estimators = n_estimators , max_depth = 8 , max_features = 0.5 , n_jobs =- 1 ), 'LGBMClassifier' : LGBMClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators , force_col_wise = True ), 'XGBClassifier' : XGBClassifier ( verbosity = verbose , learning_rate = learning_rate , n_estimators = n_estimators , objective = 'binary:logistic' , use_label_encoder = False ), 'CatBoostClassifier' : CatBoostClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators ) } print ( f 'Training on { len ( train_x ) : , } samples with { len ( feature_cols ) } features' ) for key , model in models . items (): print ( f 'Fitting model { key } ' ) model . fit ( train_x , train_y ) # Calculate probabilities for each model on the test dataset probs_columns = [ 'StartPrice_probability' ] for key , model in models . items (): probs_column_key = f 'prob_ { key } ' # Calculate runner win probability dog_win_probs = model . predict_proba ( test_x )[:, 1 ] test_data [ probs_column_key ] = dog_win_probs # Normalise probabilities test_data [ probs_column_key ] = test_data . groupby ( 'FastTrack_RaceId' )[ f 'prob_ { key } ' ] . apply ( lambda x : x / sum ( x )) probs_columns . append ( probs_column_key ) # Calculate model strike rate and Brier score across models # Create a boolean column for whether a dog has the higehst model prediction in a race. # Do the same for the starting price as a comparison test_dataset_size = test_data [ 'FastTrack_RaceId' ] . nunique () odds_win_prediction = test_data . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x == max ( x )) odds_win_prediction_percent = len ( test_data [( odds_win_prediction == True ) & ( test_data [ 'win' ] == 1 )]) / test_dataset_size brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ 'StartPrice_probability' ]) print ( f 'Starting Price strike rate: { odds_win_prediction_percent : .2% } Brier score: { brier_score : .8 } ' ) for key , model in models . items (): predicted_winners = test_data . groupby ( 'FastTrack_RaceId' )[ f 'prob_ { key } ' ] . apply ( lambda x : x == max ( x )) strike_rate = len ( test_data [( predicted_winners == True ) & ( test_data [ 'win' ] == 1 )]) / test_data [ 'FastTrack_RaceId' ] . nunique () brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ f 'prob_ { key } ' ]) print ( f ' { key . ljust ( 30 ) } strike rate: { strike_rate : .2% } Brier score: { brier_score : .8 } ' ) # Visualise model predictions # Display and format sample results def highlight_max ( s , props = '' ): return np . where ( s == np . nanmax ( s . values ), props , '' ) def highlight_min ( s , props = '' ): return np . where ( s == np . nanmin ( s . values ), props , '' ) test_data [ probs_columns ] . sample ( 20 ) . style \\ . bar ( color = '#FFA07A' , vmin = 0.01 , vmax = 0.25 , axis = 1 ) \\ . apply ( highlight_max , props = 'color:red;' , axis = 1 ) \\ . apply ( highlight_min , props = 'color:blue;' , axis = 1 ) ## Display models feature importance from sklearn.preprocessing import normalize total_feature_importances = [] # Individual models feature importance for key , model in models . items (): figure ( figsize = ( 10 , 24 ), dpi = 80 ) if isinstance ( model , LogisticRegression ): feature_importance = model . coef_ [ 0 ] else : feature_importance = model . feature_importances_ feature_importance = normalize ( feature_importance [:, np . newaxis ], axis = 0 ) . ravel () total_feature_importances . append ( feature_importance ) pyplot . barh ( feature_cols , feature_importance ) pyplot . xlabel ( f ' { key } Features Importance' ) pyplot . show () # Overall feature importance avg_feature_importances = np . asarray ( total_feature_importances ) . mean ( axis = 0 ) figure ( figsize = ( 10 , 24 ), dpi = 80 ) pyplot . barh ( feature_cols , avg_feature_importances ) pyplot . xlabel ( 'Overall Features Importance' ) pyplot . show () Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Greyhound modelling in Python"},{"location":"modelling/greyhoundModellingPython/#greyhound-modelling-in-python","text":"| Building a Greyhound Racing model with Scikit-learn Logistic Regression and Ensemble Learning","title":"Greyhound modelling in Python"},{"location":"modelling/greyhoundModellingPython/#workshop","text":"This tutorial was written by Bruno Chauvet and was originally published on Github . It is shared here with his permission. This tutorial follows on logically from the Greyhound form Fasttrack tutorial we shared previously. As always please reach out with feedback, suggestions or queries, or feel free to submit a pull request if you catch some bugs or have other improvements!","title":"Workshop"},{"location":"modelling/greyhoundModellingPython/#overview","text":"This tutorial will walk you through the different steps required to generate Greyhound racing winning probabilities Download historic greyhound data from FastTrack API Cleanse and normalise the data Generate features using raw data Build and train classification models Evaluate models' performances Evaluate feature importance","title":"Overview"},{"location":"modelling/greyhoundModellingPython/#requirements","text":"You will need a Betfair API app key. If you don't have one please follow the steps outlined on the The Automation Hub You will need your own FastTrack security key. To apply for one, contact the Betfair automation team . This notebook and accompanying files is shared on betfair-downunder 's Github . # Import libraries import os import sys # Allow imports from src folder module_path = os . path . abspath ( os . path . join ( '../src' )) if module_path not in sys . path : sys . path . append ( module_path ) from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from dateutil import tz from pandas.tseries.offsets import MonthEnd from sklearn.preprocessing import MinMaxScaler import itertools import math import numpy as np import pandas as pd import fasttrack as ft from dotenv import load_dotenv load_dotenv () True","title":"Requirements"},{"location":"modelling/greyhoundModellingPython/#note-fasttrack-api-key","text":"If you follow README instructions to run this notebook locally, you should have configured a .env file with your FastTrack API key. Otherwise you can set your API key below. # Validate FastTrack API connection api_key = os . getenv ( 'FAST_TRACK_API_KEY' , '<REPLACE WITH YOUR KEY>' ) client = ft . Fasttrack ( api_key ) track_codes = client . listTracks () Valid Security Key","title":"Note - FastTrack API key"},{"location":"modelling/greyhoundModellingPython/#1-download-historic-greyhound-data-from-fasttrack-api","text":"The cell below downloads FastTrack AU race data for the past few months. Data is cached locally in the data folder so it can easily be reused for further processing. Depending on the amount of data to retrieve, this can take a few hours. # Import race data excluding NZ races au_tracks_filter = list ( track_codes [ track_codes [ 'state' ] != 'NZ' ][ 'track_code' ]) # Time window to import data # First day of the month 46 months back from now date_from = ( datetime . today () - relativedelta ( months = 46 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # First day of previous month date_to = ( datetime . today () - relativedelta ( months = 1 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # Dataframes to populate data with race_details = pd . DataFrame () dog_results = pd . DataFrame () # For each month, either fetch data from API or use local CSV file if we already have downloaded it for start in pd . date_range ( date_from , date_to , freq = 'MS' ): start_date = start . strftime ( \"%Y-%m- %d \" ) end_date = ( start + MonthEnd ( 1 )) . strftime ( \"%Y-%m- %d \" ) try : filename_races = f 'FT_AU_RACES_ { start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { start_date } .csv' filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' print ( f 'Loading data from { start_date } to { end_date } ' ) if os . path . isfile ( filepath_races ): # Load local CSV file month_race_details = pd . read_csv ( filepath_races ) month_dog_results = pd . read_csv ( filepath_dogs ) else : # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( start_date , end_date , au_tracks_filter ) month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) # Combine monthly data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) except : print ( f 'Could not load data from { start_date } to { end_date } ' ) Loading data from 2017-12-01 to 2017-12-31 Loading data from 2018-01-01 to 2018-01-31 Loading data from 2018-02-01 to 2018-02-28 Loading data from 2018-03-01 to 2018-03-31 Loading data from 2018-04-01 to 2018-04-30 Loading data from 2018-05-01 to 2018-05-31 Loading data from 2018-06-01 to 2018-06-30 Loading data from 2018-07-01 to 2018-07-31 Loading data from 2018-08-01 to 2018-08-31 /home/bruno/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) Loading data from 2018-09-01 to 2018-09-30 Loading data from 2018-10-01 to 2018-10-31 Loading data from 2018-11-01 to 2018-11-30 Loading data from 2018-12-01 to 2018-12-31 Loading data from 2019-01-01 to 2019-01-31 Loading data from 2019-02-01 to 2019-02-28 Loading data from 2019-03-01 to 2019-03-31 Loading data from 2019-04-01 to 2019-04-30 Loading data from 2019-05-01 to 2019-05-31 Loading data from 2019-06-01 to 2019-06-30 Loading data from 2019-07-01 to 2019-07-31 Loading data from 2019-08-01 to 2019-08-31 Loading data from 2019-09-01 to 2019-09-30 Loading data from 2019-10-01 to 2019-10-31 Loading data from 2019-11-01 to 2019-11-30 Loading data from 2019-12-01 to 2019-12-31 Loading data from 2020-01-01 to 2020-01-31 Loading data from 2020-02-01 to 2020-02-29 Loading data from 2020-03-01 to 2020-03-31 Loading data from 2020-04-01 to 2020-04-30 Loading data from 2020-05-01 to 2020-05-31 Loading data from 2020-06-01 to 2020-06-30 Loading data from 2020-07-01 to 2020-07-31 Loading data from 2020-08-01 to 2020-08-31 Loading data from 2020-09-01 to 2020-09-30 Loading data from 2020-10-01 to 2020-10-31 Loading data from 2020-11-01 to 2020-11-30 Loading data from 2020-12-01 to 2020-12-31 Loading data from 2021-01-01 to 2021-01-31 Loading data from 2021-02-01 to 2021-02-28 Loading data from 2021-03-01 to 2021-03-31 Loading data from 2021-04-01 to 2021-04-30 Loading data from 2021-05-01 to 2021-05-31 Loading data from 2021-06-01 to 2021-06-30 Loading data from 2021-07-01 to 2021-07-31 /home/bruno/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (10,12) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) Loading data from 2021-08-01 to 2021-08-31 Loading data from 2021-09-01 to 2021-09-30 To better understand the data we retrieved, let's print the first few rows # Race data race_details . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id RaceNum RaceName RaceTime Distance RaceGrade Track date 0 278896185 1 TRIPLE M BENDIGO 93.5 02:54PM 425m Grade 6 Bendigo 01 Dec 17 1 278896189 2 GOLDEN CITY CONCRETE PUMPING 03:17PM 500m Mixed 6/7 Bendigo 01 Dec 17 2 275589809 3 RAILWAY STATION HOTEL FINAL 03:38PM 500m Mixed 6/7 Final Bendigo 01 Dec 17 3 278896183 4 MCIVOR RD VETERINARY CLINIC 03:58PM 425m Grade 5 Bendigo 01 Dec 17 4 278896179 5 GRV VIC BRED SERIES HT1 04:24PM 425m Grade 5 Heat Bendigo 01 Dec 17 # Individual dogs results dog_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } @id Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR Checks Comments SplitMargin RunTime Prizemoney RaceId TrainerId TrainerName 0 124886334 1 VANDA MICK 2.0 2 32.0 $2.80F NaN 0.49 NaN S/231 0 NaN 6.79 24.66 NaN 278896185 66993 M Ellis 1 2027130024 2 DYNA ZAD 7.0 7 24.2 $6.60 NaN 0.49 0.49 M/843 4 NaN 6.95 24.69 NaN 278896185 115912 M Delbridge 2 1448760015 3 KLONDIKE GOLD 4.0 4 33.3 $16.60 NaN 1.83 1.34 M/422 0 NaN 6.81 24.79 NaN 278896185 94459 R Hayes 3 1449650024 4 FROSTY TIARA 3.0 3 26.8 $22.00 NaN 2.94 1.11 S/114 0 NaN 6.75 24.86 NaN 278896185 87428 R Morgan 4 118782592 5 GNOCCHI 1.0 1 29.6 $8.60 NaN 6.50 3.56 S/355 0 NaN 6.80 25.11 NaN 278896185 138164 J La Rosa","title":"1. Download historic greyhound data from FastTrack API"},{"location":"modelling/greyhoundModellingPython/#2-cleanse-and-normalise-the-data","text":"Here we do some basic data manipulation and cleansing to get variables into format that we can work with # Clean up the race dataset race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) # Clean up the dogs results dataset dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) # Combine dogs results with race attributes dog_results = dog_results . merge ( race_details [[ 'FastTrack_RaceId' , 'Distance' , 'RaceGrade' , 'Track' , 'date_dt' ]], how = 'left' , on = 'FastTrack_RaceId' ) # Convert StartPrice to probability dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x is None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' )) if isinstance ( x , str ) else x ) dog_results [ 'StartPrice_probability' ] = ( 1 / dog_results [ 'StartPrice' ]) . fillna ( 0 ) dog_results [ 'StartPrice_probability' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x / x . sum ()) # Discard entries without results (scratched or did not finish) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results [ 'Box' ] = dog_results [ 'Box' ] . astype ( int ) # Clean up other attributes dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'SplitMargin' ] = dog_results [ 'SplitMargin' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) . fillna ( 0 ) dog_results [ 'Place' ] = pd . to_numeric ( dog_results [ 'Place' ] . apply ( lambda x : x . replace ( \"=\" , \"\" ) if isinstance ( x , str ) else 0 ), errors = 'coerce' ) . fillna ( 0 ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x == 1 else 0 ) The cell below shows some normalisation techniques - Apply Log base 10 transformation to Prizemoney and Place - Apply inverse transformation to Place - Combine RunTime and Distance to generate Speed value # Normalise some of the raw values dog_results [ 'Prizemoney_norm' ] = np . log10 ( dog_results [ 'Prizemoney' ] + 1 ) / 12 dog_results [ 'Place_inv' ] = ( 1 / dog_results [ 'Place' ]) . fillna ( 0 ) dog_results [ 'Place_log' ] = np . log10 ( dog_results [ 'Place' ] + 1 ) . fillna ( 0 ) dog_results [ 'RunSpeed' ] = ( dog_results [ 'RunTime' ] / dog_results [ 'Distance' ]) . fillna ( 0 )","title":"2. Cleanse and normalise the data"},{"location":"modelling/greyhoundModellingPython/#3-generate-features-using-raw-data","text":"","title":"3. Generate features using raw data"},{"location":"modelling/greyhoundModellingPython/#calculate-median-winner-time-by-trackdistance","text":"To compare individual runner times, we extract the median winner time for each Track/Distance and use it as a reference time. # Calculate median winner time per track/distance win_results = dog_results [ dog_results [ 'win' ] == 1 ] median_win_time = pd . DataFrame ( data = win_results [ win_results [ 'RunTime' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'RunTime' ] . median ()) . rename ( columns = { \"RunTime\" : \"RunTime_median\" }) . reset_index () median_win_split_time = pd . DataFrame ( data = win_results [ win_results [ 'SplitMargin' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'SplitMargin' ] . median ()) . rename ( columns = { \"SplitMargin\" : \"SplitMargin_median\" }) . reset_index () median_win_time . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Track Distance RunTime_median 0 Albion Park 331 19.180 1 Albion Park 395 22.860 2 Albion Park 520 30.220 3 Albion Park 600 35.100 4 Albion Park 710 42.005","title":"Calculate median winner time by track/distance"},{"location":"modelling/greyhoundModellingPython/#calculate-track-speed-index","text":"Some tracks are run faster than other, we calculate here a speed_index using the track reference time over the travelled distance. The lower the speed_index , the faster the track is. We use MinMaxScaler to scale speed_index values between zero and one. # Calculate track speed index median_win_time [ 'speed_index' ] = ( median_win_time [ 'RunTime_median' ] / median_win_time [ 'Distance' ]) median_win_time [ 'speed_index' ] = MinMaxScaler () . fit_transform ( median_win_time [[ 'speed_index' ]]) median_win_time . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Track Distance RunTime_median speed_index 0 Albion Park 331 19.180 0.471787 1 Albion Park 395 22.860 0.460736 2 Albion Park 520 30.220 0.497773 3 Albion Park 600 35.100 0.556644 4 Albion Park 710 42.005 0.657970","title":"Calculate Track speed index"},{"location":"modelling/greyhoundModellingPython/#compare-individual-times-with-track-reference-time","text":"For each dog result, we compare the runner time with the reference time using the formula (track reference time) / (runner time) and normalise the result. The higher the value, the quicker the dog was. # Compare dogs finish time with median winner time dog_results = dog_results . merge ( median_win_time , on = [ 'Track' , 'Distance' ], how = 'left' ) dog_results = dog_results . merge ( median_win_split_time , on = [ 'Track' , 'Distance' ], how = 'left' ) # Normalise time comparison dog_results [ 'RunTime_norm' ] = ( dog_results [ 'RunTime_median' ] / dog_results [ 'RunTime' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'RunTime_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'RunTime_norm' ]]) dog_results [ 'SplitMargin_norm' ] = ( dog_results [ 'SplitMargin_median' ] / dog_results [ 'SplitMargin' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'SplitMargin_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'SplitMargin_norm' ]]) dog_results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FastTrack_DogId Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 ... win Prizemoney_norm Place_inv Place_log RunSpeed RunTime_median speed_index SplitMargin_median RunTime_norm SplitMargin_norm 0 124886334 1.0 VANDA MICK 2 2 32.0 2.8 NaN 0.49 NaN ... 1 0.0 1.000000 0.301030 0.058024 24.21 0.321642 6.63 0.408759 0.382180 1 2027130024 2.0 DYNA ZAD 7 7 24.2 6.6 NaN 0.49 0.49 ... 0 0.0 0.500000 0.477121 0.058094 24.21 0.321642 6.63 0.402795 0.269784 2 1448760015 3.0 KLONDIKE GOLD 4 4 33.3 16.6 NaN 1.83 1.34 ... 0 0.0 0.333333 0.602060 0.058329 24.21 0.321642 6.63 0.383017 0.367841 3 1449650024 4.0 FROSTY TIARA 3 3 26.8 22.0 NaN 2.94 1.11 ... 0 0.0 0.250000 0.698970 0.058494 24.21 0.321642 6.63 0.369268 0.411111 4 118782592 5.0 GNOCCHI 1 1 29.6 8.6 NaN 6.50 3.56 ... 0 0.0 0.200000 0.778151 0.059082 24.21 0.321642 6.63 0.320789 0.375000 5 rows \u00d7 34 columns","title":"Compare individual times with track reference time"},{"location":"modelling/greyhoundModellingPython/#barrier-winning-probabilities","text":"The barrier dogs start from play a big part in the race so we calculate the winning percentage for each barrier/track/distance # Calculate box winning percentage for each track/distance box_win_percent = pd . DataFrame ( data = dog_results . groupby ([ 'Track' , 'Distance' , 'Box' ])[ 'win' ] . mean ()) . rename ( columns = { \"win\" : \"box_win_percent\" }) . reset_index () # Add to dog results dataframe dog_results = dog_results . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) # Display example of barrier winning probabilities display ( box_win_percent . head ( 8 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Track Distance Box box_win_percent 0 Albion Park 331 1 0.195652 1 Albion Park 331 2 0.153472 2 Albion Park 331 3 0.125446 3 Albion Park 331 4 0.124615 4 Albion Park 331 5 0.116135 5 Albion Park 331 6 0.105144 6 Albion Park 331 7 0.104770 7 Albion Park 331 8 0.115095","title":"Barrier winning probabilities"},{"location":"modelling/greyhoundModellingPython/#generate-time-based-features","text":"Now that we have a set of basic features for individual dog results, we need to aggregate them into a single feature vector. To do so, we calculate the min , max , mean , median , std of features previously caculated over different time windows 28 , 91 and 365 days: - RunTime_norm - SplitMargin_norm - Place_inv - Place_log - Prizemoney_norm Depending on the dataset size, this can take several minutes # Generate rolling window features dataset = dog_results . copy () dataset = dataset . set_index ([ 'FastTrack_DogId' , 'date_dt' ]) . sort_index () # Use rolling window of 28, 91 and 365 days rolling_windows = [ '28D' , '91D' , '365D' ] # Features to use for rolling windows calculation features = [ 'RunTime_norm' , 'SplitMargin_norm' , 'Place_inv' , 'Place_log' , 'Prizemoney_norm' ] # Aggregation functions to apply aggregates = [ 'min' , 'max' , 'mean' , 'median' , 'std' ] # Keep track of generated feature names feature_cols = [ 'speed_index' , 'box_win_percent' ] for rolling_window in rolling_windows : print ( f 'Processing rolling window { rolling_window } ' ) rolling_result = ( dataset . reset_index ( level = 0 ) . groupby ( 'FastTrack_DogId' )[ features ] . rolling ( rolling_window ) . agg ( aggregates ) . groupby ( level = 0 ) . shift ( 1 ) ) # Generate list of rolling window feature names (eg: RunTime_norm_min_365D) agg_features_cols = [ f ' { f } _ { a } _ { rolling_window } ' for f , a in itertools . product ( features , aggregates )] # Add features to dataset dataset [ agg_features_cols ] = rolling_result # Keep track of generated feature names feature_cols . extend ( agg_features_cols ) Processing rolling window 28D Processing rolling window 91D Processing rolling window 365D # Replace missing values with 0 dataset . fillna ( 0 , inplace = True ) display ( dataset . head ( 8 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Place DogName Box Rug Weight StartPrice Handicap Margin1 Margin2 PIR ... Place_log_min_365D Place_log_max_365D Place_log_mean_365D Place_log_median_365D Place_log_std_365D Prizemoney_norm_min_365D Prizemoney_norm_max_365D Prizemoney_norm_mean_365D Prizemoney_norm_median_365D Prizemoney_norm_std_365D FastTrack_DogId date_dt -2143487296 2017-12-14 6.0 JEWELLED COIN 7 7 26.6 13.1 0.0 8.25 0.14 55 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 2017-12-21 4.0 JEWELLED COIN 5 5 26.8 9.7 0.0 13.50 3.00 555 ... 0.845098 0.845098 0.845098 0.845098 0.000000 0.0 0.000000 0.000000 0.000000 0.000000 2017-12-26 3.0 JEWELLED COIN 3 3 27.1 21.5 0.0 6.75 2.29 642 ... 0.698970 0.845098 0.772034 0.772034 0.103328 0.0 0.142298 0.071149 0.071149 0.100620 2017-12-30 7.0 JEWELLED COIN 7 9 26.4 48.1 0.0 21.75 2.29 7777 ... 0.602060 0.845098 0.715376 0.698970 0.122347 0.0 0.204697 0.115665 0.142298 0.104915 2018-01-02 8.0 JEWELLED COIN 5 5 26.8 32.7 0.0 15.75 0.00 888 ... 0.602060 0.903090 0.762305 0.772034 0.137070 0.0 0.204697 0.086749 0.071149 0.103357 2018-01-08 4.0 JEWELLED COIN 1 1 27.2 2.5 0.0 6.50 1.29 5443 ... 0.602060 0.954243 0.800692 0.845098 0.146490 0.0 0.204697 0.069399 0.000000 0.097556 2018-01-10 2.0 JEWELLED COIN 5 5 27.3 8.5 0.0 2.00 2.14 442 ... 0.602060 0.954243 0.783738 0.772034 0.137448 0.0 0.204697 0.080926 0.069282 0.091711 2018-01-17 2.0 JEWELLED COIN 3 3 27.4 7.3 0.0 5.25 5.14 433 ... 0.477121 0.954243 0.739936 0.698970 0.170804 0.0 0.204697 0.098329 0.138563 0.095547 8 rows \u00d7 108 columns As we use up to a year of data to generate our feature set, we exclude the first year of the dataset from our training dataset # Only keep data after 2018-12-01 model_df = dataset . reset_index () feature_cols = np . unique ( feature_cols ) . tolist () model_df = model_df [ model_df [ 'date_dt' ] >= '2018-12-01' ] model_df = model_df [[ 'date_dt' , 'FastTrack_RaceId' , 'DogName' , 'win' , 'StartPrice_probability' ] + feature_cols ] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )]","title":"Generate time-based features"},{"location":"modelling/greyhoundModellingPython/#4-build-and-train-regression-models","text":"","title":"4. Build and train regression models"},{"location":"modelling/greyhoundModellingPython/#logistic-regression","text":"The dataset is split between training and validation: - train dataset: from 2018-12-01 to 2020-12-31 - validation dataset: from 2021-01-01 The next cell trains a LogisticRegression model and uses the win flag ( 0=lose, 1=win ) as a target. from matplotlib import pyplot from matplotlib.pyplot import figure from sklearn.linear_model import LogisticRegression # Split the data into train and test data train_data = model_df [ model_df [ 'date_dt' ] < '2021-01-01' ] . reset_index ( drop = True ) . sample ( frac = 1 ) test_data = model_df [ model_df [ 'date_dt' ] >= '2021-01-01' ] . reset_index ( drop = True ) # Use our previously built features set columns as Training vector # Use win flag as Target vector train_x , train_y = train_data [ feature_cols ], train_data [ 'win' ] test_x , test_y = test_data [ feature_cols ], test_data [ 'win' ] # Build a LogisticRegression model model = LogisticRegression ( verbose = 0 , solver = 'saga' , n_jobs =- 1 ) # Train the model print ( f 'Training on { len ( train_x ) : , } samples with { len ( feature_cols ) } features' ) model . fit ( train_x , train_y ) Training on 630,306 samples with 77 features LogisticRegression(n_jobs=-1, solver='saga')","title":"Logistic regression"},{"location":"modelling/greyhoundModellingPython/#5-evaluate-model-predictions","text":"Now that we have trained our model, we can generate predictions on the test dataset # Generate runner win predictions dog_win_probabilities = model . predict_proba ( test_x )[:, 1 ] test_data [ 'prob_LogisticRegression' ] = dog_win_probabilities # Normalise probabilities test_data [ 'prob_LogisticRegression' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x / sum ( x ))","title":"5. Evaluate model predictions"},{"location":"modelling/greyhoundModellingPython/#model-strike-rate","text":"Knowing how often a model correctly predicts the winner is one of the most important metrics # Create a boolean column for whether a dog has the higehst model prediction in a race test_dataset_size = test_data [ 'FastTrack_RaceId' ] . nunique () odds_win_prediction = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x == max ( x )) odds_win_prediction_percent = len ( test_data [( odds_win_prediction == True ) & ( test_data [ 'win' ] == 1 )]) / test_dataset_size print ( f \"LogisticRegression strike rate: { odds_win_prediction_percent : .2% } \" ) LogisticRegression strike rate: 32.57%","title":"Model strike rate"},{"location":"modelling/greyhoundModellingPython/#brier-score","text":"The Brier score measures the mean squared difference between the predicted probability and the actual outcome. The smaller the Brier score loss, the better. from sklearn.metrics import brier_score_loss brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ 'prob_LogisticRegression' ]) print ( f 'LogisticRegression Brier score: { brier_score : .8 } ' ) LogisticRegression Brier score: 0.11074995","title":"Brier score"},{"location":"modelling/greyhoundModellingPython/#predictions-distribution","text":"To get a better feel of what our models are predicting, we can plot the generated probabilities' distribution and compare them with Start Prices probabilities' distribution. import matplotlib.pyplot as plt import seaborn as sns bins = 100 sns . displot ( data = [ test_data [ 'prob_LogisticRegression' ], test_data [ 'StartPrice_probability' ]], kind = \"hist\" , bins = bins , height = 7 , aspect = 2 ) plt . title ( 'StartPrice vs LogisticRegression probabilities distribution' ) plt . xlabel ( 'Probability' ) plt . show () Probabilities generated by the logistic regression model follow a slightly different distribution. Scikit-learn framework offers various hyper parameters to fine tune a model and achieve better performances.","title":"Predictions' distribution"},{"location":"modelling/greyhoundModellingPython/#predictions-calibration","text":"We want to ensure that probabilities generated by our model match real world probabilities. Calibration curves help us understand if a model needs to be calibrated. from sklearn.calibration import calibration_curve bins = 100 fig = plt . figure ( figsize = ( 12 , 9 )) # Generate calibration curves based on our probabilities cal_y , cal_x = calibration_curve ( test_data [ 'win' ], test_data [ 'prob_LogisticRegression' ], n_bins = bins ) # Plot against reference line plt . plot ( cal_x , cal_y , marker = 'o' , linewidth = 1 ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], '--' , color = 'gray' ) plt . title ( \"LogisticRegression calibration curve\" ); A model is perfectly calibrated if the grouped values (bins) follow the dotted line. Our model generates probabilities that need to be calibrated. To get our model to generate more accurate probabilities, we would need to generate better features, test various modelling approaches and calibrate generated probabilities.","title":"Predictions calibration"},{"location":"modelling/greyhoundModellingPython/#compare-other-types-of-classification-models","text":"The next cell trains different classification models using Scikit-learn unified API: - GradientBoostingClassifier - RandomForestClassifier - LGBMClassifier - XGBClassifier - CatBoostClassifier Depending on dataset size and compute capacity, this can take several minutes from matplotlib import pyplot from matplotlib.pyplot import figure from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestClassifier # Gradient Boosting Machines libraries from lightgbm import LGBMClassifier from xgboost import XGBClassifier from catboost import CatBoostClassifier # Common models parameters verbose = 0 learning_rate = 0.1 n_estimators = 100 # Train different types of models models = { 'LogisticRegression' : LogisticRegression ( verbose = 0 , solver = 'saga' , n_jobs =- 1 ), 'GradientBoostingClassifier' : GradientBoostingClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators , max_depth = 3 , max_features = 0.25 ), 'RandomForestClassifier' : RandomForestClassifier ( verbose = verbose , n_estimators = n_estimators , max_depth = 8 , max_features = 0.5 , n_jobs =- 1 ), 'LGBMClassifier' : LGBMClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators , force_col_wise = True ), 'XGBClassifier' : XGBClassifier ( verbosity = verbose , learning_rate = learning_rate , n_estimators = n_estimators , objective = 'binary:logistic' , use_label_encoder = False ), 'CatBoostClassifier' : CatBoostClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators ) } print ( f 'Training on { len ( train_x ) : , } samples with { len ( feature_cols ) } features' ) for key , model in models . items (): print ( f 'Fitting model { key } ' ) model . fit ( train_x , train_y ) Training on 630,306 samples with 77 features Fitting model LogisticRegression Fitting model GradientBoostingClassifier Fitting model RandomForestClassifier Fitting model LGBMClassifier Fitting model XGBClassifier Fitting model CatBoostClassifier # Calculate probabilities for each model on the test dataset probs_columns = [ 'StartPrice_probability' ] for key , model in models . items (): probs_column_key = f 'prob_ { key } ' # Calculate runner win probability dog_win_probs = model . predict_proba ( test_x )[:, 1 ] test_data [ probs_column_key ] = dog_win_probs # Normalise probabilities test_data [ probs_column_key ] = test_data . groupby ( 'FastTrack_RaceId' )[ f 'prob_ { key } ' ] . apply ( lambda x : x / sum ( x )) probs_columns . append ( probs_column_key )","title":"Compare other types of classification models"},{"location":"modelling/greyhoundModellingPython/#calculate-models-strike-rate-and-brier-score","text":"Here we compare the strike rate of the different models' predictions with the start price strike rate. # Create a boolean column for whether a dog has the higehst model prediction in a race. # Do the same for the starting price as a comparison test_dataset_size = test_data [ 'FastTrack_RaceId' ] . nunique () odds_win_prediction = test_data . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x == max ( x )) odds_win_prediction_percent = len ( test_data [( odds_win_prediction == True ) & ( test_data [ 'win' ] == 1 )]) / test_dataset_size brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ 'StartPrice_probability' ]) print ( f 'Starting Price strike rate: { odds_win_prediction_percent : .2% } Brier score: { brier_score : .8 } ' ) for key , model in models . items (): predicted_winners = test_data . groupby ( 'FastTrack_RaceId' )[ f 'prob_ { key } ' ] . apply ( lambda x : x == max ( x )) strike_rate = len ( test_data [( predicted_winners == True ) & ( test_data [ 'win' ] == 1 )]) / test_data [ 'FastTrack_RaceId' ] . nunique () brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ f 'prob_ { key } ' ]) print ( f ' { key . ljust ( 30 ) } strike rate: { strike_rate : .2% } Brier score: { brier_score : .8 } ' ) Starting Price strike rate: 42.24% Brier score: 0.1008106 LogisticRegression strike rate: 32.57% Brier score: 0.11074995 GradientBoostingClassifier strike rate: 33.31% Brier score: 0.1105322 RandomForestClassifier strike rate: 33.24% Brier score: 0.11110442 LGBMClassifier strike rate: 33.40% Brier score: 0.11024272 XGBClassifier strike rate: 33.45% Brier score: 0.11019414 CatBoostClassifier strike rate: 33.33% Brier score: 0.11038785","title":"Calculate models strike rate and Brier score"},{"location":"modelling/greyhoundModellingPython/#visualise-models-predictions","text":"Here we generate some probabilities using our trained models' and compare them with the start price. In blue the lowest prediction and in red the highest prediction generated by the different models. # Display and format sample results def highlight_max ( s , props = '' ): return np . where ( s == np . nanmax ( s . values ), props , '' ) def highlight_min ( s , props = '' ): return np . where ( s == np . nanmin ( s . values ), props , '' ) test_data [ probs_columns ] . sample ( 20 ) . style \\ . bar ( color = '#FFA07A' , vmin = 0.01 , vmax = 0.25 , axis = 1 ) \\ . apply ( highlight_max , props = 'color:red;' , axis = 1 ) \\ . apply ( highlight_min , props = 'color:blue;' , axis = 1 ) #T_f2591_row0_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 65.8%, transparent 65.8%); color: blue; } #T_f2591_row0_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 91.4%, transparent 91.4%); color: red; } #T_f2591_row0_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 84.8%, transparent 84.8%); } #T_f2591_row0_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 78.7%, transparent 78.7%); } #T_f2591_row0_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 84.4%, transparent 84.4%); } #T_f2591_row0_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 79.7%, transparent 79.7%); } #T_f2591_row0_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 86.1%, transparent 86.1%); } #T_f2591_row1_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 37.4%, transparent 37.4%); } #T_f2591_row1_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 35.2%, transparent 35.2%); } #T_f2591_row1_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 49.5%, transparent 49.5%); } #T_f2591_row1_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 61.3%, transparent 61.3%); color: red; } #T_f2591_row1_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 42.2%, transparent 42.2%); } #T_f2591_row1_col5, #T_f2591_row15_col0, #T_f2591_row16_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 22.6%, transparent 22.6%); color: blue; } #T_f2591_row1_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 48.1%, transparent 48.1%); } #T_f2591_row2_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 1.4%, transparent 1.4%); color: blue; } #T_f2591_row2_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 22.4%, transparent 22.4%); } #T_f2591_row2_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 25.1%, transparent 25.1%); } #T_f2591_row2_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 35.9%, transparent 35.9%); color: red; } #T_f2591_row2_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 28.0%, transparent 28.0%); } #T_f2591_row2_col5, #T_f2591_row2_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 29.0%, transparent 29.0%); } #T_f2591_row3_col0, #T_f2591_row5_col0, #T_f2591_row6_col0, #T_f2591_row7_col1, #T_f2591_row9_col0, #T_f2591_row17_col0, #T_f2591_row19_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 100.0%, transparent 100.0%); color: red; } #T_f2591_row3_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 26.8%, transparent 26.8%); } #T_f2591_row3_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 34.7%, transparent 34.7%); } #T_f2591_row3_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 32.4%, transparent 32.4%); } #T_f2591_row3_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 25.4%, transparent 25.4%); color: blue; } #T_f2591_row3_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 27.0%, transparent 27.0%); } #T_f2591_row3_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 34.3%, transparent 34.3%); } #T_f2591_row4_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 6.5%, transparent 6.5%); } #T_f2591_row4_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 9.1%, transparent 9.1%); } #T_f2591_row4_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 7.8%, transparent 7.8%); } #T_f2591_row4_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 11.4%, transparent 11.4%); color: red; } #T_f2591_row4_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 7.6%, transparent 7.6%); } #T_f2591_row4_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 6.4%, transparent 6.4%); } #T_f2591_row4_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 6.3%, transparent 6.3%); color: blue; } #T_f2591_row5_col1, #T_f2591_row17_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 100.0%, transparent 100.0%); color: blue; } #T_f2591_row5_col2, #T_f2591_row5_col3, #T_f2591_row5_col4, #T_f2591_row5_col5, #T_f2591_row5_col6, #T_f2591_row7_col0, #T_f2591_row17_col1, #T_f2591_row17_col2, #T_f2591_row17_col4, #T_f2591_row17_col5, #T_f2591_row17_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 100.0%, transparent 100.0%); } #T_f2591_row6_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 53.0%, transparent 53.0%); } #T_f2591_row6_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.5%, transparent 43.5%); } #T_f2591_row6_col3, #T_f2591_row18_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 42.9%, transparent 42.9%); } #T_f2591_row6_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 34.7%, transparent 34.7%); color: blue; } #T_f2591_row6_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 40.9%, transparent 40.9%); } #T_f2591_row6_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 48.6%, transparent 48.6%); } #T_f2591_row7_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 80.9%, transparent 80.9%); } #T_f2591_row7_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 74.2%, transparent 74.2%); color: blue; } #T_f2591_row7_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 84.0%, transparent 84.0%); } #T_f2591_row7_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 77.8%, transparent 77.8%); } #T_f2591_row7_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 87.2%, transparent 87.2%); } #T_f2591_row8_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 48.3%, transparent 48.3%); color: red; } #T_f2591_row8_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 40.3%, transparent 40.3%); } #T_f2591_row8_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 37.8%, transparent 37.8%); color: blue; } #T_f2591_row8_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.8%, transparent 43.8%); } #T_f2591_row8_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 42.1%, transparent 42.1%); } #T_f2591_row8_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 45.2%, transparent 45.2%); } #T_f2591_row8_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 40.7%, transparent 40.7%); } #T_f2591_row9_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 63.5%, transparent 63.5%); } #T_f2591_row9_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 62.1%, transparent 62.1%); } #T_f2591_row9_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 49.8%, transparent 49.8%); color: blue; } #T_f2591_row9_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 74.4%, transparent 74.4%); } #T_f2591_row9_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 67.6%, transparent 67.6%); } #T_f2591_row9_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 59.7%, transparent 59.7%); } #T_f2591_row10_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 61.7%, transparent 61.7%); color: blue; } #T_f2591_row10_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 91.0%, transparent 91.0%); } #T_f2591_row10_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 93.2%, transparent 93.2%); color: red; } #T_f2591_row10_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 91.1%, transparent 91.1%); } #T_f2591_row10_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 88.0%, transparent 88.0%); } #T_f2591_row10_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 86.4%, transparent 86.4%); } #T_f2591_row10_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 92.0%, transparent 92.0%); } #T_f2591_row11_col0, #T_f2591_row11_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 12.6%, transparent 12.6%); } #T_f2591_row11_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 12.3%, transparent 12.3%); } #T_f2591_row11_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 14.9%, transparent 14.9%); } #T_f2591_row11_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 16.1%, transparent 16.1%); color: red; } #T_f2591_row11_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 12.2%, transparent 12.2%); color: blue; } #T_f2591_row11_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 13.6%, transparent 13.6%); } #T_f2591_row12_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 10.9%, transparent 10.9%); color: blue; } #T_f2591_row12_col1, #T_f2591_row12_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 31.0%, transparent 31.0%); } #T_f2591_row12_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 34.2%, transparent 34.2%); } #T_f2591_row12_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 36.4%, transparent 36.4%); color: red; } #T_f2591_row12_col5, #T_f2591_row13_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 32.9%, transparent 32.9%); } #T_f2591_row12_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 33.2%, transparent 33.2%); } #T_f2591_row13_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 15.1%, transparent 15.1%); color: blue; } #T_f2591_row13_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 33.6%, transparent 33.6%); } #T_f2591_row13_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 41.5%, transparent 41.5%); } #T_f2591_row13_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 41.6%, transparent 41.6%); color: red; } #T_f2591_row13_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 41.4%, transparent 41.4%); } #T_f2591_row13_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 39.9%, transparent 39.9%); } #T_f2591_row14_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 26.7%, transparent 26.7%); } #T_f2591_row14_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 40.1%, transparent 40.1%); color: red; } #T_f2591_row14_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 27.6%, transparent 27.6%); } #T_f2591_row14_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 29.4%, transparent 29.4%); } #T_f2591_row14_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 25.1%, transparent 25.1%); color: blue; } #T_f2591_row14_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 26.1%, transparent 26.1%); } #T_f2591_row14_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 29.2%, transparent 29.2%); } #T_f2591_row15_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 46.1%, transparent 46.1%); } #T_f2591_row15_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 39.5%, transparent 39.5%); } #T_f2591_row15_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 44.9%, transparent 44.9%); } #T_f2591_row15_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 51.8%, transparent 51.8%); } #T_f2591_row15_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 56.5%, transparent 56.5%); color: red; } #T_f2591_row15_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.4%, transparent 43.4%); } #T_f2591_row16_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 56.4%, transparent 56.4%); color: red; } #T_f2591_row16_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 48.3%, transparent 48.3%); } #T_f2591_row16_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 54.8%, transparent 54.8%); } #T_f2591_row16_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 49.7%, transparent 49.7%); } #T_f2591_row16_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 45.8%, transparent 45.8%); } #T_f2591_row16_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 49.4%, transparent 49.4%); } #T_f2591_row18_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 26.3%, transparent 26.3%); color: blue; } #T_f2591_row18_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 46.6%, transparent 46.6%); color: red; } #T_f2591_row18_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.9%, transparent 43.9%); } #T_f2591_row18_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 43.0%, transparent 43.0%); } #T_f2591_row18_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 44.2%, transparent 44.2%); } #T_f2591_row18_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 36.8%, transparent 36.8%); } #T_f2591_row19_col1 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 75.6%, transparent 75.6%); } #T_f2591_row19_col2 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 69.4%, transparent 69.4%); } #T_f2591_row19_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 63.6%, transparent 63.6%); } #T_f2591_row19_col4 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 74.0%, transparent 74.0%); } #T_f2591_row19_col5 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 71.9%, transparent 71.9%); } #T_f2591_row19_col6 { width: 10em; height: 80%; background: linear-gradient(90deg,#FFA07A 60.2%, transparent 60.2%); color: blue; } StartPrice_probability prob_LogisticRegression prob_GradientBoostingClassifier prob_RandomForestClassifier prob_LGBMClassifier prob_XGBClassifier prob_CatBoostClassifier 103796 0.168011 0.229477 0.213527 0.198978 0.212484 0.201237 0.216716 148438 0.099749 0.094426 0.128795 0.157217 0.111212 0.064149 0.125434 47999 0.013440 0.063647 0.070135 0.096175 0.077220 0.079547 0.079643 226804 0.331895 0.074406 0.093237 0.087665 0.070858 0.074815 0.092281 14964 0.025668 0.031759 0.028652 0.037364 0.028186 0.025395 0.025159 139208 0.537243 0.269383 0.304885 0.300394 0.286015 0.301443 0.296410 43027 0.257494 0.137246 0.114401 0.113054 0.093332 0.108154 0.126703 151933 0.254678 0.280246 0.204094 0.188197 0.211623 0.196675 0.219325 75586 0.126036 0.106833 0.100814 0.115142 0.111017 0.118383 0.107643 114240 0.530708 0.162428 0.159064 0.129457 0.188566 0.172335 0.153399 73858 0.158025 0.228463 0.233774 0.228696 0.221210 0.217358 0.230763 174698 0.040139 0.039564 0.045854 0.048556 0.040328 0.039350 0.042728 121778 0.036096 0.084464 0.092160 0.097381 0.084467 0.088923 0.089644 227215 0.046275 0.088997 0.090732 0.109514 0.109888 0.109474 0.105836 10731 0.074040 0.106214 0.076128 0.080542 0.070221 0.072588 0.080146 88484 0.064234 0.120542 0.104910 0.117783 0.134223 0.145592 0.114074 104690 0.064302 0.145388 0.126006 0.141474 0.129206 0.120031 0.128490 201852 0.602269 0.367840 0.394838 0.364498 0.401240 0.387149 0.393707 218537 0.073158 0.121725 0.115405 0.113319 0.116149 0.098413 0.112932 119928 0.310934 0.191449 0.176635 0.162701 0.187676 0.182536 0.154486 We now have built a simple feature set and trained models using various classification techniques. To improve our model's performance, one should build a more advanced feature set and fine tune the model's hyper parameters.","title":"Visualise models predictions"},{"location":"modelling/greyhoundModellingPython/#6-display-models-features-importance","text":"from sklearn.preprocessing import normalize total_feature_importances = [] # Individual models feature importance for key , model in models . items (): figure ( figsize = ( 10 , 24 ), dpi = 80 ) if isinstance ( model , LogisticRegression ): feature_importance = model . coef_ [ 0 ] else : feature_importance = model . feature_importances_ feature_importance = normalize ( feature_importance [:, np . newaxis ], axis = 0 ) . ravel () total_feature_importances . append ( feature_importance ) pyplot . barh ( feature_cols , feature_importance ) pyplot . xlabel ( f ' { key } Features Importance' ) pyplot . show () # Overall feature importance avg_feature_importances = np . asarray ( total_feature_importances ) . mean ( axis = 0 ) figure ( figsize = ( 10 , 24 ), dpi = 80 ) pyplot . barh ( feature_cols , avg_feature_importances ) pyplot . xlabel ( 'Overall Features Importance' ) pyplot . show ()","title":"6. Display models' features' importance"},{"location":"modelling/greyhoundModellingPython/#conclusion","text":"This notebook shows an approach to building a simple feature set and training fundamental models using various classification techniques. Analysing these models using metrics such as strike rate and Beyer score indicates that these models have poor performances compared to market starting prices and therefore probably should not be used for betting. To improve your model's performances, the logical next steps would generally be to create new features to add to the dataset, apply various normalisation and standardisation techniques and fine-tune hyper-parameters when training models. This process does take time and effort to get right but is an approach that can be both fun, challenging and sometimes rewarding.","title":"Conclusion"},{"location":"modelling/greyhoundModellingPython/#complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github # Import libraries import os import sys # Allow imports from src folder module_path = os . path . abspath ( os . path . join ( '../src' )) if module_path not in sys . path : sys . path . append ( module_path ) from datetime import datetime , timedelta from dateutil.relativedelta import relativedelta from dateutil import tz from pandas.tseries.offsets import MonthEnd from sklearn.preprocessing import MinMaxScaler import itertools import math import numpy as np import pandas as pd import fasttrack as ft from dotenv import load_dotenv load_dotenv () # Validate FastTrack API connection api_key = os . getenv ( 'FAST_TRACK_API_KEY' , '<REPLACE WITH YOUR KEY>' ) client = ft . Fasttrack ( api_key ) track_codes = client . listTracks () # Import race data excluding NZ races au_tracks_filter = list ( track_codes [ track_codes [ 'state' ] != 'NZ' ][ 'track_code' ]) # Time window to import data # First day of the month 46 months back from now date_from = ( datetime . today () - relativedelta ( months = 46 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # First day of previous month date_to = ( datetime . today () - relativedelta ( months = 1 )) . replace ( day = 1 ) . strftime ( '%Y-%m- %d ' ) # Dataframes to populate data with race_details = pd . DataFrame () dog_results = pd . DataFrame () # For each month, either fetch data from API or use local CSV file if we already have downloaded it for start in pd . date_range ( date_from , date_to , freq = 'MS' ): start_date = start . strftime ( \"%Y-%m- %d \" ) end_date = ( start + MonthEnd ( 1 )) . strftime ( \"%Y-%m- %d \" ) try : filename_races = f 'FT_AU_RACES_ { start_date } .csv' filename_dogs = f 'FT_AU_DOGS_ { start_date } .csv' filepath_races = f '../data/ { filename_races } ' filepath_dogs = f '../data/ { filename_dogs } ' print ( f 'Loading data from { start_date } to { end_date } ' ) if os . path . isfile ( filepath_races ): # Load local CSV file month_race_details = pd . read_csv ( filepath_races ) month_dog_results = pd . read_csv ( filepath_dogs ) else : # Fetch data from API month_race_details , month_dog_results = client . getRaceResults ( start_date , end_date , au_tracks_filter ) month_race_details . to_csv ( filepath_races , index = False ) month_dog_results . to_csv ( filepath_dogs , index = False ) # Combine monthly data race_details = race_details . append ( month_race_details , ignore_index = True ) dog_results = dog_results . append ( month_dog_results , ignore_index = True ) except : print ( f 'Could not load data from { start_date } to { end_date } ' ) ## Cleanse and normalise the data # Clean up the race dataset race_details = race_details . rename ( columns = { '@id' : 'FastTrack_RaceId' }) race_details [ 'Distance' ] = race_details [ 'Distance' ] . apply ( lambda x : int ( x . replace ( \"m\" , \"\" ))) race_details [ 'date_dt' ] = pd . to_datetime ( race_details [ 'date' ], format = ' %d %b %y' ) # Clean up the dogs results dataset dog_results = dog_results . rename ( columns = { '@id' : 'FastTrack_DogId' , 'RaceId' : 'FastTrack_RaceId' }) # Combine dogs results with race attributes dog_results = dog_results . merge ( race_details [[ 'FastTrack_RaceId' , 'Distance' , 'RaceGrade' , 'Track' , 'date_dt' ]], how = 'left' , on = 'FastTrack_RaceId' ) # Convert StartPrice to probability dog_results [ 'StartPrice' ] = dog_results [ 'StartPrice' ] . apply ( lambda x : None if x is None else float ( x . replace ( '$' , '' ) . replace ( 'F' , '' )) if isinstance ( x , str ) else x ) dog_results [ 'StartPrice_probability' ] = ( 1 / dog_results [ 'StartPrice' ]) . fillna ( 0 ) dog_results [ 'StartPrice_probability' ] = dog_results . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x / x . sum ()) # Discard entries without results (scratched or did not finish) dog_results = dog_results [ ~ dog_results [ 'Box' ] . isnull ()] dog_results [ 'Box' ] = dog_results [ 'Box' ] . astype ( int ) # Clean up other attributes dog_results [ 'RunTime' ] = dog_results [ 'RunTime' ] . astype ( float ) dog_results [ 'SplitMargin' ] = dog_results [ 'SplitMargin' ] . astype ( float ) dog_results [ 'Prizemoney' ] = dog_results [ 'Prizemoney' ] . astype ( float ) . fillna ( 0 ) dog_results [ 'Place' ] = pd . to_numeric ( dog_results [ 'Place' ] . apply ( lambda x : x . replace ( \"=\" , \"\" ) if isinstance ( x , str ) else 0 ), errors = 'coerce' ) . fillna ( 0 ) dog_results [ 'win' ] = dog_results [ 'Place' ] . apply ( lambda x : 1 if x == 1 else 0 ) # Normalise some of the raw values dog_results [ 'Prizemoney_norm' ] = np . log10 ( dog_results [ 'Prizemoney' ] + 1 ) / 12 dog_results [ 'Place_inv' ] = ( 1 / dog_results [ 'Place' ]) . fillna ( 0 ) dog_results [ 'Place_log' ] = np . log10 ( dog_results [ 'Place' ] + 1 ) . fillna ( 0 ) dog_results [ 'RunSpeed' ] = ( dog_results [ 'RunTime' ] / dog_results [ 'Distance' ]) . fillna ( 0 ) ## Generate features using raw data # Calculate median winner time per track/distance win_results = dog_results [ dog_results [ 'win' ] == 1 ] median_win_time = pd . DataFrame ( data = win_results [ win_results [ 'RunTime' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'RunTime' ] . median ()) . rename ( columns = { \"RunTime\" : \"RunTime_median\" }) . reset_index () median_win_split_time = pd . DataFrame ( data = win_results [ win_results [ 'SplitMargin' ] > 0 ] . groupby ([ 'Track' , 'Distance' ])[ 'SplitMargin' ] . median ()) . rename ( columns = { \"SplitMargin\" : \"SplitMargin_median\" }) . reset_index () median_win_time . head () # Calculate track speed index median_win_time [ 'speed_index' ] = ( median_win_time [ 'RunTime_median' ] / median_win_time [ 'Distance' ]) median_win_time [ 'speed_index' ] = MinMaxScaler () . fit_transform ( median_win_time [[ 'speed_index' ]]) median_win_time . head () # Compare dogs finish time with median winner time dog_results = dog_results . merge ( median_win_time , on = [ 'Track' , 'Distance' ], how = 'left' ) dog_results = dog_results . merge ( median_win_split_time , on = [ 'Track' , 'Distance' ], how = 'left' ) # Normalise time comparison dog_results [ 'RunTime_norm' ] = ( dog_results [ 'RunTime_median' ] / dog_results [ 'RunTime' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'RunTime_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'RunTime_norm' ]]) dog_results [ 'SplitMargin_norm' ] = ( dog_results [ 'SplitMargin_median' ] / dog_results [ 'SplitMargin' ]) . clip ( 0.9 , 1.1 ) dog_results [ 'SplitMargin_norm' ] = MinMaxScaler () . fit_transform ( dog_results [[ 'SplitMargin_norm' ]]) dog_results . head () # Calculate box winning percentage for each track/distance box_win_percent = pd . DataFrame ( data = dog_results . groupby ([ 'Track' , 'Distance' , 'Box' ])[ 'win' ] . mean ()) . rename ( columns = { \"win\" : \"box_win_percent\" }) . reset_index () # Add to dog results dataframe dog_results = dog_results . merge ( box_win_percent , on = [ 'Track' , 'Distance' , 'Box' ], how = 'left' ) # Display example of barrier winning probabilities display ( box_win_percent . head ( 8 )) # Generate rolling window features dataset = dog_results . copy () dataset = dataset . set_index ([ 'FastTrack_DogId' , 'date_dt' ]) . sort_index () # Use rolling window of 28, 91 and 365 days rolling_windows = [ '28D' , '91D' , '365D' ] # Features to use for rolling windows calculation features = [ 'RunTime_norm' , 'SplitMargin_norm' , 'Place_inv' , 'Place_log' , 'Prizemoney_norm' ] # Aggregation functions to apply aggregates = [ 'min' , 'max' , 'mean' , 'median' , 'std' ] # Keep track of generated feature names feature_cols = [ 'speed_index' , 'box_win_percent' ] for rolling_window in rolling_windows : print ( f 'Processing rolling window { rolling_window } ' ) rolling_result = ( dataset . reset_index ( level = 0 ) . groupby ( 'FastTrack_DogId' )[ features ] . rolling ( rolling_window ) . agg ( aggregates ) . shift ( 1 ) ) # Generate list of rolling window feature names (eg: RunTime_norm_min_365D) agg_features_cols = [ f ' { f } _ { a } _ { rolling_window } ' for f , a in itertools . product ( features , aggregates )] # Add features to dataset dataset [ agg_features_cols ] = rolling_result # Keep track of generated feature names feature_cols . extend ( agg_features_cols ) # Replace missing values with 0 dataset . fillna ( 0 , inplace = True ) display ( dataset . head ( 8 )) # Only keep data after 2018-12-01 model_df = dataset . reset_index () feature_cols = np . unique ( feature_cols ) . tolist () model_df = model_df [ model_df [ 'date_dt' ] >= '2018-12-01' ] model_df = model_df [[ 'date_dt' , 'FastTrack_RaceId' , 'DogName' , 'win' , 'StartPrice_probability' ] + feature_cols ] # Only train model off of races where each dog has a value for each feature races_exclude = model_df [ model_df . isnull () . any ( axis = 1 )][ 'FastTrack_RaceId' ] . drop_duplicates () model_df = model_df [ ~ model_df [ 'FastTrack_RaceId' ] . isin ( races_exclude )] ## Build and train Regression models from matplotlib import pyplot from matplotlib.pyplot import figure from sklearn.linear_model import LogisticRegression # Split the data into train and test data train_data = model_df [ model_df [ 'date_dt' ] < '2021-01-01' ] . reset_index ( drop = True ) . sample ( frac = 1 ) test_data = model_df [ model_df [ 'date_dt' ] >= '2021-01-01' ] . reset_index ( drop = True ) # Use our previously built features set columns as Training vector # Use win flag as Target vector train_x , train_y = train_data [ feature_cols ], train_data [ 'win' ] test_x , test_y = test_data [ feature_cols ], test_data [ 'win' ] # Build a LogisticRegression model model = LogisticRegression ( verbose = 0 , solver = 'saga' , n_jobs =- 1 ) # Train the model print ( f 'Training on { len ( train_x ) : , } samples with { len ( feature_cols ) } features' ) model . fit ( train_x , train_y ) # Generate runner win predictions dog_win_probabilities = model . predict_proba ( test_x )[:, 1 ] test_data [ 'prob_LogisticRegression' ] = dog_win_probabilities # Normalise probabilities test_data [ 'prob_LogisticRegression' ] = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x / sum ( x )) # Create a boolean column for whether a dog has the higehst model prediction in a race test_dataset_size = test_data [ 'FastTrack_RaceId' ] . nunique () odds_win_prediction = test_data . groupby ( 'FastTrack_RaceId' )[ 'prob_LogisticRegression' ] . apply ( lambda x : x == max ( x )) odds_win_prediction_percent = len ( test_data [( odds_win_prediction == True ) & ( test_data [ 'win' ] == 1 )]) / test_dataset_size print ( f \"LogisticRegression strike rate: { odds_win_prediction_percent : .2% } \" ) from sklearn.metrics import brier_score_loss brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ 'prob_LogisticRegression' ]) print ( f 'LogisticRegression Brier score: { brier_score : .8 } ' ) # Predictions distribution import matplotlib.pyplot as plt import seaborn as sns bins = 100 sns . displot ( data = [ test_data [ 'prob_LogisticRegression' ], test_data [ 'StartPrice_probability' ]], kind = \"hist\" , bins = bins , height = 7 , aspect = 2 ) plt . title ( 'StartPrice vs LogisticRegression probabilities distribution' ) plt . xlabel ( 'Probability' ) plt . show () # Predictions calibration from sklearn.calibration import calibration_curve bins = 100 fig = plt . figure ( figsize = ( 12 , 9 )) # Generate calibration curves based on our probabilities cal_y , cal_x = calibration_curve ( test_data [ 'win' ], test_data [ 'prob_LogisticRegression' ], n_bins = bins ) # Plot against reference line plt . plot ( cal_x , cal_y , marker = 'o' , linewidth = 1 ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], '--' , color = 'gray' ) plt . title ( \"LogisticRegression calibration curve\" ); # Other classification models from matplotlib import pyplot from matplotlib.pyplot import figure from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestClassifier # Gradient Boosting Machines libraries from lightgbm import LGBMClassifier from xgboost import XGBClassifier from catboost import CatBoostClassifier # Common models parameters verbose = 0 learning_rate = 0.1 n_estimators = 100 # Train different types of models models = { 'LogisticRegression' : LogisticRegression ( verbose = 0 , solver = 'saga' , n_jobs =- 1 ), 'GradientBoostingClassifier' : GradientBoostingClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators , max_depth = 3 , max_features = 0.25 ), 'RandomForestClassifier' : RandomForestClassifier ( verbose = verbose , n_estimators = n_estimators , max_depth = 8 , max_features = 0.5 , n_jobs =- 1 ), 'LGBMClassifier' : LGBMClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators , force_col_wise = True ), 'XGBClassifier' : XGBClassifier ( verbosity = verbose , learning_rate = learning_rate , n_estimators = n_estimators , objective = 'binary:logistic' , use_label_encoder = False ), 'CatBoostClassifier' : CatBoostClassifier ( verbose = verbose , learning_rate = learning_rate , n_estimators = n_estimators ) } print ( f 'Training on { len ( train_x ) : , } samples with { len ( feature_cols ) } features' ) for key , model in models . items (): print ( f 'Fitting model { key } ' ) model . fit ( train_x , train_y ) # Calculate probabilities for each model on the test dataset probs_columns = [ 'StartPrice_probability' ] for key , model in models . items (): probs_column_key = f 'prob_ { key } ' # Calculate runner win probability dog_win_probs = model . predict_proba ( test_x )[:, 1 ] test_data [ probs_column_key ] = dog_win_probs # Normalise probabilities test_data [ probs_column_key ] = test_data . groupby ( 'FastTrack_RaceId' )[ f 'prob_ { key } ' ] . apply ( lambda x : x / sum ( x )) probs_columns . append ( probs_column_key ) # Calculate model strike rate and Brier score across models # Create a boolean column for whether a dog has the higehst model prediction in a race. # Do the same for the starting price as a comparison test_dataset_size = test_data [ 'FastTrack_RaceId' ] . nunique () odds_win_prediction = test_data . groupby ( 'FastTrack_RaceId' )[ 'StartPrice_probability' ] . apply ( lambda x : x == max ( x )) odds_win_prediction_percent = len ( test_data [( odds_win_prediction == True ) & ( test_data [ 'win' ] == 1 )]) / test_dataset_size brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ 'StartPrice_probability' ]) print ( f 'Starting Price strike rate: { odds_win_prediction_percent : .2% } Brier score: { brier_score : .8 } ' ) for key , model in models . items (): predicted_winners = test_data . groupby ( 'FastTrack_RaceId' )[ f 'prob_ { key } ' ] . apply ( lambda x : x == max ( x )) strike_rate = len ( test_data [( predicted_winners == True ) & ( test_data [ 'win' ] == 1 )]) / test_data [ 'FastTrack_RaceId' ] . nunique () brier_score = brier_score_loss ( test_data [ 'win' ], test_data [ f 'prob_ { key } ' ]) print ( f ' { key . ljust ( 30 ) } strike rate: { strike_rate : .2% } Brier score: { brier_score : .8 } ' ) # Visualise model predictions # Display and format sample results def highlight_max ( s , props = '' ): return np . where ( s == np . nanmax ( s . values ), props , '' ) def highlight_min ( s , props = '' ): return np . where ( s == np . nanmin ( s . values ), props , '' ) test_data [ probs_columns ] . sample ( 20 ) . style \\ . bar ( color = '#FFA07A' , vmin = 0.01 , vmax = 0.25 , axis = 1 ) \\ . apply ( highlight_max , props = 'color:red;' , axis = 1 ) \\ . apply ( highlight_min , props = 'color:blue;' , axis = 1 ) ## Display models feature importance from sklearn.preprocessing import normalize total_feature_importances = [] # Individual models feature importance for key , model in models . items (): figure ( figsize = ( 10 , 24 ), dpi = 80 ) if isinstance ( model , LogisticRegression ): feature_importance = model . coef_ [ 0 ] else : feature_importance = model . feature_importances_ feature_importance = normalize ( feature_importance [:, np . newaxis ], axis = 0 ) . ravel () total_feature_importances . append ( feature_importance ) pyplot . barh ( feature_cols , feature_importance ) pyplot . xlabel ( f ' { key } Features Importance' ) pyplot . show () # Overall feature importance avg_feature_importances = np . asarray ( total_feature_importances ) . mean ( axis = 0 ) figure ( figsize = ( 10 , 24 ), dpi = 80 ) pyplot . barh ( feature_cols , avg_feature_importances ) pyplot . xlabel ( 'Overall Features Importance' ) pyplot . show ()","title":"Complete Code"},{"location":"modelling/greyhoundModellingPython/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/greyhoundModellingPythonREADME/","text":"Betfair Greyhound Modelling Project Setup This project assume you have a developmemnt environment with Python and pip installed Clone project git clone https://github.com/BrunoChauvet/greyhound-modelling.git Install python libraries cd greyhound-modellin pip install --user -r requirements.txt Create a configuration file named .env in the rot folder and set required values: FAST_TRACK_API_KEY=<your key> Launch notebook jupyter notebook Notebooks Below is the list of notebooks with detailed examples Logistic Regression logistic_regression provides a step-by-step tutorial from fetching Greyhound racing data from FastTrack API to generating win probabilities using Scikit-learn and various Regression techniques. Feature Importance TODO Models Ensemble TODO","title":"Betfair Greyhound Modelling"},{"location":"modelling/greyhoundModellingPythonREADME/#betfair-greyhound-modelling","text":"","title":"Betfair Greyhound Modelling"},{"location":"modelling/greyhoundModellingPythonREADME/#project-setup","text":"This project assume you have a developmemnt environment with Python and pip installed Clone project git clone https://github.com/BrunoChauvet/greyhound-modelling.git Install python libraries cd greyhound-modellin pip install --user -r requirements.txt Create a configuration file named .env in the rot folder and set required values: FAST_TRACK_API_KEY=<your key> Launch notebook jupyter notebook","title":"Project Setup"},{"location":"modelling/greyhoundModellingPythonREADME/#notebooks","text":"Below is the list of notebooks with detailed examples","title":"Notebooks"},{"location":"modelling/greyhoundModellingPythonREADME/#logistic-regression","text":"logistic_regression provides a step-by-step tutorial from fetching Greyhound racing data from FastTrack API to generating win probabilities using Scikit-learn and various Regression techniques.","title":"Logistic Regression"},{"location":"modelling/greyhoundModellingPythonREADME/#feature-importance","text":"TODO","title":"Feature Importance"},{"location":"modelling/greyhoundModellingPythonREADME/#models-ensemble","text":"TODO","title":"Models Ensemble"},{"location":"modelling/howToModel/","text":"Intro to modelling Want to learn how to create your own predictive model using sports or racing data, but you don\u2019t know where to start? We\u2019re here to help. The Data Scientists at Betfair have put together the first few steps we suggest you take to get you started on your data modelling journey. We also run occasional data modelling workshops to help you get the basics down \u2013 reach out and let us know if you\u2019re interested in being notified about upcoming data events. Choose your language There are lots of programming languages to choose from. For our data modelling workshops we work in R and Python, as they\u2019re both relatively easy to learn and designed for working with data. If you\u2019re new to these languages, here are some resources that will help get you set up. Language 1: R What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Take a look at the some of the existing R libraries you can use if you want to connect to our API, including abettor and our Data Scientists\u2019 R repo . Language 2: Python What is Python? Download and install Anaconda Distribution \u2013 this will install Python and a heap of data science packages along with it Find a data source Finding quality data is crucial to being able to create a successful model. We have lots of historical Exchange data that we\u2019re happy to share, and there are lots of other sources of sports or racing specific data available online, depending on what you\u2019re looking for. For our workshops we use historical NBA odds data from the Exchange ( which you can download directly from here , along with NBA game data from a variety of sources including: ESPN.com NBA.com basketball-reference.com Stattleship\u2019s API Learn to Program Okay, so easier said than done, but you don't actually need a high level of programming knowledge to be able to build a decent model, and there are so many excellent resources available online that the barrier to entry is much lower than it's been in the past. These are some of our favourites if you want to learn to use R or Python for data modelling: Dataquest \u2013 free coding resource for learning both Python and R for data science Datacamp \u2013 another popular free resource to learn both R and Python for data science Codeacademy \u2013 free online programming courses with community engagement We've also shared a R repo for connecting with our API , which might make that part of the learning process easier for you, if you go down that path. Learn how to model data We\u2019ve put together some articles to give you an introduction to some of the different approaches you can take to modelling data, but again there are also lots of resources available online. Here are some good places to start: Work through the modelling tutorials we've put together using AFL and soccer data This Introduction to Tennis Modelling gives a good overview of ranking-based models, regression-based models and point-based models How we used ELO and machine learning as different approaches to modelling the recent World Cup Get your hands dirty The best way to learn is by doing. Make sure you have a solid foundation knowledge to work from, then get excited, get your hands dirty and see what you can create! Here are a final few thoughts to help you decide where to from here: Make sure you\u2019ve got your betting basics and wagering fundamentals knowledge solid Learn about the importance of ratings and prices and get inspired by the models created by our Data Scientists Consider how you could use our API in building and automating your model Read about how successful some of our customers have been in their modelling journeys","title":"Intro to modelling"},{"location":"modelling/howToModel/#intro-to-modelling","text":"Want to learn how to create your own predictive model using sports or racing data, but you don\u2019t know where to start? We\u2019re here to help. The Data Scientists at Betfair have put together the first few steps we suggest you take to get you started on your data modelling journey. We also run occasional data modelling workshops to help you get the basics down \u2013 reach out and let us know if you\u2019re interested in being notified about upcoming data events.","title":"Intro to modelling"},{"location":"modelling/howToModel/#choose-your-language","text":"There are lots of programming languages to choose from. For our data modelling workshops we work in R and Python, as they\u2019re both relatively easy to learn and designed for working with data. If you\u2019re new to these languages, here are some resources that will help get you set up.","title":"Choose your language"},{"location":"modelling/howToModel/#language-1-r","text":"What is R? Download and install R \u2013 get the language set up on your computer Download and install RStudio \u2013 you\u2019ll need a program to develop in, and this one is custom-designed to work with R Take a look at the some of the existing R libraries you can use if you want to connect to our API, including abettor and our Data Scientists\u2019 R repo .","title":"Language 1: R"},{"location":"modelling/howToModel/#language-2-python","text":"What is Python? Download and install Anaconda Distribution \u2013 this will install Python and a heap of data science packages along with it","title":"Language 2: Python"},{"location":"modelling/howToModel/#find-a-data-source","text":"Finding quality data is crucial to being able to create a successful model. We have lots of historical Exchange data that we\u2019re happy to share, and there are lots of other sources of sports or racing specific data available online, depending on what you\u2019re looking for. For our workshops we use historical NBA odds data from the Exchange ( which you can download directly from here , along with NBA game data from a variety of sources including: ESPN.com NBA.com basketball-reference.com Stattleship\u2019s API","title":"Find a data source"},{"location":"modelling/howToModel/#learn-to-program","text":"Okay, so easier said than done, but you don't actually need a high level of programming knowledge to be able to build a decent model, and there are so many excellent resources available online that the barrier to entry is much lower than it's been in the past. These are some of our favourites if you want to learn to use R or Python for data modelling: Dataquest \u2013 free coding resource for learning both Python and R for data science Datacamp \u2013 another popular free resource to learn both R and Python for data science Codeacademy \u2013 free online programming courses with community engagement We've also shared a R repo for connecting with our API , which might make that part of the learning process easier for you, if you go down that path.","title":"Learn to Program"},{"location":"modelling/howToModel/#learn-how-to-model-data","text":"We\u2019ve put together some articles to give you an introduction to some of the different approaches you can take to modelling data, but again there are also lots of resources available online. Here are some good places to start: Work through the modelling tutorials we've put together using AFL and soccer data This Introduction to Tennis Modelling gives a good overview of ranking-based models, regression-based models and point-based models How we used ELO and machine learning as different approaches to modelling the recent World Cup","title":"Learn how to model data"},{"location":"modelling/howToModel/#get-your-hands-dirty","text":"The best way to learn is by doing. Make sure you have a solid foundation knowledge to work from, then get excited, get your hands dirty and see what you can create! Here are a final few thoughts to help you decide where to from here: Make sure you\u2019ve got your betting basics and wagering fundamentals knowledge solid Learn about the importance of ratings and prices and get inspired by the models created by our Data Scientists Consider how you could use our API in building and automating your model Read about how successful some of our customers have been in their modelling journeys","title":"Get your hands dirty"},{"location":"modelling/howToModelTheAusOpen/","text":"How to model the Australian Open Betfair\u2019s Data Scientists Team are putting together a collection of articles on How to Build a Model and submit an entry to the Betfair Aus Open Datathon . This article will outline their thought process and share their approach. Subsequent articles will be posted with code examples that outline how this approach can be put into practice. Tools for creating our model We will be providing a step by step tutorial in two languages \u2013 Python and R. Tutorial in Python Tutorial in R These are the two most popular languages used in data science nowadays. Both code examples will follow identical approaches. Tournament structure The Datathon structure requires contestants to predict every possible tournament match-up only using data that is available at the start of the tournament. This means we can\u2019t use information from previous rounds (data from Round 1 matches for potential Round 2 matches and so on). For example, if we were to just train our model on all the tennis matches in the data set, our model would have been trained assuming that it had the result from previous rounds in the Australian Open. But this isn\u2019t the case, so we need to account for this nuance of the competition. We need to ensure that we don\u2019t include previous round data from the same tournament in the way we structure our features for predicting results. How to set up data and features In predictive modelling language \u2013 features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options Training the model We can train the model on every tennis match in the data set or We can only train the model on Australian Open matches Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. In the end, we decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface \u2013 hard court. Next decision is to decide the features (or the metrics we feed into the model, which makes the decision on who the winner is going to be). We don\u2019t have a definitive list of features that we will use, but we will most likely keep the number of features quite low (between 4-5). Features set Likely features may include: ELO First serve % Winners-unforced error ratio We will also use the difference between opponents' statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. This will reduce the dimensionality of the model. A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, the difference in ELO rating etc. Target variable Our target variable (what we are trying to predict) is whether player A wins or not against player B. In machine learning terms this is a classification problem. The output will be a probability number between 0 and 1. A number closer to 0 means Player B is likely to win, and a number closer to 1 will mean Player A is likely to win. Another positive of a probabilistic outcome is that they can easily be converted to odds, and can also be compared with the historical Betfair odds that have been provided, and test if our model would have been profitable for previous seasons. Sports modelling nuances Sports data is inherently complex to model. Generally when predicting something, like \u201cwill it rain today\u201d, you have information for that day, such as the temperature, which you can use in formulating your prediction. However with sports data, you cannot use the majority of information that is provided in the raw dataset, such as aces, winners, etc, as this will create what is called feature leakage \u2013 using data from after the event, which you won\u2019t have access to before the event, to predict the result. You will also need to use historic results in such a way that will have predictive power for the sports event that you are trying to predict. This means that you run into little nuances like needing to use rolling averages, as well as whether to model each match on a single row or multiple rows. In the next article in this series, we will show you how we tackle this problem using code examples that anyone can replicate. Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"How to model the Australian Open"},{"location":"modelling/howToModelTheAusOpen/#how-to-model-the-australian-open","text":"Betfair\u2019s Data Scientists Team are putting together a collection of articles on How to Build a Model and submit an entry to the Betfair Aus Open Datathon . This article will outline their thought process and share their approach. Subsequent articles will be posted with code examples that outline how this approach can be put into practice.","title":"How to model the Australian Open"},{"location":"modelling/howToModelTheAusOpen/#tools-for-creating-our-model","text":"We will be providing a step by step tutorial in two languages \u2013 Python and R. Tutorial in Python Tutorial in R These are the two most popular languages used in data science nowadays. Both code examples will follow identical approaches.","title":"Tools for creating our model"},{"location":"modelling/howToModelTheAusOpen/#tournament-structure","text":"The Datathon structure requires contestants to predict every possible tournament match-up only using data that is available at the start of the tournament. This means we can\u2019t use information from previous rounds (data from Round 1 matches for potential Round 2 matches and so on). For example, if we were to just train our model on all the tennis matches in the data set, our model would have been trained assuming that it had the result from previous rounds in the Australian Open. But this isn\u2019t the case, so we need to account for this nuance of the competition. We need to ensure that we don\u2019t include previous round data from the same tournament in the way we structure our features for predicting results.","title":"Tournament structure"},{"location":"modelling/howToModelTheAusOpen/#how-to-set-up-data-and-features","text":"In predictive modelling language \u2013 features are data metrics we use to predict an outcome or target variable. We have several choices to make before we get to the prediction phase. What are the features? How do we structure the outcome variable? What does each row mean? Do we use all data or just a subset? We narrowed it down to two options Training the model We can train the model on every tennis match in the data set or We can only train the model on Australian Open matches Doing Option 1 would mean we have a lot more data to build a strong model, but it might be challenging to work around the constraints described in the tournament structure. Doing Option 2 fits better from that angle but leaves us with very few matches to train our model on. In the end, we decided to go with an option that combines strengths from both approaches, by training the model on matches from the Aus Open and the US Open because both grand slams are played on the same surface \u2013 hard court. Next decision is to decide the features (or the metrics we feed into the model, which makes the decision on who the winner is going to be). We don\u2019t have a definitive list of features that we will use, but we will most likely keep the number of features quite low (between 4-5). Features set Likely features may include: ELO First serve % Winners-unforced error ratio We will also use the difference between opponents' statistics (Difference of Averages), such as the difference between average first serve % in a single column rather than Player 1\u2019s first serve % and Player 2\u2019s first serve % in two separate columns. This will reduce the dimensionality of the model. A typical row of the transformed data will look like this \u2013 For a match between Player A \u2013 Roger Federer and Player B \u2013 Rafael Nadal, we will have a bunch of features like the difference in first serve %, the difference in ELO rating etc.","title":"How to set up data and features"},{"location":"modelling/howToModelTheAusOpen/#target-variable","text":"Our target variable (what we are trying to predict) is whether player A wins or not against player B. In machine learning terms this is a classification problem. The output will be a probability number between 0 and 1. A number closer to 0 means Player B is likely to win, and a number closer to 1 will mean Player A is likely to win. Another positive of a probabilistic outcome is that they can easily be converted to odds, and can also be compared with the historical Betfair odds that have been provided, and test if our model would have been profitable for previous seasons.","title":"Target variable"},{"location":"modelling/howToModelTheAusOpen/#sports-modelling-nuances","text":"Sports data is inherently complex to model. Generally when predicting something, like \u201cwill it rain today\u201d, you have information for that day, such as the temperature, which you can use in formulating your prediction. However with sports data, you cannot use the majority of information that is provided in the raw dataset, such as aces, winners, etc, as this will create what is called feature leakage \u2013 using data from after the event, which you won\u2019t have access to before the event, to predict the result. You will also need to use historic results in such a way that will have predictive power for the sports event that you are trying to predict. This means that you run into little nuances like needing to use rolling averages, as well as whether to model each match on a single row or multiple rows. In the next article in this series, we will show you how we tackle this problem using code examples that anyone can replicate.","title":"Sports modelling nuances"},{"location":"modelling/howToModelTheAusOpen/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/soccerEloTutorialR/","text":"Using an Elo approach to model soccer in R This tutorial was written as part of the 2021 Euro & Copa America Datathon competition Elo modelling Elo modelling is a commonly-used approach towards creating rating systems in sport. Originally devised by Arpad Elo for ranking chess players, the popularity of Elo modelling has grown massively since its first official use in the 1960s to the point where it is now widely used to model just about every professional sporting code across the globe. From a set of Elo ratings we can construct win/loss probabilities for given match-ups between two teams using a simple formula which takes the difference between the two teams' ratings and outputs the probability of each team winning or losing the match. A very basic yet very effective approach! To read more the inner workings of Elo models, take a read of this article for a detailed run-down of how the mathematics behind an Elo rating system works in the context of tennis matches. Elo Models & Soccer Elo-based systems lend themselves particularly well to modelling soccer, so much so that publicly available Elo ratings systems ( www.eloratings.net for international soccer for example) have been adopted by professional bodies to help seed tournaments and create fairer fixtures. This tutorial aims to serve as guide of how to build a basic soccer Elo model with a particular focus on the 2021 editions of the Euro and Copa America, as they will serve as the subject of Betfair's Datathon . To follow along with this tutorial you will need two things: This code is written in R, and hence you will need to have R and RStudio running on your system if you wish to follow along. You will also need the historical data set provided for Betfair's 2021 Euro & Copa America Datathon. For full access to the data set including international soccer fixtures from 2014 to 2021 you can register for the Datathon here , or alternatively if you are reading this tutorial after the Datathon has concluded, please reach out to datathon@betfair.com.au for data access. In the meantime, click here for a sample of the data which will be enough to allow the code to run effectively. Betfair Datathon If you're interested in competing in the 2021 Euro & Copa America Datathon competition make sure you head to the Hub , register and download the bespoke data set provided and get your model submitted before 11 June for your chance to win part of the prize pool. Let's get started! Load Packages & Import Data The first thing to do is load the packages that will be required to run the Elo model and to read in the historic data from wherever it is stored on your machine. The main package to take notice of here is the elo package. As the name suggests, this package is for running Elo models. The MLmetrics package will be used towards the end of the tutorial to back-test the accuracy of our model. library(readr) library(dplyr) library(lubridate) library(elo) library(MLmetrics) raw_data <- read_csv(\"datathon_initial_form_data.csv\") When exploring a new data set, it is always good practice to first get an idea of the what is included in the data set. colnames(raw_data) Something noticeable is that the data set does not have a feature included to indicate the result of each match. Let's start by creating two new columns: one showing the result from the home team's perspective and another from the away team's perspective. We'll also create a feature to show the margin of the match, i.e. the absolute goal difference between the two sides. raw_data <- raw_data %>% mutate(home_result = case_when(home_team_goal_count > away_team_goal_count ~ 1, home_team_goal_count < away_team_goal_count ~ 0, home_team_goal_count == away_team_goal_count ~ 0.5), away_result = case_when(home_team_goal_count < away_team_goal_count ~ 1, home_team_goal_count > away_team_goal_count ~ 0, home_team_goal_count == away_team_goal_count ~ 0.5), margin = abs(home_team_goal_count - away_team_goal_count)) Let's also make sure the date_GMT column is formatted correctly as a date. We can do that using the parse_date_time function from the lubridate package. raw_data <- raw_data %>% mutate(date_GMT = parse_date_time(raw_data$date_GMT , \"mdY_HM\")) Running the Elo Model Now comes the fun part - setting the Elo model in action! There are a number of steps we could have taken before getting to this point to make the model a touch more complex (and possibly more accurate as a result), however in the interest of keeping this tutorial as accessible and easy to follow as possible let's jump straight into running an Elo model based on our data set. To run the Elo model we will need to use the elo.run function from the elo package. The elo.run function requires we input a formula as an argument which tells the function which columns list the two teams, which column is our target/outcome variable, as well as any other features we want to include in our model. We can use the home_result column we've created to identify the result from the home team's perspective. We can also input a k value which essentially dictates the maximum number of Elo points that can be won or lost in a single match. Here we could set k to be a constant (such as 30), however to add an extra layer of (very slight) complexity, we can instead choose to use a variable k value which is dictated by the margin of the match using a formula of k = 30 + 30*margin . This means that for every goal the margin increases by, the k value will increase by 30, starting with a base k value of 30 for draws ( margin equals 0 ). The idea here is simply to help account for and reward/punish teams winning/losing by bigger margins compared to closer matches. elo_model <- elo.run(data = raw_data, formula = home_result ~ home_team_name + away_team_name + k(30 + 30*margin)) Let's take a look at the last few matches in our training sample to get a better view of the Elo model in action. elo_results <- elo_model %>% as.data.frame() elo_results %>% tail(n = 10) We can also now check out what the latest Elo rankings for each team in the data set! Let's look at the top 20. Keep in mind that teams start with an Elo rating of 1500, so any team with an Elo greater 1500 can be considered above average, while those with Elo ratings below 1500 are therefore below average. final_elos <- final.elos(elo_model) final_elos %>% sort(decreasing = TRUE) %>% head(n = 20) Accounting for Draws Something that we need to acknowledge both generally and also in relation to the Euro & Copa America Datathon is that Elo models are best suited to sporting contexts with binary outcomes - i.e. win or lose - however in most soccer fixture we also have the possibility of a draw to account for. We are still able to ensure that Elo ratings update appropriately after a draw using 0.5 wins to denote the situation in which a draw occurs, but when it comes to making probabilistic win predictions using the Elo ratings (see p.A column above for example) things get a little more confusing. For the Euro & Copa America Datathon, we also need to consider the fact that draws are a possibility during group stage match-ups, therefore we need to develop a workaround option to include draw probabilities. One way to do this - and the method that will be adopted for this tutorial - is to find the historic rate at which two teams of a certain Elo prediction split ending up drawing their matches. For example, how often do matches in which an Elo model deems Team A an 80% chance of winning and Team B a 20% chance of winning result in a draw? How about a 70%-30% split? We can find these draw rates for a range of probability points between 0 and 1 and use them to redistribute win/loss probabilities accordingly. Let's start the process by finding historic draw rates. We will bucket matches at 5% increments according to the home team's probability of winning the match according to the model. draw_rates <- data.frame(win_prob = elo_model$elos[,3], win_loss_draw = elo_model$elos[,4]) %>% mutate(prob_bucket = abs(round((win_prob)*20)) / 20) %>% # Round the predicted win probabilities to the nearest 0.05 group_by(prob_bucket) %>% summarise(draw_prob = sum(ifelse(win_loss_draw == 0.5, 1, 0)) / n()) # Calculate the rate their was a draw for this win prob - elo package codes a draw as a 0.5 draw_rates %>% head(n=20) We now have data which will help us deem how likely a match-up between two teams is to end up in a draw! The next step is to merge this data in with our existing data set. We also need to include the win/loss probabilities for each match that we've already found using our Elo model so that we may tweak them to include for the possibility of a draw. We'll also pull in the actual Elo ratings for each team for completeness. data_with_probabilities <- raw_data %>% select(tournament, date_GMT, home_team_name, away_team_name, home_result, away_result) %>% # Remove some redundant columns mutate(home_elo = elo_results$elo.A - elo_results$update.A, # Add in home team's elo rating (need to subtract the points update to obtain pre-match rating) away_elo = elo_results$elo.B - elo_results$update.B, # Add in away team's elo rating (need to subtract the points update to obtain pre-match rating) home_prob = elo_results$p.A, # Add in home team's win/loss probability away_prob = 1 - home_prob) %>% # Add in away team's win/loss probability mutate(prob_bucket = round(20*home_prob)/20) %>% # Bucket the home team's win/loss probability into a rounded increment of 0.05 left_join(draw_rates, by = \"prob_bucket\") %>% # Join in our historic draw rates using the probability buckets relocate(draw_prob, .after = home_prob) %>% select(-prob_bucket) Having now brought the draw probability for each match into the data frame, we need to redistribute the win and loss probabilities so that Pr(win) + Pr(draw) + Pr(loss) sums to exactly 1. We can do this by simply subtracting the draw probability from each of the win and loss probabilities in a proportional manner. See below: data_with_probabilities <- data_with_probabilities %>% mutate(home_prob = home_prob - home_prob * draw_prob, # Redistribute home team's probabilities proportionally to create win/draw/loss probabilities away_prob = away_prob - away_prob * draw_prob) # Redistribute away team's probabilities proportionally to create win/draw/loss probabilities data_with_probabilities %>% select(home_team_name, away_team_name, home_prob, draw_prob, away_prob) %>% tail(n=10) And there you have it! We've now come up with win, draw and loss probabilities for each match-up in our data set! Keep in mind that if we were to be focusing on knockout matches (i.e. where no draws are possible), we could have just skipped the previous few steps as we already had binary win-loss probabilities as direct outputs from our Elo model. Back Testing The last step in our modelling process is to back test against a subset of our data to get an idea of our model's accuracy. We can use the MLmetrics package to run a log loss function on our subset. Let's look at how the model performed when we limit the data set to include only the most recent matches from early 2021. matches_2021 <- data_with_probabilities %>% filter(year(date_GMT) == 2021) %>% # Filter down to only 2021 matches mutate(home_win = ifelse(home_result == 1, 1, 0), # Include new columns which show the true outcome of the match draw = ifelse(home_result == 0.5, 1, 0), away_win = ifelse(away_result == 1, 1, 0)) %>% select(date_GMT, home_team_name, away_team_name, home_prob, draw_prob, away_prob, home_win, draw, away_win) # Run the multinomial log loss function from MLmetrics to output a log loss score for our sample MultiLogLoss( y_pred = matches_2021[,c(\"home_prob\", \"draw_prob\", \"away_prob\")] %>% as.matrix(), y_true = matches_2021[,c(\"home_win\", \"draw\", \"away_win\")] %>% as.matrix() ) A pretty good result for such a simple Elo model! Making Future Predictions Okay, so now our Elo model is set in place, we have the latest set of Elo ratings for each team and we've back tested our model. We can now apply our model to future matches to obtain probabilities for match-ups that are yet to occur. Again we can do this using the elo package. This time we will use the function elo.prob , which takes two teams and outputs the probability of the first team winning the match-up. Like before, this function only considers win/loss outcomes to be possible, so if we were to also be looking to generate draw probabilities for a future match-up, we can just go through the exact same process as we did previously (i.e. make a binary win/loss prediction, merge in historic draw rates for various probabilities, redistribute accordingly). Let's just keep this simple for now though and focus on win and loss probabilities. We'll put together a small dataframe of matches and see what our model thinks - we've gone for hypothetical match-ups of Brazil v Argentina, England v France, Spain v Germany and an obligatory 2006 World Cup rematch of Australia v Italy. future_matches <- data.frame( team_a = c(\"Brazil\", \"England\", \"Spain\", \"Australia\"), team_b = c(\"Argentina\", \"France\", \"Germany\", \"Italy\")) %>% mutate(elo_a = final_elos[team_a], elo_b = final_elos[team_b], team_a_win_prob = elo.prob(elo.A = elo_a, elo.B = elo_b) ) future_matches Conclusions & Areas for Improvement Elo modelling can be a surprisingly accurate modelling technique given how simple it is to implement. This tutorial gives a very basic framework from which you are free to build a more intricate model with more detailed inputs and features. Some things that you might want to consider adding to this Elo model: Home ground advantage Key match statistics (i.e. shots on target, possession %, etc.) Whether the match was a dead rubber (teams may take the foot off the gas if they don't need to win to advance to the next stage of a tournament) Selected team line-ups (were key players missing?) Remember, an Elo model can be as complex or as simple as you want it to be - in some cases it might be better to keep it basic! We hope you've found this tutorial useful - if you have any questions regarding predictive data modelling please reach out to automation@betfair.com.au . Good luck in the Datathon ! Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Elo soccer tutorial in R"},{"location":"modelling/soccerEloTutorialR/#using-an-elo-approach-to-model-soccer-in-r","text":"This tutorial was written as part of the 2021 Euro & Copa America Datathon competition","title":"Using an Elo approach to model soccer in R"},{"location":"modelling/soccerEloTutorialR/#elo-modelling","text":"Elo modelling is a commonly-used approach towards creating rating systems in sport. Originally devised by Arpad Elo for ranking chess players, the popularity of Elo modelling has grown massively since its first official use in the 1960s to the point where it is now widely used to model just about every professional sporting code across the globe. From a set of Elo ratings we can construct win/loss probabilities for given match-ups between two teams using a simple formula which takes the difference between the two teams' ratings and outputs the probability of each team winning or losing the match. A very basic yet very effective approach! To read more the inner workings of Elo models, take a read of this article for a detailed run-down of how the mathematics behind an Elo rating system works in the context of tennis matches.","title":"Elo modelling"},{"location":"modelling/soccerEloTutorialR/#elo-models-soccer","text":"Elo-based systems lend themselves particularly well to modelling soccer, so much so that publicly available Elo ratings systems ( www.eloratings.net for international soccer for example) have been adopted by professional bodies to help seed tournaments and create fairer fixtures. This tutorial aims to serve as guide of how to build a basic soccer Elo model with a particular focus on the 2021 editions of the Euro and Copa America, as they will serve as the subject of Betfair's Datathon . To follow along with this tutorial you will need two things: This code is written in R, and hence you will need to have R and RStudio running on your system if you wish to follow along. You will also need the historical data set provided for Betfair's 2021 Euro & Copa America Datathon. For full access to the data set including international soccer fixtures from 2014 to 2021 you can register for the Datathon here , or alternatively if you are reading this tutorial after the Datathon has concluded, please reach out to datathon@betfair.com.au for data access. In the meantime, click here for a sample of the data which will be enough to allow the code to run effectively. Betfair Datathon If you're interested in competing in the 2021 Euro & Copa America Datathon competition make sure you head to the Hub , register and download the bespoke data set provided and get your model submitted before 11 June for your chance to win part of the prize pool. Let's get started!","title":"Elo Models &amp; Soccer"},{"location":"modelling/soccerEloTutorialR/#load-packages-import-data","text":"The first thing to do is load the packages that will be required to run the Elo model and to read in the historic data from wherever it is stored on your machine. The main package to take notice of here is the elo package. As the name suggests, this package is for running Elo models. The MLmetrics package will be used towards the end of the tutorial to back-test the accuracy of our model. library(readr) library(dplyr) library(lubridate) library(elo) library(MLmetrics) raw_data <- read_csv(\"datathon_initial_form_data.csv\") When exploring a new data set, it is always good practice to first get an idea of the what is included in the data set. colnames(raw_data) Something noticeable is that the data set does not have a feature included to indicate the result of each match. Let's start by creating two new columns: one showing the result from the home team's perspective and another from the away team's perspective. We'll also create a feature to show the margin of the match, i.e. the absolute goal difference between the two sides. raw_data <- raw_data %>% mutate(home_result = case_when(home_team_goal_count > away_team_goal_count ~ 1, home_team_goal_count < away_team_goal_count ~ 0, home_team_goal_count == away_team_goal_count ~ 0.5), away_result = case_when(home_team_goal_count < away_team_goal_count ~ 1, home_team_goal_count > away_team_goal_count ~ 0, home_team_goal_count == away_team_goal_count ~ 0.5), margin = abs(home_team_goal_count - away_team_goal_count)) Let's also make sure the date_GMT column is formatted correctly as a date. We can do that using the parse_date_time function from the lubridate package. raw_data <- raw_data %>% mutate(date_GMT = parse_date_time(raw_data$date_GMT , \"mdY_HM\"))","title":"Load Packages &amp; Import Data"},{"location":"modelling/soccerEloTutorialR/#running-the-elo-model","text":"Now comes the fun part - setting the Elo model in action! There are a number of steps we could have taken before getting to this point to make the model a touch more complex (and possibly more accurate as a result), however in the interest of keeping this tutorial as accessible and easy to follow as possible let's jump straight into running an Elo model based on our data set. To run the Elo model we will need to use the elo.run function from the elo package. The elo.run function requires we input a formula as an argument which tells the function which columns list the two teams, which column is our target/outcome variable, as well as any other features we want to include in our model. We can use the home_result column we've created to identify the result from the home team's perspective. We can also input a k value which essentially dictates the maximum number of Elo points that can be won or lost in a single match. Here we could set k to be a constant (such as 30), however to add an extra layer of (very slight) complexity, we can instead choose to use a variable k value which is dictated by the margin of the match using a formula of k = 30 + 30*margin . This means that for every goal the margin increases by, the k value will increase by 30, starting with a base k value of 30 for draws ( margin equals 0 ). The idea here is simply to help account for and reward/punish teams winning/losing by bigger margins compared to closer matches. elo_model <- elo.run(data = raw_data, formula = home_result ~ home_team_name + away_team_name + k(30 + 30*margin)) Let's take a look at the last few matches in our training sample to get a better view of the Elo model in action. elo_results <- elo_model %>% as.data.frame() elo_results %>% tail(n = 10) We can also now check out what the latest Elo rankings for each team in the data set! Let's look at the top 20. Keep in mind that teams start with an Elo rating of 1500, so any team with an Elo greater 1500 can be considered above average, while those with Elo ratings below 1500 are therefore below average. final_elos <- final.elos(elo_model) final_elos %>% sort(decreasing = TRUE) %>% head(n = 20)","title":"Running the Elo Model"},{"location":"modelling/soccerEloTutorialR/#accounting-for-draws","text":"Something that we need to acknowledge both generally and also in relation to the Euro & Copa America Datathon is that Elo models are best suited to sporting contexts with binary outcomes - i.e. win or lose - however in most soccer fixture we also have the possibility of a draw to account for. We are still able to ensure that Elo ratings update appropriately after a draw using 0.5 wins to denote the situation in which a draw occurs, but when it comes to making probabilistic win predictions using the Elo ratings (see p.A column above for example) things get a little more confusing. For the Euro & Copa America Datathon, we also need to consider the fact that draws are a possibility during group stage match-ups, therefore we need to develop a workaround option to include draw probabilities. One way to do this - and the method that will be adopted for this tutorial - is to find the historic rate at which two teams of a certain Elo prediction split ending up drawing their matches. For example, how often do matches in which an Elo model deems Team A an 80% chance of winning and Team B a 20% chance of winning result in a draw? How about a 70%-30% split? We can find these draw rates for a range of probability points between 0 and 1 and use them to redistribute win/loss probabilities accordingly. Let's start the process by finding historic draw rates. We will bucket matches at 5% increments according to the home team's probability of winning the match according to the model. draw_rates <- data.frame(win_prob = elo_model$elos[,3], win_loss_draw = elo_model$elos[,4]) %>% mutate(prob_bucket = abs(round((win_prob)*20)) / 20) %>% # Round the predicted win probabilities to the nearest 0.05 group_by(prob_bucket) %>% summarise(draw_prob = sum(ifelse(win_loss_draw == 0.5, 1, 0)) / n()) # Calculate the rate their was a draw for this win prob - elo package codes a draw as a 0.5 draw_rates %>% head(n=20) We now have data which will help us deem how likely a match-up between two teams is to end up in a draw! The next step is to merge this data in with our existing data set. We also need to include the win/loss probabilities for each match that we've already found using our Elo model so that we may tweak them to include for the possibility of a draw. We'll also pull in the actual Elo ratings for each team for completeness. data_with_probabilities <- raw_data %>% select(tournament, date_GMT, home_team_name, away_team_name, home_result, away_result) %>% # Remove some redundant columns mutate(home_elo = elo_results$elo.A - elo_results$update.A, # Add in home team's elo rating (need to subtract the points update to obtain pre-match rating) away_elo = elo_results$elo.B - elo_results$update.B, # Add in away team's elo rating (need to subtract the points update to obtain pre-match rating) home_prob = elo_results$p.A, # Add in home team's win/loss probability away_prob = 1 - home_prob) %>% # Add in away team's win/loss probability mutate(prob_bucket = round(20*home_prob)/20) %>% # Bucket the home team's win/loss probability into a rounded increment of 0.05 left_join(draw_rates, by = \"prob_bucket\") %>% # Join in our historic draw rates using the probability buckets relocate(draw_prob, .after = home_prob) %>% select(-prob_bucket) Having now brought the draw probability for each match into the data frame, we need to redistribute the win and loss probabilities so that Pr(win) + Pr(draw) + Pr(loss) sums to exactly 1. We can do this by simply subtracting the draw probability from each of the win and loss probabilities in a proportional manner. See below: data_with_probabilities <- data_with_probabilities %>% mutate(home_prob = home_prob - home_prob * draw_prob, # Redistribute home team's probabilities proportionally to create win/draw/loss probabilities away_prob = away_prob - away_prob * draw_prob) # Redistribute away team's probabilities proportionally to create win/draw/loss probabilities data_with_probabilities %>% select(home_team_name, away_team_name, home_prob, draw_prob, away_prob) %>% tail(n=10) And there you have it! We've now come up with win, draw and loss probabilities for each match-up in our data set! Keep in mind that if we were to be focusing on knockout matches (i.e. where no draws are possible), we could have just skipped the previous few steps as we already had binary win-loss probabilities as direct outputs from our Elo model.","title":"Accounting for Draws"},{"location":"modelling/soccerEloTutorialR/#back-testing","text":"The last step in our modelling process is to back test against a subset of our data to get an idea of our model's accuracy. We can use the MLmetrics package to run a log loss function on our subset. Let's look at how the model performed when we limit the data set to include only the most recent matches from early 2021. matches_2021 <- data_with_probabilities %>% filter(year(date_GMT) == 2021) %>% # Filter down to only 2021 matches mutate(home_win = ifelse(home_result == 1, 1, 0), # Include new columns which show the true outcome of the match draw = ifelse(home_result == 0.5, 1, 0), away_win = ifelse(away_result == 1, 1, 0)) %>% select(date_GMT, home_team_name, away_team_name, home_prob, draw_prob, away_prob, home_win, draw, away_win) # Run the multinomial log loss function from MLmetrics to output a log loss score for our sample MultiLogLoss( y_pred = matches_2021[,c(\"home_prob\", \"draw_prob\", \"away_prob\")] %>% as.matrix(), y_true = matches_2021[,c(\"home_win\", \"draw\", \"away_win\")] %>% as.matrix() ) A pretty good result for such a simple Elo model!","title":"Back Testing"},{"location":"modelling/soccerEloTutorialR/#making-future-predictions","text":"Okay, so now our Elo model is set in place, we have the latest set of Elo ratings for each team and we've back tested our model. We can now apply our model to future matches to obtain probabilities for match-ups that are yet to occur. Again we can do this using the elo package. This time we will use the function elo.prob , which takes two teams and outputs the probability of the first team winning the match-up. Like before, this function only considers win/loss outcomes to be possible, so if we were to also be looking to generate draw probabilities for a future match-up, we can just go through the exact same process as we did previously (i.e. make a binary win/loss prediction, merge in historic draw rates for various probabilities, redistribute accordingly). Let's just keep this simple for now though and focus on win and loss probabilities. We'll put together a small dataframe of matches and see what our model thinks - we've gone for hypothetical match-ups of Brazil v Argentina, England v France, Spain v Germany and an obligatory 2006 World Cup rematch of Australia v Italy. future_matches <- data.frame( team_a = c(\"Brazil\", \"England\", \"Spain\", \"Australia\"), team_b = c(\"Argentina\", \"France\", \"Germany\", \"Italy\")) %>% mutate(elo_a = final_elos[team_a], elo_b = final_elos[team_b], team_a_win_prob = elo.prob(elo.A = elo_a, elo.B = elo_b) ) future_matches","title":"Making Future Predictions"},{"location":"modelling/soccerEloTutorialR/#conclusions-areas-for-improvement","text":"Elo modelling can be a surprisingly accurate modelling technique given how simple it is to implement. This tutorial gives a very basic framework from which you are free to build a more intricate model with more detailed inputs and features. Some things that you might want to consider adding to this Elo model: Home ground advantage Key match statistics (i.e. shots on target, possession %, etc.) Whether the match was a dead rubber (teams may take the foot off the gas if they don't need to win to advance to the next stage of a tournament) Selected team line-ups (were key players missing?) Remember, an Elo model can be as complex or as simple as you want it to be - in some cases it might be better to keep it basic! We hope you've found this tutorial useful - if you have any questions regarding predictive data modelling please reach out to automation@betfair.com.au . Good luck in the Datathon !","title":"Conclusions &amp; Areas for Improvement"},{"location":"modelling/soccerEloTutorialR/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/soccerModellingTutorialPython/","text":"How to model the 2021 Euro & Copa America: Python Tutorial This tutorial was written as part of the 2021 Euro & Copa America Datathon competition The Task This notebook will outline how to train a simple classification model to predict the outcome of a soccer match using the dataset provided for the datathon. Reading data from file and get a raw dataset Data cleaning and feature engineering Training a model The tutorial covers the thought process of manipulating the dataset (why and how), some simple data cleaning, feature engineering and training a classification model. The tutorial DOES NOT delve deep into the fundamentals of machine learning, advanced feature engineering or model tuning. There are some helpful hints along the way though. Betfair Datathon If you're interested in competing in the 2021 Euro & Copa America Datathon competition make sure you head to the Hub , register and download the bespoke data set provided and get your model submitted before 11 June for your chance to win part of the prize pool. # import required libraries import numpy as np import pandas as pd import os import warnings warnings . filterwarnings ( 'ignore' ) Read data from file and get a raw dataset Change the data types - date column. We need the date column in good order for our tutorial. Here's a sample of the data set we're using for this tutorial. In general, it's a good idea to evaluate data types of all columns that we work with to ensure they are correct. df = pd . read_csv ( 'soccerData.csv' ) df [ 'date_GMT' ] = pd . to_datetime ( df [ 'date_GMT' ]) Get data columns and create raw dataset For this tutorial, let's take only a few stats columns to work with. Typically we would explore all features and then decide which data to discard. Goal counts Corners Total shots Shots on target Fouls Possession raw_match_stats = df [[ 'date_GMT' , 'home_team_name' , 'away_team_name' , 'home_team_goal_count' , 'away_team_goal_count' , 'home_team_corner_count' , 'away_team_corner_count' , 'home_team_shots' , 'away_team_shots' , 'home_team_shots_on_target' , 'away_team_shots_on_target' , 'home_team_fouls' , 'away_team_fouls' , 'home_team_possession' , 'away_team_possession' ,]] Clean data As a cleaning step, we order our data by date and drop rows with NA values. raw_match_stats = raw_match_stats . sort_values ( by = [ 'date_GMT' ], ascending = False ) raw_match_stats = raw_match_stats . dropna () Raw dataset This raw dataset is structured so that each match has an individual row and stats for both teams are on that row with columns titles \"home\" and \"away\". Our goal is to build a machine learning (ML) model that can predict the result of a soccer match. Given that we have some match stats, we will aim to use that information to predict a WIN, LOSS or DRAW. raw_match_stats date_GMT home_team_name away_team_name home_team_goal_count away_team_goal_count home_team_corner_count away_team_corner_count home_team_shots away_team_shots home_team_shots_on_target away_team_shots_on_target home_team_fouls away_team_fouls home_team_possession away_team_possession 2021-03-31 18:45:00 Spain Kosovo 3 1 9.0 2.0 25.0 4.0 9.0 2.0 9.0 10.0 80.0 20.0 2021-03-31 18:45:00 Scotland Faroe Islands 4 0 1.0 5.0 14.0 8.0 8.0 3.0 9.0 13.0 65.0 35.0 2021-03-31 18:45:00 Switzerland Finland 3 2 5.0 2.0 21.0 6.0 9.0 3.0 11.0 7.0 63.0 37.0 2021-03-31 18:45:00 Lithuania Italy 0 2 4.0 5.0 8.0 29.0 3.0 11.0 14.0 13.0 34.0 66.0 2021-03-31 18:45:00 Northern Ireland Bulgaria 0 0 12.0 2.0 16.0 4.0 5.0 2.0 17.0 17.0 70.0 30.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2014-06-14 16:00:00 Colombia Greece 3 0 4.0 4.0 9.0 13.0 5.0 8.0 20.0 13.0 44.0 56.0 2014-06-13 22:00:00 Chile Australia 3 1 3.0 1.0 7.0 12.0 5.0 3.0 10.0 18.0 59.0 41.0 2014-06-13 19:00:00 Spain Netherlands 1 5 4.0 1.0 8.0 6.0 5.0 6.0 6.0 18.0 61.0 39.0 2014-06-13 16:00:00 Mexico Cameroon 1 0 2.0 5.0 7.0 12.0 3.0 4.0 10.0 10.0 55.0 45.0 2014-06-12 20:00:00 Brazil Croatia 3 1 7.0 3.0 13.0 10.0 5.0 4.0 5.0 19.0 63.0 37.0 Data cleaning and feature engineering Target variable - Match Result Our machine learning model aims to predict the result of a match. This \"result\" is called the \"target variable\". Our dataset has no columns showing the match result. We will create two columns for the results for each team. One of these would become the target variable for our ML model. # create results columns for both home and away teams (W - win, D = Draw, L = Loss). raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] == raw_match_stats [ 'away_team_goal_count' ], 'home_team_result' ] = 'D' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] > raw_match_stats [ 'away_team_goal_count' ], 'home_team_result' ] = 'W' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] < raw_match_stats [ 'away_team_goal_count' ], 'home_team_result' ] = 'L' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] == raw_match_stats [ 'away_team_goal_count' ], 'away_team_result' ] = 'D' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] > raw_match_stats [ 'away_team_goal_count' ], 'away_team_result' ] = 'L' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] < raw_match_stats [ 'away_team_goal_count' ], 'away_team_result' ] = 'W' Average pre-match stats - Five match average Great! Now we have a dataset with many rows of data, with each row representing match stats and the match result (this would become our target variable). But our goal is to build an ML model that predicts the match result prior to the start of a match. Are the stats from that match what we need to build this ML model? No! When predicting a match outcome BEFORE the start of the match, we are forced to rely on match stats available to us from previous matches. Therefore, we need a dataset with the match result (target variable) and stats for each team heading into that match. For this tutorial, we will look at the average stats for each team in the five matches preceding each match. Lets look at how we can get the average stats for the previous 5 matches for each team at each match. Split the raw_match_stats to two datasets (home_team_stats and away_team_stats). Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match). At each row of this dataset, get the team name, find the stats for that team during the last 5 matches, and average these stats (avg_stats_per_team). Add these stats to the team_stats_per_match dataset. Why did we chose five matches? Why not 10? Should we average over a time period (matches in the last year perhaps?) rather than a number? What's the least number of matches available for each competing team in the dataset? These are all interesting questions that may improve our model. # Split the raw_match_stats to two datasets (home_team_stats and away_team_stats) home_team_stats = raw_match_stats [[ 'date_GMT' , 'home_team_name' , 'home_team_goal_count' , 'home_team_corner_count' , 'home_team_shots' , 'home_team_shots_on_target' , 'home_team_fouls' , 'home_team_possession' , 'home_team_result' ,]] away_team_stats = raw_match_stats [[ 'date_GMT' , 'away_team_name' , 'away_team_goal_count' , 'away_team_corner_count' , 'away_team_shots' , 'away_team_shots_on_target' , 'away_team_fouls' , 'away_team_possession' , 'away_team_result' ,]] # rename \"home_team\" and \"away_team\" columns home_team_stats . columns = [ col . replace ( 'home_team_' , '' ) for col in home_team_stats . columns ] away_team_stats . columns = [ col . replace ( 'away_team_' , '' ) for col in away_team_stats . columns ] # stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match) team_stats_per_match = home_team_stats . append ( away_team_stats ) # At each row of this dataset, get the team name, find the stats for that team during the last 5 matches, and average these stats (avg_stats_per_team). avg_stat_columns = [ 'goals_per_match' , 'corners_per_match' , 'shots_per_match' , 'shotsOnTarget_per_match' , 'fouls_per_match' , 'possession_per_match' ] stats_list = [] for index , row in team_stats_per_match . iterrows (): team_stats_last_five_matches = team_stats_per_match . loc [( team_stats_per_match [ 'name' ] == row [ 'name' ]) & ( team_stats_per_match [ 'date_GMT' ] < row [ 'date_GMT' ])] . sort_values ( by = [ 'date_GMT' ], ascending = False ) stats_list . append ( team_stats_last_five_matches . iloc [ 0 : 5 ,:] . mean ( axis = 0 ) . values [ 0 : 6 ]) avg_stats_per_team = pd . DataFrame ( stats_list , columns = avg_stat_columns ) # Add these stats to the team_stats_per_match dataset. team_stats_per_match = pd . concat ([ team_stats_per_match . reset_index ( drop = True ), avg_stats_per_team ], axis = 1 , ignore_index = False ) Reshape average pre-match stats Now that we have the average stats for each team going into every match, we can create a dataset similar to the raw_match_stats, where each row represents both teams from one match. Re-segment the home and away teams (name Team 1 and Team 2 rather than home and away). Combine at each match to get a dataset with a row representing each match. # Re-segment the home and away teams. home_team_stats = team_stats_per_match . iloc [: int ( team_stats_per_match . shape [ 0 ] / 2 ),:] away_team_stats = team_stats_per_match . iloc [ int ( team_stats_per_match . shape [ 0 ] / 2 ):,:] home_team_stats . columns = [ 'team_1_' + str ( col ) for col in home_team_stats . columns ] away_team_stats . columns = [ 'team_2_' + str ( col ) for col in away_team_stats . columns ] # Combine at each match to get a dataset with a row representing each match. # drop the NA rows (earliest match for each team, i.e no previous stats) match_stats = pd . concat ([ home_team_stats , away_team_stats . reset_index ( drop = True )], axis = 1 , ignore_index = False ) match_stats = match_stats . dropna () . reset_index ( drop = True ) Find the difference of stats between teams In our ML model, we will take the difference between Team 1 and Team 2 average stats as features. 6 new columns are created for this. Would we be better off using the raw stats for each team as features? Can we generate any other useful features from the dataset provided? Do we need to weigh the home and away teams because home teams win more often? # create columns with average stat differences between the two teams match_stats [ 'goals_per_match_diff' ] = ( match_stats [ 'team_1_goals_per_match' ] - match_stats [ 'team_2_goals_per_match' ]) match_stats [ 'corners_per_match_diff' ] = ( match_stats [ 'team_1_corners_per_match' ] - match_stats [ 'team_2_corners_per_match' ]) match_stats [ 'shots_per_match_diff' ] = ( match_stats [ 'team_1_shots_per_match' ] - match_stats [ 'team_2_shots_per_match' ]) match_stats [ 'shotsOnTarget_per_match_diff' ] = ( match_stats [ 'team_1_shotsOnTarget_per_match' ] - match_stats [ 'team_2_shotsOnTarget_per_match' ]) match_stats [ 'fouls_per_match_diff' ] = ( match_stats [ 'team_1_fouls_per_match' ] - match_stats [ 'team_2_fouls_per_match' ]) match_stats [ 'possession_per_match_diff' ] = ( match_stats [ 'team_1_possession_per_match' ] - match_stats [ 'team_2_possession_per_match' ]) match_stats team_1_date_GMT team_1_name team_1_goal_count team_1_corner_count team_1_shots team_1_shots_on_target team_1_fouls team_1_possession team_1_result team_1_goals_per_match ... team_2_shots_per_match team_2_shotsOnTarget_per_match team_2_fouls_per_match team_2_possession_per_match goals_per_match_diff corners_per_match_diff shots_per_match_diff shotsOnTarget_per_match_diff fouls_per_match_diff possession_per_match_diff 2021-03-31 18:45:00 Spain 3 9.0 25.0 9.0 9.0 80.0 W 2.2 ... 6.6 2.4 12.2 45.0 1.8 3.4 5.2 2.4 -1.6 24.6 2021-03-31 18:45:00 Scotland 4 1.0 14.0 8.0 9.0 65.0 W 0.8 ... 9.0 3.8 14.0 46.8 -0.2 2.2 4.6 2.4 -0.2 2.0 2021-03-31 18:45:00 Switzerland 3 5.0 21.0 9.0 11.0 63.0 W 1.8 ... 7.6 2.2 10.2 36.0 0.2 2.6 5.2 3.6 3.2 15.8 2021-03-31 18:45:00 Lithuania 0 4.0 8.0 3.0 14.0 34.0 L 0.8 ... 15.6 5.4 11.4 64.4 -1.6 -4.6 -7.2 -3.0 0.2 -17.2 2021-03-31 18:45:00 Northern Ireland 0 12.0 16.0 5.0 17.0 70.0 D 0.8 ... 8.8 3.2 16.2 48.6 -0.2 0.4 0.0 -0.2 -7.0 -2.8 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2014-06-19 16:00:00 Colombia 2 5.0 8.0 5.0 12.0 40.0 W 3.0 ... 19.0 8.0 12.0 56.0 1.0 -4.0 -10.0 -3.0 8.0 -12.0 2014-06-18 22:00:00 Cameroon 0 4.0 12.0 2.0 11.0 43.0 L 0.0 ... 10.0 4.0 19.0 37.0 -1.0 2.0 2.0 0.0 -9.0 8.0 2014-06-18 19:00:00 Spain 0 7.0 18.0 9.0 14.0 63.0 L 1.0 ... 7.0 5.0 10.0 59.0 -2.0 1.0 1.0 0.0 -4.0 2.0 2014-06-18 16:00:00 Australia 2 3.0 6.0 3.0 17.0 50.0 L 1.0 ... 6.0 6.0 18.0 39.0 -4.0 0.0 6.0 -3.0 0.0 2.0 2014-06-17 19:00:00 Brazil 0 4.0 12.0 7.0 14.0 50.0 D 3.0 ... 7.0 3.0 10.0 55.0 2.0 5.0 6.0 2.0 -5.0 8.0 Train ML model In this tutorial we will: Train a model using 6 feature columns Use an 80/20 split in training/test data Use accuracy to evaluate our models It's probably worth evaluating multiple models (several models explained in this tutorial), perhaps use k-fold cross validation, and use metrics other than accuracy to evaluate a model (check the commented out code). # import required libraries from sklearn.model_selection import train_test_split from sklearn.metrics import precision_recall_fscore_support as score , confusion_matrix , roc_auc_score , classification_report , log_loss from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.linear_model import LogisticRegression Get data from our dataset Team_1_result column - target variable The difference of stats between teams (6 columns) - features Do we need to scale or normalize the feature columns in order for it to make mathematical sense to a ML model? This depends on the type of model we are training, but it's definitely worth investigating in order to achieve a high performing model. We should also investigate the dataset to check if it's balanced on all classes or if it's skewed towards a particular class (i.e are there an equal number of wins, losses and draws?). If not, would this affect model performance? target = match_stats [[ 'team_1_result' ]] . replace ([ 'W' , 'L' , 'D' ],[ 0 , 1 , 2 ]) features = match_stats [[ 'goals_per_match_diff' , 'corners_per_match_diff' , 'shots_per_match_diff' , 'shotsOnTarget_per_match_diff' , 'fouls_per_match_diff' , 'possession_per_match_diff' ]] Split test and training data We train a model on the training data, and then use test data to evaluate the performance of that model. X_train , X_test , y_train , y_test = train_test_split ( features , target , test_size = 0.2 , stratify = target ) Name and define classifiers names = [ \"Nearest Neighbors\" , \"Logistic Regression\" , \"Linear SVM\" , \"RBF SVM\" , \"Gaussian Process\" , \"Decision Tree\" , \"Random Forest\" , \"Neural Net\" , \"AdaBoost\" , \"Naive Bayes\" , \"QDA\" ] classifiers = [ KNeighborsClassifier ( 3 ), LogisticRegression (), SVC ( kernel = \"linear\" , C = 0.025 , probability = True ), SVC ( gamma = 2 , C = 1 , probability = True ), GaussianProcessClassifier ( 1.0 * RBF ( 1.0 )), DecisionTreeClassifier ( max_depth = 5 ), RandomForestClassifier ( max_depth = 5 , n_estimators = 10 , max_features = 1 ), MLPClassifier ( alpha = 1 , max_iter = 1000 ), AdaBoostClassifier (), GaussianNB (), QuadraticDiscriminantAnalysis ()] Iterate through all classifiers and get their accuracy score We can use the best performing model to make our predictions. There are several other metrics in the code that have been commented out which might provide helpful insights on model performance. for name , clf in zip ( names , classifiers ): clf . fit ( X_train , y_train ) accuracy = clf . score ( X_test , y_test ) # prediction_proba = clf.predict_proba(X_test) # logloss = log_loss(y_test,prediction_proba) # precision, recall, fscore, support = score(y_test, prediction) # conf_martrix = confusion_matrix(y_test, prediction) # clas_report = classification_report(y_test, prediction) print ( name , accuracy ) Nearest Neighbors 0.49295774647887325 Logistic Regression 0.5714285714285714 Linear SVM 0.5694164989939637 RBF SVM 0.46680080482897385 Gaussian Process 0.5633802816901409 Decision Tree 0.5432595573440644 Random Forest 0.5533199195171026 Neural Net 0.5573440643863179 AdaBoost 0.5573440643863179 Naive Bayes 0.5331991951710262 QDA 0.5674044265593562 Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Soccer modelling tutorial in Python"},{"location":"modelling/soccerModellingTutorialPython/#how-to-model-the-2021-euro-copa-america-python-tutorial","text":"This tutorial was written as part of the 2021 Euro & Copa America Datathon competition","title":"How to model the 2021 Euro &amp; Copa America: Python Tutorial"},{"location":"modelling/soccerModellingTutorialPython/#the-task","text":"This notebook will outline how to train a simple classification model to predict the outcome of a soccer match using the dataset provided for the datathon. Reading data from file and get a raw dataset Data cleaning and feature engineering Training a model The tutorial covers the thought process of manipulating the dataset (why and how), some simple data cleaning, feature engineering and training a classification model. The tutorial DOES NOT delve deep into the fundamentals of machine learning, advanced feature engineering or model tuning. There are some helpful hints along the way though. Betfair Datathon If you're interested in competing in the 2021 Euro & Copa America Datathon competition make sure you head to the Hub , register and download the bespoke data set provided and get your model submitted before 11 June for your chance to win part of the prize pool. # import required libraries import numpy as np import pandas as pd import os import warnings warnings . filterwarnings ( 'ignore' )","title":"The Task"},{"location":"modelling/soccerModellingTutorialPython/#read-data-from-file-and-get-a-raw-dataset","text":"","title":"Read data from file and get a raw dataset"},{"location":"modelling/soccerModellingTutorialPython/#change-the-data-types-date-column","text":"We need the date column in good order for our tutorial. Here's a sample of the data set we're using for this tutorial. In general, it's a good idea to evaluate data types of all columns that we work with to ensure they are correct. df = pd . read_csv ( 'soccerData.csv' ) df [ 'date_GMT' ] = pd . to_datetime ( df [ 'date_GMT' ])","title":"Change the data types - date column."},{"location":"modelling/soccerModellingTutorialPython/#get-data-columns-and-create-raw-dataset","text":"For this tutorial, let's take only a few stats columns to work with. Typically we would explore all features and then decide which data to discard. Goal counts Corners Total shots Shots on target Fouls Possession raw_match_stats = df [[ 'date_GMT' , 'home_team_name' , 'away_team_name' , 'home_team_goal_count' , 'away_team_goal_count' , 'home_team_corner_count' , 'away_team_corner_count' , 'home_team_shots' , 'away_team_shots' , 'home_team_shots_on_target' , 'away_team_shots_on_target' , 'home_team_fouls' , 'away_team_fouls' , 'home_team_possession' , 'away_team_possession' ,]]","title":"Get data columns and create raw dataset"},{"location":"modelling/soccerModellingTutorialPython/#clean-data","text":"As a cleaning step, we order our data by date and drop rows with NA values. raw_match_stats = raw_match_stats . sort_values ( by = [ 'date_GMT' ], ascending = False ) raw_match_stats = raw_match_stats . dropna ()","title":"Clean data"},{"location":"modelling/soccerModellingTutorialPython/#raw-dataset","text":"This raw dataset is structured so that each match has an individual row and stats for both teams are on that row with columns titles \"home\" and \"away\". Our goal is to build a machine learning (ML) model that can predict the result of a soccer match. Given that we have some match stats, we will aim to use that information to predict a WIN, LOSS or DRAW. raw_match_stats date_GMT home_team_name away_team_name home_team_goal_count away_team_goal_count home_team_corner_count away_team_corner_count home_team_shots away_team_shots home_team_shots_on_target away_team_shots_on_target home_team_fouls away_team_fouls home_team_possession away_team_possession 2021-03-31 18:45:00 Spain Kosovo 3 1 9.0 2.0 25.0 4.0 9.0 2.0 9.0 10.0 80.0 20.0 2021-03-31 18:45:00 Scotland Faroe Islands 4 0 1.0 5.0 14.0 8.0 8.0 3.0 9.0 13.0 65.0 35.0 2021-03-31 18:45:00 Switzerland Finland 3 2 5.0 2.0 21.0 6.0 9.0 3.0 11.0 7.0 63.0 37.0 2021-03-31 18:45:00 Lithuania Italy 0 2 4.0 5.0 8.0 29.0 3.0 11.0 14.0 13.0 34.0 66.0 2021-03-31 18:45:00 Northern Ireland Bulgaria 0 0 12.0 2.0 16.0 4.0 5.0 2.0 17.0 17.0 70.0 30.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2014-06-14 16:00:00 Colombia Greece 3 0 4.0 4.0 9.0 13.0 5.0 8.0 20.0 13.0 44.0 56.0 2014-06-13 22:00:00 Chile Australia 3 1 3.0 1.0 7.0 12.0 5.0 3.0 10.0 18.0 59.0 41.0 2014-06-13 19:00:00 Spain Netherlands 1 5 4.0 1.0 8.0 6.0 5.0 6.0 6.0 18.0 61.0 39.0 2014-06-13 16:00:00 Mexico Cameroon 1 0 2.0 5.0 7.0 12.0 3.0 4.0 10.0 10.0 55.0 45.0 2014-06-12 20:00:00 Brazil Croatia 3 1 7.0 3.0 13.0 10.0 5.0 4.0 5.0 19.0 63.0 37.0","title":"Raw dataset"},{"location":"modelling/soccerModellingTutorialPython/#data-cleaning-and-feature-engineering","text":"","title":"Data cleaning and feature engineering"},{"location":"modelling/soccerModellingTutorialPython/#target-variable-match-result","text":"Our machine learning model aims to predict the result of a match. This \"result\" is called the \"target variable\". Our dataset has no columns showing the match result. We will create two columns for the results for each team. One of these would become the target variable for our ML model. # create results columns for both home and away teams (W - win, D = Draw, L = Loss). raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] == raw_match_stats [ 'away_team_goal_count' ], 'home_team_result' ] = 'D' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] > raw_match_stats [ 'away_team_goal_count' ], 'home_team_result' ] = 'W' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] < raw_match_stats [ 'away_team_goal_count' ], 'home_team_result' ] = 'L' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] == raw_match_stats [ 'away_team_goal_count' ], 'away_team_result' ] = 'D' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] > raw_match_stats [ 'away_team_goal_count' ], 'away_team_result' ] = 'L' raw_match_stats . loc [ raw_match_stats [ 'home_team_goal_count' ] < raw_match_stats [ 'away_team_goal_count' ], 'away_team_result' ] = 'W'","title":"Target variable - Match Result"},{"location":"modelling/soccerModellingTutorialPython/#average-pre-match-stats-five-match-average","text":"Great! Now we have a dataset with many rows of data, with each row representing match stats and the match result (this would become our target variable). But our goal is to build an ML model that predicts the match result prior to the start of a match. Are the stats from that match what we need to build this ML model? No! When predicting a match outcome BEFORE the start of the match, we are forced to rely on match stats available to us from previous matches. Therefore, we need a dataset with the match result (target variable) and stats for each team heading into that match. For this tutorial, we will look at the average stats for each team in the five matches preceding each match. Lets look at how we can get the average stats for the previous 5 matches for each team at each match. Split the raw_match_stats to two datasets (home_team_stats and away_team_stats). Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match). At each row of this dataset, get the team name, find the stats for that team during the last 5 matches, and average these stats (avg_stats_per_team). Add these stats to the team_stats_per_match dataset. Why did we chose five matches? Why not 10? Should we average over a time period (matches in the last year perhaps?) rather than a number? What's the least number of matches available for each competing team in the dataset? These are all interesting questions that may improve our model. # Split the raw_match_stats to two datasets (home_team_stats and away_team_stats) home_team_stats = raw_match_stats [[ 'date_GMT' , 'home_team_name' , 'home_team_goal_count' , 'home_team_corner_count' , 'home_team_shots' , 'home_team_shots_on_target' , 'home_team_fouls' , 'home_team_possession' , 'home_team_result' ,]] away_team_stats = raw_match_stats [[ 'date_GMT' , 'away_team_name' , 'away_team_goal_count' , 'away_team_corner_count' , 'away_team_shots' , 'away_team_shots_on_target' , 'away_team_fouls' , 'away_team_possession' , 'away_team_result' ,]] # rename \"home_team\" and \"away_team\" columns home_team_stats . columns = [ col . replace ( 'home_team_' , '' ) for col in home_team_stats . columns ] away_team_stats . columns = [ col . replace ( 'away_team_' , '' ) for col in away_team_stats . columns ] # stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match) team_stats_per_match = home_team_stats . append ( away_team_stats ) # At each row of this dataset, get the team name, find the stats for that team during the last 5 matches, and average these stats (avg_stats_per_team). avg_stat_columns = [ 'goals_per_match' , 'corners_per_match' , 'shots_per_match' , 'shotsOnTarget_per_match' , 'fouls_per_match' , 'possession_per_match' ] stats_list = [] for index , row in team_stats_per_match . iterrows (): team_stats_last_five_matches = team_stats_per_match . loc [( team_stats_per_match [ 'name' ] == row [ 'name' ]) & ( team_stats_per_match [ 'date_GMT' ] < row [ 'date_GMT' ])] . sort_values ( by = [ 'date_GMT' ], ascending = False ) stats_list . append ( team_stats_last_five_matches . iloc [ 0 : 5 ,:] . mean ( axis = 0 ) . values [ 0 : 6 ]) avg_stats_per_team = pd . DataFrame ( stats_list , columns = avg_stat_columns ) # Add these stats to the team_stats_per_match dataset. team_stats_per_match = pd . concat ([ team_stats_per_match . reset_index ( drop = True ), avg_stats_per_team ], axis = 1 , ignore_index = False )","title":"Average pre-match stats - Five match average"},{"location":"modelling/soccerModellingTutorialPython/#reshape-average-pre-match-stats","text":"Now that we have the average stats for each team going into every match, we can create a dataset similar to the raw_match_stats, where each row represents both teams from one match. Re-segment the home and away teams (name Team 1 and Team 2 rather than home and away). Combine at each match to get a dataset with a row representing each match. # Re-segment the home and away teams. home_team_stats = team_stats_per_match . iloc [: int ( team_stats_per_match . shape [ 0 ] / 2 ),:] away_team_stats = team_stats_per_match . iloc [ int ( team_stats_per_match . shape [ 0 ] / 2 ):,:] home_team_stats . columns = [ 'team_1_' + str ( col ) for col in home_team_stats . columns ] away_team_stats . columns = [ 'team_2_' + str ( col ) for col in away_team_stats . columns ] # Combine at each match to get a dataset with a row representing each match. # drop the NA rows (earliest match for each team, i.e no previous stats) match_stats = pd . concat ([ home_team_stats , away_team_stats . reset_index ( drop = True )], axis = 1 , ignore_index = False ) match_stats = match_stats . dropna () . reset_index ( drop = True )","title":"Reshape average pre-match stats"},{"location":"modelling/soccerModellingTutorialPython/#find-the-difference-of-stats-between-teams","text":"In our ML model, we will take the difference between Team 1 and Team 2 average stats as features. 6 new columns are created for this. Would we be better off using the raw stats for each team as features? Can we generate any other useful features from the dataset provided? Do we need to weigh the home and away teams because home teams win more often? # create columns with average stat differences between the two teams match_stats [ 'goals_per_match_diff' ] = ( match_stats [ 'team_1_goals_per_match' ] - match_stats [ 'team_2_goals_per_match' ]) match_stats [ 'corners_per_match_diff' ] = ( match_stats [ 'team_1_corners_per_match' ] - match_stats [ 'team_2_corners_per_match' ]) match_stats [ 'shots_per_match_diff' ] = ( match_stats [ 'team_1_shots_per_match' ] - match_stats [ 'team_2_shots_per_match' ]) match_stats [ 'shotsOnTarget_per_match_diff' ] = ( match_stats [ 'team_1_shotsOnTarget_per_match' ] - match_stats [ 'team_2_shotsOnTarget_per_match' ]) match_stats [ 'fouls_per_match_diff' ] = ( match_stats [ 'team_1_fouls_per_match' ] - match_stats [ 'team_2_fouls_per_match' ]) match_stats [ 'possession_per_match_diff' ] = ( match_stats [ 'team_1_possession_per_match' ] - match_stats [ 'team_2_possession_per_match' ]) match_stats team_1_date_GMT team_1_name team_1_goal_count team_1_corner_count team_1_shots team_1_shots_on_target team_1_fouls team_1_possession team_1_result team_1_goals_per_match ... team_2_shots_per_match team_2_shotsOnTarget_per_match team_2_fouls_per_match team_2_possession_per_match goals_per_match_diff corners_per_match_diff shots_per_match_diff shotsOnTarget_per_match_diff fouls_per_match_diff possession_per_match_diff 2021-03-31 18:45:00 Spain 3 9.0 25.0 9.0 9.0 80.0 W 2.2 ... 6.6 2.4 12.2 45.0 1.8 3.4 5.2 2.4 -1.6 24.6 2021-03-31 18:45:00 Scotland 4 1.0 14.0 8.0 9.0 65.0 W 0.8 ... 9.0 3.8 14.0 46.8 -0.2 2.2 4.6 2.4 -0.2 2.0 2021-03-31 18:45:00 Switzerland 3 5.0 21.0 9.0 11.0 63.0 W 1.8 ... 7.6 2.2 10.2 36.0 0.2 2.6 5.2 3.6 3.2 15.8 2021-03-31 18:45:00 Lithuania 0 4.0 8.0 3.0 14.0 34.0 L 0.8 ... 15.6 5.4 11.4 64.4 -1.6 -4.6 -7.2 -3.0 0.2 -17.2 2021-03-31 18:45:00 Northern Ireland 0 12.0 16.0 5.0 17.0 70.0 D 0.8 ... 8.8 3.2 16.2 48.6 -0.2 0.4 0.0 -0.2 -7.0 -2.8 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2014-06-19 16:00:00 Colombia 2 5.0 8.0 5.0 12.0 40.0 W 3.0 ... 19.0 8.0 12.0 56.0 1.0 -4.0 -10.0 -3.0 8.0 -12.0 2014-06-18 22:00:00 Cameroon 0 4.0 12.0 2.0 11.0 43.0 L 0.0 ... 10.0 4.0 19.0 37.0 -1.0 2.0 2.0 0.0 -9.0 8.0 2014-06-18 19:00:00 Spain 0 7.0 18.0 9.0 14.0 63.0 L 1.0 ... 7.0 5.0 10.0 59.0 -2.0 1.0 1.0 0.0 -4.0 2.0 2014-06-18 16:00:00 Australia 2 3.0 6.0 3.0 17.0 50.0 L 1.0 ... 6.0 6.0 18.0 39.0 -4.0 0.0 6.0 -3.0 0.0 2.0 2014-06-17 19:00:00 Brazil 0 4.0 12.0 7.0 14.0 50.0 D 3.0 ... 7.0 3.0 10.0 55.0 2.0 5.0 6.0 2.0 -5.0 8.0","title":"Find the difference of stats between teams"},{"location":"modelling/soccerModellingTutorialPython/#train-ml-model","text":"In this tutorial we will: Train a model using 6 feature columns Use an 80/20 split in training/test data Use accuracy to evaluate our models It's probably worth evaluating multiple models (several models explained in this tutorial), perhaps use k-fold cross validation, and use metrics other than accuracy to evaluate a model (check the commented out code). # import required libraries from sklearn.model_selection import train_test_split from sklearn.metrics import precision_recall_fscore_support as score , confusion_matrix , roc_auc_score , classification_report , log_loss from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.linear_model import LogisticRegression","title":"Train ML model"},{"location":"modelling/soccerModellingTutorialPython/#get-data-from-our-dataset","text":"Team_1_result column - target variable The difference of stats between teams (6 columns) - features Do we need to scale or normalize the feature columns in order for it to make mathematical sense to a ML model? This depends on the type of model we are training, but it's definitely worth investigating in order to achieve a high performing model. We should also investigate the dataset to check if it's balanced on all classes or if it's skewed towards a particular class (i.e are there an equal number of wins, losses and draws?). If not, would this affect model performance? target = match_stats [[ 'team_1_result' ]] . replace ([ 'W' , 'L' , 'D' ],[ 0 , 1 , 2 ]) features = match_stats [[ 'goals_per_match_diff' , 'corners_per_match_diff' , 'shots_per_match_diff' , 'shotsOnTarget_per_match_diff' , 'fouls_per_match_diff' , 'possession_per_match_diff' ]]","title":"Get data from our dataset"},{"location":"modelling/soccerModellingTutorialPython/#split-test-and-training-data","text":"We train a model on the training data, and then use test data to evaluate the performance of that model. X_train , X_test , y_train , y_test = train_test_split ( features , target , test_size = 0.2 , stratify = target )","title":"Split test and training data"},{"location":"modelling/soccerModellingTutorialPython/#name-and-define-classifiers","text":"names = [ \"Nearest Neighbors\" , \"Logistic Regression\" , \"Linear SVM\" , \"RBF SVM\" , \"Gaussian Process\" , \"Decision Tree\" , \"Random Forest\" , \"Neural Net\" , \"AdaBoost\" , \"Naive Bayes\" , \"QDA\" ] classifiers = [ KNeighborsClassifier ( 3 ), LogisticRegression (), SVC ( kernel = \"linear\" , C = 0.025 , probability = True ), SVC ( gamma = 2 , C = 1 , probability = True ), GaussianProcessClassifier ( 1.0 * RBF ( 1.0 )), DecisionTreeClassifier ( max_depth = 5 ), RandomForestClassifier ( max_depth = 5 , n_estimators = 10 , max_features = 1 ), MLPClassifier ( alpha = 1 , max_iter = 1000 ), AdaBoostClassifier (), GaussianNB (), QuadraticDiscriminantAnalysis ()]","title":"Name and define classifiers"},{"location":"modelling/soccerModellingTutorialPython/#iterate-through-all-classifiers-and-get-their-accuracy-score","text":"We can use the best performing model to make our predictions. There are several other metrics in the code that have been commented out which might provide helpful insights on model performance. for name , clf in zip ( names , classifiers ): clf . fit ( X_train , y_train ) accuracy = clf . score ( X_test , y_test ) # prediction_proba = clf.predict_proba(X_test) # logloss = log_loss(y_test,prediction_proba) # precision, recall, fscore, support = score(y_test, prediction) # conf_martrix = confusion_matrix(y_test, prediction) # clas_report = classification_report(y_test, prediction) print ( name , accuracy ) Nearest Neighbors 0.49295774647887325 Logistic Regression 0.5714285714285714 Linear SVM 0.5694164989939637 RBF SVM 0.46680080482897385 Gaussian Process 0.5633802816901409 Decision Tree 0.5432595573440644 Random Forest 0.5533199195171026 Neural Net 0.5573440643863179 AdaBoost 0.5573440643863179 Naive Bayes 0.5331991951710262 QDA 0.5674044265593562","title":"Iterate through all classifiers and get their accuracy score"},{"location":"modelling/soccerModellingTutorialPython/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/soccerModellingTutorialR/","text":"How to model the 2021 Euro & Copa America: R Tutorial The Task This notebook will outline how to train a simple classification model to predict the outcome of a soccer match using the dataset provided for the datathon. Reading data from file and get a raw dataset Data cleaning and feature engineering Training a model The tutorial covers the thought process of manipulating the dataset (why and how), some simple data cleaning, feature engineering and training a classification model. The tutorial DOES NOT delve deep into the fundamentals of machine learning, advanced feature engineering or model tuning. There are some helpful hints along the way though. Betfair Datathon If you're interested in competing in the 2021 Euro & Copa America Datathon competition make sure you head to the Hub , register and download the bespoke data set provided and get your model submitted before 11 June for your chance to win part of the prize pool. # import required libraries library(tidyverse) library(lubridate) library(caTools) # calculate rolling average Read data from file and get a raw dataset Change the data types - date column. We need the date column in good order for our tutorial. Here's a sample of the data set we're using for this tutorial. In general, it's a good idea to evaluate data types of all columns that we work with to ensure they are correct. df = read_csv(\"SoccerData.csv\", guess_max = 2000) df = df %>% mutate(date_GMT = parse_date_time(date_GMT, '%b %d %Y - %I:%M%p', tz = \"GMT\")) Get data columns and create raw dataset For this tutorial, let's take only a few stats columns to work with. Typically we would explore all features and then decide which data to discard. Goal counts Corners Total shots Shots on target Fouls Possession raw_match_stats = df %>% select( date_GMT, home_team_name, away_team_name, home_team_goal_count, away_team_goal_count, home_team_corner_count, away_team_corner_count, home_team_shots, away_team_shots, home_team_shots_on_target, away_team_shots_on_target, home_team_fouls, away_team_fouls, home_team_possession, away_team_possession ) Clean data As a cleaning step, we order our data by date and drop rows with NA values. raw_match_stats = raw_match_stats %>% arrange(desc(date_GMT)) %>% drop_na() %>% mutate(game_id = 1:n()) Raw dataset This raw dataset is structured so that each match has an individual row and stats for both teams are on that row with columns titles \"home\" and \"away\". Our goal is to build a machine learning (ML) model that can predict the result of a soccer match. Given that we have some match stats, we will aim to use that information to predict a WIN, LOSS or DRAW. raw_match_stats Data cleaning and feature engineering Target variable - Match Result Our machine learning model aims to predict the result of a match. This \"result\" is called the \"target variable\". Our dataset has no columns showing the match result. We will create two columns for the results for each team. One of these would become the target variable for our ML model. # create results columns for both home and away teams (W - win, D = Draw, L = Loss). raw_match_stats = raw_match_stats %>% mutate(home_team_result = case_when(home_team_goal_count == away_team_goal_count ~ \"D\", home_team_goal_count > away_team_goal_count ~ \"W\", home_team_goal_count < away_team_goal_count ~ \"L\"), away_team_result = case_when(home_team_goal_count == away_team_goal_count ~ \"D\", home_team_goal_count > away_team_goal_count ~ \"L\", home_team_goal_count < away_team_goal_count ~ \"W\")) Average pre-match stats - Five match average Great! Now we have a dataset with many rows of data, with each row representing match stats and the match result (this would become our target variable). But our goal is to build an ML model that predicts the match result prior to the start of a match. Are the stats from that match what we need to build this ML model? No! When predicting a match outcome BEFORE the start of the match, we are forced to rely on match stats available to us from previous matches. Therefore, we need a dataset with the match result (target variable) and stats for each team heading into that match. For this tutorial, we will look at the average stats for each team in the five matches preceding each match. Lets look at how we can get the average stats for the previous 5 matches for each team at each match. Split the raw_match_stats to two datasets (home_team_stats and away_team_stats). Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match). At each row of this dataset, get the team name, find the stats for that team during the 4. last 5 matches, and average these stats (avg_stats_per_team). Add these stats to the team_stats_per_match dataset. Why did we chose five matches? Why not 10? Should we average over a time period (matches in the last year perhaps?) rather than a number? What's the least number of matches available for each competing team in the dataset? These are all interesting questions that may improve our model. # Split the raw_match_stats to two datasets (home_team_stats and away_team_stats). home_team_stats = raw_match_stats %>% select(game_id, date_GMT, starts_with(\"home_team\")) away_team_stats = raw_match_stats %>% select(game_id, date_GMT, starts_with(\"away_team\")) # rename \"home_team\" and \"away_team\" columns home_team_stats = rename_with(home_team_stats, ~gsub(\"home_team_\", \"\", .x)) away_team_stats = rename_with(away_team_stats, ~gsub(\"away_team_\", \"\", .x)) # Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match). team_stats_per_match = home_team_stats %>% bind_rows(away_team_stats) # At each row of this dataset, get the team name, find the stats for that team during the last 5 matches, and average these stats (avg_stats_per_team). avg_stats_per_team <- team_stats_per_match %>% group_by(name) %>% filter(n() > 1) %>% # remove countries with only 1 match arrange(date_GMT) %>% mutate_at( vars( goal_count, corner_count, shots, shots_on_target, fouls, possession ), ## Columns for which we want a rolling mean .funs = ~ runmean( x = dplyr::lag(.x), k = 5, endrule = \"mean\", align = \"right\" )## Rolling mean for last 5 matches ) %>% rename( goals_per_match = goal_count, corners_per_match = corner_count, shots_per_match = shots, shotsOnTarget_per_match = shots_on_target, fouls_per_match = fouls, possession_per_match = possession ) Reshape average pre-match stats Now that we have the average stats for each team going into every match, we can create a dataset similar to the raw_match_stats, where each row represents both teams from one match. Re-segment the home and away teams (name Team 1 and Team 2 rather than home and away). Combine at each match to get a dataset with a row representing each match. # Add these stats to the home/away dataset home_team_stats <- home_team_stats %>% left_join(avg_stats_per_team) %>% arrange(game_id) %>% rename_with(~paste0(\"team_1_\", .)) away_team_stats <- away_team_stats %>% left_join(avg_stats_per_team) %>% arrange(game_id) %>% rename_with(~paste0(\"team_2_\", .)) # Combine at each match to get a dataset with a row representing each match. # drop the NA rows ( earliest match for each team, i.e no previous stats) match_stats = home_team_stats %>% bind_cols(away_team_stats) %>% drop_na() Find the difference of stats between teams In our ML model, we will take the difference between Team 1 and Team 2 average stats as features. 6 new columns are created for this. Would we be better off using the raw stats for each team as features? Can we generate any other useful features from the dataset provided? Do we need to weigh the home and away teams because home teams win more often? # create columns with average stat differences between the two teams match_stats = match_stats %>% mutate(goals_per_match_diff = team_1_goals_per_match - team_2_goals_per_match, corners_per_match_diff = team_1_corners_per_match - team_2_corners_per_match, shots_per_match_diff = team_1_shots_per_match - team_2_shots_per_match, shotsOnTarget_per_match_diff = team_1_shotsOnTarget_per_match - team_2_shotsOnTarget_per_match, fouls_per_match_diff = team_1_fouls_per_match - team_2_fouls_per_match, possession_per_match_diff = team_1_possession_per_match - team_2_possession_per_match) match_stats Train ML model In this tutorial we will: Train a model using 6 feature columns Use an 80/20 split in training/test data Use accuracy to evaluate our models It's probably worth evaluating multiple models (several models explained in this tutorial), perhaps use k-fold cross validation, and use metrics other than accuracy to evaluate a model (check the commented out code). library(tidymodels) library(discrim) library(kknn) # kknn library(kernlab) # svm library(naivebayes) #naive bayes library(rpart) # decision tree library(nnet) # logistic regression and neural net library(ranger) # random forest (modified by multiclass) library(xgboost) # xgboost Get data from our dataset Team_1_result column - target variable The difference of stats between teams (6 columns) - features Do we need to scale or normalize the feature columns in order for it to make mathematical sense to a ML model? This depends on the type of model we are training, but it's definitely worth investigating in order to achieve a high performing model. We should also investigate the dataset to check if it's balanced on all classes or if it's skewed towards a particular class (i.e are there an equal number of wins, losses and draws?). If not, would this affect model performance? # form a dataset with only target and features model_data = match_stats %>% mutate(target = factor(recode(team_1_result, W = 0, D = 1, L = 2))) %>% select( target, goals_per_match_diff, corners_per_match_diff, shots_per_match_diff, shotsOnTarget_per_match_diff, fouls_per_match_diff, possession_per_match_diff ) Split test and training data We train a model on the training data, and then use test data to evaluate the performance of that model. # For more details of using tidymodels package, check https://www.tidymodels.org/ # split test and train data set.seed(2021) data_split <- initial_split(model_data, strata = target, prop = 0.8) # create a recipe for data pre-processing model_recipe <- recipe(target ~ ., data = model_data) %>% step_zv(all_predictors()) %>% step_corr(all_predictors()) %>% prep() Name and define classifiers # Define the models ## knn model knn_model <- nearest_neighbor() %>% set_engine(\"kknn\") %>% set_mode(\"classification\") ## svm model svm_model <- svm_rbf() %>% set_engine(\"kernlab\") %>% set_mode(\"classification\") ## logistic regression model lr_model <- multinom_reg() %>% set_engine(\"nnet\") %>% set_mode(\"classification\") ## naive bayes nb_model <- naive_Bayes() %>% set_engine(\"naivebayes\") %>% set_mode(\"classification\") ## decision tree model tree_model <- decision_tree() %>% set_engine(\"rpart\") %>% set_mode(\"classification\") # random forest rf_model <- rand_forest() %>% set_engine(\"ranger\") %>% set_mode(\"classification\") # boosted tree (xgboost) xgb_model <- boost_tree() %>% set_engine(\"xgboost\") %>% set_mode(\"classification\") # single layer neural network nn_model <- mlp() %>% set_engine(\"nnet\") %>% set_mode(\"classification\") Iterate through all classifiers and get their accuracy score We can use the best performing model to make our predictions. There are several other metrics in the code that have been commented out which might provide helpful insights on model performance. accuracy_results <- map_dfr( .x = list(knn_model, lr_model, nb_model, nn_model, rf_model, svm_model, tree_model, xgb_model), .f = function(the_model) { # create a work flow with recipe and model the_workflow <- workflow() %>% add_recipe(model_recipe) %>% add_model(the_model) # fit the model with training data and evaluate with test data the_fit <- the_workflow %>% last_fit(data_split) tibble(model_name = class(the_model)[1], accuracy = the_fit %>% collect_metrics() %>% filter(.metric == \"accuracy\") %>% select(.estimate) %>% pull) } ) ## Displaying accuracy of the models set in descending order accuracy_results %>% arrange(desc(accuracy)) Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Soccer modelling tutorial in R"},{"location":"modelling/soccerModellingTutorialR/#how-to-model-the-2021-euro-copa-america-r-tutorial","text":"","title":"How to model the 2021 Euro &amp; Copa America: R Tutorial"},{"location":"modelling/soccerModellingTutorialR/#the-task","text":"This notebook will outline how to train a simple classification model to predict the outcome of a soccer match using the dataset provided for the datathon. Reading data from file and get a raw dataset Data cleaning and feature engineering Training a model The tutorial covers the thought process of manipulating the dataset (why and how), some simple data cleaning, feature engineering and training a classification model. The tutorial DOES NOT delve deep into the fundamentals of machine learning, advanced feature engineering or model tuning. There are some helpful hints along the way though. Betfair Datathon If you're interested in competing in the 2021 Euro & Copa America Datathon competition make sure you head to the Hub , register and download the bespoke data set provided and get your model submitted before 11 June for your chance to win part of the prize pool. # import required libraries library(tidyverse) library(lubridate) library(caTools) # calculate rolling average","title":"The Task"},{"location":"modelling/soccerModellingTutorialR/#read-data-from-file-and-get-a-raw-dataset","text":"","title":"Read data from file and get a raw dataset"},{"location":"modelling/soccerModellingTutorialR/#change-the-data-types-date-column","text":"We need the date column in good order for our tutorial. Here's a sample of the data set we're using for this tutorial. In general, it's a good idea to evaluate data types of all columns that we work with to ensure they are correct. df = read_csv(\"SoccerData.csv\", guess_max = 2000) df = df %>% mutate(date_GMT = parse_date_time(date_GMT, '%b %d %Y - %I:%M%p', tz = \"GMT\"))","title":"Change the data types - date column."},{"location":"modelling/soccerModellingTutorialR/#get-data-columns-and-create-raw-dataset","text":"For this tutorial, let's take only a few stats columns to work with. Typically we would explore all features and then decide which data to discard. Goal counts Corners Total shots Shots on target Fouls Possession raw_match_stats = df %>% select( date_GMT, home_team_name, away_team_name, home_team_goal_count, away_team_goal_count, home_team_corner_count, away_team_corner_count, home_team_shots, away_team_shots, home_team_shots_on_target, away_team_shots_on_target, home_team_fouls, away_team_fouls, home_team_possession, away_team_possession )","title":"Get data columns and create raw dataset"},{"location":"modelling/soccerModellingTutorialR/#clean-data","text":"As a cleaning step, we order our data by date and drop rows with NA values. raw_match_stats = raw_match_stats %>% arrange(desc(date_GMT)) %>% drop_na() %>% mutate(game_id = 1:n())","title":"Clean data"},{"location":"modelling/soccerModellingTutorialR/#raw-dataset","text":"This raw dataset is structured so that each match has an individual row and stats for both teams are on that row with columns titles \"home\" and \"away\". Our goal is to build a machine learning (ML) model that can predict the result of a soccer match. Given that we have some match stats, we will aim to use that information to predict a WIN, LOSS or DRAW. raw_match_stats","title":"Raw dataset"},{"location":"modelling/soccerModellingTutorialR/#data-cleaning-and-feature-engineering","text":"","title":"Data cleaning and feature engineering"},{"location":"modelling/soccerModellingTutorialR/#target-variable-match-result","text":"Our machine learning model aims to predict the result of a match. This \"result\" is called the \"target variable\". Our dataset has no columns showing the match result. We will create two columns for the results for each team. One of these would become the target variable for our ML model. # create results columns for both home and away teams (W - win, D = Draw, L = Loss). raw_match_stats = raw_match_stats %>% mutate(home_team_result = case_when(home_team_goal_count == away_team_goal_count ~ \"D\", home_team_goal_count > away_team_goal_count ~ \"W\", home_team_goal_count < away_team_goal_count ~ \"L\"), away_team_result = case_when(home_team_goal_count == away_team_goal_count ~ \"D\", home_team_goal_count > away_team_goal_count ~ \"L\", home_team_goal_count < away_team_goal_count ~ \"W\"))","title":"Target variable - Match Result"},{"location":"modelling/soccerModellingTutorialR/#average-pre-match-stats-five-match-average","text":"Great! Now we have a dataset with many rows of data, with each row representing match stats and the match result (this would become our target variable). But our goal is to build an ML model that predicts the match result prior to the start of a match. Are the stats from that match what we need to build this ML model? No! When predicting a match outcome BEFORE the start of the match, we are forced to rely on match stats available to us from previous matches. Therefore, we need a dataset with the match result (target variable) and stats for each team heading into that match. For this tutorial, we will look at the average stats for each team in the five matches preceding each match. Lets look at how we can get the average stats for the previous 5 matches for each team at each match. Split the raw_match_stats to two datasets (home_team_stats and away_team_stats). Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match). At each row of this dataset, get the team name, find the stats for that team during the 4. last 5 matches, and average these stats (avg_stats_per_team). Add these stats to the team_stats_per_match dataset. Why did we chose five matches? Why not 10? Should we average over a time period (matches in the last year perhaps?) rather than a number? What's the least number of matches available for each competing team in the dataset? These are all interesting questions that may improve our model. # Split the raw_match_stats to two datasets (home_team_stats and away_team_stats). home_team_stats = raw_match_stats %>% select(game_id, date_GMT, starts_with(\"home_team\")) away_team_stats = raw_match_stats %>% select(game_id, date_GMT, starts_with(\"away_team\")) # rename \"home_team\" and \"away_team\" columns home_team_stats = rename_with(home_team_stats, ~gsub(\"home_team_\", \"\", .x)) away_team_stats = rename_with(away_team_stats, ~gsub(\"away_team_\", \"\", .x)) # Stack these two datasets so that each row is the stats for a team for one match (team_stats_per_match). team_stats_per_match = home_team_stats %>% bind_rows(away_team_stats) # At each row of this dataset, get the team name, find the stats for that team during the last 5 matches, and average these stats (avg_stats_per_team). avg_stats_per_team <- team_stats_per_match %>% group_by(name) %>% filter(n() > 1) %>% # remove countries with only 1 match arrange(date_GMT) %>% mutate_at( vars( goal_count, corner_count, shots, shots_on_target, fouls, possession ), ## Columns for which we want a rolling mean .funs = ~ runmean( x = dplyr::lag(.x), k = 5, endrule = \"mean\", align = \"right\" )## Rolling mean for last 5 matches ) %>% rename( goals_per_match = goal_count, corners_per_match = corner_count, shots_per_match = shots, shotsOnTarget_per_match = shots_on_target, fouls_per_match = fouls, possession_per_match = possession )","title":"Average pre-match stats - Five match average"},{"location":"modelling/soccerModellingTutorialR/#reshape-average-pre-match-stats","text":"Now that we have the average stats for each team going into every match, we can create a dataset similar to the raw_match_stats, where each row represents both teams from one match. Re-segment the home and away teams (name Team 1 and Team 2 rather than home and away). Combine at each match to get a dataset with a row representing each match. # Add these stats to the home/away dataset home_team_stats <- home_team_stats %>% left_join(avg_stats_per_team) %>% arrange(game_id) %>% rename_with(~paste0(\"team_1_\", .)) away_team_stats <- away_team_stats %>% left_join(avg_stats_per_team) %>% arrange(game_id) %>% rename_with(~paste0(\"team_2_\", .)) # Combine at each match to get a dataset with a row representing each match. # drop the NA rows ( earliest match for each team, i.e no previous stats) match_stats = home_team_stats %>% bind_cols(away_team_stats) %>% drop_na()","title":"Reshape average pre-match stats"},{"location":"modelling/soccerModellingTutorialR/#find-the-difference-of-stats-between-teams","text":"In our ML model, we will take the difference between Team 1 and Team 2 average stats as features. 6 new columns are created for this. Would we be better off using the raw stats for each team as features? Can we generate any other useful features from the dataset provided? Do we need to weigh the home and away teams because home teams win more often? # create columns with average stat differences between the two teams match_stats = match_stats %>% mutate(goals_per_match_diff = team_1_goals_per_match - team_2_goals_per_match, corners_per_match_diff = team_1_corners_per_match - team_2_corners_per_match, shots_per_match_diff = team_1_shots_per_match - team_2_shots_per_match, shotsOnTarget_per_match_diff = team_1_shotsOnTarget_per_match - team_2_shotsOnTarget_per_match, fouls_per_match_diff = team_1_fouls_per_match - team_2_fouls_per_match, possession_per_match_diff = team_1_possession_per_match - team_2_possession_per_match) match_stats","title":"Find the difference of stats between teams"},{"location":"modelling/soccerModellingTutorialR/#train-ml-model","text":"In this tutorial we will: Train a model using 6 feature columns Use an 80/20 split in training/test data Use accuracy to evaluate our models It's probably worth evaluating multiple models (several models explained in this tutorial), perhaps use k-fold cross validation, and use metrics other than accuracy to evaluate a model (check the commented out code). library(tidymodels) library(discrim) library(kknn) # kknn library(kernlab) # svm library(naivebayes) #naive bayes library(rpart) # decision tree library(nnet) # logistic regression and neural net library(ranger) # random forest (modified by multiclass) library(xgboost) # xgboost","title":"Train ML model"},{"location":"modelling/soccerModellingTutorialR/#get-data-from-our-dataset","text":"Team_1_result column - target variable The difference of stats between teams (6 columns) - features Do we need to scale or normalize the feature columns in order for it to make mathematical sense to a ML model? This depends on the type of model we are training, but it's definitely worth investigating in order to achieve a high performing model. We should also investigate the dataset to check if it's balanced on all classes or if it's skewed towards a particular class (i.e are there an equal number of wins, losses and draws?). If not, would this affect model performance? # form a dataset with only target and features model_data = match_stats %>% mutate(target = factor(recode(team_1_result, W = 0, D = 1, L = 2))) %>% select( target, goals_per_match_diff, corners_per_match_diff, shots_per_match_diff, shotsOnTarget_per_match_diff, fouls_per_match_diff, possession_per_match_diff )","title":"Get data from our dataset"},{"location":"modelling/soccerModellingTutorialR/#split-test-and-training-data","text":"We train a model on the training data, and then use test data to evaluate the performance of that model. # For more details of using tidymodels package, check https://www.tidymodels.org/ # split test and train data set.seed(2021) data_split <- initial_split(model_data, strata = target, prop = 0.8) # create a recipe for data pre-processing model_recipe <- recipe(target ~ ., data = model_data) %>% step_zv(all_predictors()) %>% step_corr(all_predictors()) %>% prep()","title":"Split test and training data"},{"location":"modelling/soccerModellingTutorialR/#name-and-define-classifiers","text":"# Define the models ## knn model knn_model <- nearest_neighbor() %>% set_engine(\"kknn\") %>% set_mode(\"classification\") ## svm model svm_model <- svm_rbf() %>% set_engine(\"kernlab\") %>% set_mode(\"classification\") ## logistic regression model lr_model <- multinom_reg() %>% set_engine(\"nnet\") %>% set_mode(\"classification\") ## naive bayes nb_model <- naive_Bayes() %>% set_engine(\"naivebayes\") %>% set_mode(\"classification\") ## decision tree model tree_model <- decision_tree() %>% set_engine(\"rpart\") %>% set_mode(\"classification\") # random forest rf_model <- rand_forest() %>% set_engine(\"ranger\") %>% set_mode(\"classification\") # boosted tree (xgboost) xgb_model <- boost_tree() %>% set_engine(\"xgboost\") %>% set_mode(\"classification\") # single layer neural network nn_model <- mlp() %>% set_engine(\"nnet\") %>% set_mode(\"classification\")","title":"Name and define classifiers"},{"location":"modelling/soccerModellingTutorialR/#iterate-through-all-classifiers-and-get-their-accuracy-score","text":"We can use the best performing model to make our predictions. There are several other metrics in the code that have been commented out which might provide helpful insights on model performance. accuracy_results <- map_dfr( .x = list(knn_model, lr_model, nb_model, nn_model, rf_model, svm_model, tree_model, xgb_model), .f = function(the_model) { # create a work flow with recipe and model the_workflow <- workflow() %>% add_recipe(model_recipe) %>% add_model(the_model) # fit the model with training data and evaluate with test data the_fit <- the_workflow %>% last_fit(data_split) tibble(model_name = class(the_model)[1], accuracy = the_fit %>% collect_metrics() %>% filter(.metric == \"accuracy\") %>% select(.estimate) %>% pull) } ) ## Displaying accuracy of the models set in descending order accuracy_results %>% arrange(desc(accuracy))","title":"Iterate through all classifiers and get their accuracy score"},{"location":"modelling/soccerModellingTutorialR/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any winnings/losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"},{"location":"modelling/stakingMethods/","text":"Staking Methods and Bankroll Management Workshop This tutorial was written by Jason and was originally published on Github . It is shared here with his permission. Good staking systems should be used in combination with betting models. In this article, we\u2019ll go over the basics of staking and analyse some of the more popular staking strategies, outlining their pros and cons. Why is staking important? Many bettors with models don\u2019t have sound bankroll management, to their detriment. A good staking system is primarily about balancing risk vs reward: If we are too risk inclined and bet aggressively, there is a good chance we will suffer losses We want to maximise our returns within our risk constraints Despite many resources on the web claiming that there is an optimal way to stake, the reality is that there is no one right answer and how you stake will be a personal one which will be affected by: Your return objectives Your risk tolerance Your emotional well-being (we\u2019re not robots) Your return objectives and risk tolerance are usually the primary considerations when staking and will likely be in conflict with one another and hence will need to be balanced. Generally, chasing higher returns will mean greater levels of risk. Finding the right balance between risk and return that works for you is not an easy task and there will likely be a lot of iterating before getting it right. The third factor that influences your staking that most people overlook is the impact the size of your bets has on your emotional well-being. Remember that betting should not be stressful and if it is, it probably means that you\u2019re staking too big. Even if your bankroll can theoretically accommodate an increase in your stake sizes, be wary of whether your emotions can. Common staking systems Here, we\u2019ll go through some of the more common staking systems. However, rather than just describing each of the staking systems, we\u2019ll attempt to understand the risk/reward profile of each by simulating each staking system 10,000 times based on the following parameters: Starting bank of $1,000 Ruin defined as bank going down to $5 or less (No longer can place a minimum back BSP bet) Objective is to quadruple our money Assume we have an edge of 5% on all bets Each bet is assumed to be between odds of 1.5 and 5, with bets being drawn in this range uniformly What we\u2019re most interested in seeing is how likely it is that the staking system will succeed on the objective and on average how many bets it takes for success/failure. Overview of staking methods Staking method Description 1. Fixed Staking Fixed staking is probably the most simplistic betting system where we bet the same $ amount on every bet no matter what happens to our bankroll in the future Stake = x where x is some fixed $ amount 2a. Proportional Staking - % of bankroll Proportional staking defines the bet stake size based on a percentage of your current bank. As your bank increases, the absolute value of your stakes increase and vice versa for when your bank decreases. Stake = Bh where B is the current bank size h is some chosen fixed % 2b. Proportional Staking - bet to win certain amount A variant of proportional staking, where for each bet we stake an amount such that if we win, the winning amount is a set % of our bankroll Stake = Bk / (o-1) where B is the current bank size k is some chosen fixed % o is the odds in decimal form 3. Martingale Staking The Martingale betting system is a progressive betting system whereby after every loss, we stake an amount that (if successful) will recoup all previous consecutive losses and win the original desired amount. After every win, the betting stakes are reset to the initial desired win amount. 4. Kelly Staking Kelly staking or some variant is probably the most popular staking method amongst serious bettors. The Kelly staking formula determines bet size based on the odds of the bet and the assumed edge. Stake = B[(o-1)p - (1-p)] / (o-1) where B is the current bank size k is some chosen fixed % o is the odds in decimal form Pros/Cons Staking method Pros Cons 1. Fixed Staking - Simple to use - Does not account for the size of the current bank - Can take a long time to reach desired objective 2a. Proportional Staking - % of bankroll - Reduces the risk of ruin by reducing stakes when bank decreases - Maintains the same relative risk-reward profile no matter the size of the bank - If you start on a bad run, it can take a long time to recoup your losses 2b. Proportional Staking - bet to win certain amount - Reduces the risk of ruin by reducing stakes when bank decreases - Compared to proportionally staking a % of bankroll, this method tends to have lower odds slippage - If you start on a bad run, it can take a long time to recoup your losses 5. Martingale Staking - Each betting \u201crun\u201d has a high probability of winning - If on a string of good luck, your bank can increase quickly - It takes one bad run to wipe out your entire bank .. they do happen! - Highest probability of ruin out of all the staking methods 6. Kelly Staking - Theoretically, it gives the \u201cbest\u201d balance of risk vs reward - Varies bet size based on assumed edge - Variance is high, bank can be very erratic - Kelly assumes full knowledge of what our edge is. Given we will never know this with certainty, it may lead us to stake too much on low or negative edge bets. Simulation results The following simulation is based on artificial parameters set by Betfair to illustrate the effects of different staking strategies. Of course, gambling is not a reasonable strategy for financial betterment. Staking method Simulation results 1. Fixed Staking Bet amount: $20 % chance to achieve objective: 89% (2449 bets on average) % chance of ruin: 11% (679 bets on average) Example of simulation success 2a. Proportional Staking Bet amount: 2% of bankroll % chance to achieve objective: 97% (1563 bets on average) % chance of ruin: 3% (2893 bets on average) Example of simulation success 2b. Proportional Staking Bet amount: to win 5% of bankroll % chance to achieve objective: 96% (1074 bets on average) % chance of ruin: 4% (1776 bets on average) Example of simulation success 3. Martingale Staking Bet amount: to win 3% of bankroll % chance to achieve objective: 46% (408 bets on average) % chance of ruin: 54% (1776 bets on average) Example of simulation success 4. Kelly Staking Bet amount: Kelly formula % chance to achieve objective: 96% (1071 bets on average) % chance of ruin: 4% (2040 bets on average) Example of simulation success Things to watch out for when choosing a staking system Edge Cliff A common mistake when deploying a model is to be overly confident in your model\u2019s edge on the market. This is especially problematic when using Kelly staking where edge is a required input into the staking decision. One thing to be especially wary of when using a model\u2019s theoretical edge in staking decisions is the \u201cedge\u201d cliff where assumed edge and realised edge increase together up to a certain point before dropping markedly. The rationale behind why this happens is because we are unlikely to have perfect information, especially in mature markets where edges beyond a certain amount are unrealistic. Bets that are highlighted from a strategy as having unrealistic edges most likely arise due to the market knowing a key piece of information that we don\u2019t know (e.g. trial information in a race). Hence these bets could actually be our worst performing bets where edge is zero or negative. A Kelly betting system without acknowledging that this can occur can lead to very large decreases in our bank. Odds Slippage Before you productionise a model, it\u2019s always good to back-test the model against historic odds if possible. However, be careful about putting too much faith in the Return on Investment (ROI) of your back-test as the prices you\u2019ll actually get will unlikely be the same as the ones in your back-test (odds slippage). This is mainly due to: An exchange is dynamic, actions you take will affect the behaviours of other participants on the exchange influencing prices and volumes The back-tested ROI will unlikely scale linearly with increased bet sizes, as you stake more you will be forced to take worse prices, especially at larger odds That is not to say that you shouldn\u2019t back-test your models but you should build in buffers into the back-tests and make assumptions of reasonable but conservative slippage prices. Also, remember to test at small stakes initially, a model may be profitable but unprofitable at larger stakes .. don\u2019t jump to the wrong conclusion! Complete code Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github import random import numpy as np def FixedStakesSim ( stakeSize , bankroll , ruin , minBet , maxBet , edge , bankObj ): \"\"\" Fixed Stakes Simulation Parameters ---------- stakeSize : float stake size for each bet bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank \"\"\" betRange = maxBet - minBet dynamicBank = bankroll # Simulate bets until either objective is achieved or ruined, cap at 50,000 bets for i in range ( 50000 ): betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) winChance = ( 1 + edge ) / betOdds rand = random . uniform ( 0 , 1 ) outcome = \"Exhausted Bets\" if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) else : betPnl = - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break if i == 49999 : outcome = \"Bets exhausted\" return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( FixedStakesSim ( 20 , 1000 , 5 , 1.5 , 5 , 0.05 , 4 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) def ProportionalStakesSimA ( stakepct , bankroll , ruin , minBet , maxBet , edge , bankObj ): \"\"\" Proportional stakes simulation (staking a % of bankroll) Parameters ---------- stakepct : float the % of your dynamic bankroll that you're staking bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank \"\"\" betRange = maxBet - minBet dynamicBank = bankroll for i in range ( 50000 ): stakeSize = max ( stakepct * dynamicBank , ruin ) betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) winChance = ( 1 + edge ) / betOdds rand = random . uniform ( 0 , 1 ) if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) else : betPnl = - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break if i == 49999 : outcome = \"Bets exhausted\" return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( ProportionalStakesSimA ( 0.02 , 1000 , 5 , 1.5 , 5 , 0.05 , 4 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) def ProportionalStakesSimB ( winpct , bankroll , ruin , minBet , maxBet , edge , bankObj ): \"\"\" Proportional stakes simulation (staking to win a certain % of bankroll) Parameters ---------- winpct : float the % of your dynamic bankroll that you're staking to win bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank \"\"\" betRange = maxBet - minBet dynamicBank = bankroll for i in range ( 50000 ): betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) stakeSize = max (( dynamicBank * winpct ) / ( betOdds - 1 ), ruin ) winChance = ( 1 + edge ) / betOdds rand = random . uniform ( 0 , 1 ) if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) else : betPnl = - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break if i == 49999 : outcome = \"Bets exhausted\" return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( ProportionalStakesSimB ( 0.05 , 1000 , 5 , 1.3 , 5 , 0.05 , 4 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) def Martingale ( winamt , bankroll , ruin , minBet , maxBet , edge , bankObj ): \"\"\" Martingale staking simulation Parameters ---------- winamt : float the desired win amount for each betting run bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank \"\"\" betRange = maxBet - minBet dynamicBank = bankroll martingale_win = 1 martingale_progressive_loss = 0 for i in range ( 50000 ): betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) stakeSize = max (( winamt - martingale_progressive_loss ) / ( betOdds - 1 ), ruin ) winChance = ( 1 + edge ) / betOdds rand = random . uniform ( 0 , 1 ) outcome = \"Exhausted Bets\" if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) martingale_win = 1 martingale_progressive_loss = 0 else : betPnl = - stakeSize martingale_win = 0 martingale_progressive_loss = martingale_progressive_loss - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( Martingale ( 20 , 1000 , 5 , 1.5 , 5 , 0.05 , 4 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) def KellyStake ( bankroll , ruin , minBet , maxBet , edge , bankObj , partialKelly ): \"\"\" Kelly staking simulation Parameters ---------- bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank partialKelly: float proportion of kelly staking to bet \"\"\" betRange = maxBet - minBet dynamicBank = bankroll for i in range ( 50000 ): betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) winChance = ( 1 + edge ) / betOdds stakeSize = max ((((( betOdds - 1 ) * winChance ) - ( 1 - winChance )) / ( betOdds - 1 )) * dynamicBank * partialKelly , ruin ) rand = random . uniform ( 0 , 1 ) outcome = \"Exhausted Bets\" if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) else : betPnl = - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( KellyStake ( 1000 , 5 , 1.5 , 5 , 0.05 , 4 , 1 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) Disclaimer Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Staking Methods and Bankroll Management"},{"location":"modelling/stakingMethods/#staking-methods-and-bankroll-management","text":"","title":"Staking Methods and Bankroll Management"},{"location":"modelling/stakingMethods/#workshop","text":"This tutorial was written by Jason and was originally published on Github . It is shared here with his permission. Good staking systems should be used in combination with betting models. In this article, we\u2019ll go over the basics of staking and analyse some of the more popular staking strategies, outlining their pros and cons.","title":"Workshop"},{"location":"modelling/stakingMethods/#why-is-staking-important","text":"Many bettors with models don\u2019t have sound bankroll management, to their detriment. A good staking system is primarily about balancing risk vs reward: If we are too risk inclined and bet aggressively, there is a good chance we will suffer losses We want to maximise our returns within our risk constraints Despite many resources on the web claiming that there is an optimal way to stake, the reality is that there is no one right answer and how you stake will be a personal one which will be affected by: Your return objectives Your risk tolerance Your emotional well-being (we\u2019re not robots) Your return objectives and risk tolerance are usually the primary considerations when staking and will likely be in conflict with one another and hence will need to be balanced. Generally, chasing higher returns will mean greater levels of risk. Finding the right balance between risk and return that works for you is not an easy task and there will likely be a lot of iterating before getting it right. The third factor that influences your staking that most people overlook is the impact the size of your bets has on your emotional well-being. Remember that betting should not be stressful and if it is, it probably means that you\u2019re staking too big. Even if your bankroll can theoretically accommodate an increase in your stake sizes, be wary of whether your emotions can.","title":"Why is staking important?"},{"location":"modelling/stakingMethods/#common-staking-systems","text":"Here, we\u2019ll go through some of the more common staking systems. However, rather than just describing each of the staking systems, we\u2019ll attempt to understand the risk/reward profile of each by simulating each staking system 10,000 times based on the following parameters: Starting bank of $1,000 Ruin defined as bank going down to $5 or less (No longer can place a minimum back BSP bet) Objective is to quadruple our money Assume we have an edge of 5% on all bets Each bet is assumed to be between odds of 1.5 and 5, with bets being drawn in this range uniformly What we\u2019re most interested in seeing is how likely it is that the staking system will succeed on the objective and on average how many bets it takes for success/failure.","title":"Common staking systems"},{"location":"modelling/stakingMethods/#overview-of-staking-methods","text":"Staking method Description 1. Fixed Staking Fixed staking is probably the most simplistic betting system where we bet the same $ amount on every bet no matter what happens to our bankroll in the future Stake = x where x is some fixed $ amount 2a. Proportional Staking - % of bankroll Proportional staking defines the bet stake size based on a percentage of your current bank. As your bank increases, the absolute value of your stakes increase and vice versa for when your bank decreases. Stake = Bh where B is the current bank size h is some chosen fixed % 2b. Proportional Staking - bet to win certain amount A variant of proportional staking, where for each bet we stake an amount such that if we win, the winning amount is a set % of our bankroll Stake = Bk / (o-1) where B is the current bank size k is some chosen fixed % o is the odds in decimal form 3. Martingale Staking The Martingale betting system is a progressive betting system whereby after every loss, we stake an amount that (if successful) will recoup all previous consecutive losses and win the original desired amount. After every win, the betting stakes are reset to the initial desired win amount. 4. Kelly Staking Kelly staking or some variant is probably the most popular staking method amongst serious bettors. The Kelly staking formula determines bet size based on the odds of the bet and the assumed edge. Stake = B[(o-1)p - (1-p)] / (o-1) where B is the current bank size k is some chosen fixed % o is the odds in decimal form","title":"Overview of staking methods"},{"location":"modelling/stakingMethods/#proscons","text":"Staking method Pros Cons 1. Fixed Staking - Simple to use - Does not account for the size of the current bank - Can take a long time to reach desired objective 2a. Proportional Staking - % of bankroll - Reduces the risk of ruin by reducing stakes when bank decreases - Maintains the same relative risk-reward profile no matter the size of the bank - If you start on a bad run, it can take a long time to recoup your losses 2b. Proportional Staking - bet to win certain amount - Reduces the risk of ruin by reducing stakes when bank decreases - Compared to proportionally staking a % of bankroll, this method tends to have lower odds slippage - If you start on a bad run, it can take a long time to recoup your losses 5. Martingale Staking - Each betting \u201crun\u201d has a high probability of winning - If on a string of good luck, your bank can increase quickly - It takes one bad run to wipe out your entire bank .. they do happen! - Highest probability of ruin out of all the staking methods 6. Kelly Staking - Theoretically, it gives the \u201cbest\u201d balance of risk vs reward - Varies bet size based on assumed edge - Variance is high, bank can be very erratic - Kelly assumes full knowledge of what our edge is. Given we will never know this with certainty, it may lead us to stake too much on low or negative edge bets.","title":"Pros/Cons"},{"location":"modelling/stakingMethods/#simulation-results","text":"The following simulation is based on artificial parameters set by Betfair to illustrate the effects of different staking strategies. Of course, gambling is not a reasonable strategy for financial betterment. Staking method Simulation results 1. Fixed Staking Bet amount: $20 % chance to achieve objective: 89% (2449 bets on average) % chance of ruin: 11% (679 bets on average) Example of simulation success 2a. Proportional Staking Bet amount: 2% of bankroll % chance to achieve objective: 97% (1563 bets on average) % chance of ruin: 3% (2893 bets on average) Example of simulation success 2b. Proportional Staking Bet amount: to win 5% of bankroll % chance to achieve objective: 96% (1074 bets on average) % chance of ruin: 4% (1776 bets on average) Example of simulation success 3. Martingale Staking Bet amount: to win 3% of bankroll % chance to achieve objective: 46% (408 bets on average) % chance of ruin: 54% (1776 bets on average) Example of simulation success 4. Kelly Staking Bet amount: Kelly formula % chance to achieve objective: 96% (1071 bets on average) % chance of ruin: 4% (2040 bets on average) Example of simulation success","title":"Simulation results"},{"location":"modelling/stakingMethods/#things-to-watch-out-for-when-choosing-a-staking-system","text":"","title":"Things to watch out for when choosing a staking system"},{"location":"modelling/stakingMethods/#edge-cliff","text":"A common mistake when deploying a model is to be overly confident in your model\u2019s edge on the market. This is especially problematic when using Kelly staking where edge is a required input into the staking decision. One thing to be especially wary of when using a model\u2019s theoretical edge in staking decisions is the \u201cedge\u201d cliff where assumed edge and realised edge increase together up to a certain point before dropping markedly. The rationale behind why this happens is because we are unlikely to have perfect information, especially in mature markets where edges beyond a certain amount are unrealistic. Bets that are highlighted from a strategy as having unrealistic edges most likely arise due to the market knowing a key piece of information that we don\u2019t know (e.g. trial information in a race). Hence these bets could actually be our worst performing bets where edge is zero or negative. A Kelly betting system without acknowledging that this can occur can lead to very large decreases in our bank.","title":"Edge Cliff"},{"location":"modelling/stakingMethods/#odds-slippage","text":"Before you productionise a model, it\u2019s always good to back-test the model against historic odds if possible. However, be careful about putting too much faith in the Return on Investment (ROI) of your back-test as the prices you\u2019ll actually get will unlikely be the same as the ones in your back-test (odds slippage). This is mainly due to: An exchange is dynamic, actions you take will affect the behaviours of other participants on the exchange influencing prices and volumes The back-tested ROI will unlikely scale linearly with increased bet sizes, as you stake more you will be forced to take worse prices, especially at larger odds That is not to say that you shouldn\u2019t back-test your models but you should build in buffers into the back-tests and make assumptions of reasonable but conservative slippage prices. Also, remember to test at small stakes initially, a model may be profitable but unprofitable at larger stakes .. don\u2019t jump to the wrong conclusion!","title":"Odds Slippage"},{"location":"modelling/stakingMethods/#complete-code","text":"Run the code from your ide by using py <filename>.py , making sure you amend the path to point to your input data. Download from Github import random import numpy as np def FixedStakesSim ( stakeSize , bankroll , ruin , minBet , maxBet , edge , bankObj ): \"\"\" Fixed Stakes Simulation Parameters ---------- stakeSize : float stake size for each bet bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank \"\"\" betRange = maxBet - minBet dynamicBank = bankroll # Simulate bets until either objective is achieved or ruined, cap at 50,000 bets for i in range ( 50000 ): betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) winChance = ( 1 + edge ) / betOdds rand = random . uniform ( 0 , 1 ) outcome = \"Exhausted Bets\" if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) else : betPnl = - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break if i == 49999 : outcome = \"Bets exhausted\" return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( FixedStakesSim ( 20 , 1000 , 5 , 1.5 , 5 , 0.05 , 4 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) def ProportionalStakesSimA ( stakepct , bankroll , ruin , minBet , maxBet , edge , bankObj ): \"\"\" Proportional stakes simulation (staking a % of bankroll) Parameters ---------- stakepct : float the % of your dynamic bankroll that you're staking bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank \"\"\" betRange = maxBet - minBet dynamicBank = bankroll for i in range ( 50000 ): stakeSize = max ( stakepct * dynamicBank , ruin ) betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) winChance = ( 1 + edge ) / betOdds rand = random . uniform ( 0 , 1 ) if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) else : betPnl = - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break if i == 49999 : outcome = \"Bets exhausted\" return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( ProportionalStakesSimA ( 0.02 , 1000 , 5 , 1.5 , 5 , 0.05 , 4 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) def ProportionalStakesSimB ( winpct , bankroll , ruin , minBet , maxBet , edge , bankObj ): \"\"\" Proportional stakes simulation (staking to win a certain % of bankroll) Parameters ---------- winpct : float the % of your dynamic bankroll that you're staking to win bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank \"\"\" betRange = maxBet - minBet dynamicBank = bankroll for i in range ( 50000 ): betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) stakeSize = max (( dynamicBank * winpct ) / ( betOdds - 1 ), ruin ) winChance = ( 1 + edge ) / betOdds rand = random . uniform ( 0 , 1 ) if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) else : betPnl = - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break if i == 49999 : outcome = \"Bets exhausted\" return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( ProportionalStakesSimB ( 0.05 , 1000 , 5 , 1.3 , 5 , 0.05 , 4 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) def Martingale ( winamt , bankroll , ruin , minBet , maxBet , edge , bankObj ): \"\"\" Martingale staking simulation Parameters ---------- winamt : float the desired win amount for each betting run bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank \"\"\" betRange = maxBet - minBet dynamicBank = bankroll martingale_win = 1 martingale_progressive_loss = 0 for i in range ( 50000 ): betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) stakeSize = max (( winamt - martingale_progressive_loss ) / ( betOdds - 1 ), ruin ) winChance = ( 1 + edge ) / betOdds rand = random . uniform ( 0 , 1 ) outcome = \"Exhausted Bets\" if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) martingale_win = 1 martingale_progressive_loss = 0 else : betPnl = - stakeSize martingale_win = 0 martingale_progressive_loss = martingale_progressive_loss - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( Martingale ( 20 , 1000 , 5 , 1.5 , 5 , 0.05 , 4 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ]) def KellyStake ( bankroll , ruin , minBet , maxBet , edge , bankObj , partialKelly ): \"\"\" Kelly staking simulation Parameters ---------- bankroll : float starting bankroll ruin : float amount where if bankroll drops below, you are ruined minBet : float the minimum bet odds which you will bet at maxBet : float the maximum bet odds which you will bet at edge : float assumed edge for each bet bankObj : float your bank objective expressed as a multiple of your starting bank partialKelly: float proportion of kelly staking to bet \"\"\" betRange = maxBet - minBet dynamicBank = bankroll for i in range ( 50000 ): betOdds = round ( random . uniform ( 0 , 1 ) * betRange + minBet , 2 ) winChance = ( 1 + edge ) / betOdds stakeSize = max ((((( betOdds - 1 ) * winChance ) - ( 1 - winChance )) / ( betOdds - 1 )) * dynamicBank * partialKelly , ruin ) rand = random . uniform ( 0 , 1 ) outcome = \"Exhausted Bets\" if rand < winChance : betPnl = stakeSize * ( betOdds - 1 ) else : betPnl = - stakeSize dynamicBank = dynamicBank + betPnl if dynamicBank > bankObj * bankroll : outcome = \"Objective Achieved\" break if dynamicBank < ruin : outcome = \"Ruined\" break return [ outcome , i ] simStore = [] # Simulate 10,000 times for i in range ( 10000 ): simStore . append ( KellyStake ( 1000 , 5 , 1.5 , 5 , 0.05 , 4 , 1 )) probSuccess = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) / len ( simStore ) probRuined = len ([ i [ 0 ] for i in simStore if i [ 0 ] == 'Ruined' ]) / len ( simStore ) numbetsSuccess = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Objective Achieved' ]) numbetsRuined = np . median ([ i [ 1 ] for i in simStore if i [ 0 ] == 'Ruined' ])","title":"Complete code"},{"location":"modelling/stakingMethods/#disclaimer","text":"Note that whilst models and automated strategies are fun and rewarding to create, we can't promise that your model or betting strategy will be profitable, and we make no representations in relation to the code shared or information on this page. If you're using this code or implementing your own strategies, you do so entirely at your own risk and you are responsible for any losses incurred. Under no circumstances will Betfair be liable for any loss or damage you suffer.","title":"Disclaimer"}]}